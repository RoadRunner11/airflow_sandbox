[2020-11-02 03:56:36,209] {scheduler_job.py:155} INFO - Started process (PID=26680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:56:36,218] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:56:36,219] {logging_mixin.py:112} INFO - [2020-11-02 03:56:36,219] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:56:36,508] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:56:37,740] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:56:37,782] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:56:37,825] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:56:37,875] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:56:37,881] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:56:37,943] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:56:37,952] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 23:00:00+00:00 [success]> in ORM
[2020-11-02 03:56:37,961] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 23:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:56:37,971] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 23:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:56:37,985] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.776 seconds
[2020-11-02 03:56:50,596] {scheduler_job.py:155} INFO - Started process (PID=26790) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:56:50,599] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:56:50,608] {logging_mixin.py:112} INFO - [2020-11-02 03:56:50,608] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:56:50,837] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:56:52,044] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:56:52,076] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:56:52,120] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:56:52,169] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:56:52,175] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:56:52,203] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:56:52,276] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:56:52,285] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 23:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:56:52,294] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 00:00:00+00:00 [success]> in ORM
[2020-11-02 03:56:52,302] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:56:52,311] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:56:52,329] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.734 seconds
[2020-11-02 03:57:03,967] {scheduler_job.py:155} INFO - Started process (PID=26881) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:03,983] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:57:03,984] {logging_mixin.py:112} INFO - [2020-11-02 03:57:03,984] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:04,348] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:57:05,816] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:57:05,877] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:05,954] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:57:06,100] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:06,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:06,157] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:06,202] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:06,310] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:57:06,322] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 01:00:00+00:00 [success]> in ORM
[2020-11-02 03:57:06,333] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 01:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:57:06,345] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 01:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:57:06,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.401 seconds
[2020-11-02 03:57:18,440] {scheduler_job.py:155} INFO - Started process (PID=27005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:18,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:57:18,475] {logging_mixin.py:112} INFO - [2020-11-02 03:57:18,475] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:18,837] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:57:20,022] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:57:20,051] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:20,093] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:57:20,123] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:20,154] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:20,196] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:20,332] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:57:20,341] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:57:20,352] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.912 seconds
[2020-11-02 03:57:37,925] {scheduler_job.py:155} INFO - Started process (PID=27115) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:37,932] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:57:37,940] {logging_mixin.py:112} INFO - [2020-11-02 03:57:37,940] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:38,222] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:57:40,149] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:57:40,236] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:40,346] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:57:40,415] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:40,472] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:40,524] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:40,705] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:57:40,719] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.794 seconds
[2020-11-02 03:57:52,268] {scheduler_job.py:155} INFO - Started process (PID=27233) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:52,286] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:57:52,302] {logging_mixin.py:112} INFO - [2020-11-02 03:57:52,302] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:52,823] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:57:54,098] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:57:54,144] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:57:54,224] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:57:54,269] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:54,308] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:54,342] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:57:54,463] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:57:54,472] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 01:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:57:54,485] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.218 seconds
[2020-11-02 03:58:14,907] {scheduler_job.py:155} INFO - Started process (PID=27369) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:14,924] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:58:14,924] {logging_mixin.py:112} INFO - [2020-11-02 03:58:14,924] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:15,498] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:58:17,273] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:58:17,315] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:17,366] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:58:17,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:17,424] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:17,445] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:17,496] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:58:17,501] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.593 seconds
[2020-11-02 03:58:30,154] {scheduler_job.py:155} INFO - Started process (PID=27458) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:30,158] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:58:30,159] {logging_mixin.py:112} INFO - [2020-11-02 03:58:30,159] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:30,409] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:58:31,483] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:58:31,514] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:31,553] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:58:31,577] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:31,599] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:31,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:31,669] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:58:31,673] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.519 seconds
[2020-11-02 03:58:43,390] {scheduler_job.py:155} INFO - Started process (PID=27528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:43,399] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:58:43,400] {logging_mixin.py:112} INFO - [2020-11-02 03:58:43,400] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:43,754] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:58:45,617] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:58:45,682] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:45,750] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:58:45,824] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:45,857] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:45,885] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:45,968] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:58:45,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.586 seconds
[2020-11-02 03:58:57,762] {scheduler_job.py:155} INFO - Started process (PID=27596) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:57,772] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:58:57,774] {logging_mixin.py:112} INFO - [2020-11-02 03:58:57,773] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:57,968] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:58:58,985] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:58:59,016] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:58:59,055] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:58:59,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:59,097] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:59,115] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:58:59,167] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:58:59,175] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 23:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:58:59,186] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.424 seconds
[2020-11-02 03:59:11,034] {scheduler_job.py:155} INFO - Started process (PID=27684) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:11,039] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:59:11,040] {logging_mixin.py:112} INFO - [2020-11-02 03:59:11,040] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:11,263] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:59:12,885] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:59:12,927] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:12,969] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:59:12,999] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:13,027] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:13,060] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:13,189] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:59:13,197] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.163 seconds
[2020-11-02 03:59:25,283] {scheduler_job.py:155} INFO - Started process (PID=27751) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:25,288] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:59:25,288] {logging_mixin.py:112} INFO - [2020-11-02 03:59:25,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:25,520] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:59:27,038] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:59:27,071] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:27,124] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:59:27,147] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:27,178] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:27,201] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:27,263] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:59:27,270] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.987 seconds
[2020-11-02 03:59:38,522] {scheduler_job.py:155} INFO - Started process (PID=27816) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:38,544] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:59:38,546] {logging_mixin.py:112} INFO - [2020-11-02 03:59:38,545] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:38,829] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:59:40,330] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:59:40,373] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:40,437] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:59:40,474] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:40,507] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:40,537] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:40,641] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:59:40,653] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 03:59:40,670] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.148 seconds
[2020-11-02 03:59:52,892] {scheduler_job.py:155} INFO - Started process (PID=27911) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:52,895] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 03:59:52,896] {logging_mixin.py:112} INFO - [2020-11-02 03:59:52,896] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:53,112] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 03:59:54,260] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 03:59:54,287] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 03:59:54,322] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 03:59:54,343] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:54,365] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:54,388] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 03:59:54,439] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 03:59:54,443] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.552 seconds
[2020-11-02 04:00:06,105] {scheduler_job.py:155} INFO - Started process (PID=27974) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:00:06,109] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:00:06,109] {logging_mixin.py:112} INFO - [2020-11-02 04:00:06,109] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:00:06,363] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:00:07,422] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:00:07,453] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:00:07,497] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:00:07,558] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:07,564] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:07,589] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:07,612] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:07,635] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:07,673] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:00:07,809] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:00:07,817] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 01:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:00:07,825] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 02:00:00+00:00 [success]> in ORM
[2020-11-02 04:00:07,833] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:00:07,840] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 02:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:00:07,849] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 03:00:00.649272+00:00 [success]> in ORM
[2020-11-02 04:00:07,858] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:00:00.649272+00:00 [scheduled]> in ORM
[2020-11-02 04:00:07,867] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 03:00:00.649272+00:00 [scheduled]> in ORM
[2020-11-02 04:00:07,880] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.775 seconds
[2020-11-02 04:00:19,403] {scheduler_job.py:155} INFO - Started process (PID=28074) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:00:19,407] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:00:19,416] {logging_mixin.py:112} INFO - [2020-11-02 04:00:19,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:00:20,187] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:00:23,299] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:00:23,339] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:00:23,444] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:00:23,511] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:23,579] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:23,633] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:23,666] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:00:23,707] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:00:23,842] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:00:23,853] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 02:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:00:23,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 4.473 seconds
[2020-11-02 04:01:01,292] {scheduler_job.py:155} INFO - Started process (PID=28293) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:01,296] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:01:01,301] {logging_mixin.py:112} INFO - [2020-11-02 04:01:01,301] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:01,618] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:01:02,678] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:01:02,710] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:02,748] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:01:02,769] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:02,791] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:02,807] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:02,826] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:02,843] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:01:02,924] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:01:02,932] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 03:00:00.649272+00:00 [scheduled]> in ORM
[2020-11-02 04:01:02,946] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.654 seconds
[2020-11-02 04:01:14,512] {scheduler_job.py:155} INFO - Started process (PID=28392) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:14,527] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:01:14,533] {logging_mixin.py:112} INFO - [2020-11-02 04:01:14,533] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:14,763] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:01:15,939] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:01:15,970] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:16,013] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:01:16,037] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:16,060] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:16,079] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:16,097] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:16,115] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:01:16,187] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:01:16,194] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 23:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:01:16,208] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.696 seconds
[2020-11-02 04:01:27,770] {scheduler_job.py:155} INFO - Started process (PID=28509) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:27,774] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:01:27,774] {logging_mixin.py:112} INFO - [2020-11-02 04:01:27,774] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:27,973] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:01:28,925] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:01:28,952] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:28,991] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:01:29,014] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:29,034] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:29,052] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:29,072] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:29,093] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:01:29,157] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:01:29,161] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.391 seconds
[2020-11-02 04:01:41,193] {scheduler_job.py:155} INFO - Started process (PID=28579) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:41,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:01:41,198] {logging_mixin.py:112} INFO - [2020-11-02 04:01:41,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:41,398] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:01:42,360] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:01:42,394] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:42,445] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:01:42,469] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:42,492] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:42,509] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:42,527] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:42,544] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:01:42,615] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:01:42,619] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.426 seconds
[2020-11-02 04:01:54,415] {scheduler_job.py:155} INFO - Started process (PID=28650) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:54,420] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:01:54,421] {logging_mixin.py:112} INFO - [2020-11-02 04:01:54,420] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:54,620] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:01:55,627] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:01:55,653] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:01:55,692] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:01:55,715] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:55,737] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:55,754] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:55,771] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:01:55,788] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:01:55,852] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:01:55,858] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:01:55,869] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.453 seconds
[2020-11-02 04:02:07,787] {scheduler_job.py:155} INFO - Started process (PID=28740) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:07,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:02:07,791] {logging_mixin.py:112} INFO - [2020-11-02 04:02:07,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:07,988] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:02:08,948] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:02:08,976] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:09,016] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:02:09,038] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:09,058] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:09,076] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:09,097] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:09,118] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:02:09,183] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:02:09,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.403 seconds
[2020-11-02 04:02:21,029] {scheduler_job.py:155} INFO - Started process (PID=28805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:21,033] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:02:21,034] {logging_mixin.py:112} INFO - [2020-11-02 04:02:21,034] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:21,487] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:02:22,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:02:22,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:22,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:02:22,901] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:22,937] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:22,968] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:23,002] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:23,043] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:02:23,259] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:02:23,274] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.246 seconds
[2020-11-02 04:02:35,373] {scheduler_job.py:155} INFO - Started process (PID=28866) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:35,379] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:02:35,380] {logging_mixin.py:112} INFO - [2020-11-02 04:02:35,380] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:35,738] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:02:37,083] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:02:37,117] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:37,160] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:02:37,186] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:37,223] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:37,268] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:37,307] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:37,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:02:37,517] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:02:37,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.149 seconds
[2020-11-02 04:02:49,611] {scheduler_job.py:155} INFO - Started process (PID=28931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:49,615] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:02:49,616] {logging_mixin.py:112} INFO - [2020-11-02 04:02:49,616] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:49,901] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:02:51,369] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:02:51,406] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:02:51,450] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:02:51,471] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:51,496] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:51,518] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:51,542] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:02:51,567] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:02:51,679] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:02:51,687] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 01:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:02:51,696] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:02:51,708] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.097 seconds
[2020-11-02 04:03:03,920] {scheduler_job.py:155} INFO - Started process (PID=29022) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:03,929] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:03:03,929] {logging_mixin.py:112} INFO - [2020-11-02 04:03:03,929] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:04,202] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:03:05,701] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:03:05,734] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:05,772] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:03:05,797] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:05,825] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:05,845] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:05,869] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:05,892] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:03:06,000] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:03:06,010] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:00:00.649272+00:00 [scheduled]> in ORM
[2020-11-02 04:03:06,030] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.110 seconds
[2020-11-02 04:03:18,186] {scheduler_job.py:155} INFO - Started process (PID=29118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:18,190] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:03:18,192] {logging_mixin.py:112} INFO - [2020-11-02 04:03:18,191] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:18,466] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:03:20,021] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:03:20,127] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:20,245] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:03:20,357] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:20,472] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:20,561] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:20,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:20,719] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:03:21,006] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:03:21,024] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.839 seconds
[2020-11-02 04:03:32,418] {scheduler_job.py:155} INFO - Started process (PID=29205) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:32,423] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:03:32,424] {logging_mixin.py:112} INFO - [2020-11-02 04:03:32,424] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:32,619] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:03:33,534] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:03:33,563] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:33,601] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:03:33,622] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:33,647] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:33,669] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:33,691] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:33,714] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:03:33,811] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:03:33,818] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 23:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:03:33,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.420 seconds
[2020-11-02 04:03:45,719] {scheduler_job.py:155} INFO - Started process (PID=29289) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:45,729] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:03:45,737] {logging_mixin.py:112} INFO - [2020-11-02 04:03:45,737] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:46,011] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:03:47,430] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:03:47,469] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:47,528] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:03:47,553] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:47,568] {logging_mixin.py:112} INFO - [2020-11-02 04:03:47,568] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 23:00:00+00:00: scheduled__2020-11-01T23:00:00+00:00, externally triggered: False> failed
[2020-11-02 04:03:47,572] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:47,596] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:47,619] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:03:47,642] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:03:47,731] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:03:47,736] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.017 seconds
[2020-11-02 04:03:59,979] {scheduler_job.py:155} INFO - Started process (PID=29352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:03:59,987] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:03:59,991] {logging_mixin.py:112} INFO - [2020-11-02 04:03:59,991] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:00,425] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:04:01,688] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:04:01,734] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:01,799] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:04:01,827] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:01,868] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:01,903] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:01,932] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:04:02,041] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:04:02,054] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.075 seconds
[2020-11-02 04:04:17,508] {scheduler_job.py:155} INFO - Started process (PID=29445) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:17,517] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:04:17,518] {logging_mixin.py:112} INFO - [2020-11-02 04:04:17,517] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:17,880] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:04:19,175] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:04:19,206] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:19,251] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:04:19,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:19,325] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:19,353] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:19,379] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:04:19,477] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:04:19,485] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:04:19,501] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.993 seconds
[2020-11-02 04:04:33,462] {scheduler_job.py:155} INFO - Started process (PID=29528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:33,469] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:04:33,470] {logging_mixin.py:112} INFO - [2020-11-02 04:04:33,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:33,714] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:04:34,585] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:04:34,612] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:34,651] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:04:34,671] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:34,697] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:34,719] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:34,743] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:04:34,824] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:04:34,830] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 00:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:04:34,840] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.379 seconds
[2020-11-02 04:04:46,789] {scheduler_job.py:155} INFO - Started process (PID=29622) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:46,798] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:04:46,799] {logging_mixin.py:112} INFO - [2020-11-02 04:04:46,799] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:47,043] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:04:48,358] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:04:48,394] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:04:48,437] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:04:48,468] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:48,493] {logging_mixin.py:112} INFO - [2020-11-02 04:04:48,493] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 00:00:00+00:00: scheduled__2020-11-02T00:00:00+00:00, externally triggered: False> failed
[2020-11-02 04:04:48,500] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:48,536] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:04:48,574] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:04:48,682] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:04:48,689] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.900 seconds
[2020-11-02 04:05:01,001] {scheduler_job.py:155} INFO - Started process (PID=29696) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:01,006] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:05:01,007] {logging_mixin.py:112} INFO - [2020-11-02 04:05:01,007] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:01,358] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:05:02,814] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:05:02,864] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:02,924] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:05:02,961] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:03,009] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:03,048] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:05:03,193] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:05:03,209] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 01:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:05:03,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.229 seconds
[2020-11-02 04:05:14,314] {scheduler_job.py:155} INFO - Started process (PID=29791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:14,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:05:14,327] {logging_mixin.py:112} INFO - [2020-11-02 04:05:14,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:14,593] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:05:15,688] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:05:15,727] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:15,764] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:05:15,784] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:15,801] {logging_mixin.py:112} INFO - [2020-11-02 04:05:15,801] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 01:00:00+00:00: scheduled__2020-11-02T01:00:00+00:00, externally triggered: False> failed
[2020-11-02 04:05:15,813] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:15,840] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:05:15,894] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:05:15,901] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:05:15,911] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.597 seconds
[2020-11-02 04:05:28,532] {scheduler_job.py:155} INFO - Started process (PID=29883) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:28,537] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:05:28,538] {logging_mixin.py:112} INFO - [2020-11-02 04:05:28,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:28,780] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:05:29,675] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:05:29,702] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:29,740] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:05:29,760] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:29,786] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:05:29,840] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:05:29,847] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:00:00.649272+00:00 [scheduled]> in ORM
[2020-11-02 04:05:29,857] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.325 seconds
[2020-11-02 04:05:41,843] {scheduler_job.py:155} INFO - Started process (PID=29973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:41,846] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:05:41,847] {logging_mixin.py:112} INFO - [2020-11-02 04:05:41,847] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:42,086] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:05:43,225] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:05:43,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:43,294] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:05:43,318] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:43,348] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:05:43,415] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:05:43,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.576 seconds
[2020-11-02 04:05:55,149] {scheduler_job.py:155} INFO - Started process (PID=30041) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:55,152] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:05:55,153] {logging_mixin.py:112} INFO - [2020-11-02 04:05:55,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:55,419] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 04:05:56,400] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 04:05:56,436] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:05:56,485] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:05:56,509] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:05:56,541] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:05:56,600] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:05:56,604] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.456 seconds
[2020-11-02 04:08:32,507] {scheduler_job.py:155} INFO - Started process (PID=30216) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:32,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:08:32,514] {logging_mixin.py:112} INFO - [2020-11-02 04:08:32,514] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:32,716] {logging_mixin.py:112} INFO - [2020-11-02 04:08:32,715] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:08:32,717] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:32,739] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-02 04:08:45,786] {scheduler_job.py:155} INFO - Started process (PID=30283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:45,790] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:08:45,791] {logging_mixin.py:112} INFO - [2020-11-02 04:08:45,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:46,000] {logging_mixin.py:112} INFO - [2020-11-02 04:08:45,999] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:08:46,000] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:46,024] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.238 seconds
[2020-11-02 04:08:59,135] {scheduler_job.py:155} INFO - Started process (PID=30349) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:59,138] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:08:59,139] {logging_mixin.py:112} INFO - [2020-11-02 04:08:59,139] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:59,384] {logging_mixin.py:112} INFO - [2020-11-02 04:08:59,383] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:08:59,385] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:08:59,408] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.274 seconds
[2020-11-02 04:09:12,453] {scheduler_job.py:155} INFO - Started process (PID=30419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:12,466] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:09:12,469] {logging_mixin.py:112} INFO - [2020-11-02 04:09:12,469] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:12,759] {logging_mixin.py:112} INFO - [2020-11-02 04:09:12,758] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:09:12,760] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:12,784] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.331 seconds
[2020-11-02 04:09:25,725] {scheduler_job.py:155} INFO - Started process (PID=30483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:25,730] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:09:25,731] {logging_mixin.py:112} INFO - [2020-11-02 04:09:25,731] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:25,913] {logging_mixin.py:112} INFO - [2020-11-02 04:09:25,912] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:09:25,914] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:25,936] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.212 seconds
[2020-11-02 04:09:39,026] {scheduler_job.py:155} INFO - Started process (PID=30549) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:39,032] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:09:39,033] {logging_mixin.py:112} INFO - [2020-11-02 04:09:39,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:39,250] {logging_mixin.py:112} INFO - [2020-11-02 04:09:39,249] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:09:39,250] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:39,275] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.249 seconds
[2020-11-02 04:09:52,290] {scheduler_job.py:155} INFO - Started process (PID=30622) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:52,295] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:09:52,296] {logging_mixin.py:112} INFO - [2020-11-02 04:09:52,295] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:52,541] {logging_mixin.py:112} INFO - [2020-11-02 04:09:52,540] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:09:52,541] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:09:52,565] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.275 seconds
[2020-11-02 04:10:05,529] {scheduler_job.py:155} INFO - Started process (PID=30689) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:05,532] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:10:05,533] {logging_mixin.py:112} INFO - [2020-11-02 04:10:05,533] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:05,723] {logging_mixin.py:112} INFO - [2020-11-02 04:10:05,722] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:10:05,723] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:05,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.216 seconds
[2020-11-02 04:10:18,807] {scheduler_job.py:155} INFO - Started process (PID=30760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:18,811] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:10:18,811] {logging_mixin.py:112} INFO - [2020-11-02 04:10:18,811] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:18,998] {logging_mixin.py:112} INFO - [2020-11-02 04:10:18,998] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:10:18,999] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:19,021] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-02 04:10:32,179] {scheduler_job.py:155} INFO - Started process (PID=30829) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:32,184] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:10:32,185] {logging_mixin.py:112} INFO - [2020-11-02 04:10:32,184] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:32,384] {logging_mixin.py:112} INFO - [2020-11-02 04:10:32,383] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:10:32,384] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:32,408] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-02 04:10:45,469] {scheduler_job.py:155} INFO - Started process (PID=30906) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:45,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:10:45,477] {logging_mixin.py:112} INFO - [2020-11-02 04:10:45,477] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:45,789] {logging_mixin.py:112} INFO - [2020-11-02 04:10:45,788] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:10:45,790] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:45,823] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.355 seconds
[2020-11-02 04:10:58,710] {scheduler_job.py:155} INFO - Started process (PID=30973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:58,736] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:10:58,738] {logging_mixin.py:112} INFO - [2020-11-02 04:10:58,737] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:59,107] {logging_mixin.py:112} INFO - [2020-11-02 04:10:59,105] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:10:59,107] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:10:59,141] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.431 seconds
[2020-11-02 04:11:12,015] {scheduler_job.py:155} INFO - Started process (PID=31044) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:12,023] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:11:12,025] {logging_mixin.py:112} INFO - [2020-11-02 04:11:12,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:12,300] {logging_mixin.py:112} INFO - [2020-11-02 04:11:12,299] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:11:12,300] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:12,332] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.317 seconds
[2020-11-02 04:11:25,295] {scheduler_job.py:155} INFO - Started process (PID=31114) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:25,302] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:11:25,303] {logging_mixin.py:112} INFO - [2020-11-02 04:11:25,303] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:25,496] {logging_mixin.py:112} INFO - [2020-11-02 04:11:25,495] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:11:25,496] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:25,522] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.227 seconds
[2020-11-02 04:11:38,637] {scheduler_job.py:155} INFO - Started process (PID=31188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:38,642] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:11:38,643] {logging_mixin.py:112} INFO - [2020-11-02 04:11:38,642] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:38,907] {logging_mixin.py:112} INFO - [2020-11-02 04:11:38,906] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:11:38,908] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:38,939] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.303 seconds
[2020-11-02 04:11:51,900] {scheduler_job.py:155} INFO - Started process (PID=31256) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:51,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:11:51,905] {logging_mixin.py:112} INFO - [2020-11-02 04:11:51,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:52,089] {logging_mixin.py:112} INFO - [2020-11-02 04:11:52,088] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:11:52,089] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:11:52,112] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-02 04:12:05,171] {scheduler_job.py:155} INFO - Started process (PID=31328) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:05,175] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:12:05,176] {logging_mixin.py:112} INFO - [2020-11-02 04:12:05,176] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:05,361] {logging_mixin.py:112} INFO - [2020-11-02 04:12:05,360] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:12:05,362] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:05,385] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-02 04:12:18,472] {scheduler_job.py:155} INFO - Started process (PID=31402) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:18,477] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:12:18,478] {logging_mixin.py:112} INFO - [2020-11-02 04:12:18,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:18,687] {logging_mixin.py:112} INFO - [2020-11-02 04:12:18,686] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:12:18,687] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:18,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-02 04:12:31,730] {scheduler_job.py:155} INFO - Started process (PID=31475) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:31,734] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:12:31,736] {logging_mixin.py:112} INFO - [2020-11-02 04:12:31,735] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:32,060] {logging_mixin.py:112} INFO - [2020-11-02 04:12:32,058] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:12:32,060] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:32,097] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.367 seconds
[2020-11-02 04:12:45,130] {scheduler_job.py:155} INFO - Started process (PID=31546) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:45,134] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:12:45,134] {logging_mixin.py:112} INFO - [2020-11-02 04:12:45,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:45,462] {logging_mixin.py:112} INFO - [2020-11-02 04:12:45,461] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-02 04:12:45,462] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:45,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.364 seconds
[2020-11-02 04:12:58,354] {scheduler_job.py:155} INFO - Started process (PID=31610) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:58,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:12:58,359] {logging_mixin.py:112} INFO - [2020-11-02 04:12:58,359] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:59,647] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:12:59,691] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:12:59,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:12:59,745] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:12:59,811] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:12:59,818] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:00:00+00:00 [scheduled]> in ORM
[2020-11-02 04:12:59,825] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:00:00.649272+00:00 [scheduled]> in ORM
[2020-11-02 04:12:59,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.484 seconds
[2020-11-02 04:13:12,678] {scheduler_job.py:155} INFO - Started process (PID=31707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:12,682] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:13:12,682] {logging_mixin.py:112} INFO - [2020-11-02 04:13:12,682] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:15,587] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:15,655] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:13:15,703] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False>
[2020-11-02 04:13:15,742] {logging_mixin.py:112} INFO - [2020-11-02 04:13:15,742] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 02:00:00+00:00: scheduled__2020-11-02T02:00:00+00:00, externally triggered: False> failed
[2020-11-02 04:13:15,752] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:13:15,863] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:13:15,884] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.206 seconds
[2020-11-02 04:13:27,102] {scheduler_job.py:155} INFO - Started process (PID=31813) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:27,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:13:27,112] {logging_mixin.py:112} INFO - [2020-11-02 04:13:27,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:28,947] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:29,030] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:13:29,073] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True>
[2020-11-02 04:13:29,096] {logging_mixin.py:112} INFO - [2020-11-02 04:13:29,096] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 03:00:00.649272+00:00: manual__2020-11-02T03:00:00.649272+00:00, externally triggered: True> failed
[2020-11-02 04:13:29,106] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:13:29,111] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.010 seconds
[2020-11-02 04:13:40,457] {scheduler_job.py:155} INFO - Started process (PID=31875) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:40,461] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:13:40,462] {logging_mixin.py:112} INFO - [2020-11-02 04:13:40,462] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:41,837] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:41,892] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:13:41,922] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:13:41,927] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.471 seconds
[2020-11-02 04:13:53,723] {scheduler_job.py:155} INFO - Started process (PID=31939) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:53,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:13:53,746] {logging_mixin.py:112} INFO - [2020-11-02 04:13:53,746] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:55,363] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:13:55,413] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:13:55,443] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:13:55,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.724 seconds
[2020-11-02 04:14:06,993] {scheduler_job.py:155} INFO - Started process (PID=31996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:07,000] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:14:07,001] {logging_mixin.py:112} INFO - [2020-11-02 04:14:07,001] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:08,174] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:08,209] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:14:08,229] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:14:08,233] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.240 seconds
[2020-11-02 04:14:28,062] {scheduler_job.py:155} INFO - Started process (PID=32114) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:28,066] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:14:28,067] {logging_mixin.py:112} INFO - [2020-11-02 04:14:28,067] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:29,246] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:29,284] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:14:29,306] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:14:29,311] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-02 04:14:36,653] {scheduler_job.py:155} INFO - Started process (PID=32153) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:36,657] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:14:36,658] {logging_mixin.py:112} INFO - [2020-11-02 04:14:36,658] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:38,190] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:38,239] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:14:38,266] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:14:38,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.618 seconds
[2020-11-02 04:14:50,917] {scheduler_job.py:155} INFO - Started process (PID=32230) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:50,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:14:50,927] {logging_mixin.py:112} INFO - [2020-11-02 04:14:50,927] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:52,304] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:14:52,350] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:14:52,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:14:52,388] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.471 seconds
[2020-11-02 04:15:05,219] {scheduler_job.py:155} INFO - Started process (PID=32292) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:05,224] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:15:05,225] {logging_mixin.py:112} INFO - [2020-11-02 04:15:05,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:06,445] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:06,491] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:15:06,514] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:15:06,519] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.300 seconds
[2020-11-02 04:15:19,509] {scheduler_job.py:155} INFO - Started process (PID=32370) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:19,521] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:15:19,522] {logging_mixin.py:112} INFO - [2020-11-02 04:15:19,521] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:21,350] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:21,406] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:15:21,432] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:15:21,436] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.926 seconds
[2020-11-02 04:15:32,747] {scheduler_job.py:155} INFO - Started process (PID=32435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:32,759] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:15:32,759] {logging_mixin.py:112} INFO - [2020-11-02 04:15:32,759] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:34,086] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:34,123] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:15:34,145] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:15:34,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.402 seconds
[2020-11-02 04:15:46,014] {scheduler_job.py:155} INFO - Started process (PID=32503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:46,020] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:15:46,021] {logging_mixin.py:112} INFO - [2020-11-02 04:15:46,021] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:47,447] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:47,513] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:15:47,578] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:15:47,597] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.582 seconds
[2020-11-02 04:15:59,240] {scheduler_job.py:155} INFO - Started process (PID=32577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:15:59,245] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:15:59,246] {logging_mixin.py:112} INFO - [2020-11-02 04:15:59,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:16:00,558] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:16:00,593] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:16:00,613] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:16:00,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.377 seconds
[2020-11-02 04:16:36,908] {scheduler_job.py:155} INFO - Started process (PID=324) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:16:36,912] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:16:36,912] {logging_mixin.py:112} INFO - [2020-11-02 04:16:36,912] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:16:38,900] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:16:39,000] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:16:39,065] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:16:39,074] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.166 seconds
[2020-11-02 04:17:04,501] {scheduler_job.py:155} INFO - Started process (PID=461) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:04,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:17:04,506] {logging_mixin.py:112} INFO - [2020-11-02 04:17:04,506] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:05,620] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:05,661] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:17:05,680] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:17:05,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-02 04:17:31,018] {scheduler_job.py:155} INFO - Started process (PID=650) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:31,022] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:17:31,022] {logging_mixin.py:112} INFO - [2020-11-02 04:17:31,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:32,117] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:32,162] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:17:32,185] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:17:32,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.172 seconds
[2020-11-02 04:17:57,463] {scheduler_job.py:155} INFO - Started process (PID=798) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:57,467] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:17:57,468] {logging_mixin.py:112} INFO - [2020-11-02 04:17:57,468] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:58,595] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:17:58,636] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:17:58,657] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:17:58,663] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.200 seconds
[2020-11-02 04:18:23,957] {scheduler_job.py:155} INFO - Started process (PID=944) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:18:23,962] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:18:23,963] {logging_mixin.py:112} INFO - [2020-11-02 04:18:23,963] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:18:25,316] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:18:25,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:18:25,384] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:18:25,389] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.432 seconds
[2020-11-02 04:18:50,448] {scheduler_job.py:155} INFO - Started process (PID=1112) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:18:50,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:18:50,453] {logging_mixin.py:112} INFO - [2020-11-02 04:18:50,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:18:52,156] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:18:52,196] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:18:52,217] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:18:52,221] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.773 seconds
[2020-11-02 04:19:10,520] {scheduler_job.py:155} INFO - Started process (PID=1237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:10,534] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:19:10,540] {logging_mixin.py:112} INFO - [2020-11-02 04:19:10,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:12,568] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:12,638] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:19:12,666] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:19:12,671] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.150 seconds
[2020-11-02 04:19:25,732] {scheduler_job.py:155} INFO - Started process (PID=1428) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:25,735] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:19:25,735] {logging_mixin.py:112} INFO - [2020-11-02 04:19:25,735] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:27,380] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:27,418] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:19:27,441] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:19:27,445] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.714 seconds
[2020-11-02 04:19:38,929] {scheduler_job.py:155} INFO - Started process (PID=1565) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:38,933] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:19:38,934] {logging_mixin.py:112} INFO - [2020-11-02 04:19:38,934] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:40,168] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:40,211] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:19:40,233] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:19:40,238] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-02 04:19:52,173] {scheduler_job.py:155} INFO - Started process (PID=1670) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:52,178] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:19:52,179] {logging_mixin.py:112} INFO - [2020-11-02 04:19:52,179] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:54,066] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:19:54,133] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:19:54,175] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:19:54,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.009 seconds
[2020-11-02 04:20:05,470] {scheduler_job.py:155} INFO - Started process (PID=1817) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:05,499] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:20:05,505] {logging_mixin.py:112} INFO - [2020-11-02 04:20:05,504] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:07,149] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:07,189] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:20:07,214] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:20:07,218] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.749 seconds
[2020-11-02 04:20:19,693] {scheduler_job.py:155} INFO - Started process (PID=1901) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:19,697] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:20:19,698] {logging_mixin.py:112} INFO - [2020-11-02 04:20:19,697] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:21,416] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:21,488] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:20:21,524] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:20:21,622] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:20:21,635] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 03:20:13.109273+00:00 [success]> in ORM
[2020-11-02 04:20:21,646] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:20:13.109273+00:00 [scheduled]> in ORM
[2020-11-02 04:20:21,658] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 03:20:13.109273+00:00 [scheduled]> in ORM
[2020-11-02 04:20:21,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.992 seconds
[2020-11-02 04:20:32,996] {scheduler_job.py:155} INFO - Started process (PID=2017) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:33,009] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:20:33,010] {logging_mixin.py:112} INFO - [2020-11-02 04:20:33,010] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:35,044] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:35,099] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:20:35,149] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:20:35,218] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:20:35,225] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 03:20:13.109273+00:00 [scheduled]> in ORM
[2020-11-02 04:20:35,235] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.239 seconds
[2020-11-02 04:20:46,253] {scheduler_job.py:155} INFO - Started process (PID=2113) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:46,265] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:20:46,269] {logging_mixin.py:112} INFO - [2020-11-02 04:20:46,269] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:47,740] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:20:47,780] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:20:47,803] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:20:47,837] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:20:47,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.588 seconds
[2020-11-02 04:21:00,534] {scheduler_job.py:155} INFO - Started process (PID=2225) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:00,538] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:21:00,538] {logging_mixin.py:112} INFO - [2020-11-02 04:21:00,538] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:02,168] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:02,222] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:21:02,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:21:02,280] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:21:02,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.752 seconds
[2020-11-02 04:21:13,780] {scheduler_job.py:155} INFO - Started process (PID=2283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:13,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:21:13,786] {logging_mixin.py:112} INFO - [2020-11-02 04:21:13,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:15,239] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:15,277] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:21:15,299] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:21:15,344] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:21:15,348] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.568 seconds
[2020-11-02 04:21:36,164] {scheduler_job.py:155} INFO - Started process (PID=2386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:36,169] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:21:36,170] {logging_mixin.py:112} INFO - [2020-11-02 04:21:36,169] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:37,526] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:37,568] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:21:37,591] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:21:37,642] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:21:37,648] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.484 seconds
[2020-11-02 04:21:50,421] {scheduler_job.py:155} INFO - Started process (PID=2459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:50,425] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:21:50,426] {logging_mixin.py:112} INFO - [2020-11-02 04:21:50,426] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:51,705] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:21:51,750] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:21:51,773] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:21:51,819] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:21:51,824] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.402 seconds
[2020-11-02 04:22:08,754] {scheduler_job.py:155} INFO - Started process (PID=2544) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:08,757] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:22:08,758] {logging_mixin.py:112} INFO - [2020-11-02 04:22:08,758] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:10,027] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:10,072] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:22:10,094] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:22:10,135] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:22:10,140] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.386 seconds
[2020-11-02 04:22:23,003] {scheduler_job.py:155} INFO - Started process (PID=2617) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:23,008] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:22:23,009] {logging_mixin.py:112} INFO - [2020-11-02 04:22:23,009] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:24,664] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:24,726] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:22:24,759] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:22:24,820] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:22:24,825] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.822 seconds
[2020-11-02 04:22:36,287] {scheduler_job.py:155} INFO - Started process (PID=2692) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:36,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:22:36,292] {logging_mixin.py:112} INFO - [2020-11-02 04:22:36,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:37,529] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:37,568] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:22:37,589] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:22:37,617] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:22:37,676] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:22:37,686] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 03:22:27.937988+00:00 [success]> in ORM
[2020-11-02 04:22:37,695] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 03:22:27.937988+00:00 [scheduled]> in ORM
[2020-11-02 04:22:37,703] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 03:22:27.937988+00:00 [scheduled]> in ORM
[2020-11-02 04:22:37,716] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.429 seconds
[2020-11-02 04:22:49,588] {scheduler_job.py:155} INFO - Started process (PID=2801) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:49,591] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:22:49,607] {logging_mixin.py:112} INFO - [2020-11-02 04:22:49,607] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:51,121] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:22:51,172] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:22:51,201] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:22:51,245] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:22:51,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:22:51,320] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:20:13.109273+00:00 [scheduled]> in ORM
[2020-11-02 04:22:51,328] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:22:27.937988+00:00 [scheduled]> in ORM
[2020-11-02 04:22:51,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.751 seconds
[2020-11-02 04:23:02,979] {scheduler_job.py:155} INFO - Started process (PID=2906) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:02,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:23:02,996] {logging_mixin.py:112} INFO - [2020-11-02 04:23:02,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:04,644] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:04,683] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:23:04,703] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:23:04,737] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:23:04,777] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:23:04,782] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.802 seconds
[2020-11-02 04:23:16,205] {scheduler_job.py:155} INFO - Started process (PID=3019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:16,209] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:23:16,210] {logging_mixin.py:112} INFO - [2020-11-02 04:23:16,209] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:17,422] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:17,463] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:23:17,486] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:23:17,515] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:23:17,571] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:23:17,577] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.372 seconds
[2020-11-02 04:23:29,470] {scheduler_job.py:155} INFO - Started process (PID=3096) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:29,482] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:23:29,482] {logging_mixin.py:112} INFO - [2020-11-02 04:23:29,482] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:31,001] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:31,043] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:23:31,069] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:23:31,106] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:23:31,170] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:23:31,174] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.704 seconds
[2020-11-02 04:23:42,757] {scheduler_job.py:155} INFO - Started process (PID=3160) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:42,761] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:23:42,762] {logging_mixin.py:112} INFO - [2020-11-02 04:23:42,762] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:43,894] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:43,939] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:23:43,961] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:23:43,988] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:23:44,051] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:23:44,055] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.299 seconds
[2020-11-02 04:23:56,022] {scheduler_job.py:155} INFO - Started process (PID=3230) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:56,025] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:23:56,026] {logging_mixin.py:112} INFO - [2020-11-02 04:23:56,026] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:57,290] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:23:57,327] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:23:57,351] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:23:57,377] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:23:57,428] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:23:57,432] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.410 seconds
[2020-11-02 04:24:09,234] {scheduler_job.py:155} INFO - Started process (PID=3299) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:09,238] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:24:09,239] {logging_mixin.py:112} INFO - [2020-11-02 04:24:09,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:10,354] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:10,395] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:24:10,415] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:24:10,441] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:24:10,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:24:10,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.263 seconds
[2020-11-02 04:24:22,488] {scheduler_job.py:155} INFO - Started process (PID=3366) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:22,498] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:24:22,498] {logging_mixin.py:112} INFO - [2020-11-02 04:24:22,498] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:24,103] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:24,172] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:24:24,225] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:24:24,275] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:24:24,377] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:24:24,384] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.897 seconds
[2020-11-02 04:24:35,745] {scheduler_job.py:155} INFO - Started process (PID=3435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:35,749] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:24:35,750] {logging_mixin.py:112} INFO - [2020-11-02 04:24:35,750] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:37,079] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:37,150] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:24:37,175] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:24:37,204] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:24:37,277] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:24:37,281] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.536 seconds
[2020-11-02 04:24:49,067] {scheduler_job.py:155} INFO - Started process (PID=3502) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:49,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:24:49,072] {logging_mixin.py:112} INFO - [2020-11-02 04:24:49,072] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:50,617] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:24:50,662] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:24:50,684] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:24:50,717] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:24:50,807] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:24:50,813] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.747 seconds
[2020-11-02 04:25:02,307] {scheduler_job.py:155} INFO - Started process (PID=3577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:02,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:25:02,322] {logging_mixin.py:112} INFO - [2020-11-02 04:25:02,322] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:04,199] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:04,239] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:25:04,259] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:25:04,286] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:25:04,362] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:25:04,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.061 seconds
[2020-11-02 04:25:16,660] {scheduler_job.py:155} INFO - Started process (PID=3648) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:16,666] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:25:16,667] {logging_mixin.py:112} INFO - [2020-11-02 04:25:16,667] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:17,807] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:17,846] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:25:17,866] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:25:17,891] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:25:17,943] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:25:17,950] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:20:13.109273+00:00 [scheduled]> in ORM
[2020-11-02 04:25:17,957] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:22:27.937988+00:00 [scheduled]> in ORM
[2020-11-02 04:25:17,968] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-02 04:25:29,898] {scheduler_job.py:155} INFO - Started process (PID=3744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:29,902] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:25:29,902] {logging_mixin.py:112} INFO - [2020-11-02 04:25:29,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:31,906] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:31,977] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:25:32,041] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:25:32,172] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:25:32,349] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:25:32,361] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.463 seconds
[2020-11-02 04:25:44,256] {scheduler_job.py:155} INFO - Started process (PID=3835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:44,261] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:25:44,262] {logging_mixin.py:112} INFO - [2020-11-02 04:25:44,262] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:45,405] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:45,447] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:25:45,468] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:25:45,494] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:25:45,545] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:25:45,550] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.295 seconds
[2020-11-02 04:25:57,576] {scheduler_job.py:155} INFO - Started process (PID=3906) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:57,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:25:57,583] {logging_mixin.py:112} INFO - [2020-11-02 04:25:57,582] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:59,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:25:59,323] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:25:59,354] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:25:59,394] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:25:59,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:25:59,477] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.901 seconds
[2020-11-02 04:26:10,832] {scheduler_job.py:155} INFO - Started process (PID=3973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:10,837] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:26:10,838] {logging_mixin.py:112} INFO - [2020-11-02 04:26:10,838] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:11,938] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:11,979] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:26:11,998] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:26:12,023] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:26:12,071] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:26:12,076] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.244 seconds
[2020-11-02 04:26:24,102] {scheduler_job.py:155} INFO - Started process (PID=4043) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:24,106] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:26:24,106] {logging_mixin.py:112} INFO - [2020-11-02 04:26:24,106] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:25,252] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:25,292] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:26:25,315] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:26:25,346] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:26:25,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:26:25,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.300 seconds
[2020-11-02 04:26:37,437] {scheduler_job.py:155} INFO - Started process (PID=4120) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:37,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:26:37,445] {logging_mixin.py:112} INFO - [2020-11-02 04:26:37,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:38,894] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:38,948] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:26:38,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:26:39,014] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:26:39,142] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:26:39,147] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.711 seconds
[2020-11-02 04:26:50,672] {scheduler_job.py:155} INFO - Started process (PID=4186) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:50,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:26:50,676] {logging_mixin.py:112} INFO - [2020-11-02 04:26:50,676] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:51,796] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:26:51,845] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:26:51,866] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:26:51,893] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:26:51,947] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:26:51,953] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.281 seconds
[2020-11-02 04:27:03,971] {scheduler_job.py:155} INFO - Started process (PID=4283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:03,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:27:03,980] {logging_mixin.py:112} INFO - [2020-11-02 04:27:03,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:05,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:05,940] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:27:05,964] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:27:06,057] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:27:06,243] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:27:06,250] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.279 seconds
[2020-11-02 04:27:18,309] {scheduler_job.py:155} INFO - Started process (PID=4358) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:18,338] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:27:18,339] {logging_mixin.py:112} INFO - [2020-11-02 04:27:18,339] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:19,460] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:19,494] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:27:19,514] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:27:19,541] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:27:19,592] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:27:19,596] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.288 seconds
[2020-11-02 04:27:31,601] {scheduler_job.py:155} INFO - Started process (PID=4426) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:31,604] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:27:31,605] {logging_mixin.py:112} INFO - [2020-11-02 04:27:31,605] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:32,945] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:33,040] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:27:33,088] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:27:33,176] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:27:33,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:27:33,318] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:22:27.937988+00:00 [scheduled]> in ORM
[2020-11-02 04:27:33,330] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.729 seconds
[2020-11-02 04:27:45,034] {scheduler_job.py:155} INFO - Started process (PID=4520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:45,043] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:27:45,044] {logging_mixin.py:112} INFO - [2020-11-02 04:27:45,044] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:46,234] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:46,278] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:27:46,299] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:27:46,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:27:46,392] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:27:46,400] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:20:13.109273+00:00 [scheduled]> in ORM
[2020-11-02 04:27:46,411] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.377 seconds
[2020-11-02 04:27:58,323] {scheduler_job.py:155} INFO - Started process (PID=4612) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:58,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:27:58,327] {logging_mixin.py:112} INFO - [2020-11-02 04:27:58,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:59,566] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:27:59,608] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:27:59,629] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True>
[2020-11-02 04:27:59,644] {logging_mixin.py:112} INFO - [2020-11-02 04:27:59,644] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 03:20:13.109273+00:00: manual__2020-11-02T03:20:13.109273+00:00, externally triggered: True> failed
[2020-11-02 04:27:59,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:27:59,702] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:27:59,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.384 seconds
[2020-11-02 04:28:11,548] {scheduler_job.py:155} INFO - Started process (PID=4679) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:11,554] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:28:11,555] {logging_mixin.py:112} INFO - [2020-11-02 04:28:11,555] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:12,730] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:12,771] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:28:12,792] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:28:12,833] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:28:12,837] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.290 seconds
[2020-11-02 04:28:24,886] {scheduler_job.py:155} INFO - Started process (PID=4749) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:24,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:28:24,893] {logging_mixin.py:112} INFO - [2020-11-02 04:28:24,893] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:26,016] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:26,058] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:28:26,080] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:28:26,123] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:28:26,128] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.242 seconds
[2020-11-02 04:28:38,273] {scheduler_job.py:155} INFO - Started process (PID=4818) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:38,289] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:28:38,290] {logging_mixin.py:112} INFO - [2020-11-02 04:28:38,290] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:40,049] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:40,090] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:28:40,113] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:28:40,163] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:28:40,168] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.895 seconds
[2020-11-02 04:28:51,558] {scheduler_job.py:155} INFO - Started process (PID=4889) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:51,562] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:28:51,563] {logging_mixin.py:112} INFO - [2020-11-02 04:28:51,563] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:52,836] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:28:52,891] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:28:52,923] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:28:52,973] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:28:52,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.421 seconds
[2020-11-02 04:29:04,869] {scheduler_job.py:155} INFO - Started process (PID=4957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:04,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:29:04,894] {logging_mixin.py:112} INFO - [2020-11-02 04:29:04,893] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:07,274] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:07,323] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:29:07,356] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:29:07,426] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:29:07,435] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.567 seconds
[2020-11-02 04:29:19,246] {scheduler_job.py:155} INFO - Started process (PID=5022) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:19,253] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:29:19,254] {logging_mixin.py:112} INFO - [2020-11-02 04:29:19,253] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:20,577] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:20,627] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:29:20,655] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:29:20,707] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:29:20,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.465 seconds
[2020-11-02 04:29:32,546] {scheduler_job.py:155} INFO - Started process (PID=5094) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:32,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:29:32,551] {logging_mixin.py:112} INFO - [2020-11-02 04:29:32,551] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:33,793] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:33,831] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:29:33,852] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:29:33,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:29:33,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.352 seconds
[2020-11-02 04:29:45,795] {scheduler_job.py:155} INFO - Started process (PID=5159) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:45,800] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:29:45,800] {logging_mixin.py:112} INFO - [2020-11-02 04:29:45,800] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:46,953] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:46,987] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:29:47,007] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:29:47,048] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:29:47,055] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:22:27.937988+00:00 [scheduled]> in ORM
[2020-11-02 04:29:47,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.271 seconds
[2020-11-02 04:29:59,185] {scheduler_job.py:155} INFO - Started process (PID=5249) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:29:59,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:29:59,208] {logging_mixin.py:112} INFO - [2020-11-02 04:29:59,207] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:01,045] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:01,108] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:30:01,147] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True>
[2020-11-02 04:30:01,170] {logging_mixin.py:112} INFO - [2020-11-02 04:30:01,169] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 03:22:27.937988+00:00: manual__2020-11-02T03:22:27.937988+00:00, externally triggered: True> failed
[2020-11-02 04:30:01,174] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:30:01,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.005 seconds
[2020-11-02 04:30:13,399] {scheduler_job.py:155} INFO - Started process (PID=5316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:13,405] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:30:13,406] {logging_mixin.py:112} INFO - [2020-11-02 04:30:13,405] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:14,650] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:14,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:30:14,734] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:30:14,739] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.340 seconds
[2020-11-02 04:30:26,747] {scheduler_job.py:155} INFO - Started process (PID=5386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:26,752] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:30:26,753] {logging_mixin.py:112} INFO - [2020-11-02 04:30:26,753] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:28,478] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:28,524] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:30:28,549] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:30:28,554] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.807 seconds
[2020-11-02 04:30:40,093] {scheduler_job.py:155} INFO - Started process (PID=5468) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:40,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:30:40,128] {logging_mixin.py:112} INFO - [2020-11-02 04:30:40,117] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:42,305] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:42,385] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:30:42,430] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:30:42,437] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.344 seconds
[2020-11-02 04:30:54,418] {scheduler_job.py:155} INFO - Started process (PID=5535) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:54,425] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:30:54,426] {logging_mixin.py:112} INFO - [2020-11-02 04:30:54,426] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:55,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:30:55,710] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:30:55,729] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:30:55,733] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.315 seconds
[2020-11-02 04:31:07,696] {scheduler_job.py:155} INFO - Started process (PID=5607) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:07,710] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:31:07,724] {logging_mixin.py:112} INFO - [2020-11-02 04:31:07,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:09,594] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:09,657] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:31:09,692] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:31:09,706] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.020 seconds
[2020-11-02 04:31:21,952] {scheduler_job.py:155} INFO - Started process (PID=5678) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:21,957] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:31:21,958] {logging_mixin.py:112} INFO - [2020-11-02 04:31:21,958] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:23,163] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:23,202] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:31:23,224] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:31:23,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.277 seconds
[2020-11-02 04:31:35,248] {scheduler_job.py:155} INFO - Started process (PID=5745) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:35,260] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:31:35,261] {logging_mixin.py:112} INFO - [2020-11-02 04:31:35,261] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:36,365] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:36,405] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:31:36,424] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:31:36,428] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.180 seconds
[2020-11-02 04:31:48,525] {scheduler_job.py:155} INFO - Started process (PID=5816) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:48,530] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:31:48,531] {logging_mixin.py:112} INFO - [2020-11-02 04:31:48,531] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:50,021] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:31:50,061] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:31:50,091] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:31:50,096] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.571 seconds
[2020-11-02 04:32:01,899] {scheduler_job.py:155} INFO - Started process (PID=5888) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:01,902] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:32:01,903] {logging_mixin.py:112} INFO - [2020-11-02 04:32:01,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:03,027] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:03,061] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:32:03,080] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:32:03,084] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.185 seconds
[2020-11-02 04:32:15,226] {scheduler_job.py:155} INFO - Started process (PID=5957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:15,230] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:32:15,230] {logging_mixin.py:112} INFO - [2020-11-02 04:32:15,230] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:16,314] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:16,354] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:32:16,375] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:32:16,379] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-02 04:32:28,567] {scheduler_job.py:155} INFO - Started process (PID=6031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:28,571] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:32:28,572] {logging_mixin.py:112} INFO - [2020-11-02 04:32:28,571] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:29,911] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:29,961] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:32:29,986] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:32:29,992] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.425 seconds
[2020-11-02 04:32:42,045] {scheduler_job.py:155} INFO - Started process (PID=6107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:42,095] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:32:42,097] {logging_mixin.py:112} INFO - [2020-11-02 04:32:42,096] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:44,483] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:44,542] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:32:44,569] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:32:44,574] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.540 seconds
[2020-11-02 04:32:56,271] {scheduler_job.py:155} INFO - Started process (PID=6177) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:56,276] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:32:56,276] {logging_mixin.py:112} INFO - [2020-11-02 04:32:56,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:58,053] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:32:58,108] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:32:58,150] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:32:58,157] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.885 seconds
[2020-11-02 04:33:09,592] {scheduler_job.py:155} INFO - Started process (PID=6249) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:09,606] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:33:09,618] {logging_mixin.py:112} INFO - [2020-11-02 04:33:09,612] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:11,974] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:12,023] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:33:12,051] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:33:12,055] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.468 seconds
[2020-11-02 04:33:23,991] {scheduler_job.py:155} INFO - Started process (PID=6321) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:23,998] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:33:23,999] {logging_mixin.py:112} INFO - [2020-11-02 04:33:23,999] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:25,168] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:25,214] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:33:25,238] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:33:25,243] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.252 seconds
[2020-11-02 04:33:37,307] {scheduler_job.py:155} INFO - Started process (PID=6391) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:37,312] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:33:37,312] {logging_mixin.py:112} INFO - [2020-11-02 04:33:37,312] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:38,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:38,493] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:33:38,514] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:33:38,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.212 seconds
[2020-11-02 04:33:50,556] {scheduler_job.py:155} INFO - Started process (PID=6458) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:50,560] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:33:50,561] {logging_mixin.py:112} INFO - [2020-11-02 04:33:50,560] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:51,723] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:33:51,762] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:33:51,785] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:33:51,789] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.234 seconds
[2020-11-02 04:34:03,846] {scheduler_job.py:155} INFO - Started process (PID=6523) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:03,851] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:34:03,852] {logging_mixin.py:112} INFO - [2020-11-02 04:34:03,852] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:05,412] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:05,485] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:34:05,520] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:34:05,526] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.679 seconds
[2020-11-02 04:34:17,146] {scheduler_job.py:155} INFO - Started process (PID=6594) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:17,149] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:34:17,150] {logging_mixin.py:112} INFO - [2020-11-02 04:34:17,150] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:18,585] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:18,635] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:34:18,681] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:34:18,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.540 seconds
[2020-11-02 04:34:30,549] {scheduler_job.py:155} INFO - Started process (PID=6662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:30,556] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:34:30,557] {logging_mixin.py:112} INFO - [2020-11-02 04:34:30,557] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:31,762] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:31,800] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:34:31,821] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:34:31,825] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.276 seconds
[2020-11-02 04:34:43,892] {scheduler_job.py:155} INFO - Started process (PID=6728) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:43,895] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:34:43,904] {logging_mixin.py:112} INFO - [2020-11-02 04:34:43,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:45,349] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:45,390] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:34:45,410] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:34:45,414] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.522 seconds
[2020-11-02 04:34:57,187] {scheduler_job.py:155} INFO - Started process (PID=6792) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:57,191] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:34:57,192] {logging_mixin.py:112} INFO - [2020-11-02 04:34:57,191] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:58,918] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:34:58,957] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:34:58,982] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:34:58,987] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.800 seconds
[2020-11-02 04:35:10,447] {scheduler_job.py:155} INFO - Started process (PID=6867) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:10,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:35:10,451] {logging_mixin.py:112} INFO - [2020-11-02 04:35:10,451] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:11,724] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:11,768] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:35:11,790] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:35:11,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-02 04:35:23,853] {scheduler_job.py:155} INFO - Started process (PID=6941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:23,862] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:35:23,862] {logging_mixin.py:112} INFO - [2020-11-02 04:35:23,862] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:25,152] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:25,194] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:35:25,214] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:35:25,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.366 seconds
[2020-11-02 04:35:37,216] {scheduler_job.py:155} INFO - Started process (PID=7000) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:37,221] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:35:37,222] {logging_mixin.py:112} INFO - [2020-11-02 04:35:37,221] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:38,972] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:39,029] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:35:39,102] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:35:39,109] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:35:39,190] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:35:39,205] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 02:02:00+00:00 [success]> in ORM
[2020-11-02 04:35:39,221] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 02:02:00+00:00 [scheduled]> in ORM
[2020-11-02 04:35:39,234] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 02:02:00+00:00 [scheduled]> in ORM
[2020-11-02 04:35:39,253] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.036 seconds
[2020-11-02 04:35:51,775] {scheduler_job.py:155} INFO - Started process (PID=7102) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:51,780] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:35:51,780] {logging_mixin.py:112} INFO - [2020-11-02 04:35:51,780] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:53,507] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:35:53,562] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:35:53,599] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:35:53,662] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:35:53,678] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:02:00+00:00 [scheduled]> in ORM
[2020-11-02 04:35:53,697] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.922 seconds
[2020-11-02 04:36:05,163] {scheduler_job.py:155} INFO - Started process (PID=7200) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:05,167] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:36:05,168] {logging_mixin.py:112} INFO - [2020-11-02 04:36:05,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:07,883] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:07,964] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:36:08,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:36:08,093] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:36:08,102] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.939 seconds
[2020-11-02 04:36:19,458] {scheduler_job.py:155} INFO - Started process (PID=7313) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:19,467] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:36:19,474] {logging_mixin.py:112} INFO - [2020-11-02 04:36:19,474] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:21,097] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:21,157] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:36:21,182] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:36:21,232] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:36:21,237] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.778 seconds
[2020-11-02 04:36:32,945] {scheduler_job.py:155} INFO - Started process (PID=7373) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:32,950] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:36:32,950] {logging_mixin.py:112} INFO - [2020-11-02 04:36:32,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:34,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:34,304] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:36:34,327] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:36:34,369] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:36:34,374] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.428 seconds
[2020-11-02 04:36:46,230] {scheduler_job.py:155} INFO - Started process (PID=7440) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:46,234] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:36:46,235] {logging_mixin.py:112} INFO - [2020-11-02 04:36:46,235] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:47,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:36:47,530] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:36:47,552] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:36:47,602] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:36:47,606] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.376 seconds
[2020-11-02 04:37:04,662] {scheduler_job.py:155} INFO - Started process (PID=7526) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:04,671] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:37:04,680] {logging_mixin.py:112} INFO - [2020-11-02 04:37:04,680] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:06,422] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:06,478] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:37:06,504] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:37:06,567] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:37:06,572] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.910 seconds
[2020-11-02 04:37:31,398] {scheduler_job.py:155} INFO - Started process (PID=7659) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:31,402] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:37:31,402] {logging_mixin.py:112} INFO - [2020-11-02 04:37:31,402] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:33,006] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:33,053] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:37:33,076] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:37:33,117] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:37:33,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.723 seconds
[2020-11-02 04:37:58,015] {scheduler_job.py:155} INFO - Started process (PID=7791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:58,021] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:37:58,022] {logging_mixin.py:112} INFO - [2020-11-02 04:37:58,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:59,328] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:37:59,401] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:37:59,441] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:37:59,500] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:37:59,506] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.491 seconds
[2020-11-02 04:38:24,740] {scheduler_job.py:155} INFO - Started process (PID=7925) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:38:24,744] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:38:24,745] {logging_mixin.py:112} INFO - [2020-11-02 04:38:24,745] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:38:25,892] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:38:25,935] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:38:25,957] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:38:26,000] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:38:26,008] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:02:00+00:00 [scheduled]> in ORM
[2020-11-02 04:38:26,018] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.278 seconds
[2020-11-02 04:38:51,392] {scheduler_job.py:155} INFO - Started process (PID=8087) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:38:51,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:38:51,397] {logging_mixin.py:112} INFO - [2020-11-02 04:38:51,397] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:38:52,509] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:38:52,561] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:38:52,582] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:38:52,626] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:38:52,630] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.238 seconds
[2020-11-02 04:39:18,170] {scheduler_job.py:155} INFO - Started process (PID=8230) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:39:18,174] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:39:18,174] {logging_mixin.py:112} INFO - [2020-11-02 04:39:18,174] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:39:19,360] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:39:19,399] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:39:19,420] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:39:19,460] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:39:19,464] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.293 seconds
[2020-11-02 04:39:44,946] {scheduler_job.py:155} INFO - Started process (PID=8375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:39:44,949] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:39:44,949] {logging_mixin.py:112} INFO - [2020-11-02 04:39:44,949] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:39:46,285] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:39:46,324] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:39:46,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:39:46,390] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:39:46,394] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.448 seconds
[2020-11-02 04:40:11,983] {scheduler_job.py:155} INFO - Started process (PID=8513) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:40:11,989] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:40:11,990] {logging_mixin.py:112} INFO - [2020-11-02 04:40:11,989] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:40:13,102] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:40:13,136] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:40:13,157] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:40:13,196] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:40:13,200] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.217 seconds
[2020-11-02 04:40:38,599] {scheduler_job.py:155} INFO - Started process (PID=8660) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:40:38,605] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:40:38,607] {logging_mixin.py:112} INFO - [2020-11-02 04:40:38,606] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:40:40,202] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:40:40,280] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:40:40,319] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:40:40,386] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:40:40,398] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:02:00+00:00 [scheduled]> in ORM
[2020-11-02 04:40:40,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.820 seconds
[2020-11-02 04:41:04,442] {scheduler_job.py:155} INFO - Started process (PID=8821) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:04,456] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:41:04,457] {logging_mixin.py:112} INFO - [2020-11-02 04:41:04,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:05,597] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:05,634] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:41:05,656] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:41:05,697] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:41:05,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.260 seconds
[2020-11-02 04:41:31,049] {scheduler_job.py:155} INFO - Started process (PID=8967) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:31,054] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:41:31,055] {logging_mixin.py:112} INFO - [2020-11-02 04:41:31,055] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:32,223] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:32,262] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:41:32,282] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:41:32,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:41:32,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-02 04:41:57,836] {scheduler_job.py:155} INFO - Started process (PID=9118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:57,841] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:41:57,842] {logging_mixin.py:112} INFO - [2020-11-02 04:41:57,842] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:59,190] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:41:59,233] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:41:59,262] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:41:59,315] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:41:59,320] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.484 seconds
[2020-11-02 04:42:24,542] {scheduler_job.py:155} INFO - Started process (PID=9268) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:42:24,546] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:42:24,547] {logging_mixin.py:112} INFO - [2020-11-02 04:42:24,546] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:42:25,695] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:42:25,730] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:42:25,750] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:42:25,787] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:42:25,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-02 04:42:51,343] {scheduler_job.py:155} INFO - Started process (PID=9419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:42:51,356] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:42:51,356] {logging_mixin.py:112} INFO - [2020-11-02 04:42:51,356] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:42:52,717] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:42:52,754] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:42:52,777] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:42:52,827] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:42:52,834] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 02:02:00+00:00 [scheduled]> in ORM
[2020-11-02 04:42:52,844] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.500 seconds
[2020-11-02 04:43:18,174] {scheduler_job.py:155} INFO - Started process (PID=9582) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:43:18,181] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:43:18,183] {logging_mixin.py:112} INFO - [2020-11-02 04:43:18,182] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:43:19,502] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:43:19,546] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:43:19,569] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False>
[2020-11-02 04:43:19,585] {logging_mixin.py:112} INFO - [2020-11-02 04:43:19,585] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 02:02:00+00:00: scheduled__2020-11-02T02:02:00+00:00, externally triggered: False> failed
[2020-11-02 04:43:19,590] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:43:19,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.419 seconds
[2020-11-02 04:43:44,922] {scheduler_job.py:155} INFO - Started process (PID=9724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:43:44,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:43:44,927] {logging_mixin.py:112} INFO - [2020-11-02 04:43:44,927] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:43:46,054] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:43:46,096] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:43:46,117] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:43:46,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.199 seconds
[2020-11-02 04:44:11,625] {scheduler_job.py:155} INFO - Started process (PID=9868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:44:11,629] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:44:11,630] {logging_mixin.py:112} INFO - [2020-11-02 04:44:11,630] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:44:12,759] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:44:12,801] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:44:12,822] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:44:12,826] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.201 seconds
[2020-11-02 04:44:38,413] {scheduler_job.py:155} INFO - Started process (PID=10026) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:44:38,421] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:44:38,421] {logging_mixin.py:112} INFO - [2020-11-02 04:44:38,421] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:44:39,585] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:44:39,635] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:44:39,660] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:44:39,664] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.251 seconds
[2020-11-02 04:45:04,926] {scheduler_job.py:155} INFO - Started process (PID=10178) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:04,930] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:45:04,931] {logging_mixin.py:112} INFO - [2020-11-02 04:45:04,931] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:06,032] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:06,076] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:45:06,100] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:45:06,104] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.178 seconds
[2020-11-02 04:45:31,640] {scheduler_job.py:155} INFO - Started process (PID=10331) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:31,644] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:45:31,644] {logging_mixin.py:112} INFO - [2020-11-02 04:45:31,644] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:32,745] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:32,781] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:45:32,801] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:45:32,805] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.165 seconds
[2020-11-02 04:45:58,273] {scheduler_job.py:155} INFO - Started process (PID=10484) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:58,278] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:45:58,278] {logging_mixin.py:112} INFO - [2020-11-02 04:45:58,278] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:59,495] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:45:59,533] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:45:59,570] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:45:59,575] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.301 seconds
[2020-11-02 04:46:25,119] {scheduler_job.py:155} INFO - Started process (PID=10635) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:46:25,123] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:46:25,123] {logging_mixin.py:112} INFO - [2020-11-02 04:46:25,123] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:46:26,465] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:46:26,505] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:46:26,526] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:46:26,530] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.411 seconds
[2020-11-02 04:46:51,839] {scheduler_job.py:155} INFO - Started process (PID=10780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:46:51,843] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:46:51,843] {logging_mixin.py:112} INFO - [2020-11-02 04:46:51,843] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:46:52,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:46:52,986] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:46:53,009] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:46:53,013] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.174 seconds
[2020-11-02 04:47:18,722] {scheduler_job.py:155} INFO - Started process (PID=10923) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:47:18,726] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:47:18,726] {logging_mixin.py:112} INFO - [2020-11-02 04:47:18,726] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:47:19,851] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:47:19,906] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:47:19,930] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:47:19,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.212 seconds
[2020-11-02 04:47:45,591] {scheduler_job.py:155} INFO - Started process (PID=11065) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:47:45,596] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:47:45,597] {logging_mixin.py:112} INFO - [2020-11-02 04:47:45,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:47:46,712] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:47:46,754] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:47:46,776] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:47:46,781] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.191 seconds
[2020-11-02 04:48:12,448] {scheduler_job.py:155} INFO - Started process (PID=11207) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:48:12,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:48:12,456] {logging_mixin.py:112} INFO - [2020-11-02 04:48:12,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:48:13,564] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:48:13,605] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:48:13,629] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:48:13,633] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.186 seconds
[2020-11-02 04:48:39,242] {scheduler_job.py:155} INFO - Started process (PID=11350) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:48:39,250] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:48:39,251] {logging_mixin.py:112} INFO - [2020-11-02 04:48:39,250] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:48:40,414] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:48:40,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:48:40,477] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:48:40,481] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.240 seconds
[2020-11-02 04:49:06,100] {scheduler_job.py:155} INFO - Started process (PID=11498) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:06,104] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:49:06,104] {logging_mixin.py:112} INFO - [2020-11-02 04:49:06,104] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:07,177] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:07,218] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:49:07,237] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:49:07,240] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.140 seconds
[2020-11-02 04:49:33,044] {scheduler_job.py:155} INFO - Started process (PID=11638) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:33,049] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:49:33,049] {logging_mixin.py:112} INFO - [2020-11-02 04:49:33,049] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:34,133] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:34,171] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:49:34,192] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:49:34,196] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-02 04:49:59,871] {scheduler_job.py:155} INFO - Started process (PID=11794) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:49:59,884] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:49:59,885] {logging_mixin.py:112} INFO - [2020-11-02 04:49:59,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:01,277] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:01,317] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:50:01,338] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:50:01,342] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.471 seconds
[2020-11-02 04:50:26,742] {scheduler_job.py:155} INFO - Started process (PID=11939) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:26,747] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:50:26,747] {logging_mixin.py:112} INFO - [2020-11-02 04:50:26,747] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:27,860] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:27,900] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:50:27,920] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:50:27,924] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-02 04:50:53,441] {scheduler_job.py:155} INFO - Started process (PID=12080) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:53,447] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:50:53,448] {logging_mixin.py:112} INFO - [2020-11-02 04:50:53,447] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:54,547] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:50:54,586] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:50:54,610] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:50:54,614] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.173 seconds
[2020-11-02 04:51:20,156] {scheduler_job.py:155} INFO - Started process (PID=12223) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:51:20,162] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:51:20,163] {logging_mixin.py:112} INFO - [2020-11-02 04:51:20,162] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:51:21,348] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:51:21,388] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:51:21,409] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:51:21,413] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.257 seconds
[2020-11-02 04:51:46,822] {scheduler_job.py:155} INFO - Started process (PID=12357) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:51:46,826] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:51:46,827] {logging_mixin.py:112} INFO - [2020-11-02 04:51:46,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:51:48,252] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:51:48,299] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:51:48,322] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:51:48,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.505 seconds
[2020-11-02 04:52:13,378] {scheduler_job.py:155} INFO - Started process (PID=12496) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:52:13,383] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:52:13,384] {logging_mixin.py:112} INFO - [2020-11-02 04:52:13,384] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:52:14,970] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:52:15,023] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:52:15,056] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:52:15,062] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.683 seconds
[2020-11-02 04:52:40,080] {scheduler_job.py:155} INFO - Started process (PID=12644) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:52:40,085] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:52:40,086] {logging_mixin.py:112} INFO - [2020-11-02 04:52:40,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:52:41,220] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:52:41,258] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:52:41,279] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:52:41,283] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.204 seconds
[2020-11-02 04:53:06,949] {scheduler_job.py:155} INFO - Started process (PID=12799) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:53:06,953] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:53:06,954] {logging_mixin.py:112} INFO - [2020-11-02 04:53:06,954] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:53:08,151] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:53:08,189] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:53:08,211] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:53:08,215] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.266 seconds
[2020-11-02 04:53:33,839] {scheduler_job.py:155} INFO - Started process (PID=12951) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:53:33,847] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:53:33,848] {logging_mixin.py:112} INFO - [2020-11-02 04:53:33,847] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:53:35,166] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:53:35,204] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:53:35,226] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:53:35,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.391 seconds
[2020-11-02 04:54:00,566] {scheduler_job.py:155} INFO - Started process (PID=13097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:00,572] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:54:00,572] {logging_mixin.py:112} INFO - [2020-11-02 04:54:00,572] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:01,659] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:01,699] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:54:01,721] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:54:01,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.159 seconds
[2020-11-02 04:54:27,160] {scheduler_job.py:155} INFO - Started process (PID=13248) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:27,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:54:27,165] {logging_mixin.py:112} INFO - [2020-11-02 04:54:27,165] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:28,269] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:28,310] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:54:28,331] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:54:28,335] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.175 seconds
[2020-11-02 04:54:53,851] {scheduler_job.py:155} INFO - Started process (PID=13396) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:53,863] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:54:53,863] {logging_mixin.py:112} INFO - [2020-11-02 04:54:53,863] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:55,066] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:54:55,101] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:54:55,124] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:54:55,128] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.278 seconds
[2020-11-02 04:55:20,448] {scheduler_job.py:155} INFO - Started process (PID=13542) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:55:20,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:55:20,452] {logging_mixin.py:112} INFO - [2020-11-02 04:55:20,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:55:21,569] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:55:21,611] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:55:21,630] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:55:21,634] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.186 seconds
[2020-11-02 04:55:47,090] {scheduler_job.py:155} INFO - Started process (PID=13713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:55:47,093] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:55:47,094] {logging_mixin.py:112} INFO - [2020-11-02 04:55:47,094] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:55:48,182] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:55:48,222] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:55:48,243] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:55:48,252] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.162 seconds
[2020-11-02 04:56:13,754] {scheduler_job.py:155} INFO - Started process (PID=13859) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:56:13,757] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:56:13,758] {logging_mixin.py:112} INFO - [2020-11-02 04:56:13,758] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:56:14,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:56:14,919] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:56:14,941] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:56:14,946] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.192 seconds
[2020-11-02 04:56:40,345] {scheduler_job.py:155} INFO - Started process (PID=14008) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:56:40,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:56:40,349] {logging_mixin.py:112} INFO - [2020-11-02 04:56:40,348] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:56:41,762] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:56:41,801] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:56:41,820] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:56:41,823] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.478 seconds
[2020-11-02 04:57:06,990] {scheduler_job.py:155} INFO - Started process (PID=14154) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:57:07,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:57:07,003] {logging_mixin.py:112} INFO - [2020-11-02 04:57:07,003] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:57:08,424] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:57:08,460] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:57:08,480] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:57:08,484] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.494 seconds
[2020-11-02 04:57:33,686] {scheduler_job.py:155} INFO - Started process (PID=14301) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:57:33,691] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:57:33,691] {logging_mixin.py:112} INFO - [2020-11-02 04:57:33,691] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:57:34,776] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:57:34,812] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:57:34,830] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:57:34,834] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.148 seconds
[2020-11-02 04:58:00,339] {scheduler_job.py:155} INFO - Started process (PID=14472) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:00,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:58:00,344] {logging_mixin.py:112} INFO - [2020-11-02 04:58:00,344] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:01,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:01,521] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:58:01,542] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:58:01,546] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.208 seconds
[2020-11-02 04:58:26,897] {scheduler_job.py:155} INFO - Started process (PID=14613) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:26,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:58:26,902] {logging_mixin.py:112} INFO - [2020-11-02 04:58:26,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:28,074] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:28,113] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:58:28,136] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:58:28,141] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.244 seconds
[2020-11-02 04:58:53,531] {scheduler_job.py:155} INFO - Started process (PID=14757) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:53,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:58:53,537] {logging_mixin.py:112} INFO - [2020-11-02 04:58:53,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:54,671] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:58:54,710] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:58:54,730] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:58:54,734] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.203 seconds
[2020-11-02 04:59:20,126] {scheduler_job.py:155} INFO - Started process (PID=14905) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:59:20,130] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:59:20,131] {logging_mixin.py:112} INFO - [2020-11-02 04:59:20,131] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:59:21,520] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:59:21,564] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:59:21,589] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:59:21,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.468 seconds
[2020-11-02 04:59:46,709] {scheduler_job.py:155} INFO - Started process (PID=15050) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:59:46,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 04:59:46,715] {logging_mixin.py:112} INFO - [2020-11-02 04:59:46,714] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:59:48,196] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 04:59:48,252] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 04:59:48,287] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 04:59:48,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.584 seconds
[2020-11-02 05:00:13,292] {scheduler_job.py:155} INFO - Started process (PID=15200) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:00:13,307] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:00:13,308] {logging_mixin.py:112} INFO - [2020-11-02 05:00:13,308] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:00:14,779] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:00:14,833] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:00:14,858] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:00:14,862] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.570 seconds
[2020-11-02 05:00:39,898] {scheduler_job.py:155} INFO - Started process (PID=15349) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:00:39,903] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:00:39,904] {logging_mixin.py:112} INFO - [2020-11-02 05:00:39,904] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:00:41,330] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:00:41,388] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:00:41,413] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:00:41,418] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.520 seconds
[2020-11-02 05:01:06,468] {scheduler_job.py:155} INFO - Started process (PID=15496) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:06,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:01:06,472] {logging_mixin.py:112} INFO - [2020-11-02 05:01:06,472] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:07,637] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:07,679] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:01:07,702] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:01:07,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.239 seconds
[2020-11-02 05:01:33,042] {scheduler_job.py:155} INFO - Started process (PID=15640) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:33,050] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:01:33,052] {logging_mixin.py:112} INFO - [2020-11-02 05:01:33,051] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:34,209] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:34,251] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:01:34,274] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:01:34,278] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.236 seconds
[2020-11-02 05:01:59,541] {scheduler_job.py:155} INFO - Started process (PID=15784) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:01:59,545] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:01:59,547] {logging_mixin.py:112} INFO - [2020-11-02 05:01:59,546] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:00,746] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:00,785] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:02:00,828] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:02:00,834] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:02:00,885] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:02:00,893] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 03:02:00+00:00 [success]> in ORM
[2020-11-02 05:02:00,902] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 03:02:00+00:00 [scheduled]> in ORM
[2020-11-02 05:02:00,909] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 03:02:00+00:00 [scheduled]> in ORM
[2020-11-02 05:02:00,923] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.382 seconds
[2020-11-02 05:02:26,124] {scheduler_job.py:155} INFO - Started process (PID=15994) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:26,130] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:02:26,130] {logging_mixin.py:112} INFO - [2020-11-02 05:02:26,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:27,297] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:27,334] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:02:27,355] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:02:27,409] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:02:27,416] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:02:00+00:00 [scheduled]> in ORM
[2020-11-02 05:02:27,426] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.303 seconds
[2020-11-02 05:02:52,621] {scheduler_job.py:155} INFO - Started process (PID=16163) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:52,625] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:02:52,626] {logging_mixin.py:112} INFO - [2020-11-02 05:02:52,625] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:53,739] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:02:53,778] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:02:53,800] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:02:53,843] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:02:53,847] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.226 seconds
[2020-11-02 05:03:19,121] {scheduler_job.py:155} INFO - Started process (PID=16308) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:03:19,124] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:03:19,125] {logging_mixin.py:112} INFO - [2020-11-02 05:03:19,125] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:03:20,322] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:03:20,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:03:20,379] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:03:20,437] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:03:20,442] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.322 seconds
[2020-11-02 05:03:45,630] {scheduler_job.py:155} INFO - Started process (PID=16459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:03:45,635] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:03:45,635] {logging_mixin.py:112} INFO - [2020-11-02 05:03:45,635] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:03:47,023] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:03:47,068] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:03:47,093] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:03:47,143] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:03:47,147] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.517 seconds
[2020-11-02 05:04:12,273] {scheduler_job.py:155} INFO - Started process (PID=16604) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:04:12,277] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:04:12,277] {logging_mixin.py:112} INFO - [2020-11-02 05:04:12,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:04:13,434] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:04:13,475] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:04:13,498] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:04:13,541] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:04:13,546] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.273 seconds
[2020-11-02 05:04:38,923] {scheduler_job.py:155} INFO - Started process (PID=16754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:04:38,928] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:04:38,929] {logging_mixin.py:112} INFO - [2020-11-02 05:04:38,929] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:04:40,194] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:04:40,263] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:04:40,290] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:04:40,335] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:04:40,342] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:02:00+00:00 [scheduled]> in ORM
[2020-11-02 05:04:40,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.431 seconds
[2020-11-02 05:05:05,548] {scheduler_job.py:155} INFO - Started process (PID=16928) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:05,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:05:05,553] {logging_mixin.py:112} INFO - [2020-11-02 05:05:05,553] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:06,959] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:07,000] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:05:07,023] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:05:07,069] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:05:07,074] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.526 seconds
[2020-11-02 05:05:32,087] {scheduler_job.py:155} INFO - Started process (PID=17085) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:32,097] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:05:32,097] {logging_mixin.py:112} INFO - [2020-11-02 05:05:32,097] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:33,515] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:33,589] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:05:33,622] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:05:33,698] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:05:33,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.620 seconds
[2020-11-02 05:05:58,690] {scheduler_job.py:155} INFO - Started process (PID=17242) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:05:58,695] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:05:58,696] {logging_mixin.py:112} INFO - [2020-11-02 05:05:58,696] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:00,059] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:00,117] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:06:00,147] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:06:00,224] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:06:00,229] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.540 seconds
[2020-11-02 05:06:25,301] {scheduler_job.py:155} INFO - Started process (PID=17375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:25,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:06:25,306] {logging_mixin.py:112} INFO - [2020-11-02 05:06:25,306] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:26,569] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:26,607] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:06:26,628] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:06:26,672] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:06:26,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.375 seconds
[2020-11-02 05:06:51,866] {scheduler_job.py:155} INFO - Started process (PID=17524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:51,873] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:06:51,874] {logging_mixin.py:112} INFO - [2020-11-02 05:06:51,874] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:53,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:06:53,302] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:06:53,332] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:06:53,380] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:06:53,393] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:02:00+00:00 [scheduled]> in ORM
[2020-11-02 05:06:53,422] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.556 seconds
[2020-11-02 05:07:18,506] {scheduler_job.py:155} INFO - Started process (PID=17676) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:07:18,520] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:07:18,529] {logging_mixin.py:112} INFO - [2020-11-02 05:07:18,529] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:07:19,989] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:07:20,034] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:07:20,056] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:07:20,101] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:07:20,104] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.598 seconds
[2020-11-02 05:07:45,026] {scheduler_job.py:155} INFO - Started process (PID=17802) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:07:45,030] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:07:45,031] {logging_mixin.py:112} INFO - [2020-11-02 05:07:45,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:07:46,349] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:07:46,395] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:07:46,420] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:07:46,468] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:07:46,472] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.446 seconds
[2020-11-02 05:08:11,615] {scheduler_job.py:155} INFO - Started process (PID=17951) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:08:11,628] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:08:11,632] {logging_mixin.py:112} INFO - [2020-11-02 05:08:11,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:08:13,171] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:08:13,205] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:08:13,228] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:08:13,276] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:08:13,281] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.665 seconds
[2020-11-02 05:08:38,190] {scheduler_job.py:155} INFO - Started process (PID=18086) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:08:38,194] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:08:38,195] {logging_mixin.py:112} INFO - [2020-11-02 05:08:38,194] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:08:39,686] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:08:39,727] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:08:39,753] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:08:39,806] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:08:39,816] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.626 seconds
[2020-11-02 05:09:04,748] {scheduler_job.py:155} INFO - Started process (PID=18224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:09:04,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 05:09:04,752] {logging_mixin.py:112} INFO - [2020-11-02 05:09:04,752] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:09:06,333] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 05:09:06,387] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 05:09:06,434] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 05:09:06,498] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 05:09:06,509] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 03:02:00+00:00 [scheduled]> in ORM
[2020-11-02 05:09:06,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.775 seconds
[2020-11-02 11:37:53,358] {scheduler_job.py:155} INFO - Started process (PID=18417) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:37:53,367] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:37:53,368] {logging_mixin.py:112} INFO - [2020-11-02 11:37:53,368] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:37:55,227] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:37:55,282] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:37:55,338] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:37:55,357] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False>
[2020-11-02 11:37:55,374] {logging_mixin.py:112} INFO - [2020-11-02 11:37:55,374] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 03:02:00+00:00: scheduled__2020-11-02T03:02:00+00:00, externally triggered: False> failed
[2020-11-02 11:37:55,378] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:37:55,474] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:37:55,491] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 04:02:00+00:00 [success]> in ORM
[2020-11-02 11:37:55,506] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:37:55,527] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:37:55,548] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.190 seconds
[2020-11-02 11:38:20,007] {scheduler_job.py:155} INFO - Started process (PID=18624) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:38:20,010] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:38:20,011] {logging_mixin.py:112} INFO - [2020-11-02 11:38:20,011] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:38:21,255] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:38:21,292] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:38:21,332] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:21,336] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:21,366] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:21,428] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:38:21,436] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 05:02:00+00:00 [success]> in ORM
[2020-11-02 11:38:21,444] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:38:21,452] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:38:21,462] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.455 seconds
[2020-11-02 11:38:52,081] {scheduler_job.py:155} INFO - Started process (PID=18816) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:38:52,084] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:38:52,085] {logging_mixin.py:112} INFO - [2020-11-02 11:38:52,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:38:53,468] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:38:53,506] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:38:53,542] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:53,547] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:53,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:53,609] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:38:53,705] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:38:53,712] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 06:02:00+00:00 [success]> in ORM
[2020-11-02 11:38:53,719] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:38:53,727] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:38:53,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.680 seconds
[2020-11-02 11:39:21,627] {scheduler_job.py:155} INFO - Started process (PID=19025) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:39:21,675] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:39:21,677] {logging_mixin.py:112} INFO - [2020-11-02 11:39:21,676] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:39:22,907] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:39:22,945] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:39:22,990] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:22,995] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:23,025] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:23,056] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:23,087] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:23,257] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:39:23,264] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 07:02:00+00:00 [success]> in ORM
[2020-11-02 11:39:23,271] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:39:23,278] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:39:23,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.665 seconds
[2020-11-02 11:39:51,605] {scheduler_job.py:155} INFO - Started process (PID=19219) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:39:51,615] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:39:51,617] {logging_mixin.py:112} INFO - [2020-11-02 11:39:51,617] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:39:52,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:39:53,004] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:39:53,068] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:53,073] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:53,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:53,132] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:53,160] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:53,189] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:39:53,332] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:39:53,340] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 08:02:00+00:00 [success]> in ORM
[2020-11-02 11:39:53,352] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:39:53,358] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:39:53,374] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.769 seconds
[2020-11-02 11:40:18,607] {scheduler_job.py:155} INFO - Started process (PID=19390) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:40:18,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:40:18,611] {logging_mixin.py:112} INFO - [2020-11-02 11:40:18,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:40:19,943] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:40:20,032] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:40:20,105] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,187] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,278] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,343] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,394] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,436] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:40:20,614] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:40:20,621] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:40:20,630] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 09:02:00+00:00 [success]> in ORM
[2020-11-02 11:40:20,639] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:40:20,648] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:40:20,664] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.057 seconds
[2020-11-02 11:40:59,722] {scheduler_job.py:155} INFO - Started process (PID=19623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:40:59,730] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:40:59,731] {logging_mixin.py:112} INFO - [2020-11-02 11:40:59,731] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:41:00,880] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:41:00,924] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:41:00,950] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:00,968] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:00,995] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:01,022] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:01,055] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:01,081] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:01,237] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:41:01,244] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:41:01,252] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:41:01,259] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:41:01,272] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.550 seconds
[2020-11-02 11:41:39,808] {scheduler_job.py:155} INFO - Started process (PID=19849) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:41:39,812] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:41:39,813] {logging_mixin.py:112} INFO - [2020-11-02 11:41:39,813] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:41:40,965] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:41:41,009] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:41:41,029] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:41,049] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:41,065] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:41,111] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:41,156] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:41,205] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:41:41,433] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:41:41,442] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:41:41,450] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:41:41,457] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:41:41,469] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.660 seconds
[2020-11-02 11:42:26,414] {scheduler_job.py:155} INFO - Started process (PID=20064) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:42:26,427] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:42:26,430] {logging_mixin.py:112} INFO - [2020-11-02 11:42:26,429] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:42:27,648] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:42:27,687] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:42:27,727] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:42:27,770] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:42:27,807] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:42:27,841] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:42:27,878] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:42:27,945] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:42:28,127] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:42:28,134] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:42:28,141] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:42:28,154] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.740 seconds
[2020-11-02 11:43:19,248] {scheduler_job.py:155} INFO - Started process (PID=20309) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:43:19,267] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:43:19,268] {logging_mixin.py:112} INFO - [2020-11-02 11:43:19,268] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:43:21,974] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:43:22,039] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:43:22,080] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:22,131] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:22,162] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:22,194] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:22,226] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:22,272] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:22,661] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:43:22,677] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:43:22,690] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:43:22,697] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:43:22,704] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:43:22,717] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.469 seconds
[2020-11-02 11:43:44,523] {scheduler_job.py:155} INFO - Started process (PID=20483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:43:44,527] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 11:43:44,528] {logging_mixin.py:112} INFO - [2020-11-02 11:43:44,528] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:43:45,779] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 11:43:45,835] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 11:43:45,866] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:45,897] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:45,922] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:45,939] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:45,956] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:45,973] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 11:43:46,122] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 11:43:46,129] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:43:46,136] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 11:43:46,146] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.623 seconds
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [2020-11-02 12:44:02,344] {scheduler_job.py:155} INFO - Started process (PID=2861) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:44:02,358] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:44:02,360] {logging_mixin.py:112} INFO - [2020-11-02 12:44:02,359] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:44:03,889] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:44:03,927] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:44:03,973] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:03,977] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,008] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,037] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,054] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,070] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,086] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:04,283] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:44:04,290] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,296] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,304] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,311] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,317] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,324] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,330] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,343] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,357] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,368] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,380] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,392] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,401] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 10:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,414] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 10:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:04,423] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 10:02:00+00:00 [success]> in ORM
[2020-11-02 12:44:04,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.103 seconds
[2020-11-02 12:44:17,611] {scheduler_job.py:155} INFO - Started process (PID=2976) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:44:17,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:44:17,615] {logging_mixin.py:112} INFO - [2020-11-02 12:44:17,615] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:44:19,901] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:44:19,967] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:44:20,017] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,091] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,197] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,356] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,434] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,481] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:44:20,778] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:44:20,800] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 10:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:44:20,831] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.219 seconds
[2020-11-02 12:46:24,453] {scheduler_job.py:155} INFO - Started process (PID=3623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:24,485] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:46:24,491] {logging_mixin.py:112} INFO - [2020-11-02 12:46:24,490] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:25,954] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:25,994] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:46:26,014] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,041] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,068] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,139] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,182] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,215] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:26,349] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:46:26,358] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:46:26,366] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:46:26,375] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:46:26,385] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:46:26,399] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:46:26,422] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:46:26,445] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.991 seconds
[2020-11-02 12:46:37,599] {scheduler_job.py:155} INFO - Started process (PID=3719) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:37,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:46:37,614] {logging_mixin.py:112} INFO - [2020-11-02 12:46:37,614] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:39,801] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:39,850] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:46:39,880] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:39,924] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:39,954] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:39,981] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:40,012] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:40,062] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:40,085] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:40,166] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:46:40,171] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.572 seconds
[2020-11-02 12:46:51,935] {scheduler_job.py:155} INFO - Started process (PID=3831) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:51,945] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:46:51,947] {logging_mixin.py:112} INFO - [2020-11-02 12:46:51,947] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:53,116] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:46:53,169] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:46:53,196] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,223] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,247] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,273] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,294] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,317] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,351] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:46:53,451] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:46:53,456] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.521 seconds
[2020-11-02 12:47:38,522] {scheduler_job.py:155} INFO - Started process (PID=4050) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:47:38,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:47:38,540] {logging_mixin.py:112} INFO - [2020-11-02 12:47:38,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:47:40,083] {logging_mixin.py:112} INFO - [2020-11-02 12:47:40,077] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:47:40,084] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:47:40,112] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.589 seconds
[2020-11-02 12:47:51,897] {scheduler_job.py:155} INFO - Started process (PID=4107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:47:51,905] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:47:51,906] {logging_mixin.py:112} INFO - [2020-11-02 12:47:51,906] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:47:53,490] {logging_mixin.py:112} INFO - [2020-11-02 12:47:53,488] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:47:53,490] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:47:53,514] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.617 seconds
[2020-11-02 12:48:05,323] {scheduler_job.py:155} INFO - Started process (PID=4171) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:05,331] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:48:05,336] {logging_mixin.py:112} INFO - [2020-11-02 12:48:05,336] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:07,026] {logging_mixin.py:112} INFO - [2020-11-02 12:48:07,023] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:48:07,027] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:07,057] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.734 seconds
[2020-11-02 12:48:18,732] {scheduler_job.py:155} INFO - Started process (PID=4228) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:18,740] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:48:18,741] {logging_mixin.py:112} INFO - [2020-11-02 12:48:18,741] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:19,949] {logging_mixin.py:112} INFO - [2020-11-02 12:48:19,947] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:48:19,950] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:19,981] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-02 12:48:32,182] {scheduler_job.py:155} INFO - Started process (PID=4288) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:32,187] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:48:32,188] {logging_mixin.py:112} INFO - [2020-11-02 12:48:32,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:33,553] {logging_mixin.py:112} INFO - [2020-11-02 12:48:33,550] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:48:33,554] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:33,577] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.395 seconds
[2020-11-02 12:48:45,526] {scheduler_job.py:155} INFO - Started process (PID=4355) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:45,535] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:48:45,537] {logging_mixin.py:112} INFO - [2020-11-02 12:48:45,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:46,863] {logging_mixin.py:112} INFO - [2020-11-02 12:48:46,861] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:48:46,864] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:46,888] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.362 seconds
[2020-11-02 12:48:58,860] {scheduler_job.py:155} INFO - Started process (PID=4417) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:48:58,878] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:48:58,880] {logging_mixin.py:112} INFO - [2020-11-02 12:48:58,879] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:00,391] {logging_mixin.py:112} INFO - [2020-11-02 12:49:00,388] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:49:00,391] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:00,416] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.556 seconds
[2020-11-02 12:49:12,263] {scheduler_job.py:155} INFO - Started process (PID=4480) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:12,273] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:49:12,278] {logging_mixin.py:112} INFO - [2020-11-02 12:49:12,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:13,622] {logging_mixin.py:112} INFO - [2020-11-02 12:49:13,620] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:49:13,623] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:13,648] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.385 seconds
[2020-11-02 12:49:25,607] {scheduler_job.py:155} INFO - Started process (PID=4543) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:25,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:49:25,612] {logging_mixin.py:112} INFO - [2020-11-02 12:49:25,612] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:26,950] {logging_mixin.py:112} INFO - [2020-11-02 12:49:26,946] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:49:26,951] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:27,005] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.398 seconds
[2020-11-02 12:49:39,036] {scheduler_job.py:155} INFO - Started process (PID=4604) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:39,057] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:49:39,069] {logging_mixin.py:112} INFO - [2020-11-02 12:49:39,069] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:40,753] {logging_mixin.py:112} INFO - [2020-11-02 12:49:40,748] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:49:40,754] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:40,785] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.750 seconds
[2020-11-02 12:49:52,473] {scheduler_job.py:155} INFO - Started process (PID=4670) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:52,480] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:49:52,481] {logging_mixin.py:112} INFO - [2020-11-02 12:49:52,481] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:53,631] {logging_mixin.py:112} INFO - [2020-11-02 12:49:53,628] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:49:53,631] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:49:53,665] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.192 seconds
[2020-11-02 12:50:06,018] {scheduler_job.py:155} INFO - Started process (PID=4727) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:06,024] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:50:06,025] {logging_mixin.py:112} INFO - [2020-11-02 12:50:06,025] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:07,453] {logging_mixin.py:112} INFO - [2020-11-02 12:50:07,451] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:50:07,454] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:07,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.464 seconds
[2020-11-02 12:50:19,374] {scheduler_job.py:155} INFO - Started process (PID=4788) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:19,380] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:50:19,381] {logging_mixin.py:112} INFO - [2020-11-02 12:50:19,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:20,829] {logging_mixin.py:112} INFO - [2020-11-02 12:50:20,827] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:50:20,830] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:20,859] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.485 seconds
[2020-11-02 12:50:32,684] {scheduler_job.py:155} INFO - Started process (PID=4848) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:32,691] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:50:32,692] {logging_mixin.py:112} INFO - [2020-11-02 12:50:32,691] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:34,384] {logging_mixin.py:112} INFO - [2020-11-02 12:50:34,381] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:50:34,384] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:34,411] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.727 seconds
[2020-11-02 12:50:45,941] {scheduler_job.py:155} INFO - Started process (PID=4914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:45,946] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:50:45,947] {logging_mixin.py:112} INFO - [2020-11-02 12:50:45,947] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:47,573] {logging_mixin.py:112} INFO - [2020-11-02 12:50:47,571] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 30, in <module>
    from airflow.utils.date import days_ago
ModuleNotFoundError: No module named 'airflow.utils.date'
[2020-11-02 12:50:47,573] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:50:47,599] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.658 seconds
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [2020-11-02 12:55:21,868] {scheduler_job.py:155} INFO - Started process (PID=1992) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:55:21,877] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:55:21,878] {logging_mixin.py:112} INFO - [2020-11-02 12:55:21,878] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:55:23,395] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:55:23,444] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:55:23,477] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,514] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,541] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,569] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,595] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,623] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,650] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:23,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:55:23,847] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,857] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,868] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,879] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,891] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,904] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,913] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 10:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:55:23,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.069 seconds
[2020-11-02 12:55:48,615] {scheduler_job.py:155} INFO - Started process (PID=2288) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:55:48,637] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:55:48,637] {logging_mixin.py:112} INFO - [2020-11-02 12:55:48,637] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:55:52,031] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:55:52,109] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:55:52,154] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,214] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,295] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,383] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,455] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,510] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,572] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:55:52,739] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:55:52,751] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 4.135 seconds
[2020-11-02 12:57:07,997] {scheduler_job.py:155} INFO - Started process (PID=2706) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:57:08,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:57:08,011] {logging_mixin.py:112} INFO - [2020-11-02 12:57:08,011] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:57:09,709] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:57:09,767] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:57:09,796] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:09,848] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:09,887] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:09,926] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:09,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:10,014] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:10,066] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:10,394] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:57:10,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.405 seconds
[2020-11-02 12:57:34,930] {scheduler_job.py:155} INFO - Started process (PID=2828) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:57:34,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:57:34,943] {logging_mixin.py:112} INFO - [2020-11-02 12:57:34,942] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:57:36,973] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:57:37,043] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:57:37,070] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,101] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,128] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,152] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,177] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,206] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,230] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:57:37,425] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:57:37,439] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 10:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:57:37,459] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.530 seconds
[2020-11-02 12:58:01,773] {scheduler_job.py:155} INFO - Started process (PID=2975) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:01,782] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:58:01,784] {logging_mixin.py:112} INFO - [2020-11-02 12:58:01,783] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:03,544] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:03,590] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:58:03,610] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,637] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,662] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,688] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,718] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,777] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:03,913] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:58:03,921] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:58:03,933] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:58:03,945] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.171 seconds
[2020-11-02 12:58:28,370] {scheduler_job.py:155} INFO - Started process (PID=3150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:28,387] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:58:28,389] {logging_mixin.py:112} INFO - [2020-11-02 12:58:28,389] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:29,694] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:29,737] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:58:29,755] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:29,785] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:29,808] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:29,833] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:29,868] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:29,893] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:29,916] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:30,056] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:58:30,066] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:58:30,077] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:58:30,087] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:58:30,101] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.731 seconds
[2020-11-02 12:58:55,154] {scheduler_job.py:155} INFO - Started process (PID=3337) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:55,168] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:58:55,172] {logging_mixin.py:112} INFO - [2020-11-02 12:58:55,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:56,395] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:58:56,443] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:58:56,465] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,492] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,517] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,540] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,566] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,591] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,616] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:58:56,738] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:58:56,745] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:58:56,758] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.604 seconds
[2020-11-02 12:59:25,795] {scheduler_job.py:155} INFO - Started process (PID=3507) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:59:25,802] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:59:25,804] {logging_mixin.py:112} INFO - [2020-11-02 12:59:25,803] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:59:27,107] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:59:27,150] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:59:27,168] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,196] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,221] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,244] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,269] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,292] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,316] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:27,451] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:59:27,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.660 seconds
[2020-11-02 12:59:52,680] {scheduler_job.py:155} INFO - Started process (PID=3637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:59:52,686] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 12:59:52,687] {logging_mixin.py:112} INFO - [2020-11-02 12:59:52,687] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:59:53,967] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 12:59:54,012] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 12:59:54,029] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,060] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,083] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,108] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,135] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,159] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,182] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 12:59:54,318] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 12:59:54,326] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 10:02:00+00:00 [scheduled]> in ORM
[2020-11-02 12:59:54,338] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.658 seconds
[2020-11-02 13:00:19,363] {scheduler_job.py:155} INFO - Started process (PID=3792) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:00:19,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:00:19,378] {logging_mixin.py:112} INFO - [2020-11-02 13:00:19,378] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:00:20,897] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:00:20,962] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:00:21,007] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,050] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,085] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,145] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,176] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,202] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:21,214] {logging_mixin.py:112} INFO - [2020-11-02 13:00:21,214] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:02:00+00:00: scheduled__2020-11-02T10:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:00:21,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:00:21,330] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 09:02:00+00:00 [scheduled]> in ORM
[2020-11-02 13:00:21,342] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.979 seconds
[2020-11-02 13:00:45,908] {scheduler_job.py:155} INFO - Started process (PID=3941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:00:45,925] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:00:45,925] {logging_mixin.py:112} INFO - [2020-11-02 13:00:45,925] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:00:47,843] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:00:47,947] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:00:47,974] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:48,017] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:48,054] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:48,104] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:48,158] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:48,193] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False>
[2020-11-02 13:00:48,214] {logging_mixin.py:112} INFO - [2020-11-02 13:00:48,214] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 09:02:00+00:00: scheduled__2020-11-02T09:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:00:48,388] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:00:48,413] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 06:02:00+00:00 [scheduled]> in ORM
[2020-11-02 13:00:48,427] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 07:02:00+00:00 [scheduled]> in ORM
[2020-11-02 13:00:48,442] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 08:02:00+00:00 [scheduled]> in ORM
[2020-11-02 13:00:48,468] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.560 seconds
[2020-11-02 13:01:12,432] {scheduler_job.py:155} INFO - Started process (PID=4119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:01:12,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:01:12,446] {logging_mixin.py:112} INFO - [2020-11-02 13:01:12,446] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:01:14,485] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:01:14,692] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:01:14,756] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:14,860] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:14,913] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:14,936] {logging_mixin.py:112} INFO - [2020-11-02 13:01:14,936] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 06:02:00+00:00: scheduled__2020-11-02T06:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:01:14,945] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:14,968] {logging_mixin.py:112} INFO - [2020-11-02 13:01:14,968] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 07:02:00+00:00: scheduled__2020-11-02T07:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:01:14,971] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:15,062] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:01:15,335] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:01:15,350] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 04:02:00+00:00 [scheduled]> in ORM
[2020-11-02 13:01:15,365] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 05:02:00+00:00 [scheduled]> in ORM
[2020-11-02 13:01:15,383] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-02 12:00:58.517184+00:00 [success]> in ORM
[2020-11-02 13:01:15,403] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-02 12:00:58.517184+00:00 [scheduled]> in ORM
[2020-11-02 13:01:15,433] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-02 12:00:58.517184+00:00 [scheduled]> in ORM
[2020-11-02 13:01:15,495] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.063 seconds
[2020-11-02 13:01:47,096] {scheduler_job.py:155} INFO - Started process (PID=4359) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:01:47,101] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:01:47,102] {logging_mixin.py:112} INFO - [2020-11-02 13:01:47,102] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:01:49,510] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:01:49,596] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:01:49,627] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:49,692] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:49,738] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False>
[2020-11-02 13:01:49,764] {logging_mixin.py:112} INFO - [2020-11-02 13:01:49,763] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 08:02:00+00:00: scheduled__2020-11-02T08:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:01:49,768] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:01:49,885] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:01:49,903] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 12:00:58.517184+00:00 [scheduled]> in ORM
[2020-11-02 13:01:49,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.839 seconds
[2020-11-02 13:02:31,161] {scheduler_job.py:155} INFO - Started process (PID=4575) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:02:31,169] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:02:31,170] {logging_mixin.py:112} INFO - [2020-11-02 13:02:31,170] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:02:32,514] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:02:32,559] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:02:32,580] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False>
[2020-11-02 13:02:32,596] {logging_mixin.py:112} INFO - [2020-11-02 13:02:32,596] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 04:02:00+00:00: scheduled__2020-11-02T04:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:02:32,600] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False>
[2020-11-02 13:02:32,614] {logging_mixin.py:112} INFO - [2020-11-02 13:02:32,614] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 05:02:00+00:00: scheduled__2020-11-02T05:02:00+00:00, externally triggered: False> failed
[2020-11-02 13:02:32,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:02:32,668] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:02:32,672] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.511 seconds
[2020-11-02 13:02:58,034] {scheduler_job.py:155} INFO - Started process (PID=4693) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:02:58,054] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:02:58,064] {logging_mixin.py:112} INFO - [2020-11-02 13:02:58,064] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:02:59,755] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:02:59,804] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:02:59,825] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:02:59,890] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:02:59,900] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.866 seconds
[2020-11-02 13:03:24,648] {scheduler_job.py:155} INFO - Started process (PID=4823) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:03:24,654] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:03:24,655] {logging_mixin.py:112} INFO - [2020-11-02 13:03:24,655] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:03:25,888] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:03:25,931] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:03:25,949] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:03:25,999] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:03:26,006] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.358 seconds
[2020-11-02 13:03:51,189] {scheduler_job.py:155} INFO - Started process (PID=4951) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:03:51,207] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:03:51,208] {logging_mixin.py:112} INFO - [2020-11-02 13:03:51,208] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:03:52,990] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:03:53,031] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:03:53,049] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:03:53,094] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:03:53,099] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.911 seconds
[2020-11-02 13:04:17,891] {scheduler_job.py:155} INFO - Started process (PID=5083) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:04:17,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:04:17,905] {logging_mixin.py:112} INFO - [2020-11-02 13:04:17,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:04:20,187] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:04:20,316] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:04:20,367] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:04:20,470] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:04:20,486] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 12:00:58.517184+00:00 [scheduled]> in ORM
[2020-11-02 13:04:20,513] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.622 seconds
[2020-11-02 13:04:44,856] {scheduler_job.py:155} INFO - Started process (PID=5228) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:04:44,871] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:04:44,874] {logging_mixin.py:112} INFO - [2020-11-02 13:04:44,874] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:04:46,380] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:04:46,444] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:04:46,480] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:04:46,577] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:04:46,595] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.740 seconds
[2020-11-02 13:05:11,576] {scheduler_job.py:155} INFO - Started process (PID=5351) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:05:11,581] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:05:11,581] {logging_mixin.py:112} INFO - [2020-11-02 13:05:11,581] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:05:13,086] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:05:13,138] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:05:13,158] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:05:13,231] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:05:13,236] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.660 seconds
[2020-11-02 13:05:38,338] {scheduler_job.py:155} INFO - Started process (PID=5476) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:05:38,345] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:05:38,346] {logging_mixin.py:112} INFO - [2020-11-02 13:05:38,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:05:39,720] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:05:39,765] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:05:39,786] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:05:39,840] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:05:39,847] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.509 seconds
[2020-11-02 13:06:05,177] {scheduler_job.py:155} INFO - Started process (PID=5596) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:05,189] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:06:05,191] {logging_mixin.py:112} INFO - [2020-11-02 13:06:05,191] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:06,626] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:06,676] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:06:06,695] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:06:06,752] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:06:06,765] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.588 seconds
[2020-11-02 13:06:31,851] {scheduler_job.py:155} INFO - Started process (PID=5720) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:31,856] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:06:31,856] {logging_mixin.py:112} INFO - [2020-11-02 13:06:31,856] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:33,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:33,304] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:06:33,326] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:06:33,385] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:06:33,393] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 12:00:58.517184+00:00 [scheduled]> in ORM
[2020-11-02 13:06:33,407] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.556 seconds
[2020-11-02 13:06:58,578] {scheduler_job.py:155} INFO - Started process (PID=5877) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:06:58,584] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:06:58,586] {logging_mixin.py:112} INFO - [2020-11-02 13:06:58,585] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:00,030] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:00,078] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:07:00,107] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:07:00,157] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:07:00,161] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.583 seconds
[2020-11-02 13:07:25,334] {scheduler_job.py:155} INFO - Started process (PID=6002) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:25,352] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:07:25,356] {logging_mixin.py:112} INFO - [2020-11-02 13:07:25,356] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:27,078] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:27,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:07:27,139] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:07:27,197] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:07:27,201] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.868 seconds
[2020-11-02 13:07:52,438] {scheduler_job.py:155} INFO - Started process (PID=6131) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:52,469] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:07:52,471] {logging_mixin.py:112} INFO - [2020-11-02 13:07:52,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:54,081] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:07:54,127] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:07:54,152] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:07:54,216] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:07:54,221] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.783 seconds
[2020-11-02 13:08:19,420] {scheduler_job.py:155} INFO - Started process (PID=6252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:08:19,436] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:08:19,438] {logging_mixin.py:112} INFO - [2020-11-02 13:08:19,437] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:08:20,974] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:08:21,027] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:08:21,046] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:08:21,113] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:08:21,119] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.698 seconds
[2020-11-02 13:08:46,113] {scheduler_job.py:155} INFO - Started process (PID=6383) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:08:46,126] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:08:46,128] {logging_mixin.py:112} INFO - [2020-11-02 13:08:46,128] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:08:47,695] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:08:47,735] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:08:47,759] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:08:47,822] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:08:47,830] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-02 12:00:58.517184+00:00 [scheduled]> in ORM
[2020-11-02 13:08:47,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.728 seconds
[2020-11-02 13:09:13,225] {scheduler_job.py:155} INFO - Started process (PID=6538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:09:13,239] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:09:13,240] {logging_mixin.py:112} INFO - [2020-11-02 13:09:13,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:09:14,712] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:09:14,782] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:09:14,820] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True>
[2020-11-02 13:09:14,850] {logging_mixin.py:112} INFO - [2020-11-02 13:09:14,850] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:00:58.517184+00:00: manual__2020-11-02T12:00:58.517184+00:00, externally triggered: True> failed
[2020-11-02 13:09:14,857] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:09:14,867] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.643 seconds
[2020-11-02 13:09:40,134] {scheduler_job.py:155} INFO - Started process (PID=6663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:09:40,139] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:09:40,140] {logging_mixin.py:112} INFO - [2020-11-02 13:09:40,140] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:09:41,801] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:09:41,879] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:09:41,910] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:09:41,918] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.783 seconds
[2020-11-02 13:10:07,173] {scheduler_job.py:155} INFO - Started process (PID=6791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:10:07,177] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:10:07,178] {logging_mixin.py:112} INFO - [2020-11-02 13:10:07,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:10:08,558] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:10:08,602] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:10:08,620] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:10:08,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.452 seconds
[2020-11-02 13:10:34,220] {scheduler_job.py:155} INFO - Started process (PID=6915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:10:34,231] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:10:34,234] {logging_mixin.py:112} INFO - [2020-11-02 13:10:34,233] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:10:35,602] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:10:35,646] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:10:35,672] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:10:35,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.456 seconds
[2020-11-02 13:11:00,953] {scheduler_job.py:155} INFO - Started process (PID=7037) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:00,958] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:11:00,959] {logging_mixin.py:112} INFO - [2020-11-02 13:11:00,959] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:02,363] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:02,407] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:11:02,425] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:11:02,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.476 seconds
[2020-11-02 13:11:27,983] {scheduler_job.py:155} INFO - Started process (PID=7159) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:27,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:11:28,006] {logging_mixin.py:112} INFO - [2020-11-02 13:11:28,005] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:29,745] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:29,788] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:11:29,806] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:11:29,812] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.829 seconds
[2020-11-02 13:11:54,990] {scheduler_job.py:155} INFO - Started process (PID=7277) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:54,998] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:11:54,999] {logging_mixin.py:112} INFO - [2020-11-02 13:11:54,999] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:56,677] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:11:56,729] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:11:56,748] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:11:56,753] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.763 seconds
[2020-11-02 13:12:21,872] {scheduler_job.py:155} INFO - Started process (PID=7395) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:12:21,881] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:12:21,882] {logging_mixin.py:112} INFO - [2020-11-02 13:12:21,882] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:12:23,235] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:12:23,283] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:12:23,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:12:23,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.444 seconds
[2020-11-02 13:12:48,712] {scheduler_job.py:155} INFO - Started process (PID=7520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:12:48,741] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:12:48,744] {logging_mixin.py:112} INFO - [2020-11-02 13:12:48,744] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:12:50,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:12:50,192] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:12:50,211] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:12:50,216] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.504 seconds
[2020-11-02 13:13:15,713] {scheduler_job.py:155} INFO - Started process (PID=7645) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:13:15,717] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:13:15,718] {logging_mixin.py:112} INFO - [2020-11-02 13:13:15,718] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:13:17,041] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:13:17,098] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:13:17,119] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:13:17,123] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.410 seconds
[2020-11-02 13:13:42,764] {scheduler_job.py:155} INFO - Started process (PID=7769) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:13:42,777] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:13:42,778] {logging_mixin.py:112} INFO - [2020-11-02 13:13:42,778] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:13:44,269] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:13:44,315] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:13:44,344] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:13:44,348] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.585 seconds
[2020-11-02 13:14:09,574] {scheduler_job.py:155} INFO - Started process (PID=7893) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:14:09,581] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:14:09,581] {logging_mixin.py:112} INFO - [2020-11-02 13:14:09,581] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:14:11,120] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:14:11,165] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:14:11,183] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:14:11,188] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.613 seconds
[2020-11-02 13:14:36,478] {scheduler_job.py:155} INFO - Started process (PID=8013) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:14:36,492] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:14:36,500] {logging_mixin.py:112} INFO - [2020-11-02 13:14:36,500] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:14:38,121] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:14:38,168] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:14:38,196] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:14:38,202] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.724 seconds
[2020-11-02 13:15:03,529] {scheduler_job.py:155} INFO - Started process (PID=8140) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:03,533] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:15:03,534] {logging_mixin.py:112} INFO - [2020-11-02 13:15:03,533] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:05,395] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:05,481] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:15:05,508] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:15:05,520] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.992 seconds
[2020-11-02 13:15:30,660] {scheduler_job.py:155} INFO - Started process (PID=8254) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:30,672] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:15:30,679] {logging_mixin.py:112} INFO - [2020-11-02 13:15:30,679] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:32,353] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:32,446] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:15:32,476] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:15:32,483] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.823 seconds
[2020-11-02 13:15:57,580] {scheduler_job.py:155} INFO - Started process (PID=8377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:57,596] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:15:57,597] {logging_mixin.py:112} INFO - [2020-11-02 13:15:57,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:58,963] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:15:59,010] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:15:59,034] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:15:59,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.466 seconds
[2020-11-02 13:16:24,498] {scheduler_job.py:155} INFO - Started process (PID=8497) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:16:24,505] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:16:24,506] {logging_mixin.py:112} INFO - [2020-11-02 13:16:24,506] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:16:25,888] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:16:25,942] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:16:25,965] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:16:25,970] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.472 seconds
[2020-11-02 13:16:51,376] {scheduler_job.py:155} INFO - Started process (PID=8621) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:16:51,388] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:16:51,390] {logging_mixin.py:112} INFO - [2020-11-02 13:16:51,389] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:16:52,780] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:16:52,826] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:16:52,845] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:16:52,850] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.475 seconds
[2020-11-02 13:17:18,148] {scheduler_job.py:155} INFO - Started process (PID=8753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:17:18,153] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:17:18,154] {logging_mixin.py:112} INFO - [2020-11-02 13:17:18,154] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:17:19,568] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:17:19,615] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:17:19,637] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:17:19,643] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.495 seconds
[2020-11-02 13:17:44,985] {scheduler_job.py:155} INFO - Started process (PID=8885) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:17:44,996] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:17:45,000] {logging_mixin.py:112} INFO - [2020-11-02 13:17:44,999] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:17:46,438] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:17:46,481] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:17:46,512] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:17:46,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.532 seconds
[2020-11-02 13:18:11,833] {scheduler_job.py:155} INFO - Started process (PID=9012) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:18:11,850] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:18:11,866] {logging_mixin.py:112} INFO - [2020-11-02 13:18:11,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:18:13,692] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:18:13,737] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:18:13,757] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:18:13,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.928 seconds
[2020-11-02 13:18:38,493] {scheduler_job.py:155} INFO - Started process (PID=9134) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:18:38,520] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:18:38,522] {logging_mixin.py:112} INFO - [2020-11-02 13:18:38,521] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:18:39,936] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:18:39,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:18:40,004] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:18:40,008] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.518 seconds
[2020-11-02 13:19:05,196] {scheduler_job.py:155} INFO - Started process (PID=9259) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:05,220] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:19:05,226] {logging_mixin.py:112} INFO - [2020-11-02 13:19:05,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:06,758] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:06,814] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:19:06,832] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:19:06,837] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.641 seconds
[2020-11-02 13:19:32,190] {scheduler_job.py:155} INFO - Started process (PID=9371) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:32,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:19:32,198] {logging_mixin.py:112} INFO - [2020-11-02 13:19:32,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:33,733] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:33,777] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:19:33,796] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:19:33,801] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.611 seconds
[2020-11-02 13:19:59,123] {scheduler_job.py:155} INFO - Started process (PID=9491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:19:59,131] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:19:59,132] {logging_mixin.py:112} INFO - [2020-11-02 13:19:59,132] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:00,524] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:00,584] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:20:00,604] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:20:00,610] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.487 seconds
[2020-11-02 13:20:26,001] {scheduler_job.py:155} INFO - Started process (PID=9617) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:26,018] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:20:26,020] {logging_mixin.py:112} INFO - [2020-11-02 13:20:26,019] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:27,744] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:27,790] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:20:27,817] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:20:27,822] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.821 seconds
[2020-11-02 13:20:53,015] {scheduler_job.py:155} INFO - Started process (PID=9741) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:53,025] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:20:53,027] {logging_mixin.py:112} INFO - [2020-11-02 13:20:53,027] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:54,482] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:20:54,539] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:20:54,558] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:20:54,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.554 seconds
[2020-11-02 13:21:19,789] {scheduler_job.py:155} INFO - Started process (PID=9862) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:21:19,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:21:19,798] {logging_mixin.py:112} INFO - [2020-11-02 13:21:19,797] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:21:21,218] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:21:21,269] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:21:21,298] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:21:21,304] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.516 seconds
[2020-11-02 13:21:46,816] {scheduler_job.py:155} INFO - Started process (PID=9989) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:21:46,823] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:21:46,824] {logging_mixin.py:112} INFO - [2020-11-02 13:21:46,824] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:21:48,390] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:21:48,436] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:21:48,455] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:21:48,460] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.645 seconds
[2020-11-02 13:22:13,768] {scheduler_job.py:155} INFO - Started process (PID=10105) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:22:13,778] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:22:13,779] {logging_mixin.py:112} INFO - [2020-11-02 13:22:13,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:22:15,417] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:22:15,466] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:22:15,486] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:22:15,491] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.723 seconds
[2020-11-02 13:22:40,712] {scheduler_job.py:155} INFO - Started process (PID=10237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:22:40,723] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:22:40,726] {logging_mixin.py:112} INFO - [2020-11-02 13:22:40,726] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:22:42,446] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:22:42,513] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:22:42,530] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:22:42,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.824 seconds
[2020-11-02 13:23:07,500] {scheduler_job.py:155} INFO - Started process (PID=10364) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:23:07,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:23:07,507] {logging_mixin.py:112} INFO - [2020-11-02 13:23:07,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:23:09,330] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:23:09,412] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:23:09,442] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:23:09,449] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.949 seconds
[2020-11-02 13:23:34,611] {scheduler_job.py:155} INFO - Started process (PID=10486) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:23:34,631] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:23:34,643] {logging_mixin.py:112} INFO - [2020-11-02 13:23:34,643] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:23:36,083] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:23:36,128] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:23:36,146] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:23:36,150] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.541 seconds
[2020-11-02 13:24:01,475] {scheduler_job.py:155} INFO - Started process (PID=10609) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:01,484] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:24:01,501] {logging_mixin.py:112} INFO - [2020-11-02 13:24:01,500] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:02,936] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:02,977] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:24:02,994] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:24:02,998] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.523 seconds
[2020-11-02 13:24:28,290] {scheduler_job.py:155} INFO - Started process (PID=10729) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:28,297] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:24:28,298] {logging_mixin.py:112} INFO - [2020-11-02 13:24:28,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:29,397] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:29,442] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:24:29,459] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:24:29,463] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.173 seconds
[2020-11-02 13:24:55,231] {scheduler_job.py:155} INFO - Started process (PID=10859) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:55,239] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:24:55,240] {logging_mixin.py:112} INFO - [2020-11-02 13:24:55,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:56,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:24:56,532] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:24:56,548] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:24:56,553] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.322 seconds
[2020-11-02 13:25:22,074] {scheduler_job.py:155} INFO - Started process (PID=10980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:25:22,084] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:25:22,085] {logging_mixin.py:112} INFO - [2020-11-02 13:25:22,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:25:23,233] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:25:23,274] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:25:23,292] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:25:23,298] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.224 seconds
[2020-11-02 13:25:48,886] {scheduler_job.py:155} INFO - Started process (PID=11105) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:25:48,918] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:25:48,921] {logging_mixin.py:112} INFO - [2020-11-02 13:25:48,920] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:25:50,180] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:25:50,218] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:25:50,235] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:25:50,239] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.354 seconds
[2020-11-02 13:26:15,985] {scheduler_job.py:155} INFO - Started process (PID=11225) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:26:16,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:26:16,003] {logging_mixin.py:112} INFO - [2020-11-02 13:26:16,003] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:26:17,275] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:26:17,317] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:26:17,333] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:26:17,337] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.353 seconds
[2020-11-02 13:26:42,922] {scheduler_job.py:155} INFO - Started process (PID=11344) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:26:42,947] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:26:42,947] {logging_mixin.py:112} INFO - [2020-11-02 13:26:42,947] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:26:44,239] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:26:44,293] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:26:44,309] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:26:44,313] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.390 seconds
[2020-11-02 13:27:09,793] {scheduler_job.py:155} INFO - Started process (PID=11466) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:27:09,802] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:27:09,803] {logging_mixin.py:112} INFO - [2020-11-02 13:27:09,803] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:27:11,029] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:27:11,070] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:27:11,087] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:27:11,091] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.299 seconds
[2020-11-02 13:27:36,546] {scheduler_job.py:155} INFO - Started process (PID=11593) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:27:36,556] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:27:36,558] {logging_mixin.py:112} INFO - [2020-11-02 13:27:36,558] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:27:37,750] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:27:37,795] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:27:37,812] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:27:37,816] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.270 seconds
[2020-11-02 13:28:03,350] {scheduler_job.py:155} INFO - Started process (PID=11715) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:03,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:28:03,359] {logging_mixin.py:112} INFO - [2020-11-02 13:28:03,358] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:04,679] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:04,722] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:28:04,740] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:28:04,744] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.395 seconds
[2020-11-02 13:28:30,242] {scheduler_job.py:155} INFO - Started process (PID=11835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:30,252] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:28:30,254] {logging_mixin.py:112} INFO - [2020-11-02 13:28:30,254] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:31,884] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:31,928] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:28:31,947] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:28:31,951] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.709 seconds
[2020-11-02 13:28:57,112] {scheduler_job.py:155} INFO - Started process (PID=11957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:57,123] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:28:57,125] {logging_mixin.py:112} INFO - [2020-11-02 13:28:57,124] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:58,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:28:58,311] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:28:58,332] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:28:58,337] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.225 seconds
[2020-11-02 13:29:23,974] {scheduler_job.py:155} INFO - Started process (PID=12078) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:29:23,984] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:29:23,985] {logging_mixin.py:112} INFO - [2020-11-02 13:29:23,985] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:29:25,136] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:29:25,178] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:29:25,197] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:29:25,201] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.227 seconds
[2020-11-02 13:29:50,853] {scheduler_job.py:155} INFO - Started process (PID=12198) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:29:50,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:29:50,866] {logging_mixin.py:112} INFO - [2020-11-02 13:29:50,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:29:52,121] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:29:52,160] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:29:52,176] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:29:52,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.328 seconds
[2020-11-02 13:30:17,840] {scheduler_job.py:155} INFO - Started process (PID=12314) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:30:17,852] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:30:17,853] {logging_mixin.py:112} INFO - [2020-11-02 13:30:17,853] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:30:19,101] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:30:19,141] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:30:19,158] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:30:19,162] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.322 seconds
[2020-11-02 13:30:44,847] {scheduler_job.py:155} INFO - Started process (PID=12433) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:30:44,864] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:30:44,870] {logging_mixin.py:112} INFO - [2020-11-02 13:30:44,865] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:30:46,263] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:30:46,300] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:30:46,316] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:30:46,324] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.477 seconds
[2020-11-02 13:31:11,758] {scheduler_job.py:155} INFO - Started process (PID=12565) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:31:11,769] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:31:11,770] {logging_mixin.py:112} INFO - [2020-11-02 13:31:11,770] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:31:12,911] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:31:12,952] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:31:12,969] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:31:12,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.215 seconds
[2020-11-02 13:31:38,610] {scheduler_job.py:155} INFO - Started process (PID=12683) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:31:38,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:31:38,621] {logging_mixin.py:112} INFO - [2020-11-02 13:31:38,620] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:31:39,870] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:31:39,910] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:31:39,927] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:31:39,931] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.321 seconds
[2020-11-02 13:32:05,507] {scheduler_job.py:155} INFO - Started process (PID=12804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:05,517] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:32:05,519] {logging_mixin.py:112} INFO - [2020-11-02 13:32:05,519] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:06,742] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:06,785] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:32:06,801] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:32:06,805] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.298 seconds
[2020-11-02 13:32:32,608] {scheduler_job.py:155} INFO - Started process (PID=12930) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:32,633] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:32:32,644] {logging_mixin.py:112} INFO - [2020-11-02 13:32:32,644] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:34,122] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:34,164] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:32:34,181] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:32:34,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.582 seconds
[2020-11-02 13:32:59,483] {scheduler_job.py:155} INFO - Started process (PID=13052) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:32:59,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:32:59,496] {logging_mixin.py:112} INFO - [2020-11-02 13:32:59,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:00,664] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:00,705] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:33:00,721] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:33:00,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.243 seconds
[2020-11-02 13:33:26,374] {scheduler_job.py:155} INFO - Started process (PID=13178) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:26,383] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:33:26,385] {logging_mixin.py:112} INFO - [2020-11-02 13:33:26,384] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:27,850] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:27,932] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:33:27,967] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:33:27,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.602 seconds
[2020-11-02 13:33:53,108] {scheduler_job.py:155} INFO - Started process (PID=13312) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:53,114] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:33:53,115] {logging_mixin.py:112} INFO - [2020-11-02 13:33:53,115] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:54,473] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:33:54,516] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:33:54,534] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:33:54,539] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.431 seconds
[2020-11-02 13:34:20,044] {scheduler_job.py:155} INFO - Started process (PID=13427) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:34:20,086] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:34:20,088] {logging_mixin.py:112} INFO - [2020-11-02 13:34:20,087] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:34:21,441] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:34:21,485] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:34:21,502] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:34:21,506] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.462 seconds
[2020-11-02 13:34:46,972] {scheduler_job.py:155} INFO - Started process (PID=13551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:34:46,981] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:34:46,983] {logging_mixin.py:112} INFO - [2020-11-02 13:34:46,983] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:34:48,447] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:34:48,492] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:34:48,510] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:34:48,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.543 seconds
[2020-11-02 13:35:13,800] {scheduler_job.py:155} INFO - Started process (PID=13678) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:35:13,816] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:35:13,819] {logging_mixin.py:112} INFO - [2020-11-02 13:35:13,818] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:35:15,064] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:35:15,105] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:35:15,121] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:35:15,126] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.326 seconds
[2020-11-02 13:35:40,668] {scheduler_job.py:155} INFO - Started process (PID=13800) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:35:40,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:35:40,685] {logging_mixin.py:112} INFO - [2020-11-02 13:35:40,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:35:42,092] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:35:42,133] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:35:42,149] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:35:42,153] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.485 seconds
[2020-11-02 13:36:07,508] {scheduler_job.py:155} INFO - Started process (PID=13922) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:36:07,517] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:36:07,518] {logging_mixin.py:112} INFO - [2020-11-02 13:36:07,518] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:36:08,756] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:36:08,797] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:36:08,813] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:36:08,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-02 13:36:34,127] {scheduler_job.py:155} INFO - Started process (PID=14046) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:36:34,135] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:36:34,137] {logging_mixin.py:112} INFO - [2020-11-02 13:36:34,136] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:36:35,255] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:36:35,299] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:36:35,316] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:36:35,320] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.193 seconds
[2020-11-02 13:37:01,110] {scheduler_job.py:155} INFO - Started process (PID=14168) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:01,116] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:37:01,117] {logging_mixin.py:112} INFO - [2020-11-02 13:37:01,117] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:02,339] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:02,377] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:37:02,395] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:37:02,399] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.289 seconds
[2020-11-02 13:37:28,061] {scheduler_job.py:155} INFO - Started process (PID=14283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:28,073] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:37:28,074] {logging_mixin.py:112} INFO - [2020-11-02 13:37:28,073] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:29,983] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:30,030] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:37:30,052] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:37:30,058] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.997 seconds
[2020-11-02 13:37:55,069] {scheduler_job.py:155} INFO - Started process (PID=14393) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:55,082] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:37:55,083] {logging_mixin.py:112} INFO - [2020-11-02 13:37:55,083] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:56,538] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:37:56,579] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:37:56,596] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:37:56,600] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.531 seconds
[2020-11-02 13:38:21,942] {scheduler_job.py:155} INFO - Started process (PID=14502) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:38:21,955] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:38:21,958] {logging_mixin.py:112} INFO - [2020-11-02 13:38:21,957] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:38:23,398] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:38:23,449] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:38:23,479] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:38:23,484] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.542 seconds
[2020-11-02 13:38:48,853] {scheduler_job.py:155} INFO - Started process (PID=14615) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:38:48,863] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:38:48,865] {logging_mixin.py:112} INFO - [2020-11-02 13:38:48,864] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:38:50,238] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:38:50,282] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:38:50,304] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:38:50,309] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.456 seconds
[2020-11-02 13:39:15,643] {scheduler_job.py:155} INFO - Started process (PID=14731) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:39:15,650] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:39:15,652] {logging_mixin.py:112} INFO - [2020-11-02 13:39:15,651] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:39:16,802] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:39:16,841] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:39:16,859] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:39:16,863] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.221 seconds
[2020-11-02 13:39:42,573] {scheduler_job.py:155} INFO - Started process (PID=14845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:39:42,581] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:39:42,583] {logging_mixin.py:112} INFO - [2020-11-02 13:39:42,583] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:39:43,943] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:39:43,988] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:39:44,007] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:39:44,012] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.439 seconds
[2020-11-02 13:40:09,305] {scheduler_job.py:155} INFO - Started process (PID=14962) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:40:09,312] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:40:09,313] {logging_mixin.py:112} INFO - [2020-11-02 13:40:09,313] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:40:10,522] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:40:10,562] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:40:10,582] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:40:10,587] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.282 seconds
[2020-11-02 13:40:36,171] {scheduler_job.py:155} INFO - Started process (PID=15077) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:40:36,185] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:40:36,186] {logging_mixin.py:112} INFO - [2020-11-02 13:40:36,185] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:40:37,639] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:40:37,684] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:40:37,704] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:40:37,709] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.538 seconds
[2020-11-02 13:41:03,100] {scheduler_job.py:155} INFO - Started process (PID=15191) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:03,103] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:41:03,104] {logging_mixin.py:112} INFO - [2020-11-02 13:41:03,104] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:04,558] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:04,601] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:41:04,618] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:41:04,622] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.522 seconds
[2020-11-02 13:41:30,059] {scheduler_job.py:155} INFO - Started process (PID=15301) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:30,068] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:41:30,069] {logging_mixin.py:112} INFO - [2020-11-02 13:41:30,069] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:31,570] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:31,617] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:41:31,639] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:41:31,645] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.586 seconds
[2020-11-02 13:41:56,823] {scheduler_job.py:155} INFO - Started process (PID=15417) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:56,828] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:41:56,829] {logging_mixin.py:112} INFO - [2020-11-02 13:41:56,829] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:58,197] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:41:58,236] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:41:58,253] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:41:58,257] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.435 seconds
[2020-11-02 13:42:22,638] {scheduler_job.py:155} INFO - Started process (PID=15533) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:42:22,644] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:42:22,645] {logging_mixin.py:112} INFO - [2020-11-02 13:42:22,644] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:42:24,391] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:42:24,442] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:42:24,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:42:24,479] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.841 seconds
[2020-11-02 13:42:49,480] {scheduler_job.py:155} INFO - Started process (PID=15646) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:42:49,484] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:42:49,485] {logging_mixin.py:112} INFO - [2020-11-02 13:42:49,485] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:42:51,180] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:42:51,228] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:42:51,249] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:42:51,256] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.776 seconds
[2020-11-02 13:43:16,252] {scheduler_job.py:155} INFO - Started process (PID=15758) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:43:16,260] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:43:16,261] {logging_mixin.py:112} INFO - [2020-11-02 13:43:16,261] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:43:17,527] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:43:17,567] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:43:17,584] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:43:17,588] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.337 seconds
[2020-11-02 13:43:43,091] {scheduler_job.py:155} INFO - Started process (PID=15870) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:43:43,100] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:43:43,101] {logging_mixin.py:112} INFO - [2020-11-02 13:43:43,101] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:43:44,308] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:43:44,352] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:43:44,369] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:43:44,373] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.283 seconds
[2020-11-02 13:44:09,847] {scheduler_job.py:155} INFO - Started process (PID=15982) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:44:09,855] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:44:09,856] {logging_mixin.py:112} INFO - [2020-11-02 13:44:09,856] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:44:11,163] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:44:11,239] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:44:11,272] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:44:11,280] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.433 seconds
[2020-11-02 13:44:36,735] {scheduler_job.py:155} INFO - Started process (PID=16092) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:44:36,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:44:36,748] {logging_mixin.py:112} INFO - [2020-11-02 13:44:36,748] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:44:38,205] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:44:38,260] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:44:38,277] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:44:38,281] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.547 seconds
[2020-11-02 13:45:03,847] {scheduler_job.py:155} INFO - Started process (PID=16202) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:03,851] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:45:03,852] {logging_mixin.py:112} INFO - [2020-11-02 13:45:03,852] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:05,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:05,316] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:45:05,354] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:45:05,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.520 seconds
[2020-11-02 13:45:30,726] {scheduler_job.py:155} INFO - Started process (PID=16316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:30,734] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:45:30,736] {logging_mixin.py:112} INFO - [2020-11-02 13:45:30,735] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:32,075] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:32,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:45:32,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:45:32,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.419 seconds
[2020-11-02 13:45:57,536] {scheduler_job.py:155} INFO - Started process (PID=16446) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:57,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:45:57,540] {logging_mixin.py:112} INFO - [2020-11-02 13:45:57,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:58,697] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:45:58,744] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:45:58,763] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:45:58,768] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-02 13:46:24,209] {scheduler_job.py:155} INFO - Started process (PID=16570) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:46:24,217] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:46:24,218] {logging_mixin.py:112} INFO - [2020-11-02 13:46:24,218] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:46:25,493] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:46:25,541] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:46:25,561] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:46:25,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.359 seconds
[2020-11-02 13:46:50,954] {scheduler_job.py:155} INFO - Started process (PID=16702) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:46:50,992] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:46:50,994] {logging_mixin.py:112} INFO - [2020-11-02 13:46:50,994] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:46:52,733] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:46:52,799] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:46:52,827] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:46:52,834] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.879 seconds
[2020-11-02 13:47:17,745] {scheduler_job.py:155} INFO - Started process (PID=16847) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:47:17,750] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:47:17,751] {logging_mixin.py:112} INFO - [2020-11-02 13:47:17,751] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:47:18,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:47:18,919] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:47:18,938] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:47:18,942] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.197 seconds
[2020-11-02 13:47:44,353] {scheduler_job.py:155} INFO - Started process (PID=16987) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:47:44,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:47:44,358] {logging_mixin.py:112} INFO - [2020-11-02 13:47:44,357] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:47:45,452] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:47:45,503] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:47:45,522] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:47:45,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.176 seconds
[2020-11-02 13:48:10,799] {scheduler_job.py:155} INFO - Started process (PID=17134) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:48:10,808] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:48:10,809] {logging_mixin.py:112} INFO - [2020-11-02 13:48:10,809] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:48:12,125] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:48:12,171] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:48:12,193] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:48:12,198] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.399 seconds
[2020-11-02 13:48:37,327] {scheduler_job.py:155} INFO - Started process (PID=17274) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:48:37,331] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:48:37,332] {logging_mixin.py:112} INFO - [2020-11-02 13:48:37,331] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:48:38,427] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:48:38,471] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:48:38,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:48:38,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.171 seconds
[2020-11-02 13:49:03,895] {scheduler_job.py:155} INFO - Started process (PID=17420) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:03,900] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:49:03,901] {logging_mixin.py:112} INFO - [2020-11-02 13:49:03,900] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:04,984] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:05,024] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:49:05,041] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:49:05,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.150 seconds
[2020-11-02 13:49:30,632] {scheduler_job.py:155} INFO - Started process (PID=17562) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:30,636] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:49:30,637] {logging_mixin.py:112} INFO - [2020-11-02 13:49:30,637] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:31,727] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:31,767] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:49:31,787] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:49:31,792] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.160 seconds
[2020-11-02 13:49:57,272] {scheduler_job.py:155} INFO - Started process (PID=17705) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:57,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:49:57,276] {logging_mixin.py:112} INFO - [2020-11-02 13:49:57,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:58,400] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:49:58,450] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:49:58,469] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:49:58,473] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.202 seconds
[2020-11-02 13:50:23,681] {scheduler_job.py:155} INFO - Started process (PID=17855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:50:23,685] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:50:23,686] {logging_mixin.py:112} INFO - [2020-11-02 13:50:23,685] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:50:24,671] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:50:24,720] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:50:24,739] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:50:24,743] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-02 13:50:50,146] {scheduler_job.py:155} INFO - Started process (PID=18005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:50:50,150] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:50:50,151] {logging_mixin.py:112} INFO - [2020-11-02 13:50:50,151] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:50:51,144] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:50:51,182] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:50:51,198] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:50:51,202] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.056 seconds
[2020-11-02 13:51:16,691] {scheduler_job.py:155} INFO - Started process (PID=18152) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:51:16,705] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:51:16,705] {logging_mixin.py:112} INFO - [2020-11-02 13:51:16,705] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:51:17,818] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:51:17,860] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:51:17,877] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:51:17,881] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.190 seconds
[2020-11-02 13:51:43,191] {scheduler_job.py:155} INFO - Started process (PID=18292) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:51:43,195] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:51:43,196] {logging_mixin.py:112} INFO - [2020-11-02 13:51:43,196] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:51:44,387] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:51:44,426] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:51:44,447] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:51:44,452] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.262 seconds
[2020-11-02 13:52:09,758] {scheduler_job.py:155} INFO - Started process (PID=18442) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:52:09,763] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:52:09,764] {logging_mixin.py:112} INFO - [2020-11-02 13:52:09,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:52:10,746] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:52:10,790] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:52:10,806] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:52:10,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.052 seconds
[2020-11-02 13:52:36,271] {scheduler_job.py:155} INFO - Started process (PID=18598) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:52:36,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:52:36,276] {logging_mixin.py:112} INFO - [2020-11-02 13:52:36,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:52:37,371] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:52:37,413] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:52:37,430] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:52:37,436] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.165 seconds
[2020-11-02 13:53:02,707] {scheduler_job.py:155} INFO - Started process (PID=18741) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:02,711] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:53:02,712] {logging_mixin.py:112} INFO - [2020-11-02 13:53:02,711] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:03,796] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:03,841] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:53:03,859] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:53:03,865] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.158 seconds
[2020-11-02 13:53:29,245] {scheduler_job.py:155} INFO - Started process (PID=18878) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:29,251] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:53:29,252] {logging_mixin.py:112} INFO - [2020-11-02 13:53:29,252] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:30,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:30,744] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:53:30,767] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:53:30,772] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.527 seconds
[2020-11-02 13:53:55,789] {scheduler_job.py:155} INFO - Started process (PID=19008) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:55,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:53:55,796] {logging_mixin.py:112} INFO - [2020-11-02 13:53:55,796] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:56,984] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:53:57,031] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:53:57,050] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:53:57,055] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.266 seconds
[2020-11-02 13:54:22,236] {scheduler_job.py:155} INFO - Started process (PID=19148) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:54:22,241] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:54:22,250] {logging_mixin.py:112} INFO - [2020-11-02 13:54:22,250] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:54:24,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:54:24,306] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:54:24,337] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:54:24,347] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.111 seconds
[2020-11-02 13:54:48,638] {scheduler_job.py:155} INFO - Started process (PID=19276) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:54:48,657] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:54:48,658] {logging_mixin.py:112} INFO - [2020-11-02 13:54:48,658] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:54:50,086] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:54:50,143] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:54:50,171] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:54:50,176] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.538 seconds
[2020-11-02 13:55:15,317] {scheduler_job.py:155} INFO - Started process (PID=19422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:55:15,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:55:15,323] {logging_mixin.py:112} INFO - [2020-11-02 13:55:15,323] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:55:16,332] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:55:16,368] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:55:16,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:55:16,386] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.070 seconds
[2020-11-02 13:55:42,073] {scheduler_job.py:155} INFO - Started process (PID=19575) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:55:42,079] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:55:42,080] {logging_mixin.py:112} INFO - [2020-11-02 13:55:42,079] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:55:43,107] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:55:43,180] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:55:43,208] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:55:43,214] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.142 seconds
[2020-11-02 13:56:08,616] {scheduler_job.py:155} INFO - Started process (PID=19721) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:56:08,620] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:56:08,621] {logging_mixin.py:112} INFO - [2020-11-02 13:56:08,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:56:09,613] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:56:09,657] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:56:09,672] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:56:09,675] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.059 seconds
[2020-11-02 13:56:35,098] {scheduler_job.py:155} INFO - Started process (PID=19869) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:56:35,103] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:56:35,103] {logging_mixin.py:112} INFO - [2020-11-02 13:56:35,103] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:56:36,073] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:56:36,114] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:56:36,131] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:56:36,134] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.037 seconds
[2020-11-02 13:57:01,680] {scheduler_job.py:155} INFO - Started process (PID=20024) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:01,684] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:57:01,684] {logging_mixin.py:112} INFO - [2020-11-02 13:57:01,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:02,724] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:02,762] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:57:02,787] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:57:02,794] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.114 seconds
[2020-11-02 13:57:28,174] {scheduler_job.py:155} INFO - Started process (PID=20171) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:28,177] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:57:28,178] {logging_mixin.py:112} INFO - [2020-11-02 13:57:28,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:29,147] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:29,190] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:57:29,208] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:57:29,212] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.038 seconds
[2020-11-02 13:57:54,685] {scheduler_job.py:155} INFO - Started process (PID=20320) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:54,701] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:57:54,703] {logging_mixin.py:112} INFO - [2020-11-02 13:57:54,703] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:55,943] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:57:56,008] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:57:56,035] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:57:56,041] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.357 seconds
[2020-11-02 13:58:21,280] {scheduler_job.py:155} INFO - Started process (PID=20462) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:58:21,283] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:58:21,284] {logging_mixin.py:112} INFO - [2020-11-02 13:58:21,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:58:22,559] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:58:22,636] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:58:22,667] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:58:22,680] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.400 seconds
[2020-11-02 13:58:47,829] {scheduler_job.py:155} INFO - Started process (PID=20608) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:58:47,834] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:58:47,835] {logging_mixin.py:112} INFO - [2020-11-02 13:58:47,835] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:58:48,817] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:58:48,858] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:58:48,874] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:58:48,885] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.056 seconds
[2020-11-02 13:59:14,314] {scheduler_job.py:155} INFO - Started process (PID=20754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:59:14,318] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:59:14,319] {logging_mixin.py:112} INFO - [2020-11-02 13:59:14,318] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:59:15,318] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:59:15,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:59:15,374] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:59:15,378] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.064 seconds
[2020-11-02 13:59:40,783] {scheduler_job.py:155} INFO - Started process (PID=20902) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:59:40,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 13:59:40,790] {logging_mixin.py:112} INFO - [2020-11-02 13:59:40,790] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:59:41,835] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 13:59:41,884] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 13:59:41,907] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 13:59:41,911] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.128 seconds
[2020-11-02 14:00:07,393] {scheduler_job.py:155} INFO - Started process (PID=21048) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:00:07,399] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:00:07,400] {logging_mixin.py:112} INFO - [2020-11-02 14:00:07,400] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:00:08,393] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:00:08,430] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:00:08,446] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:00:08,450] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.057 seconds
[2020-11-02 14:00:33,827] {scheduler_job.py:155} INFO - Started process (PID=21198) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:00:33,834] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:00:33,835] {logging_mixin.py:112} INFO - [2020-11-02 14:00:33,835] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:00:35,224] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:00:35,265] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:00:35,281] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:00:35,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.458 seconds
[2020-11-02 14:01:00,287] {scheduler_job.py:155} INFO - Started process (PID=21338) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:00,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:01:00,292] {logging_mixin.py:112} INFO - [2020-11-02 14:01:00,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:01,296] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:01,336] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:01:01,351] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:01:01,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.068 seconds
[2020-11-02 14:01:26,688] {scheduler_job.py:155} INFO - Started process (PID=21491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:26,691] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:01:26,692] {logging_mixin.py:112} INFO - [2020-11-02 14:01:26,692] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:27,981] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:28,021] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:01:28,038] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:01:28,042] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.355 seconds
[2020-11-02 14:01:53,140] {scheduler_job.py:155} INFO - Started process (PID=21637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:53,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:01:53,147] {logging_mixin.py:112} INFO - [2020-11-02 14:01:53,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:54,141] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:01:54,185] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:01:54,201] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:01:54,204] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.064 seconds
[2020-11-02 14:02:19,771] {scheduler_job.py:155} INFO - Started process (PID=21785) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:02:19,777] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:02:19,779] {logging_mixin.py:112} INFO - [2020-11-02 14:02:19,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:02:20,757] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:02:20,798] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:02:20,816] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:02:20,819] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.048 seconds
[2020-11-02 14:02:46,288] {scheduler_job.py:155} INFO - Started process (PID=21931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:02:46,294] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:02:46,294] {logging_mixin.py:112} INFO - [2020-11-02 14:02:46,294] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:02:47,307] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:02:47,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:02:47,377] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:02:47,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.094 seconds
[2020-11-02 14:03:12,945] {scheduler_job.py:155} INFO - Started process (PID=22081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:03:12,952] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:03:12,954] {logging_mixin.py:112} INFO - [2020-11-02 14:03:12,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:03:13,931] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:03:13,968] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:03:13,983] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:03:13,987] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.041 seconds
[2020-11-02 14:03:39,570] {scheduler_job.py:155} INFO - Started process (PID=22224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:03:39,574] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:03:39,575] {logging_mixin.py:112} INFO - [2020-11-02 14:03:39,574] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:03:40,542] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:03:40,592] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:03:40,618] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:03:40,623] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.052 seconds
[2020-11-02 14:04:06,062] {scheduler_job.py:155} INFO - Started process (PID=22366) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:06,065] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:04:06,066] {logging_mixin.py:112} INFO - [2020-11-02 14:04:06,066] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:07,047] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:07,091] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:04:07,107] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:04:07,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.049 seconds
[2020-11-02 14:04:32,817] {scheduler_job.py:155} INFO - Started process (PID=22516) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:32,821] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:04:32,822] {logging_mixin.py:112} INFO - [2020-11-02 14:04:32,821] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:34,228] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:34,265] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:04:34,280] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:04:34,283] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.467 seconds
[2020-11-02 14:04:59,270] {scheduler_job.py:155} INFO - Started process (PID=22658) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:04:59,274] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:04:59,275] {logging_mixin.py:112} INFO - [2020-11-02 14:04:59,275] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:00,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:00,525] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:05:00,545] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:05:00,550] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-02 14:05:25,753] {scheduler_job.py:155} INFO - Started process (PID=22808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:25,759] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:05:25,759] {logging_mixin.py:112} INFO - [2020-11-02 14:05:25,759] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:26,737] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:26,788] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:05:26,805] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:05:26,809] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.056 seconds
[2020-11-02 14:05:52,236] {scheduler_job.py:155} INFO - Started process (PID=22950) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:52,239] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:05:52,240] {logging_mixin.py:112} INFO - [2020-11-02 14:05:52,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:53,207] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:05:53,253] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:05:53,269] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:05:53,273] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.037 seconds
[2020-11-02 14:06:18,739] {scheduler_job.py:155} INFO - Started process (PID=23097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:06:18,744] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:06:18,745] {logging_mixin.py:112} INFO - [2020-11-02 14:06:18,745] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:06:19,716] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:06:19,754] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:06:19,771] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:06:19,775] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.036 seconds
[2020-11-02 14:06:45,249] {scheduler_job.py:155} INFO - Started process (PID=23247) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:06:45,253] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:06:45,253] {logging_mixin.py:112} INFO - [2020-11-02 14:06:45,253] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:06:46,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:06:46,295] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:06:46,310] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:06:46,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.065 seconds
[2020-11-02 14:07:11,692] {scheduler_job.py:155} INFO - Started process (PID=23407) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:07:11,697] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:07:11,698] {logging_mixin.py:112} INFO - [2020-11-02 14:07:11,697] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:07:12,781] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:07:12,823] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:07:12,841] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:07:12,845] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-02 14:07:38,269] {scheduler_job.py:155} INFO - Started process (PID=23551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:07:38,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:07:38,276] {logging_mixin.py:112} INFO - [2020-11-02 14:07:38,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:07:39,487] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:07:39,541] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:07:39,563] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:07:39,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.299 seconds
[2020-11-02 14:08:04,888] {scheduler_job.py:155} INFO - Started process (PID=23697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:04,892] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:08:04,892] {logging_mixin.py:112} INFO - [2020-11-02 14:08:04,892] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:06,062] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:06,099] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:08:06,117] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:08:06,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-02 14:08:31,400] {scheduler_job.py:155} INFO - Started process (PID=23841) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:31,404] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:08:31,405] {logging_mixin.py:112} INFO - [2020-11-02 14:08:31,404] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:32,409] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:32,448] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:08:32,463] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:08:32,467] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.067 seconds
[2020-11-02 14:08:58,077] {scheduler_job.py:155} INFO - Started process (PID=23997) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:58,082] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:08:58,083] {logging_mixin.py:112} INFO - [2020-11-02 14:08:58,083] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:59,095] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:08:59,130] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:08:59,145] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:08:59,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.072 seconds
[2020-11-02 14:09:24,504] {scheduler_job.py:155} INFO - Started process (PID=24141) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:09:24,507] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:09:24,508] {logging_mixin.py:112} INFO - [2020-11-02 14:09:24,508] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:09:25,691] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:09:25,736] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:09:25,755] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:09:25,759] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.255 seconds
[2020-11-02 14:09:51,095] {scheduler_job.py:155} INFO - Started process (PID=24291) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:09:51,098] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:09:51,099] {logging_mixin.py:112} INFO - [2020-11-02 14:09:51,099] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:09:52,183] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:09:52,218] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:09:52,235] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:09:52,239] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.144 seconds
[2020-11-02 14:10:17,756] {scheduler_job.py:155} INFO - Started process (PID=24435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:10:17,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:10:17,763] {logging_mixin.py:112} INFO - [2020-11-02 14:10:17,763] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:10:18,743] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:10:18,783] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:10:18,800] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:10:18,804] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.048 seconds
[2020-11-02 14:10:44,358] {scheduler_job.py:155} INFO - Started process (PID=24574) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:10:44,362] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:10:44,362] {logging_mixin.py:112} INFO - [2020-11-02 14:10:44,362] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:10:45,510] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:10:45,544] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:10:45,563] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:10:45,568] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.210 seconds
[2020-11-02 14:11:10,917] {scheduler_job.py:155} INFO - Started process (PID=24723) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:11:10,930] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:11:10,930] {logging_mixin.py:112} INFO - [2020-11-02 14:11:10,930] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:11:12,080] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:11:12,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:11:12,137] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:11:12,141] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.231 seconds
[2020-11-02 14:11:37,340] {scheduler_job.py:155} INFO - Started process (PID=24869) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:11:37,345] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:11:37,346] {logging_mixin.py:112} INFO - [2020-11-02 14:11:37,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:11:38,527] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:11:38,567] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:11:38,583] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:11:38,587] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-02 14:12:03,871] {scheduler_job.py:155} INFO - Started process (PID=25012) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:03,875] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:12:03,875] {logging_mixin.py:112} INFO - [2020-11-02 14:12:03,875] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:04,936] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:04,977] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:12:04,995] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:12:05,000] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.129 seconds
[2020-11-02 14:12:30,398] {scheduler_job.py:155} INFO - Started process (PID=25158) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:30,403] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:12:30,404] {logging_mixin.py:112} INFO - [2020-11-02 14:12:30,404] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:31,473] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:31,515] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:12:31,532] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:12:31,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.138 seconds
[2020-11-02 14:12:57,010] {scheduler_job.py:155} INFO - Started process (PID=25302) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:57,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:12:57,017] {logging_mixin.py:112} INFO - [2020-11-02 14:12:57,017] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:58,335] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:12:58,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:12:58,396] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:12:58,400] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.390 seconds
[2020-11-02 14:13:23,644] {scheduler_job.py:155} INFO - Started process (PID=25448) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:13:23,648] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:13:23,649] {logging_mixin.py:112} INFO - [2020-11-02 14:13:23,649] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:13:24,710] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:13:24,753] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:13:24,770] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:13:24,775] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.131 seconds
[2020-11-02 14:13:50,242] {scheduler_job.py:155} INFO - Started process (PID=25595) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:13:50,249] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:13:50,250] {logging_mixin.py:112} INFO - [2020-11-02 14:13:50,250] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:13:51,922] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:13:51,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:13:52,012] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:13:52,018] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.776 seconds
[2020-11-02 14:14:16,655] {scheduler_job.py:155} INFO - Started process (PID=25735) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:14:16,666] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:14:16,667] {logging_mixin.py:112} INFO - [2020-11-02 14:14:16,667] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:14:17,867] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:14:17,918] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:14:17,937] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:14:17,942] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.287 seconds
[2020-11-02 14:14:43,023] {scheduler_job.py:155} INFO - Started process (PID=25885) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:14:43,030] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:14:43,034] {logging_mixin.py:112} INFO - [2020-11-02 14:14:43,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:14:44,817] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:14:44,856] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:14:44,872] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:14:44,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.854 seconds
[2020-11-02 14:15:09,460] {scheduler_job.py:155} INFO - Started process (PID=26031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:15:09,464] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:15:09,466] {logging_mixin.py:112} INFO - [2020-11-02 14:15:09,465] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:15:11,216] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:15:11,261] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:15:11,279] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:15:11,284] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.824 seconds
[2020-11-02 14:15:35,899] {scheduler_job.py:155} INFO - Started process (PID=26174) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:15:35,903] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:15:35,904] {logging_mixin.py:112} INFO - [2020-11-02 14:15:35,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:15:36,885] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:15:36,941] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:15:36,956] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:15:36,959] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.060 seconds
[2020-11-02 14:16:02,360] {scheduler_job.py:155} INFO - Started process (PID=26319) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:02,364] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:16:02,365] {logging_mixin.py:112} INFO - [2020-11-02 14:16:02,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:03,386] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:03,428] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:16:03,443] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:16:03,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.088 seconds
[2020-11-02 14:16:28,899] {scheduler_job.py:155} INFO - Started process (PID=26462) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:28,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:16:28,905] {logging_mixin.py:112} INFO - [2020-11-02 14:16:28,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:29,901] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:29,938] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:16:29,953] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:16:29,956] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.057 seconds
[2020-11-02 14:16:55,325] {scheduler_job.py:155} INFO - Started process (PID=26612) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:55,331] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:16:55,331] {logging_mixin.py:112} INFO - [2020-11-02 14:16:55,331] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:56,444] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:16:56,482] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:16:56,498] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:16:56,503] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.178 seconds
[2020-11-02 14:17:21,929] {scheduler_job.py:155} INFO - Started process (PID=26759) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:17:21,933] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:17:21,934] {logging_mixin.py:112} INFO - [2020-11-02 14:17:21,933] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:17:22,943] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:17:22,981] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:17:22,998] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:17:23,002] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.073 seconds
[2020-11-02 14:17:48,493] {scheduler_job.py:155} INFO - Started process (PID=26902) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:17:48,498] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:17:48,499] {logging_mixin.py:112} INFO - [2020-11-02 14:17:48,499] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:17:49,519] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:17:49,557] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:17:49,572] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:17:49,575] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.082 seconds
[2020-11-02 14:18:15,030] {scheduler_job.py:155} INFO - Started process (PID=27053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:18:15,037] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:18:15,038] {logging_mixin.py:112} INFO - [2020-11-02 14:18:15,038] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:18:16,258] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:18:16,293] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:18:16,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:18:16,315] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.285 seconds
[2020-11-02 14:18:41,457] {scheduler_job.py:155} INFO - Started process (PID=27188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:18:41,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:18:41,461] {logging_mixin.py:112} INFO - [2020-11-02 14:18:41,461] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:18:42,602] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:18:42,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:18:42,661] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:18:42,665] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.208 seconds
[2020-11-02 14:19:08,142] {scheduler_job.py:155} INFO - Started process (PID=27326) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:19:08,147] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:19:08,148] {logging_mixin.py:112} INFO - [2020-11-02 14:19:08,148] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:19:09,209] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:19:09,248] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:19:09,268] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:19:09,275] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.133 seconds
[2020-11-02 14:19:34,578] {scheduler_job.py:155} INFO - Started process (PID=27464) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:19:34,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:19:34,583] {logging_mixin.py:112} INFO - [2020-11-02 14:19:34,583] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:19:35,950] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:19:36,017] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:19:36,043] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:19:36,051] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.473 seconds
[2020-11-02 14:20:01,027] {scheduler_job.py:155} INFO - Started process (PID=27612) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:01,032] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:20:01,038] {logging_mixin.py:112} INFO - [2020-11-02 14:20:01,033] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:02,041] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:02,088] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:20:02,110] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:20:02,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.086 seconds
[2020-11-02 14:20:27,512] {scheduler_job.py:155} INFO - Started process (PID=27761) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:27,518] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:20:27,519] {logging_mixin.py:112} INFO - [2020-11-02 14:20:27,518] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:28,676] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:28,727] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:20:28,755] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:20:28,760] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-02 14:20:54,030] {scheduler_job.py:155} INFO - Started process (PID=27907) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:54,034] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:20:54,035] {logging_mixin.py:112} INFO - [2020-11-02 14:20:54,034] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:55,189] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:20:55,227] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:20:55,243] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:20:55,247] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.217 seconds
[2020-11-02 14:21:20,497] {scheduler_job.py:155} INFO - Started process (PID=28060) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:21:20,504] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:21:20,506] {logging_mixin.py:112} INFO - [2020-11-02 14:21:20,506] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:21:22,001] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:21:22,038] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:21:22,054] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:21:22,057] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.560 seconds
[2020-11-02 14:21:47,185] {scheduler_job.py:155} INFO - Started process (PID=28205) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:21:47,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:21:47,197] {logging_mixin.py:112} INFO - [2020-11-02 14:21:47,197] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:21:48,576] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:21:48,620] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:21:48,637] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:21:48,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.456 seconds
[2020-11-02 14:22:13,747] {scheduler_job.py:155} INFO - Started process (PID=28355) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:22:13,755] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:22:13,756] {logging_mixin.py:112} INFO - [2020-11-02 14:22:13,755] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:22:14,891] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:22:14,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:22:14,946] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:22:14,950] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.203 seconds
[2020-11-02 14:22:40,294] {scheduler_job.py:155} INFO - Started process (PID=28499) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:22:40,297] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:22:40,298] {logging_mixin.py:112} INFO - [2020-11-02 14:22:40,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:22:41,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:22:41,538] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:22:41,555] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:22:41,560] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.266 seconds
[2020-11-02 14:23:06,859] {scheduler_job.py:155} INFO - Started process (PID=28641) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:06,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:23:06,868] {logging_mixin.py:112} INFO - [2020-11-02 14:23:06,867] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:08,060] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:08,112] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:23:08,127] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:23:08,131] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.272 seconds
[2020-11-02 14:23:33,292] {scheduler_job.py:155} INFO - Started process (PID=28783) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:33,298] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:23:33,298] {logging_mixin.py:112} INFO - [2020-11-02 14:23:33,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:34,406] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:34,472] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:23:34,498] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:23:34,504] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.211 seconds
[2020-11-02 14:23:59,766] {scheduler_job.py:155} INFO - Started process (PID=28931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:23:59,769] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:23:59,770] {logging_mixin.py:112} INFO - [2020-11-02 14:23:59,770] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:00,809] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:00,848] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:24:00,865] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:24:00,869] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-02 14:24:26,223] {scheduler_job.py:155} INFO - Started process (PID=29077) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:26,227] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:24:26,227] {logging_mixin.py:112} INFO - [2020-11-02 14:24:26,227] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:27,214] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:27,260] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:24:27,282] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:24:27,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-02 14:24:52,679] {scheduler_job.py:155} INFO - Started process (PID=29222) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:52,684] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:24:52,685] {logging_mixin.py:112} INFO - [2020-11-02 14:24:52,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:53,953] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:24:53,994] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:24:54,012] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:24:54,017] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.337 seconds
[2020-11-02 14:25:19,287] {scheduler_job.py:155} INFO - Started process (PID=29355) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:25:19,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:25:19,293] {logging_mixin.py:112} INFO - [2020-11-02 14:25:19,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:25:20,679] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:25:20,728] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:25:20,749] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:25:20,754] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.468 seconds
[2020-11-02 14:25:45,888] {scheduler_job.py:155} INFO - Started process (PID=29499) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:25:45,894] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:25:45,894] {logging_mixin.py:112} INFO - [2020-11-02 14:25:45,894] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:25:47,005] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:25:47,046] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:25:47,061] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:25:47,065] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.178 seconds
[2020-11-02 14:26:12,389] {scheduler_job.py:155} INFO - Started process (PID=29648) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:26:12,392] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:26:12,393] {logging_mixin.py:112} INFO - [2020-11-02 14:26:12,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:26:13,373] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:26:13,411] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:26:13,428] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:26:13,432] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.043 seconds
[2020-11-02 14:26:39,012] {scheduler_job.py:155} INFO - Started process (PID=29801) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:26:39,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:26:39,018] {logging_mixin.py:112} INFO - [2020-11-02 14:26:39,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:26:40,096] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:26:40,139] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:26:40,160] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:26:40,164] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.152 seconds
[2020-11-02 14:27:05,465] {scheduler_job.py:155} INFO - Started process (PID=29954) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:05,469] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:27:05,469] {logging_mixin.py:112} INFO - [2020-11-02 14:27:05,469] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:06,454] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:06,509] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:27:06,525] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:27:06,528] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.063 seconds
[2020-11-02 14:27:32,027] {scheduler_job.py:155} INFO - Started process (PID=30108) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:32,031] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:27:32,032] {logging_mixin.py:112} INFO - [2020-11-02 14:27:32,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:33,106] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:33,146] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:27:33,162] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:27:33,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.140 seconds
[2020-11-02 14:27:58,589] {scheduler_job.py:155} INFO - Started process (PID=30248) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:58,593] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:27:58,594] {logging_mixin.py:112} INFO - [2020-11-02 14:27:58,594] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:59,648] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:27:59,698] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:27:59,722] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:27:59,726] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.137 seconds
[2020-11-02 14:28:25,074] {scheduler_job.py:155} INFO - Started process (PID=30394) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:28:25,088] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:28:25,089] {logging_mixin.py:112} INFO - [2020-11-02 14:28:25,088] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:28:26,669] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:28:26,709] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:28:26,729] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:28:26,733] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.660 seconds
[2020-11-02 14:28:51,497] {scheduler_job.py:155} INFO - Started process (PID=30528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:28:51,509] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:28:51,509] {logging_mixin.py:112} INFO - [2020-11-02 14:28:51,509] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:28:53,253] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:28:53,295] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:28:53,320] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:28:53,326] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.829 seconds
[2020-11-02 14:29:17,955] {scheduler_job.py:155} INFO - Started process (PID=30661) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:29:17,959] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:29:17,960] {logging_mixin.py:112} INFO - [2020-11-02 14:29:17,960] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:29:19,253] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:29:19,315] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:29:19,335] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:29:19,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.385 seconds
[2020-11-02 14:29:44,379] {scheduler_job.py:155} INFO - Started process (PID=30803) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:29:44,382] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:29:44,383] {logging_mixin.py:112} INFO - [2020-11-02 14:29:44,383] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:29:45,755] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:29:45,827] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:29:45,854] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:29:45,861] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.482 seconds
[2020-11-02 14:30:10,833] {scheduler_job.py:155} INFO - Started process (PID=30949) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:30:10,838] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:30:10,839] {logging_mixin.py:112} INFO - [2020-11-02 14:30:10,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:30:11,818] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:30:11,865] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:30:11,886] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:30:11,890] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.056 seconds
[2020-11-02 14:30:37,213] {scheduler_job.py:155} INFO - Started process (PID=31090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:30:37,217] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:30:37,217] {logging_mixin.py:112} INFO - [2020-11-02 14:30:37,217] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:30:38,316] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:30:38,366] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:30:38,390] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:30:38,396] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-02 14:31:03,830] {scheduler_job.py:155} INFO - Started process (PID=31234) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:03,834] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:31:03,835] {logging_mixin.py:112} INFO - [2020-11-02 14:31:03,834] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:04,807] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:04,846] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:31:04,863] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:31:04,867] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.037 seconds
[2020-11-02 14:31:30,356] {scheduler_job.py:155} INFO - Started process (PID=31383) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:30,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:31:30,374] {logging_mixin.py:112} INFO - [2020-11-02 14:31:30,367] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:31,654] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:31,698] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:31:31,720] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:31:31,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.369 seconds
[2020-11-02 14:31:56,782] {scheduler_job.py:155} INFO - Started process (PID=31528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:56,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:31:56,788] {logging_mixin.py:112} INFO - [2020-11-02 14:31:56,787] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:57,916] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:31:57,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:31:57,992] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:31:57,996] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.214 seconds
[2020-11-02 14:32:23,268] {scheduler_job.py:155} INFO - Started process (PID=31677) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:32:23,273] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:32:23,275] {logging_mixin.py:112} INFO - [2020-11-02 14:32:23,274] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:32:24,361] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:32:24,401] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:32:24,418] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:32:24,424] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.156 seconds
[2020-11-02 14:32:49,760] {scheduler_job.py:155} INFO - Started process (PID=31822) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:32:49,767] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:32:49,768] {logging_mixin.py:112} INFO - [2020-11-02 14:32:49,768] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:32:50,856] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:32:50,897] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:32:50,915] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:32:50,919] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.160 seconds
[2020-11-02 14:33:16,233] {scheduler_job.py:155} INFO - Started process (PID=31971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:33:16,238] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:33:16,238] {logging_mixin.py:112} INFO - [2020-11-02 14:33:16,238] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:33:17,445] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:33:17,518] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:33:17,552] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:33:17,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.325 seconds
[2020-11-02 14:33:42,673] {scheduler_job.py:155} INFO - Started process (PID=32118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:33:42,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:33:42,677] {logging_mixin.py:112} INFO - [2020-11-02 14:33:42,677] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:33:43,745] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:33:43,786] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:33:43,803] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:33:43,807] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.134 seconds
[2020-11-02 14:34:09,287] {scheduler_job.py:155} INFO - Started process (PID=32265) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:34:09,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:34:09,293] {logging_mixin.py:112} INFO - [2020-11-02 14:34:09,293] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:34:10,545] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:34:10,592] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:34:10,611] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:34:10,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.329 seconds
[2020-11-02 14:34:35,712] {scheduler_job.py:155} INFO - Started process (PID=32412) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:34:35,715] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:34:35,716] {logging_mixin.py:112} INFO - [2020-11-02 14:34:35,716] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:34:36,859] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:34:36,901] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:34:36,916] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:34:36,920] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.208 seconds
[2020-11-02 14:35:02,119] {scheduler_job.py:155} INFO - Started process (PID=32556) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:02,123] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:35:02,127] {logging_mixin.py:112} INFO - [2020-11-02 14:35:02,127] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:03,485] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:03,527] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:35:03,547] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:35:03,552] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.433 seconds
[2020-11-02 14:35:28,565] {scheduler_job.py:155} INFO - Started process (PID=32698) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:28,569] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:35:28,570] {logging_mixin.py:112} INFO - [2020-11-02 14:35:28,570] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:29,733] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:29,773] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:35:29,789] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:35:29,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.229 seconds
[2020-11-02 14:35:54,991] {scheduler_job.py:155} INFO - Started process (PID=382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:54,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:35:54,996] {logging_mixin.py:112} INFO - [2020-11-02 14:35:54,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:56,167] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:35:56,208] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:35:56,224] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:35:56,229] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.237 seconds
[2020-11-02 14:36:21,439] {scheduler_job.py:155} INFO - Started process (PID=537) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:36:21,443] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:36:21,443] {logging_mixin.py:112} INFO - [2020-11-02 14:36:21,443] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:36:22,525] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:36:22,570] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:36:22,587] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:36:22,591] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.151 seconds
[2020-11-02 14:36:47,872] {scheduler_job.py:155} INFO - Started process (PID=719) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:36:47,876] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:36:47,877] {logging_mixin.py:112} INFO - [2020-11-02 14:36:47,877] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:36:48,938] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:36:48,980] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:36:49,001] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:36:49,005] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.134 seconds
[2020-11-02 14:37:14,415] {scheduler_job.py:155} INFO - Started process (PID=874) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:37:14,421] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:37:14,422] {logging_mixin.py:112} INFO - [2020-11-02 14:37:14,422] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:37:15,503] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:37:15,551] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:37:15,575] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:37:15,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.164 seconds
[2020-11-02 14:37:40,839] {scheduler_job.py:155} INFO - Started process (PID=1041) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:37:40,843] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:37:40,844] {logging_mixin.py:112} INFO - [2020-11-02 14:37:40,843] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:37:41,944] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:37:41,989] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:37:42,015] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:37:42,021] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.182 seconds
[2020-11-02 14:38:07,274] {scheduler_job.py:155} INFO - Started process (PID=1212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:38:07,298] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:38:07,298] {logging_mixin.py:112} INFO - [2020-11-02 14:38:07,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:38:08,946] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:38:09,009] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:38:09,041] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:38:09,046] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.772 seconds
[2020-11-02 14:38:33,808] {scheduler_job.py:155} INFO - Started process (PID=1526) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:38:33,829] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:38:33,830] {logging_mixin.py:112} INFO - [2020-11-02 14:38:33,829] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:38:35,638] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:38:35,704] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:38:35,733] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:38:35,740] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.931 seconds
[2020-11-02 14:39:00,324] {scheduler_job.py:155} INFO - Started process (PID=1718) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:00,328] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:39:00,329] {logging_mixin.py:112} INFO - [2020-11-02 14:39:00,329] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:01,338] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:01,383] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:39:01,399] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:39:01,403] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.079 seconds
[2020-11-02 14:39:26,818] {scheduler_job.py:155} INFO - Started process (PID=1904) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:26,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:39:26,825] {logging_mixin.py:112} INFO - [2020-11-02 14:39:26,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:27,922] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:27,961] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:39:27,981] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:39:27,985] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.167 seconds
[2020-11-02 14:39:53,306] {scheduler_job.py:155} INFO - Started process (PID=2047) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:53,313] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:39:53,314] {logging_mixin.py:112} INFO - [2020-11-02 14:39:53,314] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:54,890] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:39:54,934] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:39:54,954] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:39:54,960] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.654 seconds
[2020-11-02 14:40:19,739] {scheduler_job.py:155} INFO - Started process (PID=2244) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:40:19,745] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:40:19,745] {logging_mixin.py:112} INFO - [2020-11-02 14:40:19,745] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:40:21,098] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:40:21,139] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:40:21,155] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:40:21,160] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.421 seconds
[2020-11-02 14:40:46,299] {scheduler_job.py:155} INFO - Started process (PID=2451) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:40:46,306] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:40:46,307] {logging_mixin.py:112} INFO - [2020-11-02 14:40:46,307] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:40:47,514] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:40:47,581] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:40:47,609] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:40:47,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.318 seconds
[2020-11-02 14:41:12,699] {scheduler_job.py:155} INFO - Started process (PID=2622) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:41:12,704] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:41:12,705] {logging_mixin.py:112} INFO - [2020-11-02 14:41:12,704] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:41:13,802] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:41:13,858] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:41:13,882] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:41:13,889] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.189 seconds
[2020-11-02 14:41:39,161] {scheduler_job.py:155} INFO - Started process (PID=2767) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:41:39,171] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:41:39,178] {logging_mixin.py:112} INFO - [2020-11-02 14:41:39,177] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:41:40,478] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:41:40,529] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:41:40,552] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:41:40,559] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.398 seconds
[2020-11-02 14:42:05,690] {scheduler_job.py:155} INFO - Started process (PID=2913) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:05,697] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:42:05,699] {logging_mixin.py:112} INFO - [2020-11-02 14:42:05,698] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:06,937] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:06,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:42:07,005] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:42:07,013] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.323 seconds
[2020-11-02 14:42:32,078] {scheduler_job.py:155} INFO - Started process (PID=3051) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:32,088] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:42:32,092] {logging_mixin.py:112} INFO - [2020-11-02 14:42:32,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:33,270] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:33,309] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:42:33,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:42:33,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-02 14:42:58,563] {scheduler_job.py:155} INFO - Started process (PID=3191) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:58,568] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:42:58,569] {logging_mixin.py:112} INFO - [2020-11-02 14:42:58,569] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:59,738] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:42:59,788] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:42:59,807] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:42:59,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-02 14:43:25,118] {scheduler_job.py:155} INFO - Started process (PID=3331) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:43:25,123] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:43:25,124] {logging_mixin.py:112} INFO - [2020-11-02 14:43:25,123] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:43:26,229] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:43:26,270] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:43:26,289] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:43:26,294] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.176 seconds
[2020-11-02 14:43:51,588] {scheduler_job.py:155} INFO - Started process (PID=3472) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:43:51,592] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:43:51,592] {logging_mixin.py:112} INFO - [2020-11-02 14:43:51,592] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:43:52,773] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:43:52,823] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:43:52,849] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:43:52,853] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.266 seconds
[2020-11-02 14:44:18,212] {scheduler_job.py:155} INFO - Started process (PID=3613) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:44:18,216] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:44:18,217] {logging_mixin.py:112} INFO - [2020-11-02 14:44:18,216] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:44:19,221] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:44:19,258] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:44:19,273] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:44:19,277] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.065 seconds
[2020-11-02 14:44:44,796] {scheduler_job.py:155} INFO - Started process (PID=3756) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:44:44,816] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:44:44,818] {logging_mixin.py:112} INFO - [2020-11-02 14:44:44,818] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:44:45,936] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:44:45,982] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:44:45,998] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:44:46,003] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.206 seconds
[2020-11-02 14:45:11,353] {scheduler_job.py:155} INFO - Started process (PID=3903) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:45:11,362] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:45:11,364] {logging_mixin.py:112} INFO - [2020-11-02 14:45:11,362] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:45:12,597] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:45:12,636] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:45:12,653] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:45:12,656] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.304 seconds
[2020-11-02 14:45:37,896] {scheduler_job.py:155} INFO - Started process (PID=4042) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:45:37,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:45:37,902] {logging_mixin.py:112} INFO - [2020-11-02 14:45:37,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:45:38,887] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:45:38,930] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:45:38,947] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:45:38,950] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.054 seconds
[2020-11-02 14:46:04,396] {scheduler_job.py:155} INFO - Started process (PID=4190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:04,400] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:46:04,400] {logging_mixin.py:112} INFO - [2020-11-02 14:46:04,400] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:05,378] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:05,422] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:46:05,440] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:46:05,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.048 seconds
[2020-11-02 14:46:31,010] {scheduler_job.py:155} INFO - Started process (PID=4341) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:31,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:46:31,018] {logging_mixin.py:112} INFO - [2020-11-02 14:46:31,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:32,011] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:32,050] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:46:32,065] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:46:32,068] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.058 seconds
[2020-11-02 14:46:57,712] {scheduler_job.py:155} INFO - Started process (PID=4491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:57,717] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:46:57,718] {logging_mixin.py:112} INFO - [2020-11-02 14:46:57,717] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:58,721] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:46:58,761] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:46:58,780] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:46:58,785] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.073 seconds
[2020-11-02 14:47:24,407] {scheduler_job.py:155} INFO - Started process (PID=4644) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:47:24,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:47:24,414] {logging_mixin.py:112} INFO - [2020-11-02 14:47:24,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:47:25,395] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:47:25,436] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:47:25,451] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:47:25,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.048 seconds
[2020-11-02 14:47:50,821] {scheduler_job.py:155} INFO - Started process (PID=4789) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:47:50,825] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:47:50,826] {logging_mixin.py:112} INFO - [2020-11-02 14:47:50,826] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:47:51,923] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:47:51,959] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:47:51,975] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:47:51,979] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.158 seconds
[2020-11-02 14:48:17,289] {scheduler_job.py:155} INFO - Started process (PID=4942) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:48:17,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:48:17,293] {logging_mixin.py:112} INFO - [2020-11-02 14:48:17,293] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:48:18,511] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:48:18,554] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:48:18,571] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:48:18,575] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.287 seconds
[2020-11-02 14:48:43,837] {scheduler_job.py:155} INFO - Started process (PID=5079) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:48:43,841] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:48:43,842] {logging_mixin.py:112} INFO - [2020-11-02 14:48:43,842] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:48:44,915] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:48:44,955] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:48:44,984] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:48:44,988] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.151 seconds
[2020-11-02 14:49:10,448] {scheduler_job.py:155} INFO - Started process (PID=5229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:49:10,454] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:49:10,454] {logging_mixin.py:112} INFO - [2020-11-02 14:49:10,454] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:49:11,461] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:49:11,505] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:49:11,523] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:49:11,527] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-02 14:49:37,029] {scheduler_job.py:155} INFO - Started process (PID=5380) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:49:37,033] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:49:37,033] {logging_mixin.py:112} INFO - [2020-11-02 14:49:37,033] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:49:38,034] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:49:38,077] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:49:38,094] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:49:38,098] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.069 seconds
[2020-11-02 14:50:03,501] {scheduler_job.py:155} INFO - Started process (PID=5529) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:03,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:50:03,508] {logging_mixin.py:112} INFO - [2020-11-02 14:50:03,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:04,506] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:04,551] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:50:04,566] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:50:04,570] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.069 seconds
[2020-11-02 14:50:30,079] {scheduler_job.py:155} INFO - Started process (PID=5677) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:30,085] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:50:30,086] {logging_mixin.py:112} INFO - [2020-11-02 14:50:30,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:31,082] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:31,122] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:50:31,137] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:50:31,141] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-02 14:50:56,621] {scheduler_job.py:155} INFO - Started process (PID=5822) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:56,626] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:50:56,627] {logging_mixin.py:112} INFO - [2020-11-02 14:50:56,627] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:57,622] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:50:57,665] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:50:57,680] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:50:57,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.063 seconds
[2020-11-02 14:51:23,074] {scheduler_job.py:155} INFO - Started process (PID=5971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:51:23,086] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:51:23,091] {logging_mixin.py:112} INFO - [2020-11-02 14:51:23,091] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:51:24,244] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:51:24,288] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:51:24,307] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:51:24,312] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.238 seconds
[2020-11-02 14:51:49,917] {scheduler_job.py:155} INFO - Started process (PID=6112) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:51:49,933] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:51:49,934] {logging_mixin.py:112} INFO - [2020-11-02 14:51:49,933] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:51:51,130] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:51:51,170] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:51:51,188] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:51:51,192] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.275 seconds
[2020-11-02 14:52:16,595] {scheduler_job.py:155} INFO - Started process (PID=6262) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:52:16,598] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:52:16,599] {logging_mixin.py:112} INFO - [2020-11-02 14:52:16,599] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:52:17,580] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:52:17,627] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:52:17,642] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:52:17,645] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.051 seconds
[2020-11-02 14:52:43,255] {scheduler_job.py:155} INFO - Started process (PID=6407) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:52:43,262] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:52:43,263] {logging_mixin.py:112} INFO - [2020-11-02 14:52:43,263] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:52:44,269] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:52:44,309] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:52:44,324] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:52:44,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.073 seconds
[2020-11-02 14:53:09,881] {scheduler_job.py:155} INFO - Started process (PID=6564) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:53:09,886] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:53:09,888] {logging_mixin.py:112} INFO - [2020-11-02 14:53:09,887] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:53:10,900] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:53:10,942] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:53:10,959] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:53:10,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.082 seconds
[2020-11-02 14:53:36,400] {scheduler_job.py:155} INFO - Started process (PID=6710) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:53:36,404] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:53:36,405] {logging_mixin.py:112} INFO - [2020-11-02 14:53:36,405] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:53:37,385] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:53:37,428] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:53:37,445] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:53:37,449] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.048 seconds
[2020-11-02 14:54:02,914] {scheduler_job.py:155} INFO - Started process (PID=6861) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:02,919] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:54:02,919] {logging_mixin.py:112} INFO - [2020-11-02 14:54:02,919] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:04,028] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:04,073] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:54:04,092] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:54:04,096] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.182 seconds
[2020-11-02 14:54:29,373] {scheduler_job.py:155} INFO - Started process (PID=7005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:29,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:54:29,378] {logging_mixin.py:112} INFO - [2020-11-02 14:54:29,378] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:30,356] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:30,399] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:54:30,419] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:54:30,424] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.051 seconds
[2020-11-02 14:54:56,057] {scheduler_job.py:155} INFO - Started process (PID=7150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:56,061] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:54:56,062] {logging_mixin.py:112} INFO - [2020-11-02 14:54:56,062] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:57,256] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:54:57,297] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:54:57,314] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:54:57,318] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.261 seconds
[2020-11-02 14:55:22,577] {scheduler_job.py:155} INFO - Started process (PID=7289) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:55:22,594] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:55:22,602] {logging_mixin.py:112} INFO - [2020-11-02 14:55:22,594] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:55:23,932] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:55:23,988] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:55:24,007] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:55:24,012] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.435 seconds
[2020-11-02 14:55:49,082] {scheduler_job.py:155} INFO - Started process (PID=7425) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:55:49,088] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:55:49,089] {logging_mixin.py:112} INFO - [2020-11-02 14:55:49,089] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:55:50,584] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:55:50,632] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:55:50,651] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:55:50,656] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.574 seconds
[2020-11-02 14:56:15,424] {scheduler_job.py:155} INFO - Started process (PID=7564) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:56:15,428] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:56:15,429] {logging_mixin.py:112} INFO - [2020-11-02 14:56:15,428] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:56:16,983] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:56:17,030] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:56:17,049] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:56:17,053] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.630 seconds
[2020-11-02 14:56:41,876] {scheduler_job.py:155} INFO - Started process (PID=7708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:56:41,881] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:56:41,881] {logging_mixin.py:112} INFO - [2020-11-02 14:56:41,881] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:56:43,108] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:56:43,153] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:56:43,175] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:56:43,180] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.303 seconds
[2020-11-02 14:57:08,364] {scheduler_job.py:155} INFO - Started process (PID=7857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:57:08,369] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:57:08,369] {logging_mixin.py:112} INFO - [2020-11-02 14:57:08,369] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:57:09,730] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:57:09,778] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:57:09,798] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:57:09,803] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.438 seconds
[2020-11-02 14:57:34,910] {scheduler_job.py:155} INFO - Started process (PID=8005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:57:34,913] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:57:34,914] {logging_mixin.py:112} INFO - [2020-11-02 14:57:34,913] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:57:36,136] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:57:36,185] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:57:36,207] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:57:36,211] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.302 seconds
[2020-11-02 14:58:01,321] {scheduler_job.py:155} INFO - Started process (PID=8152) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:01,325] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:58:01,326] {logging_mixin.py:112} INFO - [2020-11-02 14:58:01,326] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:02,887] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:02,948] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:58:02,974] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:58:02,979] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.658 seconds
[2020-11-02 14:58:27,808] {scheduler_job.py:155} INFO - Started process (PID=8280) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:27,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:58:27,816] {logging_mixin.py:112} INFO - [2020-11-02 14:58:27,815] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:29,430] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:29,476] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:58:29,496] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:58:29,501] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.693 seconds
[2020-11-02 14:58:54,188] {scheduler_job.py:155} INFO - Started process (PID=8415) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:54,192] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:58:54,193] {logging_mixin.py:112} INFO - [2020-11-02 14:58:54,192] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:55,539] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:58:55,597] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:58:55,625] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:58:55,630] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.441 seconds
[2020-11-02 14:59:20,790] {scheduler_job.py:155} INFO - Started process (PID=8557) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:59:20,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:59:20,796] {logging_mixin.py:112} INFO - [2020-11-02 14:59:20,796] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:59:21,979] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:59:22,022] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:59:22,038] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:59:22,043] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.252 seconds
[2020-11-02 14:59:47,257] {scheduler_job.py:155} INFO - Started process (PID=8702) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:59:47,262] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 14:59:47,263] {logging_mixin.py:112} INFO - [2020-11-02 14:59:47,262] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:59:48,450] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 14:59:48,489] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 14:59:48,506] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 14:59:48,510] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.253 seconds
[2020-11-02 15:00:13,671] {scheduler_job.py:155} INFO - Started process (PID=8849) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 15:00:13,678] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 15:00:13,679] {logging_mixin.py:112} INFO - [2020-11-02 15:00:13,678] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 15:00:14,845] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 15:00:14,885] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 15:00:14,902] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 15:00:14,906] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.235 seconds

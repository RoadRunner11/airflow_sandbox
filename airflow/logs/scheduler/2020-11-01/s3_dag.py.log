[2020-11-01 18:29:07,681] {scheduler_job.py:155} INFO - Started process (PID=23319) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:29:07,700] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:29:07,703] {logging_mixin.py:112} INFO - [2020-11-01 18:29:07,702] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:29:08,198] {scheduler_job.py:423} INFO - Exiting gracefully upon receiving signal 15
[2020-11-01 18:29:08,199] {scheduler_job.py:423} INFO - Exiting gracefully upon receiving signal 15
[2020-11-01 18:45:02,459] {scheduler_job.py:155} INFO - Started process (PID=23805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:45:02,462] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:45:02,463] {logging_mixin.py:112} INFO - [2020-11-01 18:45:02,463] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:45:03,677] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:45:03,722] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.263 seconds
[2020-11-01 18:45:56,550] {scheduler_job.py:155} INFO - Started process (PID=23907) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:45:56,559] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:45:56,560] {logging_mixin.py:112} INFO - [2020-11-01 18:45:56,560] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:45:57,655] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:45:57,697] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.147 seconds
[2020-11-01 18:46:50,730] {scheduler_job.py:155} INFO - Started process (PID=23997) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:46:50,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:46:50,741] {logging_mixin.py:112} INFO - [2020-11-01 18:46:50,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:46:52,178] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:46:52,213] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.483 seconds
[2020-11-01 18:47:44,849] {scheduler_job.py:155} INFO - Started process (PID=24091) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:47:44,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:47:44,864] {logging_mixin.py:112} INFO - [2020-11-01 18:47:44,863] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:47:45,978] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:47:46,015] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.166 seconds
[2020-11-01 18:48:38,985] {scheduler_job.py:155} INFO - Started process (PID=24188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:48:39,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:48:39,005] {logging_mixin.py:112} INFO - [2020-11-01 18:48:39,005] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:48:40,053] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:48:40,079] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.095 seconds
[2020-11-01 18:49:33,272] {scheduler_job.py:155} INFO - Started process (PID=24285) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:49:33,276] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:49:33,277] {logging_mixin.py:112} INFO - [2020-11-01 18:49:33,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:49:34,324] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:49:34,353] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.081 seconds
[2020-11-01 18:50:59,986] {scheduler_job.py:155} INFO - Started process (PID=24447) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:00,007] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:51:00,008] {logging_mixin.py:112} INFO - [2020-11-01 18:51:00,008] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:00,964] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:01,014] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:51:01,069] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:01,075] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:01,163] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:51:01,172] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 00:00:00+00:00 [success]> in ORM
[2020-11-01 18:51:01,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.195 seconds
[2020-11-01 18:51:14,281] {scheduler_job.py:155} INFO - Started process (PID=24514) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:14,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:51:14,293] {logging_mixin.py:112} INFO - [2020-11-01 18:51:14,293] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:15,751] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:15,793] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:51:15,833] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:15,838] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:15,874] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:15,965] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:51:15,972] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 00:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:51:15,980] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 01:00:00+00:00 [success]> in ORM
[2020-11-01 18:51:15,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.710 seconds
[2020-11-01 18:51:27,655] {scheduler_job.py:155} INFO - Started process (PID=24599) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:27,664] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:51:27,665] {logging_mixin.py:112} INFO - [2020-11-01 18:51:27,665] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:28,520] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:28,594] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:51:28,665] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:28,673] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:28,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:28,783] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:28,930] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:51:28,937] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 00:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:51:28,944] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 01:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:51:28,956] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 02:00:00+00:00 [success]> in ORM
[2020-11-01 18:51:28,969] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.314 seconds
[2020-11-01 18:51:41,037] {scheduler_job.py:155} INFO - Started process (PID=24689) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:41,053] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:51:41,054] {logging_mixin.py:112} INFO - [2020-11-01 18:51:41,054] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:43,826] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:43,905] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:51:44,062] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:44,101] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:44,217] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:44,276] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:44,376] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:44,606] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:51:44,622] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 01:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:51:44,637] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 02:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:51:44,653] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 03:00:00+00:00 [success]> in ORM
[2020-11-01 18:51:44,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.647 seconds
[2020-11-01 18:51:55,376] {scheduler_job.py:155} INFO - Started process (PID=24780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:55,380] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:51:55,381] {logging_mixin.py:112} INFO - [2020-11-01 18:51:55,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:56,304] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:51:56,342] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:51:56,392] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:56,405] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:56,424] {logging_mixin.py:112} INFO - [2020-11-01 18:51:56,424] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-01 00:00:00+00:00: scheduled__2020-11-01T00:00:00+00:00, externally triggered: False> successful
[2020-11-01 18:51:56,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:56,471] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:56,493] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:56,530] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:51:56,654] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:51:56,661] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 03:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:51:56,687] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 04:00:00+00:00 [success]> in ORM
[2020-11-01 18:51:56,710] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.334 seconds
[2020-11-01 18:52:08,832] {scheduler_job.py:155} INFO - Started process (PID=24885) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:08,846] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:52:08,847] {logging_mixin.py:112} INFO - [2020-11-01 18:52:08,847] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:09,820] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:09,864] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:52:09,904] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:09,908] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:09,931] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:09,952] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:09,974] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:10,008] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:10,141] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:52:10,147] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 02:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:10,155] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 04:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:10,161] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 05:00:00+00:00 [success]> in ORM
[2020-11-01 18:52:10,173] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.341 seconds
[2020-11-01 18:52:22,127] {scheduler_job.py:155} INFO - Started process (PID=24980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:22,132] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:52:22,133] {logging_mixin.py:112} INFO - [2020-11-01 18:52:22,133] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:23,238] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:23,318] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:52:23,404] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,409] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,438] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,481] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,547] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,577] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:23,725] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:52:23,737] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 03:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:23,748] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 05:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:23,758] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 06:00:00+00:00 [success]> in ORM
[2020-11-01 18:52:23,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.665 seconds
[2020-11-01 18:52:35,437] {scheduler_job.py:155} INFO - Started process (PID=25076) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:35,448] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:52:35,449] {logging_mixin.py:112} INFO - [2020-11-01 18:52:35,449] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:37,363] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:37,496] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:52:37,661] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:37,686] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:37,814] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:37,918] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:37,983] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:38,042] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:38,159] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:38,234] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:38,605] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:52:38,616] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 04:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:38,626] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 06:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:38,645] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 07:00:00+00:00 [success]> in ORM
[2020-11-01 18:52:38,669] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.232 seconds
[2020-11-01 18:52:54,432] {scheduler_job.py:155} INFO - Started process (PID=25193) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:54,436] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:52:54,437] {logging_mixin.py:112} INFO - [2020-11-01 18:52:54,437] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:55,417] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:52:55,471] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:52:55,541] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,582] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,605] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,629] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,653] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,676] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,711] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,734] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:52:55,871] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:52:55,877] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 05:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:55,887] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 07:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:52:55,896] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 08:00:00+00:00 [success]> in ORM
[2020-11-01 18:52:55,919] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.487 seconds
[2020-11-01 18:53:08,790] {scheduler_job.py:155} INFO - Started process (PID=25308) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:08,804] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:53:08,806] {logging_mixin.py:112} INFO - [2020-11-01 18:53:08,806] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:09,659] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:09,704] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:53:09,746] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,751] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,787] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,809] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,834] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,858] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,879] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,902] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:09,959] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:10,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:10,239] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:53:10,250] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 06:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:53:10,265] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 08:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:53:10,276] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 09:00:00+00:00 [success]> in ORM
[2020-11-01 18:53:10,290] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.500 seconds
[2020-11-01 18:53:34,040] {scheduler_job.py:155} INFO - Started process (PID=25447) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:34,044] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:53:34,045] {logging_mixin.py:112} INFO - [2020-11-01 18:53:34,045] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:35,180] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:35,291] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:53:35,387] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,405] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,454] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,507] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,565] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,602] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,665] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,708] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,731] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,752] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:35,774] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:36,115] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:53:36,127] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 07:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:53:36,154] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 09:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:53:36,176] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 10:00:00+00:00 [success]> in ORM
[2020-11-01 18:53:36,212] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.172 seconds
[2020-11-01 18:53:47,449] {scheduler_job.py:155} INFO - Started process (PID=25540) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:47,461] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:53:47,462] {logging_mixin.py:112} INFO - [2020-11-01 18:53:47,462] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:50,686] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:53:50,796] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:53:50,914] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:50,933] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,062] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,180] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,228] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,271] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,308] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,341] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,386] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,436] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:53:51,683] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:53:51,691] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 08:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:53:51,700] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 10:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:53:51,707] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 11:00:00+00:00 [success]> in ORM
[2020-11-01 18:53:51,720] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 4.271 seconds
[2020-11-01 18:54:17,422] {scheduler_job.py:155} INFO - Started process (PID=25701) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:54:17,459] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:54:17,463] {logging_mixin.py:112} INFO - [2020-11-01 18:54:17,463] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:54:20,810] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:54:20,918] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:54:21,045] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,056] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,128] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,216] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,309] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,397] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,460] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,510] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,609] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,681] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:21,813] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:22,043] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:22,148] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:22,772] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:54:22,792] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 01:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:54:22,819] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 09:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:54:22,845] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 11:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:54:22,903] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 12:00:00+00:00 [success]> in ORM
[2020-11-01 18:54:22,935] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 5.513 seconds
[2020-11-01 18:54:49,781] {scheduler_job.py:155} INFO - Started process (PID=25865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:54:49,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:54:49,807] {logging_mixin.py:112} INFO - [2020-11-01 18:54:49,807] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:54:52,912] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:54:52,983] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:54:53,064] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,074] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,165] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,209] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,286] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,359] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,448] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,523] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,564] {logging_mixin.py:112} INFO - [2020-11-01 18:54:53,563] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-01 07:00:00+00:00: scheduled__2020-11-01T07:00:00+00:00, externally triggered: False> successful
[2020-11-01 18:54:53,575] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,642] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,735] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:53,839] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:54,031] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:54,079] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:54:54,544] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:54:54,557] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 02:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:54:54,578] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 10:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:54:54,589] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 12:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:54:54,603] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 13:00:00+00:00 [success]> in ORM
[2020-11-01 18:54:54,627] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 4.846 seconds
[2020-11-01 18:55:04,031] {scheduler_job.py:155} INFO - Started process (PID=25953) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:04,040] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:55:04,041] {logging_mixin.py:112} INFO - [2020-11-01 18:55:04,041] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:05,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:06,012] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:55:06,141] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,153] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,227] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,268] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,318] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,347] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,391] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,488] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,535] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,569] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,610] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,661] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,760] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:06,863] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:07,573] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:55:07,595] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 11:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:07,618] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 13:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:07,645] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 14:00:00+00:00 [success]> in ORM
[2020-11-01 18:55:07,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.645 seconds
[2020-11-01 18:55:28,967] {scheduler_job.py:155} INFO - Started process (PID=26083) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:28,992] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:55:28,993] {logging_mixin.py:112} INFO - [2020-11-01 18:55:28,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:30,343] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:30,392] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:55:30,453] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,467] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,515] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,591] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,637] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,679] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,715] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,746] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,785] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,817] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,846] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,883] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,924] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:30,953] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:31,260] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:55:31,267] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 03:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:31,275] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 04:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:31,286] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 14:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:31,297] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 15:00:00+00:00 [success]> in ORM
[2020-11-01 18:55:31,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.347 seconds
[2020-11-01 18:55:43,339] {scheduler_job.py:155} INFO - Started process (PID=26189) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:43,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:55:43,346] {logging_mixin.py:112} INFO - [2020-11-01 18:55:43,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:44,562] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:55:44,632] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:55:44,705] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:44,713] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:44,774] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:44,833] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:44,869] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:44,923] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,005] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,087] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,144] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,195] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,255] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,298] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,340] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,380] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,511] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:45,623] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:55:46,091] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:55:46,099] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 12:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:46,107] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 13:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:46,115] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 15:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:55:46,124] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 16:00:00+00:00 [success]> in ORM
[2020-11-01 18:55:46,138] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.799 seconds
[2020-11-01 18:56:22,735] {scheduler_job.py:155} INFO - Started process (PID=26381) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:56:22,757] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:56:22,757] {logging_mixin.py:112} INFO - [2020-11-01 18:56:22,757] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:56:25,688] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:56:25,768] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:56:25,820] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:25,902] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:25,925] {logging_mixin.py:112} INFO - [2020-11-01 18:56:25,924] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-01 02:00:00+00:00: scheduled__2020-11-01T02:00:00+00:00, externally triggered: False> successful
[2020-11-01 18:56:25,929] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:25,967] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:25,993] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,028] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,062] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,092] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,124] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,153] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,186] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,213] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,236] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,265] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:26,647] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:56:26,654] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 05:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:56:26,671] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 06:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:56:26,684] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 14:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:56:26,694] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 16:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:56:26,715] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.980 seconds
[2020-11-01 18:56:38,090] {scheduler_job.py:155} INFO - Started process (PID=26488) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:56:38,107] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:56:38,114] {logging_mixin.py:112} INFO - [2020-11-01 18:56:38,114] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:56:39,764] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:56:39,851] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:56:39,876] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:39,903] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:39,944] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:39,971] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:39,995] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,022] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,058] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,149] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,239] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,309] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,446] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,508] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:40,602] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:56:41,111] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:56:41,145] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 15:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:56:41,180] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.090 seconds
[2020-11-01 18:57:27,099] {scheduler_job.py:155} INFO - Started process (PID=26724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:57:27,116] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:57:27,117] {logging_mixin.py:112} INFO - [2020-11-01 18:57:27,117] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:57:28,383] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:57:28,426] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:57:28,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,477] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,498] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,520] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,545] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,567] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,620] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,656] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,684] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,715] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,740] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,766] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,791] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,819] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:28,847] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:57:29,074] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:57:29,081] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 01:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:57:29,089] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 08:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:57:29,096] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 09:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:57:29,104] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 16:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:57:29,117] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 17:56:43.159379+00:00 [success]> in ORM
[2020-11-01 18:57:29,156] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.057 seconds
[2020-11-01 18:57:40,508] {scheduler_job.py:155} INFO - Started process (PID=26808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:57:40,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:57:40,513] {logging_mixin.py:112} INFO - [2020-11-01 18:57:40,513] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:57:41,841] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:57:41,975] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:57:42,044] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,095] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,136] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,159] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,184] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,208] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,229] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,251] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,277] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,299] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,323] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,370] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,395] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:57:42,419] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:57:42,771] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:57:42,778] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 17:56:43.159379+00:00 [scheduled]> in ORM
[2020-11-01 18:57:42,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.297 seconds
[2020-11-01 18:58:19,634] {scheduler_job.py:155} INFO - Started process (PID=27005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:58:19,662] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:58:19,667] {logging_mixin.py:112} INFO - [2020-11-01 18:58:19,667] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:58:22,100] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:58:22,184] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:58:22,218] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,272] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,327] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,363] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,401] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,452] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,491] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,551] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,595] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,621] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,669] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,724] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,788] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,857] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:58:22,933] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:58:23,524] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:58:23,550] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 10:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:58:23,560] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 11:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:58:23,579] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:56:43.159379+00:00 [scheduled]> in ORM
[2020-11-01 18:58:23,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.969 seconds
[2020-11-01 18:59:00,825] {scheduler_job.py:155} INFO - Started process (PID=27206) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:00,864] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:59:00,870] {logging_mixin.py:112} INFO - [2020-11-01 18:59:00,869] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:02,022] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:02,062] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:59:02,083] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,110] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,132] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,153] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,178] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,201] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,225] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,253] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,279] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,305] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,328] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,353] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,378] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:02,446] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:59:02,670] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:59:02,679] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 03:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:59:02,686] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 04:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:59:02,698] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.874 seconds
[2020-11-01 18:59:13,012] {scheduler_job.py:155} INFO - Started process (PID=27293) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:13,015] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:59:13,016] {logging_mixin.py:112} INFO - [2020-11-01 18:59:13,016] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:14,231] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:14,311] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:59:14,339] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,372] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,398] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,428] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,456] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,482] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,532] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,564] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,597] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,628] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,657] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,688] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,711] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:14,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:59:15,113] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:59:15,130] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 12:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:59:15,142] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 13:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:59:15,158] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.146 seconds
[2020-11-01 18:59:32,159] {scheduler_job.py:155} INFO - Started process (PID=27405) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:32,204] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:59:32,206] {logging_mixin.py:112} INFO - [2020-11-01 18:59:32,206] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:33,630] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:33,683] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:59:33,712] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,763] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,805] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,841] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,878] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,908] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,940] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:33,990] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,036] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,089] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,167] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,263] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:34,436] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:59:34,777] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:59:34,784] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.633 seconds
[2020-11-01 18:59:41,158] {scheduler_job.py:155} INFO - Started process (PID=27483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:41,171] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:59:41,173] {logging_mixin.py:112} INFO - [2020-11-01 18:59:41,172] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:42,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:42,273] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:59:42,300] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,335] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,360] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,389] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,417] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,440] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,465] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,494] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,531] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,565] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,605] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,641] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,692] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,727] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:42,765] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:59:43,158] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:59:43,177] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 05:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:59:43,194] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.036 seconds
[2020-11-01 18:59:49,448] {scheduler_job.py:155} INFO - Started process (PID=27567) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:49,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 18:59:49,452] {logging_mixin.py:112} INFO - [2020-11-01 18:59:49,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:50,422] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 18:59:50,470] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 18:59:50,502] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,538] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,572] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,596] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,640] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,663] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,684] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,707] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,729] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,751] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,772] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,794] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,816] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 18:59:50,837] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 18:59:51,043] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 18:59:51,050] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 06:00:00+00:00 [scheduled]> in ORM
[2020-11-01 18:59:51,060] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.613 seconds
[2020-11-01 19:00:14,675] {scheduler_job.py:155} INFO - Started process (PID=27708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:14,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:00:14,683] {logging_mixin.py:112} INFO - [2020-11-01 19:00:14,683] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:15,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:15,978] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:00:16,066] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,077] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,126] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,163] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,206] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,250] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,316] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,372] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,430] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,500] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,551] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,632] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,712] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,771] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,857] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,891] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:16,909] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:00:17,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:00:17,297] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 14:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:00:17,308] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 15:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:00:17,317] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 17:00:00+00:00 [success]> in ORM
[2020-11-01 19:00:17,329] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.653 seconds
[2020-11-01 19:00:28,014] {scheduler_job.py:155} INFO - Started process (PID=27796) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:28,040] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:00:28,041] {logging_mixin.py:112} INFO - [2020-11-01 19:00:28,041] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:30,246] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:30,403] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:00:30,444] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,509] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,558] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,595] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,620] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,653] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,681] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,711] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,761] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,797] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,868] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:30,945] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:31,044] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:31,108] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:31,175] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:31,252] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:00:31,723] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:00:31,732] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 01:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:00:31,742] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 17:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:00:31,764] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.750 seconds
[2020-11-01 19:00:43,277] {scheduler_job.py:155} INFO - Started process (PID=27897) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:43,281] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:00:43,284] {logging_mixin.py:112} INFO - [2020-11-01 19:00:43,283] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:44,384] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:44,466] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:00:44,499] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,540] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,616] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,645] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,673] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,701] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,731] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,756] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,783] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,811] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,837] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,867] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,894] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,921] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:44,955] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:00:45,215] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:00:45,223] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 08:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:00:45,231] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:00:45,245] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.968 seconds
[2020-11-01 19:00:56,630] {scheduler_job.py:155} INFO - Started process (PID=27993) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:56,652] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:00:56,653] {logging_mixin.py:112} INFO - [2020-11-01 19:00:56,653] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:59,453] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:00:59,509] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:00:59,535] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,587] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,623] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,676] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,797] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,871] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,898] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,924] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,947] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,971] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:00:59,995] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:00,048] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:00,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:00,151] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:00,216] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:00,267] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:01:00,698] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:01:00,720] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 09:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:01:00,736] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 16:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:01:00,767] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 4.137 seconds
[2020-11-01 19:01:25,206] {scheduler_job.py:155} INFO - Started process (PID=28146) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:01:25,210] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:01:25,211] {logging_mixin.py:112} INFO - [2020-11-01 19:01:25,211] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:01:26,141] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:01:26,181] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:01:26,212] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,228] {logging_mixin.py:112} INFO - [2020-11-01 19:01:26,228] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 01:00:00+00:00: scheduled__2020-11-01T01:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:01:26,232] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,284] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,334] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,367] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,391] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,413] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,437] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,457] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,484] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,505] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,531] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,553] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,600] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:26,624] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:01:26,964] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:01:26,971] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 10:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:01:26,992] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:56:43.159379+00:00 [scheduled]> in ORM
[2020-11-01 19:01:27,014] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.807 seconds
[2020-11-01 19:01:38,522] {scheduler_job.py:155} INFO - Started process (PID=28233) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:01:38,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:01:38,552] {logging_mixin.py:112} INFO - [2020-11-01 19:01:38,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:01:39,835] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:01:39,897] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:01:39,923] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:39,961] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:39,988] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,016] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,036] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,069] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,117] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,141] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,165] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,197] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,238] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,289] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,335] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,377] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:01:40,432] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:01:40,880] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:01:40,888] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 04:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:01:40,899] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 11:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:01:40,913] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.391 seconds
[2020-11-01 19:02:03,969] {scheduler_job.py:155} INFO - Started process (PID=28377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:03,975] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:02:03,976] {logging_mixin.py:112} INFO - [2020-11-01 19:02:03,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:05,108] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:05,150] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:02:05,171] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,198] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,221] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,243] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,267] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,290] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,313] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,338] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,361] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,387] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,410] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,437] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,459] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,481] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:05,504] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:02:05,717] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:02:05,725] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 03:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:02:05,733] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 12:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:02:05,747] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.778 seconds
[2020-11-01 19:02:18,196] {scheduler_job.py:155} INFO - Started process (PID=28483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:18,200] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:02:18,201] {logging_mixin.py:112} INFO - [2020-11-01 19:02:18,201] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:19,188] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:19,229] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:02:19,256] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,296] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,308] {logging_mixin.py:112} INFO - [2020-11-01 19:02:19,307] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 04:00:00+00:00: scheduled__2020-11-01T04:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:02:19,311] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,342] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,365] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,388] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,411] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,435] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,460] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,482] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,505] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,529] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,554] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,578] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:19,610] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:02:19,803] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:02:19,810] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 13:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:02:19,822] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.626 seconds
[2020-11-01 19:02:38,035] {scheduler_job.py:155} INFO - Started process (PID=28623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:38,038] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:02:38,039] {logging_mixin.py:112} INFO - [2020-11-01 19:02:38,039] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:38,869] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:38,913] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:02:38,934] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:38,961] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:38,982] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,001] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,021] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,047] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,093] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,131] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,168] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,210] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,280] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,315] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:39,349] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:02:39,578] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:02:39,584] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 05:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:02:39,591] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 06:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:02:39,604] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.569 seconds
[2020-11-01 19:02:50,188] {scheduler_job.py:155} INFO - Started process (PID=28710) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:50,192] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:02:50,193] {logging_mixin.py:112} INFO - [2020-11-01 19:02:50,193] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:51,069] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:02:51,110] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:02:51,130] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,145] {logging_mixin.py:112} INFO - [2020-11-01 19:02:51,145] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 03:00:00+00:00: scheduled__2020-11-01T03:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:02:51,149] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,171] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,199] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,221] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,245] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,268] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,293] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,322] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,369] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,394] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,420] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:02:51,451] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:02:51,685] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:02:51,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.502 seconds
[2020-11-01 19:03:07,017] {scheduler_job.py:155} INFO - Started process (PID=28815) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:07,023] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:03:07,024] {logging_mixin.py:112} INFO - [2020-11-01 19:03:07,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:08,740] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:08,785] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:03:08,808] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:08,838] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:08,864] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:08,895] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:08,925] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:08,956] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:08,986] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:09,014] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:09,049] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:09,083] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:09,136] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:09,175] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:09,209] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:03:09,464] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:03:09,473] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 14:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:03:09,483] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 15:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:03:09,498] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.482 seconds
[2020-11-01 19:03:21,377] {scheduler_job.py:155} INFO - Started process (PID=28917) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:21,384] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:03:21,384] {logging_mixin.py:112} INFO - [2020-11-01 19:03:21,384] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:22,422] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:22,477] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:03:22,503] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,524] {logging_mixin.py:112} INFO - [2020-11-01 19:03:22,524] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 05:00:00+00:00: scheduled__2020-11-01T05:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:03:22,528] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,546] {logging_mixin.py:112} INFO - [2020-11-01 19:03:22,545] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 06:00:00+00:00: scheduled__2020-11-01T06:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:03:22,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,583] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,608] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,634] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,661] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,691] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,713] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,734] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,756] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,778] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:22,800] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:03:22,990] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:03:22,995] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.618 seconds
[2020-11-01 19:03:35,774] {scheduler_job.py:155} INFO - Started process (PID=29008) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:35,782] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:03:35,783] {logging_mixin.py:112} INFO - [2020-11-01 19:03:35,783] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:36,970] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:37,048] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:03:37,072] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,105] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,134] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,163] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,193] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,227] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,256] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,327] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,356] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:37,384] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:03:37,633] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:03:37,646] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 08:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:03:37,656] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:03:37,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.901 seconds
[2020-11-01 19:03:49,129] {scheduler_job.py:155} INFO - Started process (PID=29101) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:49,134] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:03:49,135] {logging_mixin.py:112} INFO - [2020-11-01 19:03:49,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:50,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:03:50,320] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:03:50,342] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,371] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,398] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,451] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,476] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,508] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,543] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,580] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,611] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:03:50,647] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:03:50,835] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:03:50,843] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 09:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:03:50,854] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.725 seconds
[2020-11-01 19:04:02,466] {scheduler_job.py:155} INFO - Started process (PID=29208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:02,471] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:04:02,472] {logging_mixin.py:112} INFO - [2020-11-01 19:04:02,471] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:03,452] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:03,554] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:04:03,589] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,610] {logging_mixin.py:112} INFO - [2020-11-01 19:04:03,610] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 08:00:00+00:00: scheduled__2020-11-01T08:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:04:03,623] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,654] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,685] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,713] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,757] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,785] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,818] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,865] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,898] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:03,920] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:04:04,080] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:04:04,087] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 10:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:04:04,095] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 16:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:04:04,106] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.640 seconds
[2020-11-01 19:04:15,769] {scheduler_job.py:155} INFO - Started process (PID=29294) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:15,781] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:04:15,782] {logging_mixin.py:112} INFO - [2020-11-01 19:04:15,781] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:16,830] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:16,880] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:04:16,913] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:16,956] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:16,976] {logging_mixin.py:112} INFO - [2020-11-01 19:04:16,976] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 10:00:00+00:00: scheduled__2020-11-01T10:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:04:16,981] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,010] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,040] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,067] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,095] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,124] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,153] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:17,183] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:04:17,359] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:04:17,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.594 seconds
[2020-11-01 19:04:29,157] {scheduler_job.py:155} INFO - Started process (PID=29395) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:29,170] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:04:29,177] {logging_mixin.py:112} INFO - [2020-11-01 19:04:29,176] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:31,641] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:31,715] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:04:31,751] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:31,774] {logging_mixin.py:112} INFO - [2020-11-01 19:04:31,774] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 09:00:00+00:00: scheduled__2020-11-01T09:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:04:31,779] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:31,835] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:31,878] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:31,927] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:31,969] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:32,020] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:32,061] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:32,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:04:32,328] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:04:32,335] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 11:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:04:32,343] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:56:43.159379+00:00 [scheduled]> in ORM
[2020-11-01 19:04:32,355] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.199 seconds
[2020-11-01 19:04:46,628] {scheduler_job.py:155} INFO - Started process (PID=29503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:46,636] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:04:46,637] {logging_mixin.py:112} INFO - [2020-11-01 19:04:46,637] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:48,804] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:48,887] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:04:48,954] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:48,991] {logging_mixin.py:112} INFO - [2020-11-01 19:04:48,991] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 11:00:00+00:00: scheduled__2020-11-01T11:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:04:48,996] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:49,041] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:49,084] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:49,130] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:49,177] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:49,223] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:04:49,273] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:04:49,544] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:04:49,561] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 13:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:04:49,582] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.954 seconds
[2020-11-01 19:04:59,899] {scheduler_job.py:155} INFO - Started process (PID=29591) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:04:59,902] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:04:59,910] {logging_mixin.py:112} INFO - [2020-11-01 19:04:59,910] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:02,047] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:02,134] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:05:02,168] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:02,220] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:02,267] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:02,317] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:02,363] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:02,400] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:02,472] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:05:02,851] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:05:02,903] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 12:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:05:02,960] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.061 seconds
[2020-11-01 19:05:15,346] {scheduler_job.py:155} INFO - Started process (PID=29697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:15,353] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:05:15,354] {logging_mixin.py:112} INFO - [2020-11-01 19:05:15,354] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:16,140] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:16,178] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:05:16,200] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:16,252] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:16,264] {logging_mixin.py:112} INFO - [2020-11-01 19:05:16,264] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 13:00:00+00:00: scheduled__2020-11-01T13:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:05:16,268] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:16,291] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:16,319] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:16,347] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:16,381] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:05:16,515] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:05:16,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.172 seconds
[2020-11-01 19:05:28,688] {scheduler_job.py:155} INFO - Started process (PID=29768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:28,712] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:05:28,713] {logging_mixin.py:112} INFO - [2020-11-01 19:05:28,713] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:31,200] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:31,278] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:05:31,320] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:31,346] {logging_mixin.py:112} INFO - [2020-11-01 19:05:31,345] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 12:00:00+00:00: scheduled__2020-11-01T12:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:05:31,352] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:31,393] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:31,446] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:31,482] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:31,523] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:05:31,663] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:05:31,668] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.980 seconds
[2020-11-01 19:05:41,970] {scheduler_job.py:155} INFO - Started process (PID=29830) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:41,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:05:41,976] {logging_mixin.py:112} INFO - [2020-11-01 19:05:41,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:43,111] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:43,154] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:05:43,174] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:43,206] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:43,227] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:43,249] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:43,272] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:05:43,379] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:05:43,388] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 14:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:05:43,399] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.428 seconds
[2020-11-01 19:05:56,393] {scheduler_job.py:155} INFO - Started process (PID=29918) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:56,397] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:05:56,398] {logging_mixin.py:112} INFO - [2020-11-01 19:05:56,398] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:57,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:05:57,300] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:05:57,338] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:57,354] {logging_mixin.py:112} INFO - [2020-11-01 19:05:57,354] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 14:00:00+00:00: scheduled__2020-11-01T14:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:05:57,358] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:57,383] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:57,407] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:05:57,432] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:05:57,550] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:05:57,557] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 15:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:05:57,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.174 seconds
[2020-11-01 19:06:09,816] {scheduler_job.py:155} INFO - Started process (PID=30014) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:09,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:06:09,825] {logging_mixin.py:112} INFO - [2020-11-01 19:06:09,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:10,649] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:10,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:06:10,726] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:10,751] {logging_mixin.py:112} INFO - [2020-11-01 19:06:10,751] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 15:00:00+00:00: scheduled__2020-11-01T15:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:06:10,755] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:10,778] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:10,801] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:06:10,883] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:06:10,890] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:06:10,900] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.084 seconds
[2020-11-01 19:06:23,197] {scheduler_job.py:155} INFO - Started process (PID=30095) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:23,206] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:06:23,207] {logging_mixin.py:112} INFO - [2020-11-01 19:06:23,207] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:24,126] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:24,167] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:06:24,186] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:24,213] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:24,238] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:06:24,324] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:06:24,329] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.132 seconds
[2020-11-01 19:06:36,617] {scheduler_job.py:155} INFO - Started process (PID=30154) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:36,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:06:36,621] {logging_mixin.py:112} INFO - [2020-11-01 19:06:36,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:37,441] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:37,480] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:06:37,501] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:37,527] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:37,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:06:37,624] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:06:37,631] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 16:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:06:37,642] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.025 seconds
[2020-11-01 19:06:50,017] {scheduler_job.py:155} INFO - Started process (PID=30240) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:50,025] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:06:50,026] {logging_mixin.py:112} INFO - [2020-11-01 19:06:50,026] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:50,872] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:06:50,913] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:06:50,932] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:50,959] {logging_mixin.py:112} INFO - [2020-11-01 19:06:50,959] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 16:00:00+00:00: scheduled__2020-11-01T16:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:06:50,966] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:06:51,022] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:06:51,079] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:06:51,084] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.067 seconds
[2020-11-01 19:07:03,422] {scheduler_job.py:155} INFO - Started process (PID=30302) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:03,429] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:07:03,430] {logging_mixin.py:112} INFO - [2020-11-01 19:07:03,430] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:04,463] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:04,536] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:07:04,571] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:07:04,606] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:07:04,687] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:07:04,697] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:56:43.159379+00:00 [scheduled]> in ORM
[2020-11-01 19:07:04,709] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.287 seconds
[2020-11-01 19:07:16,766] {scheduler_job.py:155} INFO - Started process (PID=30391) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:16,782] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:07:16,784] {logging_mixin.py:112} INFO - [2020-11-01 19:07:16,783] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:17,805] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:17,867] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:07:17,895] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:07:17,931] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True>
[2020-11-01 19:07:17,947] {logging_mixin.py:112} INFO - [2020-11-01 19:07:17,946] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 17:56:43.159379+00:00: manual__2020-11-01T17:56:43.159379+00:00, externally triggered: True> failed
[2020-11-01 19:07:17,971] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:07:17,977] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.211 seconds
[2020-11-01 19:07:30,427] {scheduler_job.py:155} INFO - Started process (PID=30454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:30,432] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:07:30,433] {logging_mixin.py:112} INFO - [2020-11-01 19:07:30,432] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:31,292] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:31,341] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:07:31,360] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:07:31,405] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:07:31,409] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.982 seconds
[2020-11-01 19:07:42,852] {scheduler_job.py:155} INFO - Started process (PID=30517) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:42,858] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:07:42,860] {logging_mixin.py:112} INFO - [2020-11-01 19:07:42,859] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:43,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:43,794] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:07:43,830] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:07:43,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:07:43,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.046 seconds
[2020-11-01 19:07:57,299] {scheduler_job.py:155} INFO - Started process (PID=30583) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:57,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:07:57,306] {logging_mixin.py:112} INFO - [2020-11-01 19:07:57,306] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:58,187] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:07:58,227] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:07:58,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:07:58,321] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:07:58,325] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.026 seconds
[2020-11-01 19:08:10,562] {scheduler_job.py:155} INFO - Started process (PID=30651) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:10,572] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:08:10,574] {logging_mixin.py:112} INFO - [2020-11-01 19:08:10,574] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:11,535] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:11,611] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:08:11,631] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:08:11,675] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:08:11,678] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-01 19:08:24,165] {scheduler_job.py:155} INFO - Started process (PID=30713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:24,180] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:08:24,181] {logging_mixin.py:112} INFO - [2020-11-01 19:08:24,181] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:25,221] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:25,261] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:08:25,278] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:08:25,357] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:08:25,366] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 17:00:00+00:00 [scheduled]> in ORM
[2020-11-01 19:08:25,378] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.212 seconds
[2020-11-01 19:08:37,432] {scheduler_job.py:155} INFO - Started process (PID=30801) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:37,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:08:37,452] {logging_mixin.py:112} INFO - [2020-11-01 19:08:37,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:38,435] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:38,522] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:08:38,563] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False>
[2020-11-01 19:08:38,579] {logging_mixin.py:112} INFO - [2020-11-01 19:08:38,578] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 17:00:00+00:00: scheduled__2020-11-01T17:00:00+00:00, externally triggered: False> failed
[2020-11-01 19:08:38,583] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:08:38,590] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.158 seconds
[2020-11-01 19:08:50,883] {scheduler_job.py:155} INFO - Started process (PID=30857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:50,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:08:50,904] {logging_mixin.py:112} INFO - [2020-11-01 19:08:50,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:52,085] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:08:52,129] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:08:52,148] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:08:52,152] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.270 seconds
[2020-11-01 19:09:04,296] {scheduler_job.py:155} INFO - Started process (PID=30914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:04,303] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:09:04,304] {logging_mixin.py:112} INFO - [2020-11-01 19:09:04,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:05,247] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:05,290] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:09:05,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:09:05,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.020 seconds
[2020-11-01 19:09:16,634] {scheduler_job.py:155} INFO - Started process (PID=30980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:16,642] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:09:16,644] {logging_mixin.py:112} INFO - [2020-11-01 19:09:16,643] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:17,508] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:17,553] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:09:17,594] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:09:17,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.969 seconds
[2020-11-01 19:09:29,983] {scheduler_job.py:155} INFO - Started process (PID=31036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:29,994] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:09:29,995] {logging_mixin.py:112} INFO - [2020-11-01 19:09:29,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:30,870] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:30,912] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:09:30,933] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:09:30,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.954 seconds
[2020-11-01 19:09:43,416] {scheduler_job.py:155} INFO - Started process (PID=31104) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:43,423] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:09:43,423] {logging_mixin.py:112} INFO - [2020-11-01 19:09:43,423] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:44,405] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:44,452] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:09:44,476] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:09:44,480] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.065 seconds
[2020-11-01 19:09:57,937] {scheduler_job.py:155} INFO - Started process (PID=31171) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:57,948] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:09:57,949] {logging_mixin.py:112} INFO - [2020-11-01 19:09:57,949] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:59,112] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:09:59,178] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:09:59,207] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:09:59,216] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-01 19:10:11,290] {scheduler_job.py:155} INFO - Started process (PID=31234) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:11,296] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:10:11,296] {logging_mixin.py:112} INFO - [2020-11-01 19:10:11,296] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:12,396] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:12,451] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:10:12,474] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:10:12,481] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.190 seconds
[2020-11-01 19:10:24,637] {scheduler_job.py:155} INFO - Started process (PID=31296) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:24,641] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:10:24,642] {logging_mixin.py:112} INFO - [2020-11-01 19:10:24,642] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:25,489] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:25,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:10:25,553] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:10:25,557] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.920 seconds
[2020-11-01 19:10:37,153] {scheduler_job.py:155} INFO - Started process (PID=31348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:37,161] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:10:37,162] {logging_mixin.py:112} INFO - [2020-11-01 19:10:37,162] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:38,556] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:38,621] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:10:38,656] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:10:38,665] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.512 seconds
[2020-11-01 19:10:51,484] {scheduler_job.py:155} INFO - Started process (PID=31419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:51,492] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:10:51,495] {logging_mixin.py:112} INFO - [2020-11-01 19:10:51,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:52,727] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:10:52,769] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:10:52,789] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:10:52,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-01 19:11:04,807] {scheduler_job.py:155} INFO - Started process (PID=31478) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:04,818] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:11:04,820] {logging_mixin.py:112} INFO - [2020-11-01 19:11:04,820] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:05,804] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:05,860] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:11:05,886] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:11:05,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.084 seconds
[2020-11-01 19:11:18,123] {scheduler_job.py:155} INFO - Started process (PID=31538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:18,140] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:11:18,145] {logging_mixin.py:112} INFO - [2020-11-01 19:11:18,145] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:19,317] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:19,375] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:11:19,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:11:19,403] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-01 19:11:31,649] {scheduler_job.py:155} INFO - Started process (PID=31597) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:31,655] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:11:31,656] {logging_mixin.py:112} INFO - [2020-11-01 19:11:31,656] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:32,559] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:32,602] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:11:32,623] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:11:32,627] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.978 seconds
[2020-11-01 19:11:44,011] {scheduler_job.py:155} INFO - Started process (PID=31659) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:44,025] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:11:44,026] {logging_mixin.py:112} INFO - [2020-11-01 19:11:44,026] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:45,377] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:45,424] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:11:45,446] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:11:45,450] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.440 seconds
[2020-11-01 19:11:58,444] {scheduler_job.py:155} INFO - Started process (PID=31726) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:58,448] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:11:58,449] {logging_mixin.py:112} INFO - [2020-11-01 19:11:58,449] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:59,295] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:11:59,339] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:11:59,362] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:11:59,367] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.923 seconds
[2020-11-01 19:12:10,730] {scheduler_job.py:155} INFO - Started process (PID=31793) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:10,743] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:12:10,744] {logging_mixin.py:112} INFO - [2020-11-01 19:12:10,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:11,963] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:12,001] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:12:12,022] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:12:12,027] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.297 seconds
[2020-11-01 19:12:25,183] {scheduler_job.py:155} INFO - Started process (PID=31861) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:25,194] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:12:25,196] {logging_mixin.py:112} INFO - [2020-11-01 19:12:25,195] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:26,192] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:26,242] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:12:26,265] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:12:26,269] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.087 seconds
[2020-11-01 19:12:38,487] {scheduler_job.py:155} INFO - Started process (PID=31926) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:38,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:12:38,492] {logging_mixin.py:112} INFO - [2020-11-01 19:12:38,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:39,461] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:39,509] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:12:39,535] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:12:39,540] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.053 seconds
[2020-11-01 19:12:51,861] {scheduler_job.py:155} INFO - Started process (PID=31996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:51,869] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:12:51,870] {logging_mixin.py:112} INFO - [2020-11-01 19:12:51,870] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:52,855] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:12:52,917] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:12:52,937] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:12:52,941] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.080 seconds
[2020-11-01 19:13:05,313] {scheduler_job.py:155} INFO - Started process (PID=32051) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:05,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:13:05,328] {logging_mixin.py:112} INFO - [2020-11-01 19:13:05,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:06,449] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:06,514] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:13:06,537] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:13:06,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.229 seconds
[2020-11-01 19:13:18,535] {scheduler_job.py:155} INFO - Started process (PID=32115) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:18,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:13:18,541] {logging_mixin.py:112} INFO - [2020-11-01 19:13:18,541] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:19,947] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:20,019] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:13:20,057] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:13:20,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.531 seconds
[2020-11-01 19:13:32,141] {scheduler_job.py:155} INFO - Started process (PID=32172) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:32,157] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:13:32,159] {logging_mixin.py:112} INFO - [2020-11-01 19:13:32,158] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:33,197] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:33,269] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:13:33,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:13:33,294] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.154 seconds
[2020-11-01 19:13:45,507] {scheduler_job.py:155} INFO - Started process (PID=32235) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:45,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:13:45,514] {logging_mixin.py:112} INFO - [2020-11-01 19:13:45,514] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:46,569] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:46,652] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:13:46,682] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:13:46,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.179 seconds
[2020-11-01 19:13:59,012] {scheduler_job.py:155} INFO - Started process (PID=32294) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:59,018] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:13:59,019] {logging_mixin.py:112} INFO - [2020-11-01 19:13:59,019] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:13:59,971] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:00,028] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:14:00,056] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:14:00,061] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.049 seconds
[2020-11-01 19:14:12,470] {scheduler_job.py:155} INFO - Started process (PID=32358) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:12,476] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:14:12,477] {logging_mixin.py:112} INFO - [2020-11-01 19:14:12,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:13,809] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:13,880] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:14:13,929] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:14:13,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.467 seconds
[2020-11-01 19:14:25,942] {scheduler_job.py:155} INFO - Started process (PID=32420) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:25,952] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:14:25,953] {logging_mixin.py:112} INFO - [2020-11-01 19:14:25,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:26,809] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:26,846] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:14:26,864] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:14:26,868] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.926 seconds
[2020-11-01 19:14:38,302] {scheduler_job.py:155} INFO - Started process (PID=32478) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:38,307] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:14:38,307] {logging_mixin.py:112} INFO - [2020-11-01 19:14:38,307] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:39,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:39,561] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:14:39,601] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:14:39,607] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.306 seconds
[2020-11-01 19:14:52,618] {scheduler_job.py:155} INFO - Started process (PID=32549) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:52,642] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:14:52,652] {logging_mixin.py:112} INFO - [2020-11-01 19:14:52,651] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:53,640] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:14:53,692] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:14:53,711] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:14:53,716] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.098 seconds
[2020-11-01 19:15:06,125] {scheduler_job.py:155} INFO - Started process (PID=32607) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:06,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:15:06,130] {logging_mixin.py:112} INFO - [2020-11-01 19:15:06,129] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:07,579] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:07,633] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:15:07,663] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:15:07,668] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.543 seconds
[2020-11-01 19:15:19,538] {scheduler_job.py:155} INFO - Started process (PID=32669) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:19,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:15:19,552] {logging_mixin.py:112} INFO - [2020-11-01 19:15:19,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:22,342] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:22,410] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:15:22,438] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:15:22,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.906 seconds
[2020-11-01 19:15:32,843] {scheduler_job.py:155} INFO - Started process (PID=32729) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:32,850] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:15:32,851] {logging_mixin.py:112} INFO - [2020-11-01 19:15:32,851] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:33,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:33,981] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:15:34,010] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:15:34,020] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.177 seconds
[2020-11-01 19:15:47,383] {scheduler_job.py:155} INFO - Started process (PID=329) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:47,388] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:15:47,389] {logging_mixin.py:112} INFO - [2020-11-01 19:15:47,388] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:48,429] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:15:48,523] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:15:48,545] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:15:48,550] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.166 seconds
[2020-11-01 19:16:00,630] {scheduler_job.py:155} INFO - Started process (PID=395) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:00,634] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:16:00,635] {logging_mixin.py:112} INFO - [2020-11-01 19:16:00,634] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:01,590] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:01,638] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:16:01,659] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:16:01,663] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.033 seconds
[2020-11-01 19:16:14,130] {scheduler_job.py:155} INFO - Started process (PID=455) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:14,143] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:16:14,145] {logging_mixin.py:112} INFO - [2020-11-01 19:16:14,144] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:15,120] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:15,168] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:16:15,192] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:16:15,197] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.067 seconds
[2020-11-01 19:16:27,498] {scheduler_job.py:155} INFO - Started process (PID=547) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:27,517] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:16:27,518] {logging_mixin.py:112} INFO - [2020-11-01 19:16:27,518] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:28,477] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:28,530] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:16:28,553] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:16:28,559] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-01 19:16:39,851] {scheduler_job.py:155} INFO - Started process (PID=611) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:39,857] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:16:39,858] {logging_mixin.py:112} INFO - [2020-11-01 19:16:39,857] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:41,438] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:41,500] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:16:41,529] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:16:41,535] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.684 seconds
[2020-11-01 19:16:54,265] {scheduler_job.py:155} INFO - Started process (PID=687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:54,276] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:16:54,277] {logging_mixin.py:112} INFO - [2020-11-01 19:16:54,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:55,305] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:16:55,389] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:16:55,417] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:16:55,423] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.158 seconds
[2020-11-01 19:17:07,610] {scheduler_job.py:155} INFO - Started process (PID=753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:07,627] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:17:07,630] {logging_mixin.py:112} INFO - [2020-11-01 19:17:07,629] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:08,826] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:08,868] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:17:08,887] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:17:08,893] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.284 seconds
[2020-11-01 19:17:21,012] {scheduler_job.py:155} INFO - Started process (PID=819) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:21,026] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:17:21,031] {logging_mixin.py:112} INFO - [2020-11-01 19:17:21,031] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:22,043] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:22,135] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:17:22,186] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:17:22,195] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-01 19:17:34,426] {scheduler_job.py:155} INFO - Started process (PID=874) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:34,432] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:17:34,433] {logging_mixin.py:112} INFO - [2020-11-01 19:17:34,432] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:35,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:35,486] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:17:35,515] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:17:35,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.096 seconds
[2020-11-01 19:17:47,782] {scheduler_job.py:155} INFO - Started process (PID=933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:47,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:17:47,790] {logging_mixin.py:112} INFO - [2020-11-01 19:17:47,789] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:49,034] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:17:49,087] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:17:49,116] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:17:49,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.339 seconds
[2020-11-01 19:18:01,316] {scheduler_job.py:155} INFO - Started process (PID=994) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:01,338] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:18:01,340] {logging_mixin.py:112} INFO - [2020-11-01 19:18:01,340] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:02,268] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:02,310] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:18:02,331] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:18:02,335] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.019 seconds
[2020-11-01 19:18:14,757] {scheduler_job.py:155} INFO - Started process (PID=1079) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:14,829] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:18:14,862] {logging_mixin.py:112} INFO - [2020-11-01 19:18:14,861] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:17,799] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:17,888] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:18:17,933] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:18:17,939] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.183 seconds
[2020-11-01 19:18:28,862] {scheduler_job.py:155} INFO - Started process (PID=1161) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:28,866] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:18:28,866] {logging_mixin.py:112} INFO - [2020-11-01 19:18:28,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:29,991] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:30,036] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:18:30,057] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:18:30,062] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.200 seconds
[2020-11-01 19:18:43,155] {scheduler_job.py:155} INFO - Started process (PID=1255) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:43,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:18:43,165] {logging_mixin.py:112} INFO - [2020-11-01 19:18:43,165] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:43,979] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:44,024] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:18:44,042] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:18:44,047] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.891 seconds
[2020-11-01 19:18:55,397] {scheduler_job.py:155} INFO - Started process (PID=1446) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:55,416] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:18:55,421] {logging_mixin.py:112} INFO - [2020-11-01 19:18:55,420] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:56,960] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:18:56,998] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:18:57,017] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:18:57,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.625 seconds
[2020-11-01 19:19:09,679] {scheduler_job.py:155} INFO - Started process (PID=1577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:09,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:19:09,684] {logging_mixin.py:112} INFO - [2020-11-01 19:19:09,683] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:10,495] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:10,537] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:19:10,557] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:19:10,561] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.882 seconds
[2020-11-01 19:19:21,915] {scheduler_job.py:155} INFO - Started process (PID=1687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:21,922] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:19:21,929] {logging_mixin.py:112} INFO - [2020-11-01 19:19:21,924] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:22,976] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:23,106] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:19:23,134] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:19:23,140] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.226 seconds
[2020-11-01 19:19:36,173] {scheduler_job.py:155} INFO - Started process (PID=1833) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:36,177] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:19:36,178] {logging_mixin.py:112} INFO - [2020-11-01 19:19:36,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:37,011] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:37,054] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:19:37,074] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:19:37,079] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.906 seconds
[2020-11-01 19:19:48,365] {scheduler_job.py:155} INFO - Started process (PID=1927) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:48,369] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:19:48,370] {logging_mixin.py:112} INFO - [2020-11-01 19:19:48,370] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:49,144] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:19:49,185] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:19:49,203] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:19:49,208] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.843 seconds
[2020-11-01 19:20:01,557] {scheduler_job.py:155} INFO - Started process (PID=2005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:01,563] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:20:01,564] {logging_mixin.py:112} INFO - [2020-11-01 19:20:01,564] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:02,464] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:02,509] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:20:02,533] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:20:02,537] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-01 19:20:14,876] {scheduler_job.py:155} INFO - Started process (PID=2081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:14,879] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:20:14,880] {logging_mixin.py:112} INFO - [2020-11-01 19:20:14,880] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:15,665] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:15,705] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:20:15,724] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:20:15,729] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.853 seconds
[2020-11-01 19:20:28,074] {scheduler_job.py:155} INFO - Started process (PID=2163) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:28,079] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:20:28,081] {logging_mixin.py:112} INFO - [2020-11-01 19:20:28,081] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:28,931] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:28,970] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:20:28,989] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:20:28,993] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.920 seconds
[2020-11-01 19:20:41,456] {scheduler_job.py:155} INFO - Started process (PID=2246) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:41,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:20:41,461] {logging_mixin.py:112} INFO - [2020-11-01 19:20:41,461] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:42,280] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:42,324] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:20:42,345] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:20:42,350] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.894 seconds
[2020-11-01 19:20:54,733] {scheduler_job.py:155} INFO - Started process (PID=2324) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:54,737] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:20:54,738] {logging_mixin.py:112} INFO - [2020-11-01 19:20:54,738] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:55,658] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:20:55,694] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:20:55,712] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:20:55,715] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.982 seconds
[2020-11-01 19:21:08,077] {scheduler_job.py:155} INFO - Started process (PID=2398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:08,081] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:21:08,082] {logging_mixin.py:112} INFO - [2020-11-01 19:21:08,082] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:08,854] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:08,896] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:21:08,916] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:21:08,921] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.845 seconds
[2020-11-01 19:21:21,335] {scheduler_job.py:155} INFO - Started process (PID=2472) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:21,340] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:21:21,341] {logging_mixin.py:112} INFO - [2020-11-01 19:21:21,340] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:22,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:22,172] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:21:22,191] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:21:22,195] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.860 seconds
[2020-11-01 19:21:34,537] {scheduler_job.py:155} INFO - Started process (PID=2545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:34,565] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:21:34,565] {logging_mixin.py:112} INFO - [2020-11-01 19:21:34,565] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:35,333] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:35,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:21:35,400] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:21:35,404] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 19:21:47,812] {scheduler_job.py:155} INFO - Started process (PID=2622) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:47,818] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:21:47,819] {logging_mixin.py:112} INFO - [2020-11-01 19:21:47,819] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:48,607] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:21:48,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:21:48,662] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:21:48,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.855 seconds
[2020-11-01 19:22:00,949] {scheduler_job.py:155} INFO - Started process (PID=2697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:00,954] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:22:00,955] {logging_mixin.py:112} INFO - [2020-11-01 19:22:00,955] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:01,828] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:01,871] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:22:01,891] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:22:01,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.946 seconds
[2020-11-01 19:22:14,236] {scheduler_job.py:155} INFO - Started process (PID=2771) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:14,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:22:14,240] {logging_mixin.py:112} INFO - [2020-11-01 19:22:14,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:15,041] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:15,081] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:22:15,100] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:22:15,104] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.868 seconds
[2020-11-01 19:22:27,452] {scheduler_job.py:155} INFO - Started process (PID=2854) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:27,463] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:22:27,464] {logging_mixin.py:112} INFO - [2020-11-01 19:22:27,464] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:28,430] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:28,496] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:22:28,554] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:22:28,566] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.114 seconds
[2020-11-01 19:22:41,740] {scheduler_job.py:155} INFO - Started process (PID=2931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:41,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:22:41,753] {logging_mixin.py:112} INFO - [2020-11-01 19:22:41,752] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:42,701] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:42,764] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:22:42,785] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:22:42,789] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.050 seconds
[2020-11-01 19:22:54,935] {scheduler_job.py:155} INFO - Started process (PID=3011) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:54,939] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:22:54,939] {logging_mixin.py:112} INFO - [2020-11-01 19:22:54,939] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:55,826] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:22:55,867] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:22:55,887] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:22:55,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.956 seconds
[2020-11-01 19:23:07,148] {scheduler_job.py:155} INFO - Started process (PID=3076) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:07,152] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:23:07,153] {logging_mixin.py:112} INFO - [2020-11-01 19:23:07,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:08,088] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:08,129] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:23:08,155] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:23:08,159] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.011 seconds
[2020-11-01 19:23:21,408] {scheduler_job.py:155} INFO - Started process (PID=3143) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:21,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:23:21,415] {logging_mixin.py:112} INFO - [2020-11-01 19:23:21,415] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:22,903] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:22,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:23:22,986] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:23:22,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.584 seconds
[2020-11-01 19:23:34,615] {scheduler_job.py:155} INFO - Started process (PID=3204) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:34,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:23:34,622] {logging_mixin.py:112} INFO - [2020-11-01 19:23:34,622] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:35,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:35,538] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:23:35,560] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:23:35,564] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.949 seconds
[2020-11-01 19:23:46,874] {scheduler_job.py:155} INFO - Started process (PID=3273) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:46,879] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:23:46,880] {logging_mixin.py:112} INFO - [2020-11-01 19:23:46,880] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:47,716] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:23:47,756] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:23:47,775] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:23:47,780] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.906 seconds
[2020-11-01 19:24:00,101] {scheduler_job.py:155} INFO - Started process (PID=3346) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:00,114] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:24:00,115] {logging_mixin.py:112} INFO - [2020-11-01 19:24:00,115] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:01,574] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:01,627] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:24:01,653] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:24:01,658] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.557 seconds
[2020-11-01 19:24:14,313] {scheduler_job.py:155} INFO - Started process (PID=3412) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:14,318] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:24:14,319] {logging_mixin.py:112} INFO - [2020-11-01 19:24:14,318] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:15,431] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:15,487] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:24:15,516] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:24:15,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.209 seconds
[2020-11-01 19:24:27,496] {scheduler_job.py:155} INFO - Started process (PID=3486) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:27,501] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:24:27,502] {logging_mixin.py:112} INFO - [2020-11-01 19:24:27,502] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:28,472] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:28,514] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:24:28,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:24:28,552] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.055 seconds
[2020-11-01 19:24:40,766] {scheduler_job.py:155} INFO - Started process (PID=3545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:40,770] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:24:40,771] {logging_mixin.py:112} INFO - [2020-11-01 19:24:40,770] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:41,991] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:42,035] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:24:42,058] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:24:42,063] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.298 seconds
[2020-11-01 19:24:53,954] {scheduler_job.py:155} INFO - Started process (PID=3609) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:53,958] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:24:53,958] {logging_mixin.py:112} INFO - [2020-11-01 19:24:53,958] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:55,175] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:24:55,227] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:24:55,255] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:24:55,262] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-01 19:25:07,128] {scheduler_job.py:155} INFO - Started process (PID=3679) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:07,132] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:25:07,132] {logging_mixin.py:112} INFO - [2020-11-01 19:25:07,132] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:07,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:07,990] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:25:08,018] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:25:08,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.894 seconds
[2020-11-01 19:25:19,396] {scheduler_job.py:155} INFO - Started process (PID=3750) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:19,425] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:25:19,430] {logging_mixin.py:112} INFO - [2020-11-01 19:25:19,430] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:20,475] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:20,541] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:25:20,564] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:25:20,569] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.173 seconds
[2020-11-01 19:25:33,592] {scheduler_job.py:155} INFO - Started process (PID=3824) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:33,597] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:25:33,597] {logging_mixin.py:112} INFO - [2020-11-01 19:25:33,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:34,527] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:34,572] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:25:34,605] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:25:34,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.020 seconds
[2020-11-01 19:25:46,853] {scheduler_job.py:155} INFO - Started process (PID=3892) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:46,856] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:25:46,857] {logging_mixin.py:112} INFO - [2020-11-01 19:25:46,857] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:47,697] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:47,738] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:25:47,758] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:25:47,763] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.910 seconds
[2020-11-01 19:25:59,052] {scheduler_job.py:155} INFO - Started process (PID=3962) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:59,060] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:25:59,060] {logging_mixin.py:112} INFO - [2020-11-01 19:25:59,060] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:25:59,966] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:00,013] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:26:00,045] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:26:00,050] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.998 seconds
[2020-11-01 19:26:12,284] {scheduler_job.py:155} INFO - Started process (PID=4034) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:12,288] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:26:12,288] {logging_mixin.py:112} INFO - [2020-11-01 19:26:12,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:13,418] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:13,468] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:26:13,489] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:26:13,493] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.210 seconds
[2020-11-01 19:26:26,487] {scheduler_job.py:155} INFO - Started process (PID=4107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:26,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:26:26,495] {logging_mixin.py:112} INFO - [2020-11-01 19:26:26,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:27,328] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:27,374] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:26:27,397] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:26:27,401] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.914 seconds
[2020-11-01 19:26:38,651] {scheduler_job.py:155} INFO - Started process (PID=4173) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:38,655] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:26:38,657] {logging_mixin.py:112} INFO - [2020-11-01 19:26:38,656] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:39,595] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:39,643] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:26:39,665] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:26:39,670] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.019 seconds
[2020-11-01 19:26:51,927] {scheduler_job.py:155} INFO - Started process (PID=4267) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:51,932] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:26:51,933] {logging_mixin.py:112} INFO - [2020-11-01 19:26:51,933] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:52,749] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:26:52,790] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:26:52,809] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:26:52,814] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.887 seconds
[2020-11-01 19:27:05,132] {scheduler_job.py:155} INFO - Started process (PID=4343) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:05,137] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:27:05,138] {logging_mixin.py:112} INFO - [2020-11-01 19:27:05,138] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:06,278] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:06,338] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:27:06,366] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:27:06,380] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-01 19:27:19,373] {scheduler_job.py:155} INFO - Started process (PID=4418) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:19,380] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:27:19,381] {logging_mixin.py:112} INFO - [2020-11-01 19:27:19,380] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:20,343] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:20,400] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:27:20,427] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:27:20,432] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.059 seconds
[2020-11-01 19:27:32,610] {scheduler_job.py:155} INFO - Started process (PID=4492) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:32,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:27:32,633] {logging_mixin.py:112} INFO - [2020-11-01 19:27:32,633] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:33,622] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:33,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:27:33,752] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:27:33,757] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.147 seconds
[2020-11-01 19:27:45,815] {scheduler_job.py:155} INFO - Started process (PID=4563) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:45,821] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:27:45,822] {logging_mixin.py:112} INFO - [2020-11-01 19:27:45,822] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:46,709] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:46,750] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:27:46,769] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:27:46,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.958 seconds
[2020-11-01 19:27:58,046] {scheduler_job.py:155} INFO - Started process (PID=4636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:58,050] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:27:58,051] {logging_mixin.py:112} INFO - [2020-11-01 19:27:58,051] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:58,897] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:27:58,937] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:27:58,957] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:27:58,961] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.915 seconds
[2020-11-01 19:28:11,214] {scheduler_job.py:155} INFO - Started process (PID=4705) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:11,219] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:28:11,220] {logging_mixin.py:112} INFO - [2020-11-01 19:28:11,220] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:12,125] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:12,166] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:28:12,185] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:28:12,189] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.976 seconds
[2020-11-01 19:28:24,450] {scheduler_job.py:155} INFO - Started process (PID=4782) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:24,454] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:28:24,454] {logging_mixin.py:112} INFO - [2020-11-01 19:28:24,454] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:25,226] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:25,269] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:28:25,289] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:28:25,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.843 seconds
[2020-11-01 19:28:37,644] {scheduler_job.py:155} INFO - Started process (PID=4855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:37,649] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:28:37,649] {logging_mixin.py:112} INFO - [2020-11-01 19:28:37,649] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:38,475] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:38,529] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:28:38,553] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:28:38,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.914 seconds
[2020-11-01 19:28:50,924] {scheduler_job.py:155} INFO - Started process (PID=4929) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:50,928] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:28:50,928] {logging_mixin.py:112} INFO - [2020-11-01 19:28:50,928] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:51,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:28:52,000] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:28:52,025] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:28:52,031] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.107 seconds
[2020-11-01 19:29:05,106] {scheduler_job.py:155} INFO - Started process (PID=5013) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:05,121] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:29:05,122] {logging_mixin.py:112} INFO - [2020-11-01 19:29:05,122] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:06,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:06,634] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:29:06,681] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:29:06,689] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.584 seconds
[2020-11-01 19:29:18,323] {scheduler_job.py:155} INFO - Started process (PID=5089) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:18,327] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:29:18,327] {logging_mixin.py:112} INFO - [2020-11-01 19:29:18,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:19,240] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:19,285] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:29:19,304] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:29:19,308] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.986 seconds
[2020-11-01 19:29:30,487] {scheduler_job.py:155} INFO - Started process (PID=5155) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:30,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:29:30,492] {logging_mixin.py:112} INFO - [2020-11-01 19:29:30,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:31,328] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:31,383] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:29:31,414] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:29:31,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.931 seconds
[2020-11-01 19:29:43,828] {scheduler_job.py:155} INFO - Started process (PID=5226) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:43,836] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:29:43,837] {logging_mixin.py:112} INFO - [2020-11-01 19:29:43,837] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:45,248] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:45,298] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:29:45,324] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:29:45,332] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.504 seconds
[2020-11-01 19:29:58,108] {scheduler_job.py:155} INFO - Started process (PID=5291) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:58,112] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:29:58,113] {logging_mixin.py:112} INFO - [2020-11-01 19:29:58,113] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:59,238] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:29:59,291] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:29:59,320] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:29:59,324] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.216 seconds
[2020-11-01 19:30:11,305] {scheduler_job.py:155} INFO - Started process (PID=5356) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:11,309] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:30:11,310] {logging_mixin.py:112} INFO - [2020-11-01 19:30:11,310] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:12,427] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:12,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:30:12,500] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:30:12,504] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.199 seconds
[2020-11-01 19:30:24,535] {scheduler_job.py:155} INFO - Started process (PID=5426) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:24,539] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:30:24,540] {logging_mixin.py:112} INFO - [2020-11-01 19:30:24,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:25,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:25,528] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:30:25,562] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:30:25,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.032 seconds
[2020-11-01 19:30:37,769] {scheduler_job.py:155} INFO - Started process (PID=5493) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:37,788] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:30:37,793] {logging_mixin.py:112} INFO - [2020-11-01 19:30:37,792] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:39,295] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:39,367] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:30:39,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:30:39,403] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.635 seconds
[2020-11-01 19:30:51,034] {scheduler_job.py:155} INFO - Started process (PID=5553) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:51,038] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:30:51,039] {logging_mixin.py:112} INFO - [2020-11-01 19:30:51,039] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:52,129] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:30:52,247] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:30:52,287] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:30:52,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.259 seconds
[2020-11-01 19:31:04,218] {scheduler_job.py:155} INFO - Started process (PID=5626) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:04,235] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:31:04,238] {logging_mixin.py:112} INFO - [2020-11-01 19:31:04,237] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:05,444] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:05,495] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:31:05,533] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:31:05,539] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.321 seconds
[2020-11-01 19:31:17,474] {scheduler_job.py:155} INFO - Started process (PID=5690) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:17,481] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:31:17,482] {logging_mixin.py:112} INFO - [2020-11-01 19:31:17,482] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:18,784] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:18,878] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:31:18,913] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:31:18,927] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.452 seconds
[2020-11-01 19:31:30,747] {scheduler_job.py:155} INFO - Started process (PID=5751) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:30,755] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:31:30,756] {logging_mixin.py:112} INFO - [2020-11-01 19:31:30,756] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:31,893] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:31,949] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:31:31,972] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:31:31,981] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.234 seconds
[2020-11-01 19:31:44,008] {scheduler_job.py:155} INFO - Started process (PID=5810) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:44,015] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:31:44,016] {logging_mixin.py:112} INFO - [2020-11-01 19:31:44,016] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:45,016] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:45,090] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:31:45,118] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:31:45,127] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.119 seconds
[2020-11-01 19:31:57,319] {scheduler_job.py:155} INFO - Started process (PID=5878) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:57,323] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:31:57,324] {logging_mixin.py:112} INFO - [2020-11-01 19:31:57,324] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:58,211] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:31:58,253] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:31:58,273] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:31:58,277] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.959 seconds
[2020-11-01 19:32:09,569] {scheduler_job.py:155} INFO - Started process (PID=5942) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:09,575] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:32:09,575] {logging_mixin.py:112} INFO - [2020-11-01 19:32:09,575] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:10,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:10,886] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:32:10,912] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:32:10,916] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-01 19:32:23,908] {scheduler_job.py:155} INFO - Started process (PID=6021) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:23,916] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:32:23,917] {logging_mixin.py:112} INFO - [2020-11-01 19:32:23,917] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:25,120] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:25,165] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:32:25,185] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:32:25,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.282 seconds
[2020-11-01 19:32:37,185] {scheduler_job.py:155} INFO - Started process (PID=6086) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:37,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:32:37,189] {logging_mixin.py:112} INFO - [2020-11-01 19:32:37,189] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:38,329] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:38,396] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:32:38,421] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:32:38,426] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.241 seconds
[2020-11-01 19:32:50,394] {scheduler_job.py:155} INFO - Started process (PID=6149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:50,401] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:32:50,403] {logging_mixin.py:112} INFO - [2020-11-01 19:32:50,403] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:51,489] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:32:51,575] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:32:51,597] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:32:51,601] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.207 seconds
[2020-11-01 19:33:03,617] {scheduler_job.py:155} INFO - Started process (PID=6214) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:03,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:33:03,622] {logging_mixin.py:112} INFO - [2020-11-01 19:33:03,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:04,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:04,540] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:33:04,561] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:33:04,565] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.948 seconds
[2020-11-01 19:33:15,872] {scheduler_job.py:155} INFO - Started process (PID=6276) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:15,876] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:33:15,876] {logging_mixin.py:112} INFO - [2020-11-01 19:33:15,876] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:16,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:16,996] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:33:17,025] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:33:17,031] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.159 seconds
[2020-11-01 19:33:30,235] {scheduler_job.py:155} INFO - Started process (PID=6352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:30,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:33:30,241] {logging_mixin.py:112} INFO - [2020-11-01 19:33:30,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:31,217] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:31,269] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:33:31,292] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:33:31,296] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-01 19:33:43,497] {scheduler_job.py:155} INFO - Started process (PID=6417) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:43,508] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:33:43,512] {logging_mixin.py:112} INFO - [2020-11-01 19:33:43,512] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:44,540] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:44,631] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:33:44,687] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:33:44,691] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.195 seconds
[2020-11-01 19:33:56,729] {scheduler_job.py:155} INFO - Started process (PID=6481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:56,734] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:33:56,735] {logging_mixin.py:112} INFO - [2020-11-01 19:33:56,735] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:57,750] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:33:57,813] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:33:57,845] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:33:57,851] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.122 seconds
[2020-11-01 19:34:09,941] {scheduler_job.py:155} INFO - Started process (PID=6558) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:09,955] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:34:09,956] {logging_mixin.py:112} INFO - [2020-11-01 19:34:09,956] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:11,141] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:11,184] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:34:11,207] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:34:11,211] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.270 seconds
[2020-11-01 19:34:23,242] {scheduler_job.py:155} INFO - Started process (PID=6626) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:23,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:34:23,247] {logging_mixin.py:112} INFO - [2020-11-01 19:34:23,247] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:24,088] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:24,132] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:34:24,152] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:34:24,157] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.915 seconds
[2020-11-01 19:34:35,445] {scheduler_job.py:155} INFO - Started process (PID=6690) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:35,449] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:34:35,450] {logging_mixin.py:112} INFO - [2020-11-01 19:34:35,450] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:36,428] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:36,495] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:34:36,539] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:34:36,546] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.102 seconds
[2020-11-01 19:34:49,737] {scheduler_job.py:155} INFO - Started process (PID=6764) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:49,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:34:49,743] {logging_mixin.py:112} INFO - [2020-11-01 19:34:49,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:50,643] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:34:50,685] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:34:50,705] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:34:50,710] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.973 seconds
[2020-11-01 19:35:01,952] {scheduler_job.py:155} INFO - Started process (PID=6835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:01,956] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:35:01,956] {logging_mixin.py:112} INFO - [2020-11-01 19:35:01,956] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:02,761] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:02,803] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:35:02,823] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:35:02,828] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.875 seconds
[2020-11-01 19:35:15,181] {scheduler_job.py:155} INFO - Started process (PID=6910) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:15,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:35:15,187] {logging_mixin.py:112} INFO - [2020-11-01 19:35:15,187] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:16,010] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:16,050] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:35:16,070] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:35:16,074] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.893 seconds
[2020-11-01 19:35:28,381] {scheduler_job.py:155} INFO - Started process (PID=6982) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:28,385] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:35:28,386] {logging_mixin.py:112} INFO - [2020-11-01 19:35:28,385] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:29,177] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:29,219] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:35:29,238] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:35:29,242] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.862 seconds
[2020-11-01 19:35:41,596] {scheduler_job.py:155} INFO - Started process (PID=7054) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:41,599] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:35:41,599] {logging_mixin.py:112} INFO - [2020-11-01 19:35:41,599] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:42,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:42,726] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:35:42,750] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:35:42,755] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.159 seconds
[2020-11-01 19:35:55,830] {scheduler_job.py:155} INFO - Started process (PID=7128) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:55,837] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:35:55,838] {logging_mixin.py:112} INFO - [2020-11-01 19:35:55,838] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:56,669] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:35:56,718] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:35:56,740] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:35:56,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.915 seconds
[2020-11-01 19:36:08,069] {scheduler_job.py:155} INFO - Started process (PID=7194) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:08,073] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:36:08,073] {logging_mixin.py:112} INFO - [2020-11-01 19:36:08,073] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:08,872] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:08,912] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:36:08,932] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:36:08,936] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 19:36:21,308] {scheduler_job.py:155} INFO - Started process (PID=7264) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:21,314] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:36:21,314] {logging_mixin.py:112} INFO - [2020-11-01 19:36:21,314] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:22,223] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:22,267] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:36:22,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:36:22,294] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.986 seconds
[2020-11-01 19:36:34,547] {scheduler_job.py:155} INFO - Started process (PID=7333) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:34,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:36:34,552] {logging_mixin.py:112} INFO - [2020-11-01 19:36:34,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:35,415] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:35,463] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:36:35,487] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:36:35,492] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.945 seconds
[2020-11-01 19:36:47,775] {scheduler_job.py:155} INFO - Started process (PID=7408) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:47,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:36:47,786] {logging_mixin.py:112} INFO - [2020-11-01 19:36:47,785] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:49,422] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:36:49,468] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:36:49,492] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:36:49,499] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.723 seconds
[2020-11-01 19:37:02,005] {scheduler_job.py:155} INFO - Started process (PID=7476) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:02,011] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:37:02,011] {logging_mixin.py:112} INFO - [2020-11-01 19:37:02,011] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:02,915] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:02,951] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:37:02,971] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:37:02,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.970 seconds
[2020-11-01 19:37:14,240] {scheduler_job.py:155} INFO - Started process (PID=7545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:14,244] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:37:14,245] {logging_mixin.py:112} INFO - [2020-11-01 19:37:14,245] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:15,676] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:15,737] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:37:15,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:37:15,781] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.541 seconds
[2020-11-01 19:37:28,449] {scheduler_job.py:155} INFO - Started process (PID=7633) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:28,453] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:37:28,454] {logging_mixin.py:112} INFO - [2020-11-01 19:37:28,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:29,454] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:29,521] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:37:29,562] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:37:29,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.119 seconds
[2020-11-01 19:37:41,677] {scheduler_job.py:155} INFO - Started process (PID=7705) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:41,684] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:37:41,685] {logging_mixin.py:112} INFO - [2020-11-01 19:37:41,685] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:43,022] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:43,082] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:37:43,104] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:37:43,109] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.431 seconds
[2020-11-01 19:37:54,995] {scheduler_job.py:155} INFO - Started process (PID=7775) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:54,999] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:37:54,999] {logging_mixin.py:112} INFO - [2020-11-01 19:37:54,999] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:55,949] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:37:56,011] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:37:56,048] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:37:56,057] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-01 19:38:08,217] {scheduler_job.py:155} INFO - Started process (PID=7844) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:08,224] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:38:08,225] {logging_mixin.py:112} INFO - [2020-11-01 19:38:08,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:09,070] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:09,131] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:38:09,164] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:38:09,173] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.955 seconds
[2020-11-01 19:38:20,362] {scheduler_job.py:155} INFO - Started process (PID=7913) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:20,366] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:38:20,367] {logging_mixin.py:112} INFO - [2020-11-01 19:38:20,367] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:21,196] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:21,238] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:38:21,258] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:38:21,262] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.901 seconds
[2020-11-01 19:38:33,602] {scheduler_job.py:155} INFO - Started process (PID=7984) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:33,606] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:38:33,607] {logging_mixin.py:112} INFO - [2020-11-01 19:38:33,607] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:34,439] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:34,484] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:38:34,516] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:38:34,521] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.919 seconds
[2020-11-01 19:38:46,817] {scheduler_job.py:155} INFO - Started process (PID=8060) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:46,837] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:38:46,839] {logging_mixin.py:112} INFO - [2020-11-01 19:38:46,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:47,862] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:38:47,903] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:38:47,922] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:38:47,927] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.110 seconds
[2020-11-01 19:39:01,014] {scheduler_job.py:155} INFO - Started process (PID=8133) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:01,019] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:39:01,020] {logging_mixin.py:112} INFO - [2020-11-01 19:39:01,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:01,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:01,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:39:01,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:39:01,896] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.882 seconds
[2020-11-01 19:39:13,250] {scheduler_job.py:155} INFO - Started process (PID=8201) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:13,254] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:39:13,254] {logging_mixin.py:112} INFO - [2020-11-01 19:39:13,254] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:14,584] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:14,623] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:39:14,642] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:39:14,647] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.397 seconds
[2020-11-01 19:39:27,508] {scheduler_job.py:155} INFO - Started process (PID=8274) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:27,512] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:39:27,513] {logging_mixin.py:112} INFO - [2020-11-01 19:39:27,512] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:28,332] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:28,374] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:39:28,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:39:28,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.894 seconds
[2020-11-01 19:39:39,693] {scheduler_job.py:155} INFO - Started process (PID=8342) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:39,698] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:39:39,699] {logging_mixin.py:112} INFO - [2020-11-01 19:39:39,699] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:40,640] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:40,681] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:39:40,708] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:39:40,713] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.019 seconds
[2020-11-01 19:39:53,941] {scheduler_job.py:155} INFO - Started process (PID=8422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:53,945] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:39:53,946] {logging_mixin.py:112} INFO - [2020-11-01 19:39:53,945] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:54,851] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:39:54,899] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:39:54,927] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:39:54,932] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.991 seconds
[2020-11-01 19:40:06,211] {scheduler_job.py:155} INFO - Started process (PID=8490) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:06,214] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:40:06,215] {logging_mixin.py:112} INFO - [2020-11-01 19:40:06,215] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:07,046] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:07,111] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:40:07,145] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:40:07,151] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.941 seconds
[2020-11-01 19:40:19,388] {scheduler_job.py:155} INFO - Started process (PID=8564) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:19,393] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:40:19,394] {logging_mixin.py:112} INFO - [2020-11-01 19:40:19,394] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:20,287] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:20,326] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:40:20,346] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:40:20,350] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.962 seconds
[2020-11-01 19:40:32,627] {scheduler_job.py:155} INFO - Started process (PID=8635) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:32,632] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:40:32,632] {logging_mixin.py:112} INFO - [2020-11-01 19:40:32,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:33,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:33,481] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:40:33,503] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:40:33,519] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.892 seconds
[2020-11-01 19:40:45,883] {scheduler_job.py:155} INFO - Started process (PID=8710) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:45,894] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:40:45,895] {logging_mixin.py:112} INFO - [2020-11-01 19:40:45,895] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:46,909] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:40:46,981] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:40:47,015] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:40:47,019] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.136 seconds
[2020-11-01 19:41:00,112] {scheduler_job.py:155} INFO - Started process (PID=8783) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:00,116] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:41:00,116] {logging_mixin.py:112} INFO - [2020-11-01 19:41:00,116] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:01,172] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:01,248] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:41:01,292] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:41:01,297] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.184 seconds
[2020-11-01 19:41:13,388] {scheduler_job.py:155} INFO - Started process (PID=8842) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:13,397] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:41:13,398] {logging_mixin.py:112} INFO - [2020-11-01 19:41:13,398] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:14,242] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:14,293] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:41:14,317] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:41:14,325] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.937 seconds
[2020-11-01 19:41:25,728] {scheduler_job.py:155} INFO - Started process (PID=8903) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:25,756] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:41:25,761] {logging_mixin.py:112} INFO - [2020-11-01 19:41:25,761] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:27,244] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:27,309] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:41:27,340] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:41:27,346] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.618 seconds
[2020-11-01 19:41:40,123] {scheduler_job.py:155} INFO - Started process (PID=8973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:40,132] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:41:40,133] {logging_mixin.py:112} INFO - [2020-11-01 19:41:40,133] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:40,941] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:40,983] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:41:41,002] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:41:41,008] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.885 seconds
[2020-11-01 19:41:52,345] {scheduler_job.py:155} INFO - Started process (PID=9043) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:52,352] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:41:52,353] {logging_mixin.py:112} INFO - [2020-11-01 19:41:52,353] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:53,322] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:41:53,365] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:41:53,391] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:41:53,397] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.052 seconds
[2020-11-01 19:42:06,567] {scheduler_job.py:155} INFO - Started process (PID=9119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:06,571] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:42:06,572] {logging_mixin.py:112} INFO - [2020-11-01 19:42:06,571] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:07,439] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:07,489] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:42:07,510] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:42:07,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.950 seconds
[2020-11-01 19:42:18,883] {scheduler_job.py:155} INFO - Started process (PID=9190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:18,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:42:18,888] {logging_mixin.py:112} INFO - [2020-11-01 19:42:18,888] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:20,331] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:20,371] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:42:20,393] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:42:20,397] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.515 seconds
[2020-11-01 19:42:33,134] {scheduler_job.py:155} INFO - Started process (PID=9267) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:33,139] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:42:33,140] {logging_mixin.py:112} INFO - [2020-11-01 19:42:33,140] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:33,965] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:34,005] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:42:34,026] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:42:34,031] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.897 seconds
[2020-11-01 19:42:45,407] {scheduler_job.py:155} INFO - Started process (PID=9332) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:45,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:42:45,416] {logging_mixin.py:112} INFO - [2020-11-01 19:42:45,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:46,424] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:46,476] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:42:46,500] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:42:46,505] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.098 seconds
[2020-11-01 19:42:59,598] {scheduler_job.py:155} INFO - Started process (PID=9411) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:42:59,602] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:42:59,603] {logging_mixin.py:112} INFO - [2020-11-01 19:42:59,603] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:00,443] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:00,485] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:43:00,504] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:43:00,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.910 seconds
[2020-11-01 19:43:11,824] {scheduler_job.py:155} INFO - Started process (PID=9477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:11,828] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:43:11,829] {logging_mixin.py:112} INFO - [2020-11-01 19:43:11,829] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:12,710] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:12,777] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:43:12,808] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:43:12,821] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.997 seconds
[2020-11-01 19:43:25,025] {scheduler_job.py:155} INFO - Started process (PID=9551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:25,030] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:43:25,031] {logging_mixin.py:112} INFO - [2020-11-01 19:43:25,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:25,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:25,919] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:43:25,938] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:43:25,943] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.918 seconds
[2020-11-01 19:43:38,276] {scheduler_job.py:155} INFO - Started process (PID=9626) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:38,280] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:43:38,280] {logging_mixin.py:112} INFO - [2020-11-01 19:43:38,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:39,131] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:39,174] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:43:39,197] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:43:39,201] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.925 seconds
[2020-11-01 19:43:51,532] {scheduler_job.py:155} INFO - Started process (PID=9700) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:51,535] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:43:51,536] {logging_mixin.py:112} INFO - [2020-11-01 19:43:51,536] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:52,467] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:43:52,532] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:43:52,554] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:43:52,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.026 seconds
[2020-11-01 19:44:05,768] {scheduler_job.py:155} INFO - Started process (PID=9772) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:05,772] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:44:05,773] {logging_mixin.py:112} INFO - [2020-11-01 19:44:05,772] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:06,771] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:06,813] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:44:06,832] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:44:06,836] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.069 seconds
[2020-11-01 19:44:19,132] {scheduler_job.py:155} INFO - Started process (PID=9845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:19,136] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:44:19,136] {logging_mixin.py:112} INFO - [2020-11-01 19:44:19,136] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:20,118] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:20,197] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:44:20,240] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:44:20,254] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.123 seconds
[2020-11-01 19:44:32,332] {scheduler_job.py:155} INFO - Started process (PID=9916) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:32,340] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:44:32,341] {logging_mixin.py:112} INFO - [2020-11-01 19:44:32,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:33,348] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:33,427] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:44:33,464] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:44:33,468] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.136 seconds
[2020-11-01 19:44:45,561] {scheduler_job.py:155} INFO - Started process (PID=9997) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:45,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:44:45,592] {logging_mixin.py:112} INFO - [2020-11-01 19:44:45,591] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:46,426] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:46,477] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:44:46,498] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:44:46,502] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.941 seconds
[2020-11-01 19:44:57,778] {scheduler_job.py:155} INFO - Started process (PID=10062) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:57,781] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:44:57,782] {logging_mixin.py:112} INFO - [2020-11-01 19:44:57,782] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:58,583] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:44:58,624] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:44:58,644] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:44:58,649] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.871 seconds
[2020-11-01 19:45:11,024] {scheduler_job.py:155} INFO - Started process (PID=10136) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:11,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:45:11,030] {logging_mixin.py:112} INFO - [2020-11-01 19:45:11,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:11,966] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:12,010] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:45:12,037] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:45:12,042] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.018 seconds
[2020-11-01 19:45:25,234] {scheduler_job.py:155} INFO - Started process (PID=10209) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:25,238] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:45:25,240] {logging_mixin.py:112} INFO - [2020-11-01 19:45:25,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:26,074] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:26,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:45:26,142] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:45:26,147] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.914 seconds
[2020-11-01 19:45:37,479] {scheduler_job.py:155} INFO - Started process (PID=10271) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:37,485] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:45:37,486] {logging_mixin.py:112} INFO - [2020-11-01 19:45:37,486] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:38,593] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:38,645] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:45:38,668] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:45:38,671] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.192 seconds
[2020-11-01 19:45:51,723] {scheduler_job.py:155} INFO - Started process (PID=10347) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:51,727] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:45:51,728] {logging_mixin.py:112} INFO - [2020-11-01 19:45:51,728] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:52,637] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:45:52,678] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:45:52,696] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:45:52,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.977 seconds
[2020-11-01 19:46:03,944] {scheduler_job.py:155} INFO - Started process (PID=10412) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:03,951] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:46:03,952] {logging_mixin.py:112} INFO - [2020-11-01 19:46:03,952] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:04,869] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:04,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:46:04,956] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:46:04,961] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.017 seconds
[2020-11-01 19:46:18,156] {scheduler_job.py:155} INFO - Started process (PID=10481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:18,161] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:46:18,161] {logging_mixin.py:112} INFO - [2020-11-01 19:46:18,161] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:19,080] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:19,120] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:46:19,139] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:46:19,143] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.988 seconds
[2020-11-01 19:46:30,351] {scheduler_job.py:155} INFO - Started process (PID=10551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:30,355] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:46:30,355] {logging_mixin.py:112} INFO - [2020-11-01 19:46:30,355] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:31,220] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:31,287] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:46:31,314] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:46:31,319] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.968 seconds
[2020-11-01 19:46:43,572] {scheduler_job.py:155} INFO - Started process (PID=10623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:43,576] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:46:43,576] {logging_mixin.py:112} INFO - [2020-11-01 19:46:43,576] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:44,352] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:44,395] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:46:44,418] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:46:44,422] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.850 seconds
[2020-11-01 19:46:56,772] {scheduler_job.py:155} INFO - Started process (PID=10697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:56,776] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:46:56,776] {logging_mixin.py:112} INFO - [2020-11-01 19:46:56,776] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:57,649] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:46:57,708] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:46:57,730] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:46:57,735] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.963 seconds
[2020-11-01 19:47:10,023] {scheduler_job.py:155} INFO - Started process (PID=10766) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:10,028] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:47:10,029] {logging_mixin.py:112} INFO - [2020-11-01 19:47:10,029] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:10,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:10,986] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:47:11,006] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:47:11,010] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.986 seconds
[2020-11-01 19:47:23,281] {scheduler_job.py:155} INFO - Started process (PID=10841) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:23,287] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:47:23,288] {logging_mixin.py:112} INFO - [2020-11-01 19:47:23,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:24,651] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:24,734] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:47:24,770] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:47:24,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.503 seconds
[2020-11-01 19:47:37,475] {scheduler_job.py:155} INFO - Started process (PID=10919) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:37,480] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:47:37,481] {logging_mixin.py:112} INFO - [2020-11-01 19:47:37,481] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:38,331] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:38,380] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:47:38,406] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:47:38,411] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.935 seconds
[2020-11-01 19:47:49,717] {scheduler_job.py:155} INFO - Started process (PID=10982) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:49,723] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:47:49,727] {logging_mixin.py:112} INFO - [2020-11-01 19:47:49,727] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:50,534] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:47:50,578] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:47:50,597] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:47:50,601] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.884 seconds
[2020-11-01 19:48:02,916] {scheduler_job.py:155} INFO - Started process (PID=11056) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:02,938] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:48:02,939] {logging_mixin.py:112} INFO - [2020-11-01 19:48:02,939] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:03,992] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:04,077] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:48:04,105] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:48:04,111] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.195 seconds
[2020-11-01 19:48:17,159] {scheduler_job.py:155} INFO - Started process (PID=11126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:17,163] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:48:17,164] {logging_mixin.py:112} INFO - [2020-11-01 19:48:17,164] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:18,017] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:18,061] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:48:18,080] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:48:18,084] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.926 seconds
[2020-11-01 19:48:29,382] {scheduler_job.py:155} INFO - Started process (PID=11191) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:29,387] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:48:29,387] {logging_mixin.py:112} INFO - [2020-11-01 19:48:29,387] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:30,489] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:30,535] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:48:30,555] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:48:30,559] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.177 seconds
[2020-11-01 19:48:43,623] {scheduler_job.py:155} INFO - Started process (PID=11263) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:43,628] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:48:43,628] {logging_mixin.py:112} INFO - [2020-11-01 19:48:43,628] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:44,476] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:44,523] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:48:44,543] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:48:44,547] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.924 seconds
[2020-11-01 19:48:55,869] {scheduler_job.py:155} INFO - Started process (PID=11335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:55,876] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:48:55,885] {logging_mixin.py:112} INFO - [2020-11-01 19:48:55,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:57,178] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:48:57,240] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:48:57,266] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:48:57,272] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.403 seconds
[2020-11-01 19:49:10,137] {scheduler_job.py:155} INFO - Started process (PID=11405) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:10,141] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:49:10,142] {logging_mixin.py:112} INFO - [2020-11-01 19:49:10,142] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:11,044] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:11,088] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:49:11,109] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:49:11,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.976 seconds
[2020-11-01 19:49:22,331] {scheduler_job.py:155} INFO - Started process (PID=11479) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:22,344] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:49:22,345] {logging_mixin.py:112} INFO - [2020-11-01 19:49:22,345] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:23,562] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:23,615] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:49:23,636] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:49:23,640] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-01 19:49:36,593] {scheduler_job.py:155} INFO - Started process (PID=11551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:36,600] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:49:36,601] {logging_mixin.py:112} INFO - [2020-11-01 19:49:36,601] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:37,405] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:37,451] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:49:37,474] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:49:37,479] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.886 seconds
[2020-11-01 19:49:48,873] {scheduler_job.py:155} INFO - Started process (PID=11620) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:48,882] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:49:48,896] {logging_mixin.py:112} INFO - [2020-11-01 19:49:48,896] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:50,062] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:49:50,137] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:49:50,177] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:49:50,186] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.313 seconds
[2020-11-01 19:50:03,207] {scheduler_job.py:155} INFO - Started process (PID=11688) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:03,212] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:50:03,213] {logging_mixin.py:112} INFO - [2020-11-01 19:50:03,213] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:04,112] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:04,149] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:50:04,169] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:50:04,173] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.966 seconds
[2020-11-01 19:50:15,479] {scheduler_job.py:155} INFO - Started process (PID=11757) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:15,483] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:50:15,483] {logging_mixin.py:112} INFO - [2020-11-01 19:50:15,483] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:16,275] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:16,317] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:50:16,336] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:50:16,341] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.862 seconds
[2020-11-01 19:50:28,672] {scheduler_job.py:155} INFO - Started process (PID=11832) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:28,680] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:50:28,681] {logging_mixin.py:112} INFO - [2020-11-01 19:50:28,681] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:29,859] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:29,905] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:50:29,930] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:50:29,935] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.263 seconds
[2020-11-01 19:50:42,895] {scheduler_job.py:155} INFO - Started process (PID=11900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:42,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:50:42,902] {logging_mixin.py:112} INFO - [2020-11-01 19:50:42,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:43,958] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:43,998] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:50:44,030] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:50:44,037] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.142 seconds
[2020-11-01 19:50:56,074] {scheduler_job.py:155} INFO - Started process (PID=11973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:56,079] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:50:56,080] {logging_mixin.py:112} INFO - [2020-11-01 19:50:56,079] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:57,448] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:50:57,498] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:50:57,527] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:50:57,533] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.459 seconds
[2020-11-01 19:51:09,355] {scheduler_job.py:155} INFO - Started process (PID=12037) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:09,361] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:51:09,361] {logging_mixin.py:112} INFO - [2020-11-01 19:51:09,361] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:10,271] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:10,315] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:51:10,334] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:51:10,338] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.983 seconds
[2020-11-01 19:51:21,564] {scheduler_job.py:155} INFO - Started process (PID=12098) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:21,569] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:51:21,570] {logging_mixin.py:112} INFO - [2020-11-01 19:51:21,570] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:22,415] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:22,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:51:22,476] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:51:22,480] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.916 seconds
[2020-11-01 19:51:34,786] {scheduler_job.py:155} INFO - Started process (PID=12170) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:34,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:51:34,792] {logging_mixin.py:112} INFO - [2020-11-01 19:51:34,792] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:35,908] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:35,955] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:51:35,978] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:51:35,982] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.197 seconds
[2020-11-01 19:51:48,992] {scheduler_job.py:155} INFO - Started process (PID=12233) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:48,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:51:48,998] {logging_mixin.py:112} INFO - [2020-11-01 19:51:48,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:50,262] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:51:50,326] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:51:50,353] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:51:50,360] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.368 seconds
[2020-11-01 19:52:02,218] {scheduler_job.py:155} INFO - Started process (PID=12292) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:02,232] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:52:02,234] {logging_mixin.py:112} INFO - [2020-11-01 19:52:02,233] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:03,574] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:03,632] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:52:03,658] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:52:03,664] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.447 seconds
[2020-11-01 19:52:15,408] {scheduler_job.py:155} INFO - Started process (PID=12343) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:15,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:52:15,414] {logging_mixin.py:112} INFO - [2020-11-01 19:52:15,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:16,540] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:16,608] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:52:16,635] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:52:16,640] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-01 19:52:28,636] {scheduler_job.py:155} INFO - Started process (PID=12403) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:28,640] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:52:28,646] {logging_mixin.py:112} INFO - [2020-11-01 19:52:28,646] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:30,281] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:30,368] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:52:30,405] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:52:30,413] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.777 seconds
[2020-11-01 19:52:41,954] {scheduler_job.py:155} INFO - Started process (PID=12459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:41,962] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:52:41,964] {logging_mixin.py:112} INFO - [2020-11-01 19:52:41,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:43,008] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:43,056] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:52:43,082] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:52:43,088] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.134 seconds
[2020-11-01 19:52:55,163] {scheduler_job.py:155} INFO - Started process (PID=12511) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:55,167] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:52:55,168] {logging_mixin.py:112} INFO - [2020-11-01 19:52:55,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:56,051] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:52:56,098] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:52:56,126] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:52:56,131] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.969 seconds
[2020-11-01 19:53:07,360] {scheduler_job.py:155} INFO - Started process (PID=12571) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:07,364] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:53:07,365] {logging_mixin.py:112} INFO - [2020-11-01 19:53:07,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:08,251] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:08,295] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:53:08,317] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:53:08,322] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.962 seconds
[2020-11-01 19:53:20,593] {scheduler_job.py:155} INFO - Started process (PID=12627) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:20,599] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:53:20,600] {logging_mixin.py:112} INFO - [2020-11-01 19:53:20,600] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:21,521] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:21,579] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:53:21,606] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:53:21,611] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.019 seconds
[2020-11-01 19:53:34,779] {scheduler_job.py:155} INFO - Started process (PID=12694) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:34,783] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:53:34,784] {logging_mixin.py:112} INFO - [2020-11-01 19:53:34,784] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:35,725] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:35,774] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:53:35,804] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:53:35,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.031 seconds
[2020-11-01 19:53:47,998] {scheduler_job.py:155} INFO - Started process (PID=12747) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:48,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:53:48,002] {logging_mixin.py:112} INFO - [2020-11-01 19:53:48,002] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:48,840] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:53:48,883] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:53:48,902] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:53:48,906] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.908 seconds
[2020-11-01 19:54:00,217] {scheduler_job.py:155} INFO - Started process (PID=12805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:00,228] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:54:00,229] {logging_mixin.py:112} INFO - [2020-11-01 19:54:00,228] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:01,612] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:01,658] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:54:01,681] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:54:01,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.469 seconds
[2020-11-01 19:54:14,465] {scheduler_job.py:155} INFO - Started process (PID=12859) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:14,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:54:14,471] {logging_mixin.py:112} INFO - [2020-11-01 19:54:14,471] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:15,370] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:15,413] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:54:15,432] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:54:15,438] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.973 seconds
[2020-11-01 19:54:26,677] {scheduler_job.py:155} INFO - Started process (PID=12910) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:26,682] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:54:26,683] {logging_mixin.py:112} INFO - [2020-11-01 19:54:26,682] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:27,495] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:27,555] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:54:27,575] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:54:27,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.902 seconds
[2020-11-01 19:54:39,907] {scheduler_job.py:155} INFO - Started process (PID=12969) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:39,912] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:54:39,916] {logging_mixin.py:112} INFO - [2020-11-01 19:54:39,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:40,891] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:40,957] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:54:40,999] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:54:41,010] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-01 19:54:54,163] {scheduler_job.py:155} INFO - Started process (PID=13026) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:54,169] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:54:54,169] {logging_mixin.py:112} INFO - [2020-11-01 19:54:54,169] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:55,114] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:54:55,156] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:54:55,183] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:54:55,188] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.024 seconds
[2020-11-01 19:55:07,376] {scheduler_job.py:155} INFO - Started process (PID=13086) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:07,381] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:55:07,382] {logging_mixin.py:112} INFO - [2020-11-01 19:55:07,382] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:08,394] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:08,440] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:55:08,464] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:55:08,469] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.093 seconds
[2020-11-01 19:55:20,611] {scheduler_job.py:155} INFO - Started process (PID=13138) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:20,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:55:20,615] {logging_mixin.py:112} INFO - [2020-11-01 19:55:20,615] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:21,550] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:21,597] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:55:21,623] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:55:21,630] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.020 seconds
[2020-11-01 19:55:33,804] {scheduler_job.py:155} INFO - Started process (PID=13195) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:33,808] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:55:33,808] {logging_mixin.py:112} INFO - [2020-11-01 19:55:33,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:35,043] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:35,080] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:55:35,100] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:55:35,105] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.301 seconds
[2020-11-01 19:55:47,114] {scheduler_job.py:155} INFO - Started process (PID=13246) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:47,121] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:55:47,122] {logging_mixin.py:112} INFO - [2020-11-01 19:55:47,122] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:47,926] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:47,968] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:55:47,988] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:55:47,992] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.879 seconds
[2020-11-01 19:55:59,339] {scheduler_job.py:155} INFO - Started process (PID=13299) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:55:59,344] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:55:59,349] {logging_mixin.py:112} INFO - [2020-11-01 19:55:59,348] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:00,662] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:00,704] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:56:00,724] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:56:00,729] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.390 seconds
[2020-11-01 19:56:13,582] {scheduler_job.py:155} INFO - Started process (PID=13357) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:13,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:56:13,589] {logging_mixin.py:112} INFO - [2020-11-01 19:56:13,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:14,528] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:14,586] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:56:14,612] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:56:14,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.035 seconds
[2020-11-01 19:56:26,769] {scheduler_job.py:155} INFO - Started process (PID=13409) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:26,773] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:56:26,774] {logging_mixin.py:112} INFO - [2020-11-01 19:56:26,774] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:27,648] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:27,694] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:56:27,714] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:56:27,718] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.950 seconds
[2020-11-01 19:56:38,965] {scheduler_job.py:155} INFO - Started process (PID=13465) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:38,970] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:56:38,971] {logging_mixin.py:112} INFO - [2020-11-01 19:56:38,971] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:40,378] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:40,452] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:56:40,486] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:56:40,498] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.533 seconds
[2020-11-01 19:56:53,219] {scheduler_job.py:155} INFO - Started process (PID=13520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:53,224] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:56:53,226] {logging_mixin.py:112} INFO - [2020-11-01 19:56:53,226] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:54,683] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:56:54,775] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:56:54,811] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:56:54,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.599 seconds
[2020-11-01 19:57:06,473] {scheduler_job.py:155} INFO - Started process (PID=13595) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:06,488] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:57:06,495] {logging_mixin.py:112} INFO - [2020-11-01 19:57:06,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:08,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:08,202] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:57:08,226] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:57:08,231] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.758 seconds
[2020-11-01 19:57:19,686] {scheduler_job.py:155} INFO - Started process (PID=13651) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:19,690] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:57:19,691] {logging_mixin.py:112} INFO - [2020-11-01 19:57:19,691] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:20,567] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:20,611] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:57:20,632] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:57:20,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.951 seconds
[2020-11-01 19:57:31,889] {scheduler_job.py:155} INFO - Started process (PID=13703) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:31,894] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:57:31,895] {logging_mixin.py:112} INFO - [2020-11-01 19:57:31,895] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:33,325] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:33,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:57:33,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:57:33,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.513 seconds
[2020-11-01 19:57:46,140] {scheduler_job.py:155} INFO - Started process (PID=13766) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:46,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:57:46,147] {logging_mixin.py:112} INFO - [2020-11-01 19:57:46,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:47,056] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:47,098] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:57:47,118] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:57:47,122] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.982 seconds
[2020-11-01 19:57:58,344] {scheduler_job.py:155} INFO - Started process (PID=13817) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:58,353] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:57:58,354] {logging_mixin.py:112} INFO - [2020-11-01 19:57:58,354] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:59,746] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:57:59,809] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:57:59,839] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:57:59,844] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.500 seconds
[2020-11-01 19:58:12,562] {scheduler_job.py:155} INFO - Started process (PID=13877) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:12,574] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:58:12,574] {logging_mixin.py:112} INFO - [2020-11-01 19:58:12,574] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:13,728] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:13,776] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:58:13,802] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:58:13,807] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.245 seconds
[2020-11-01 19:58:25,802] {scheduler_job.py:155} INFO - Started process (PID=13929) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:25,807] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:58:25,809] {logging_mixin.py:112} INFO - [2020-11-01 19:58:25,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:26,860] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:26,934] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:58:26,970] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:58:26,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.174 seconds
[2020-11-01 19:58:39,012] {scheduler_job.py:155} INFO - Started process (PID=13986) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:39,016] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:58:39,017] {logging_mixin.py:112} INFO - [2020-11-01 19:58:39,016] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:40,203] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:40,269] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:58:40,309] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:58:40,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.305 seconds
[2020-11-01 19:58:52,252] {scheduler_job.py:155} INFO - Started process (PID=14038) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:52,256] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:58:52,257] {logging_mixin.py:112} INFO - [2020-11-01 19:58:52,257] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:53,107] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:58:53,149] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:58:53,171] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:58:53,175] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.923 seconds
[2020-11-01 19:59:04,456] {scheduler_job.py:155} INFO - Started process (PID=14093) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:04,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:59:04,460] {logging_mixin.py:112} INFO - [2020-11-01 19:59:04,460] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:05,716] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:05,831] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:59:05,859] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:59:05,865] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.408 seconds
[2020-11-01 19:59:18,720] {scheduler_job.py:155} INFO - Started process (PID=14150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:18,725] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:59:18,725] {logging_mixin.py:112} INFO - [2020-11-01 19:59:18,725] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:20,087] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:20,159] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:59:20,192] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:59:20,200] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.480 seconds
[2020-11-01 19:59:31,917] {scheduler_job.py:155} INFO - Started process (PID=14201) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:31,922] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:59:31,924] {logging_mixin.py:112} INFO - [2020-11-01 19:59:31,923] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:32,827] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:32,880] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:59:32,904] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:59:32,909] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.991 seconds
[2020-11-01 19:59:44,126] {scheduler_job.py:155} INFO - Started process (PID=14259) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:44,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:59:44,130] {logging_mixin.py:112} INFO - [2020-11-01 19:59:44,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:45,112] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:45,173] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:59:45,199] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:59:45,204] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.079 seconds
[2020-11-01 19:59:58,354] {scheduler_job.py:155} INFO - Started process (PID=14316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:58,359] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 19:59:58,360] {logging_mixin.py:112} INFO - [2020-11-01 19:59:58,360] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:59,315] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 19:59:59,355] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 19:59:59,376] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 19:59:59,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.028 seconds
[2020-11-01 20:00:11,582] {scheduler_job.py:155} INFO - Started process (PID=14393) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:11,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:00:11,588] {logging_mixin.py:112} INFO - [2020-11-01 20:00:11,587] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:12,416] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:12,468] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:00:12,525] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:00:12,532] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:00:12,611] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:00:12,621] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 18:00:00+00:00 [success]> in ORM
[2020-11-01 20:00:12,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.054 seconds
[2020-11-01 20:00:24,803] {scheduler_job.py:155} INFO - Started process (PID=14445) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:24,807] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:00:24,808] {logging_mixin.py:112} INFO - [2020-11-01 20:00:24,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:25,683] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:25,724] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:00:25,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:00:25,805] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:00:25,813] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_from_S3 2020-11-01 18:00:00+00:00 [scheduled]> in ORM
[2020-11-01 20:00:25,825] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.022 seconds
[2020-11-01 20:00:38,018] {scheduler_job.py:155} INFO - Started process (PID=14533) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:38,027] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:00:38,027] {logging_mixin.py:112} INFO - [2020-11-01 20:00:38,027] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:39,200] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:39,263] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:00:39,301] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:00:39,360] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:00:39,367] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 18:00:00+00:00 [scheduled]> in ORM
[2020-11-01 20:00:39,377] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.359 seconds
[2020-11-01 20:00:51,263] {scheduler_job.py:155} INFO - Started process (PID=14612) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:51,268] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:00:51,269] {logging_mixin.py:112} INFO - [2020-11-01 20:00:51,269] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:52,282] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:00:52,331] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:00:52,357] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:00:52,408] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:00:52,412] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.150 seconds
[2020-11-01 20:01:04,559] {scheduler_job.py:155} INFO - Started process (PID=14663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:04,563] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:01:04,564] {logging_mixin.py:112} INFO - [2020-11-01 20:01:04,563] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:05,420] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:05,462] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:01:05,481] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:01:05,525] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:01:05,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.970 seconds
[2020-11-01 20:01:16,745] {scheduler_job.py:155} INFO - Started process (PID=14719) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:16,853] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:01:16,854] {logging_mixin.py:112} INFO - [2020-11-01 20:01:16,853] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:17,580] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:17,623] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:01:17,647] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:01:17,696] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:01:17,701] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.956 seconds
[2020-11-01 20:01:30,055] {scheduler_job.py:155} INFO - Started process (PID=14777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:30,059] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:01:30,060] {logging_mixin.py:112} INFO - [2020-11-01 20:01:30,060] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:30,836] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:30,879] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:01:30,906] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:01:30,959] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:01:30,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.908 seconds
[2020-11-01 20:01:43,336] {scheduler_job.py:155} INFO - Started process (PID=14853) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:43,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:01:43,341] {logging_mixin.py:112} INFO - [2020-11-01 20:01:43,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:44,113] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:44,157] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:01:44,177] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:01:44,220] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:01:44,225] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.888 seconds
[2020-11-01 20:01:56,680] {scheduler_job.py:155} INFO - Started process (PID=14927) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:56,685] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:01:56,686] {logging_mixin.py:112} INFO - [2020-11-01 20:01:56,686] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:57,428] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:01:57,471] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:01:57,489] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:01:57,531] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:01:57,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.855 seconds
[2020-11-01 20:02:09,889] {scheduler_job.py:155} INFO - Started process (PID=15003) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:09,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:02:09,893] {logging_mixin.py:112} INFO - [2020-11-01 20:02:09,893] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:10,858] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:10,915] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:02:10,942] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:02:11,020] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:02:11,030] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.141 seconds
[2020-11-01 20:02:24,215] {scheduler_job.py:155} INFO - Started process (PID=15076) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:24,220] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:02:24,221] {logging_mixin.py:112} INFO - [2020-11-01 20:02:24,221] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:24,999] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:25,043] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:02:25,063] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:02:25,105] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:02:25,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.895 seconds
[2020-11-01 20:02:36,430] {scheduler_job.py:155} INFO - Started process (PID=15144) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:36,435] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:02:36,436] {logging_mixin.py:112} INFO - [2020-11-01 20:02:36,435] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:37,283] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:37,339] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:02:37,366] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:02:37,417] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:02:37,421] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.990 seconds
[2020-11-01 20:02:49,691] {scheduler_job.py:155} INFO - Started process (PID=15219) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:49,695] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:02:49,696] {logging_mixin.py:112} INFO - [2020-11-01 20:02:49,695] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:50,474] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:02:50,519] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:02:50,537] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:02:50,577] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:02:50,584] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 18:00:00+00:00 [scheduled]> in ORM
[2020-11-01 20:02:50,595] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.905 seconds
[2020-11-01 20:03:02,893] {scheduler_job.py:155} INFO - Started process (PID=15309) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:02,899] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:03:02,900] {logging_mixin.py:112} INFO - [2020-11-01 20:03:02,899] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:03,735] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:03,782] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:03:03,802] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:03:03,843] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:03:03,847] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.954 seconds
[2020-11-01 20:03:16,098] {scheduler_job.py:155} INFO - Started process (PID=15382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:16,104] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:03:16,105] {logging_mixin.py:112} INFO - [2020-11-01 20:03:16,104] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:16,986] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:17,026] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:03:17,048] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:03:17,093] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:03:17,097] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.000 seconds
[2020-11-01 20:03:30,384] {scheduler_job.py:155} INFO - Started process (PID=15459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:30,389] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:03:30,390] {logging_mixin.py:112} INFO - [2020-11-01 20:03:30,390] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:31,119] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:31,161] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:03:31,181] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:03:31,222] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:03:31,226] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.842 seconds
[2020-11-01 20:03:42,630] {scheduler_job.py:155} INFO - Started process (PID=15529) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:42,643] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:03:42,646] {logging_mixin.py:112} INFO - [2020-11-01 20:03:42,645] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:43,612] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:43,653] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:03:43,674] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:03:43,727] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:03:43,731] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.101 seconds
[2020-11-01 20:03:56,879] {scheduler_job.py:155} INFO - Started process (PID=15600) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:56,884] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:03:56,884] {logging_mixin.py:112} INFO - [2020-11-01 20:03:56,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:57,972] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:03:58,026] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:03:58,050] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:03:58,106] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:03:58,111] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-01 20:04:10,105] {scheduler_job.py:155} INFO - Started process (PID=15671) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:10,109] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:04:10,109] {logging_mixin.py:112} INFO - [2020-11-01 20:04:10,109] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:11,525] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:11,574] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:04:11,608] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:04:11,655] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:04:11,660] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.554 seconds
[2020-11-01 20:04:23,369] {scheduler_job.py:155} INFO - Started process (PID=15732) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:23,374] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:04:23,375] {logging_mixin.py:112} INFO - [2020-11-01 20:04:23,374] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:24,169] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:24,225] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:04:24,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:04:24,287] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:04:24,292] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.923 seconds
[2020-11-01 20:04:35,687] {scheduler_job.py:155} INFO - Started process (PID=15795) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:35,694] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:04:35,695] {logging_mixin.py:112} INFO - [2020-11-01 20:04:35,694] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:36,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:36,495] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:04:36,516] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:04:36,559] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:04:36,564] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.877 seconds
[2020-11-01 20:04:48,901] {scheduler_job.py:155} INFO - Started process (PID=15872) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:48,905] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:04:48,905] {logging_mixin.py:112} INFO - [2020-11-01 20:04:48,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:49,801] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:04:49,846] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:04:49,865] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:04:49,919] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:04:49,927] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.027 seconds
[2020-11-01 20:05:03,291] {scheduler_job.py:155} INFO - Started process (PID=15954) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:03,307] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:05:03,308] {logging_mixin.py:112} INFO - [2020-11-01 20:05:03,307] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:04,052] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:04,095] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:05:04,114] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:05:04,169] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:05:04,176] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 18:00:00+00:00 [scheduled]> in ORM
[2020-11-01 20:05:04,192] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.901 seconds
[2020-11-01 20:05:15,462] {scheduler_job.py:155} INFO - Started process (PID=16044) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:15,466] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:05:15,466] {logging_mixin.py:112} INFO - [2020-11-01 20:05:15,466] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:16,442] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:16,508] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:05:16,551] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:05:16,605] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:05:16,609] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.147 seconds
[2020-11-01 20:05:29,763] {scheduler_job.py:155} INFO - Started process (PID=16121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:29,768] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:05:29,769] {logging_mixin.py:112} INFO - [2020-11-01 20:05:29,769] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:30,502] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:30,546] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:05:30,566] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:05:30,625] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:05:30,630] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 20:05:42,020] {scheduler_job.py:155} INFO - Started process (PID=16189) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:42,024] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:05:42,025] {logging_mixin.py:112} INFO - [2020-11-01 20:05:42,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:42,852] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:42,907] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:05:42,928] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:05:42,971] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:05:42,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.955 seconds
[2020-11-01 20:05:55,293] {scheduler_job.py:155} INFO - Started process (PID=16259) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:55,298] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:05:55,298] {logging_mixin.py:112} INFO - [2020-11-01 20:05:55,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:56,006] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:05:56,071] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:05:56,100] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:05:56,142] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:05:56,147] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.853 seconds
[2020-11-01 20:06:08,642] {scheduler_job.py:155} INFO - Started process (PID=16329) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:08,646] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:06:08,647] {logging_mixin.py:112} INFO - [2020-11-01 20:06:08,647] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:09,413] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:09,461] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:06:09,481] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:06:09,521] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:06:09,526] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.883 seconds
[2020-11-01 20:06:21,898] {scheduler_job.py:155} INFO - Started process (PID=16408) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:21,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:06:21,905] {logging_mixin.py:112} INFO - [2020-11-01 20:06:21,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:22,760] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:22,830] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:06:22,869] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:06:22,929] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:06:22,939] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.041 seconds
[2020-11-01 20:06:36,238] {scheduler_job.py:155} INFO - Started process (PID=16477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:36,244] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:06:36,245] {logging_mixin.py:112} INFO - [2020-11-01 20:06:36,245] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:37,194] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:37,242] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:06:37,264] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:06:37,332] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:06:37,337] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.099 seconds
[2020-11-01 20:06:49,447] {scheduler_job.py:155} INFO - Started process (PID=16545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:49,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:06:49,453] {logging_mixin.py:112} INFO - [2020-11-01 20:06:49,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:50,284] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:06:50,330] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:06:50,351] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:06:50,394] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:06:50,399] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.952 seconds
[2020-11-01 20:07:01,773] {scheduler_job.py:155} INFO - Started process (PID=16608) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:01,778] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:07:01,779] {logging_mixin.py:112} INFO - [2020-11-01 20:07:01,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:02,563] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:02,604] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:07:02,624] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:07:02,671] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:07:02,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.902 seconds
[2020-11-01 20:07:14,991] {scheduler_job.py:155} INFO - Started process (PID=16685) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:14,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:07:14,999] {logging_mixin.py:112} INFO - [2020-11-01 20:07:14,999] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:15,872] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:15,917] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:07:15,941] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:07:16,032] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:07:16,065] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 18:00:00+00:00 [scheduled]> in ORM
[2020-11-01 20:07:16,094] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-01 20:07:29,251] {scheduler_job.py:155} INFO - Started process (PID=16777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:29,256] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:07:29,258] {logging_mixin.py:112} INFO - [2020-11-01 20:07:29,257] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:30,024] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:30,065] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:07:30,085] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False>
[2020-11-01 20:07:30,100] {logging_mixin.py:112} INFO - [2020-11-01 20:07:30,100] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 18:00:00+00:00: scheduled__2020-11-01T18:00:00+00:00, externally triggered: False> failed
[2020-11-01 20:07:30,105] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:07:30,109] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.858 seconds
[2020-11-01 20:07:41,497] {scheduler_job.py:155} INFO - Started process (PID=16848) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:41,511] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:07:41,512] {logging_mixin.py:112} INFO - [2020-11-01 20:07:41,511] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:42,547] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:42,609] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:07:42,632] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:07:42,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.138 seconds
[2020-11-01 20:07:55,758] {scheduler_job.py:155} INFO - Started process (PID=16919) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:55,781] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:07:55,781] {logging_mixin.py:112} INFO - [2020-11-01 20:07:55,781] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:56,704] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:07:56,777] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:07:56,818] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:07:56,833] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.075 seconds
[2020-11-01 20:08:08,922] {scheduler_job.py:155} INFO - Started process (PID=16986) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:08,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:08:08,926] {logging_mixin.py:112} INFO - [2020-11-01 20:08:08,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:09,696] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:09,757] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:08:09,778] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:08:09,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.861 seconds
[2020-11-01 20:08:21,139] {scheduler_job.py:155} INFO - Started process (PID=17052) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:21,143] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:08:21,144] {logging_mixin.py:112} INFO - [2020-11-01 20:08:21,143] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:21,934] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:21,993] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:08:22,015] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:08:22,020] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.881 seconds
[2020-11-01 20:08:34,450] {scheduler_job.py:155} INFO - Started process (PID=17125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:34,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:08:34,456] {logging_mixin.py:112} INFO - [2020-11-01 20:08:34,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:35,313] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:35,376] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:08:35,410] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:08:35,417] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.967 seconds
[2020-11-01 20:08:47,768] {scheduler_job.py:155} INFO - Started process (PID=17202) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:47,780] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:08:47,781] {logging_mixin.py:112} INFO - [2020-11-01 20:08:47,780] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:48,656] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:08:48,701] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:08:48,720] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:08:48,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.958 seconds
[2020-11-01 20:09:01,051] {scheduler_job.py:155} INFO - Started process (PID=17272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:01,055] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:09:01,056] {logging_mixin.py:112} INFO - [2020-11-01 20:09:01,056] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:01,833] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:01,877] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:09:01,900] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:09:01,905] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.854 seconds
[2020-11-01 20:09:14,303] {scheduler_job.py:155} INFO - Started process (PID=17345) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:14,307] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:09:14,308] {logging_mixin.py:112} INFO - [2020-11-01 20:09:14,308] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:15,081] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:15,134] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:09:15,159] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:09:15,165] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.862 seconds
[2020-11-01 20:09:27,661] {scheduler_job.py:155} INFO - Started process (PID=17415) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:27,665] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:09:27,666] {logging_mixin.py:112} INFO - [2020-11-01 20:09:27,666] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:28,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:28,527] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:09:28,549] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:09:28,554] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.893 seconds
[2020-11-01 20:09:40,835] {scheduler_job.py:155} INFO - Started process (PID=17487) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:40,838] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:09:40,839] {logging_mixin.py:112} INFO - [2020-11-01 20:09:40,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:41,625] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:41,674] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:09:41,710] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:09:41,715] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.880 seconds
[2020-11-01 20:09:54,041] {scheduler_job.py:155} INFO - Started process (PID=17564) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:54,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:09:54,047] {logging_mixin.py:112} INFO - [2020-11-01 20:09:54,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:54,817] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:09:54,855] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:09:54,873] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:09:54,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.836 seconds
[2020-11-01 20:10:07,335] {scheduler_job.py:155} INFO - Started process (PID=17633) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:07,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:10:07,342] {logging_mixin.py:112} INFO - [2020-11-01 20:10:07,342] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:08,169] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:08,214] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:10:08,235] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:10:08,239] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.904 seconds
[2020-11-01 20:10:20,611] {scheduler_job.py:155} INFO - Started process (PID=17707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:20,615] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:10:20,634] {logging_mixin.py:112} INFO - [2020-11-01 20:10:20,634] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:21,646] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:21,705] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:10:21,724] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:10:21,731] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.120 seconds
[2020-11-01 20:10:34,801] {scheduler_job.py:155} INFO - Started process (PID=17784) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:34,805] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:10:34,806] {logging_mixin.py:112} INFO - [2020-11-01 20:10:34,806] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:35,609] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:35,647] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:10:35,670] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:10:35,674] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.874 seconds
[2020-11-01 20:10:47,034] {scheduler_job.py:155} INFO - Started process (PID=17874) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:47,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:10:47,047] {logging_mixin.py:112} INFO - [2020-11-01 20:10:47,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:48,010] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:10:48,064] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:10:48,085] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:10:48,090] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.056 seconds
[2020-11-01 20:11:01,284] {scheduler_job.py:155} INFO - Started process (PID=17947) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:01,290] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:11:01,290] {logging_mixin.py:112} INFO - [2020-11-01 20:11:01,290] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:02,104] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:02,161] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:11:02,188] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:11:02,193] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.909 seconds
[2020-11-01 20:11:13,533] {scheduler_job.py:155} INFO - Started process (PID=18017) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:13,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:11:13,537] {logging_mixin.py:112} INFO - [2020-11-01 20:11:13,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:14,334] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:14,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:11:14,405] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:11:14,409] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.877 seconds
[2020-11-01 20:11:26,824] {scheduler_job.py:155} INFO - Started process (PID=18088) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:26,832] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:11:26,837] {logging_mixin.py:112} INFO - [2020-11-01 20:11:26,837] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:27,755] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:27,805] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:11:27,827] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:11:27,831] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.007 seconds
[2020-11-01 20:11:41,052] {scheduler_job.py:155} INFO - Started process (PID=18162) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:41,056] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:11:41,057] {logging_mixin.py:112} INFO - [2020-11-01 20:11:41,056] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:41,807] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:41,850] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:11:41,870] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:11:41,874] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.822 seconds
[2020-11-01 20:11:53,282] {scheduler_job.py:155} INFO - Started process (PID=18231) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:53,287] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:11:53,288] {logging_mixin.py:112} INFO - [2020-11-01 20:11:53,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:54,190] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:11:54,242] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:11:54,279] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:11:54,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.003 seconds
[2020-11-01 20:12:07,543] {scheduler_job.py:155} INFO - Started process (PID=18306) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:07,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:12:07,552] {logging_mixin.py:112} INFO - [2020-11-01 20:12:07,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:08,302] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:08,345] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:12:08,365] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:12:08,370] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.826 seconds
[2020-11-01 20:12:19,733] {scheduler_job.py:155} INFO - Started process (PID=18375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:19,738] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:12:19,739] {logging_mixin.py:112} INFO - [2020-11-01 20:12:19,739] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:20,953] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:20,995] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:12:21,023] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:12:21,029] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.296 seconds
[2020-11-01 20:12:33,956] {scheduler_job.py:155} INFO - Started process (PID=18449) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:33,963] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:12:33,963] {logging_mixin.py:112} INFO - [2020-11-01 20:12:33,963] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:34,826] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:34,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:12:34,898] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:12:34,902] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.947 seconds
[2020-11-01 20:12:46,144] {scheduler_job.py:155} INFO - Started process (PID=18523) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:46,151] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:12:46,152] {logging_mixin.py:112} INFO - [2020-11-01 20:12:46,152] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:46,883] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:46,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:12:46,949] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:12:46,953] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.810 seconds
[2020-11-01 20:12:59,361] {scheduler_job.py:155} INFO - Started process (PID=18593) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:12:59,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:12:59,366] {logging_mixin.py:112} INFO - [2020-11-01 20:12:59,366] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:00,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:00,263] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:13:00,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:13:00,292] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.931 seconds
[2020-11-01 20:13:12,633] {scheduler_job.py:155} INFO - Started process (PID=18666) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:12,643] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:13:12,647] {logging_mixin.py:112} INFO - [2020-11-01 20:13:12,647] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:13,456] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:13,501] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:13:13,526] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:13:13,531] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.898 seconds
[2020-11-01 20:13:25,847] {scheduler_job.py:155} INFO - Started process (PID=18735) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:25,850] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:13:25,851] {logging_mixin.py:112} INFO - [2020-11-01 20:13:25,851] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:26,599] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:26,646] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:13:26,668] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:13:26,674] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.828 seconds
[2020-11-01 20:13:39,198] {scheduler_job.py:155} INFO - Started process (PID=18804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:39,204] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:13:39,204] {logging_mixin.py:112} INFO - [2020-11-01 20:13:39,204] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:39,979] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:40,021] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:13:40,041] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:13:40,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.847 seconds
[2020-11-01 20:13:52,425] {scheduler_job.py:155} INFO - Started process (PID=18880) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:52,433] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:13:52,434] {logging_mixin.py:112} INFO - [2020-11-01 20:13:52,434] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:53,551] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:13:53,596] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:13:53,616] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:13:53,621] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.196 seconds
[2020-11-01 20:14:06,704] {scheduler_job.py:155} INFO - Started process (PID=18955) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:06,709] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:14:06,710] {logging_mixin.py:112} INFO - [2020-11-01 20:14:06,709] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:07,538] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:07,605] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:14:07,644] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:14:07,651] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.947 seconds
[2020-11-01 20:14:18,874] {scheduler_job.py:155} INFO - Started process (PID=19023) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:18,877] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:14:18,878] {logging_mixin.py:112} INFO - [2020-11-01 20:14:18,878] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:19,725] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:19,768] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:14:19,791] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:14:19,795] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.921 seconds
[2020-11-01 20:14:32,105] {scheduler_job.py:155} INFO - Started process (PID=19094) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:32,118] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:14:32,119] {logging_mixin.py:112} INFO - [2020-11-01 20:14:32,119] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:32,899] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:32,949] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:14:32,969] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:14:32,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.868 seconds
[2020-11-01 20:14:45,334] {scheduler_job.py:155} INFO - Started process (PID=19167) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:45,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:14:45,349] {logging_mixin.py:112} INFO - [2020-11-01 20:14:45,349] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:46,383] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:46,473] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:14:46,499] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:14:46,504] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.170 seconds
[2020-11-01 20:14:59,549] {scheduler_job.py:155} INFO - Started process (PID=19239) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:14:59,555] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:14:59,556] {logging_mixin.py:112} INFO - [2020-11-01 20:14:59,556] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:00,323] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:00,365] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:15:00,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:15:00,388] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.838 seconds
[2020-11-01 20:15:11,750] {scheduler_job.py:155} INFO - Started process (PID=19311) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:11,755] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:15:11,756] {logging_mixin.py:112} INFO - [2020-11-01 20:15:11,756] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:12,526] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:12,572] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:15:12,593] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:15:12,598] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.849 seconds
[2020-11-01 20:15:24,974] {scheduler_job.py:155} INFO - Started process (PID=19382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:24,991] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:15:24,992] {logging_mixin.py:112} INFO - [2020-11-01 20:15:24,992] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:25,898] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:25,947] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:15:25,986] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:15:25,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.017 seconds
[2020-11-01 20:15:39,267] {scheduler_job.py:155} INFO - Started process (PID=19454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:39,271] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:15:39,272] {logging_mixin.py:112} INFO - [2020-11-01 20:15:39,272] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:40,044] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:40,092] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:15:40,115] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:15:40,119] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.852 seconds
[2020-11-01 20:15:51,475] {scheduler_job.py:155} INFO - Started process (PID=19548) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:51,479] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:15:51,480] {logging_mixin.py:112} INFO - [2020-11-01 20:15:51,479] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:52,487] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:15:52,572] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:15:52,606] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:15:52,610] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.135 seconds
[2020-11-01 20:16:05,730] {scheduler_job.py:155} INFO - Started process (PID=19620) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:05,734] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:16:05,735] {logging_mixin.py:112} INFO - [2020-11-01 20:16:05,735] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:06,508] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:06,553] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:16:06,583] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:16:06,588] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.859 seconds
[2020-11-01 20:16:17,961] {scheduler_job.py:155} INFO - Started process (PID=19688) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:17,964] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:16:17,965] {logging_mixin.py:112} INFO - [2020-11-01 20:16:17,965] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:18,743] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:18,792] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:16:18,812] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:16:18,815] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.855 seconds
[2020-11-01 20:16:31,199] {scheduler_job.py:155} INFO - Started process (PID=19755) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:31,212] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:16:31,213] {logging_mixin.py:112} INFO - [2020-11-01 20:16:31,212] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:32,060] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:32,122] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:16:32,155] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:16:32,161] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.962 seconds
[2020-11-01 20:16:44,495] {scheduler_job.py:155} INFO - Started process (PID=19831) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:44,504] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:16:44,505] {logging_mixin.py:112} INFO - [2020-11-01 20:16:44,504] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:45,330] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:45,398] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:16:45,427] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:16:45,431] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.936 seconds
[2020-11-01 20:16:57,730] {scheduler_job.py:155} INFO - Started process (PID=19903) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:57,746] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:16:57,754] {logging_mixin.py:112} INFO - [2020-11-01 20:16:57,754] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:58,532] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:16:58,587] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:16:58,608] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:16:58,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.883 seconds
[2020-11-01 20:17:11,043] {scheduler_job.py:155} INFO - Started process (PID=19977) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:11,049] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:17:11,049] {logging_mixin.py:112} INFO - [2020-11-01 20:17:11,049] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:11,851] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:11,906] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 20:17:11,931] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 20:17:11,938] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.895 seconds
[2020-11-01 20:17:24,329] {scheduler_job.py:155} INFO - Started process (PID=20049) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:24,342] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:17:24,342] {logging_mixin.py:112} INFO - [2020-11-01 20:17:24,342] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:24,494] {logging_mixin.py:112} INFO - [2020-11-01 20:17:24,493] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:17:24,494] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:24,519] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-01 20:17:37,636] {scheduler_job.py:155} INFO - Started process (PID=20105) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:37,648] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:17:37,655] {logging_mixin.py:112} INFO - [2020-11-01 20:17:37,653] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:37,861] {logging_mixin.py:112} INFO - [2020-11-01 20:17:37,859] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:17:37,861] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:37,904] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-01 20:17:50,837] {scheduler_job.py:155} INFO - Started process (PID=20166) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:50,844] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:17:50,847] {logging_mixin.py:112} INFO - [2020-11-01 20:17:50,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:50,996] {logging_mixin.py:112} INFO - [2020-11-01 20:17:50,995] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:17:50,997] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:17:51,032] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.194 seconds
[2020-11-01 20:18:04,057] {scheduler_job.py:155} INFO - Started process (PID=20231) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:04,062] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:18:04,063] {logging_mixin.py:112} INFO - [2020-11-01 20:18:04,063] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:04,185] {logging_mixin.py:112} INFO - [2020-11-01 20:18:04,184] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:18:04,185] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:04,234] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-01 20:18:17,297] {scheduler_job.py:155} INFO - Started process (PID=20294) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:17,302] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:18:17,303] {logging_mixin.py:112} INFO - [2020-11-01 20:18:17,303] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:17,432] {logging_mixin.py:112} INFO - [2020-11-01 20:18:17,431] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:18:17,433] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:17,460] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-01 20:18:30,546] {scheduler_job.py:155} INFO - Started process (PID=20360) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:30,550] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:18:30,551] {logging_mixin.py:112} INFO - [2020-11-01 20:18:30,551] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:30,664] {logging_mixin.py:112} INFO - [2020-11-01 20:18:30,663] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:18:30,665] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:30,696] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:18:43,782] {scheduler_job.py:155} INFO - Started process (PID=20422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:43,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:18:43,786] {logging_mixin.py:112} INFO - [2020-11-01 20:18:43,785] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:43,893] {logging_mixin.py:112} INFO - [2020-11-01 20:18:43,892] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:18:43,894] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:43,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-01 20:18:56,963] {scheduler_job.py:155} INFO - Started process (PID=20491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:56,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:18:56,976] {logging_mixin.py:112} INFO - [2020-11-01 20:18:56,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:57,114] {logging_mixin.py:112} INFO - [2020-11-01 20:18:57,113] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:18:57,114] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:18:57,142] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-01 20:19:10,180] {scheduler_job.py:155} INFO - Started process (PID=20548) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:10,184] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:19:10,184] {logging_mixin.py:112} INFO - [2020-11-01 20:19:10,184] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:10,290] {logging_mixin.py:112} INFO - [2020-11-01 20:19:10,289] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:19:10,291] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:10,315] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-01 20:19:23,522] {scheduler_job.py:155} INFO - Started process (PID=20609) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:23,527] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:19:23,528] {logging_mixin.py:112} INFO - [2020-11-01 20:19:23,527] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:23,647] {logging_mixin.py:112} INFO - [2020-11-01 20:19:23,646] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:19:23,647] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:23,674] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 20:19:36,725] {scheduler_job.py:155} INFO - Started process (PID=20674) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:36,730] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:19:36,731] {logging_mixin.py:112} INFO - [2020-11-01 20:19:36,731] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:36,873] {logging_mixin.py:112} INFO - [2020-11-01 20:19:36,871] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:19:36,873] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:36,922] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.196 seconds
[2020-11-01 20:19:49,987] {scheduler_job.py:155} INFO - Started process (PID=20740) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:49,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:19:49,998] {logging_mixin.py:112} INFO - [2020-11-01 20:19:49,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:50,184] {logging_mixin.py:112} INFO - [2020-11-01 20:19:50,182] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:19:50,184] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:19:50,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-01 20:20:03,247] {scheduler_job.py:155} INFO - Started process (PID=20804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:03,251] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:20:03,252] {logging_mixin.py:112} INFO - [2020-11-01 20:20:03,252] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:03,363] {logging_mixin.py:112} INFO - [2020-11-01 20:20:03,362] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:20:03,364] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:03,391] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-01 20:20:16,537] {scheduler_job.py:155} INFO - Started process (PID=20868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:16,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:20:16,541] {logging_mixin.py:112} INFO - [2020-11-01 20:20:16,541] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:16,650] {logging_mixin.py:112} INFO - [2020-11-01 20:20:16,648] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:20:16,651] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:16,682] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-01 20:20:29,816] {scheduler_job.py:155} INFO - Started process (PID=20932) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:29,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:20:29,825] {logging_mixin.py:112} INFO - [2020-11-01 20:20:29,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:30,123] {logging_mixin.py:112} INFO - [2020-11-01 20:20:30,121] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:20:30,124] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:30,175] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.359 seconds
[2020-11-01 20:20:43,099] {scheduler_job.py:155} INFO - Started process (PID=20992) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:43,107] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:20:43,108] {logging_mixin.py:112} INFO - [2020-11-01 20:20:43,108] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:43,214] {logging_mixin.py:112} INFO - [2020-11-01 20:20:43,213] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:20:43,214] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:43,239] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-01 20:20:56,300] {scheduler_job.py:155} INFO - Started process (PID=21054) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:56,306] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:20:56,313] {logging_mixin.py:112} INFO - [2020-11-01 20:20:56,313] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:56,471] {logging_mixin.py:112} INFO - [2020-11-01 20:20:56,469] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:20:56,471] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:20:56,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-01 20:21:09,556] {scheduler_job.py:155} INFO - Started process (PID=21130) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:09,561] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:21:09,563] {logging_mixin.py:112} INFO - [2020-11-01 20:21:09,563] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:09,718] {logging_mixin.py:112} INFO - [2020-11-01 20:21:09,716] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:21:09,718] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:09,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-01 20:21:22,823] {scheduler_job.py:155} INFO - Started process (PID=21192) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:22,827] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:21:22,827] {logging_mixin.py:112} INFO - [2020-11-01 20:21:22,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:22,949] {logging_mixin.py:112} INFO - [2020-11-01 20:21:22,945] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:21:22,950] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:22,976] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-01 20:21:36,012] {scheduler_job.py:155} INFO - Started process (PID=21256) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:36,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:21:36,018] {logging_mixin.py:112} INFO - [2020-11-01 20:21:36,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:36,113] {logging_mixin.py:112} INFO - [2020-11-01 20:21:36,112] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:21:36,114] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:36,142] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-01 20:21:49,298] {scheduler_job.py:155} INFO - Started process (PID=21321) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:49,304] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:21:49,305] {logging_mixin.py:112} INFO - [2020-11-01 20:21:49,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:49,437] {logging_mixin.py:112} INFO - [2020-11-01 20:21:49,435] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:21:49,437] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:21:49,474] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-01 20:22:02,505] {scheduler_job.py:155} INFO - Started process (PID=21384) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:02,509] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:22:02,510] {logging_mixin.py:112} INFO - [2020-11-01 20:22:02,510] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:02,656] {logging_mixin.py:112} INFO - [2020-11-01 20:22:02,651] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:22:02,656] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:02,706] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-01 20:22:15,765] {scheduler_job.py:155} INFO - Started process (PID=21441) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:15,769] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:22:15,770] {logging_mixin.py:112} INFO - [2020-11-01 20:22:15,770] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:15,881] {logging_mixin.py:112} INFO - [2020-11-01 20:22:15,879] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:22:15,881] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:15,906] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-01 20:22:28,981] {scheduler_job.py:155} INFO - Started process (PID=21507) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:28,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:22:29,001] {logging_mixin.py:112} INFO - [2020-11-01 20:22:29,001] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:29,144] {logging_mixin.py:112} INFO - [2020-11-01 20:22:29,143] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:22:29,144] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:29,170] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-01 20:22:42,331] {scheduler_job.py:155} INFO - Started process (PID=21569) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:42,336] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:22:42,336] {logging_mixin.py:112} INFO - [2020-11-01 20:22:42,336] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:42,439] {logging_mixin.py:112} INFO - [2020-11-01 20:22:42,438] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:22:42,439] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:42,463] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:22:55,735] {scheduler_job.py:155} INFO - Started process (PID=21637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:55,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:22:55,740] {logging_mixin.py:112} INFO - [2020-11-01 20:22:55,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:55,846] {logging_mixin.py:112} INFO - [2020-11-01 20:22:55,845] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:22:55,847] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:22:55,873] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-01 20:23:09,008] {scheduler_job.py:155} INFO - Started process (PID=21700) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:09,014] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:23:09,017] {logging_mixin.py:112} INFO - [2020-11-01 20:23:09,016] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:09,144] {logging_mixin.py:112} INFO - [2020-11-01 20:23:09,143] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:23:09,144] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:09,169] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-01 20:23:22,240] {scheduler_job.py:155} INFO - Started process (PID=21764) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:22,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:23:22,247] {logging_mixin.py:112} INFO - [2020-11-01 20:23:22,247] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:22,378] {logging_mixin.py:112} INFO - [2020-11-01 20:23:22,376] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:23:22,378] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:22,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-01 20:23:35,530] {scheduler_job.py:155} INFO - Started process (PID=21848) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:35,543] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:23:35,547] {logging_mixin.py:112} INFO - [2020-11-01 20:23:35,545] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:35,790] {logging_mixin.py:112} INFO - [2020-11-01 20:23:35,786] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:23:35,790] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:35,831] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.301 seconds
[2020-11-01 20:23:48,792] {scheduler_job.py:155} INFO - Started process (PID=21910) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:48,796] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:23:48,796] {logging_mixin.py:112} INFO - [2020-11-01 20:23:48,796] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:48,911] {logging_mixin.py:112} INFO - [2020-11-01 20:23:48,910] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:23:48,911] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:23:48,943] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:24:01,976] {scheduler_job.py:155} INFO - Started process (PID=21978) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:01,982] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:24:01,982] {logging_mixin.py:112} INFO - [2020-11-01 20:24:01,982] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:02,236] {logging_mixin.py:112} INFO - [2020-11-01 20:24:02,234] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:24:02,236] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:02,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.296 seconds
[2020-11-01 20:24:15,244] {scheduler_job.py:155} INFO - Started process (PID=22039) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:15,248] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:24:15,248] {logging_mixin.py:112} INFO - [2020-11-01 20:24:15,248] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:15,352] {logging_mixin.py:112} INFO - [2020-11-01 20:24:15,351] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:24:15,352] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:15,376] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:24:28,616] {scheduler_job.py:155} INFO - Started process (PID=22099) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:28,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:24:28,621] {logging_mixin.py:112} INFO - [2020-11-01 20:24:28,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:28,817] {logging_mixin.py:112} INFO - [2020-11-01 20:24:28,816] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:24:28,817] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:28,850] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-01 20:24:41,840] {scheduler_job.py:155} INFO - Started process (PID=22162) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:41,844] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:24:41,845] {logging_mixin.py:112} INFO - [2020-11-01 20:24:41,844] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:42,053] {logging_mixin.py:112} INFO - [2020-11-01 20:24:42,051] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:24:42,053] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:42,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.250 seconds
[2020-11-01 20:24:55,113] {scheduler_job.py:155} INFO - Started process (PID=22226) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:55,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:24:55,118] {logging_mixin.py:112} INFO - [2020-11-01 20:24:55,118] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:55,232] {logging_mixin.py:112} INFO - [2020-11-01 20:24:55,230] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:24:55,232] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:24:55,258] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-01 20:25:08,383] {scheduler_job.py:155} INFO - Started process (PID=22290) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:08,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:25:08,392] {logging_mixin.py:112} INFO - [2020-11-01 20:25:08,392] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:08,491] {logging_mixin.py:112} INFO - [2020-11-01 20:25:08,490] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:25:08,491] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:08,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:25:21,704] {scheduler_job.py:155} INFO - Started process (PID=22352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:21,708] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:25:21,709] {logging_mixin.py:112} INFO - [2020-11-01 20:25:21,709] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:21,872] {logging_mixin.py:112} INFO - [2020-11-01 20:25:21,871] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:25:21,873] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:21,900] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-01 20:25:34,928] {scheduler_job.py:155} INFO - Started process (PID=22416) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:34,937] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:25:34,939] {logging_mixin.py:112} INFO - [2020-11-01 20:25:34,938] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:35,225] {logging_mixin.py:112} INFO - [2020-11-01 20:25:35,223] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:25:35,226] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:35,266] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.338 seconds
[2020-11-01 20:25:48,191] {scheduler_job.py:155} INFO - Started process (PID=22475) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:48,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:25:48,199] {logging_mixin.py:112} INFO - [2020-11-01 20:25:48,199] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:48,313] {logging_mixin.py:112} INFO - [2020-11-01 20:25:48,312] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:25:48,314] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:25:48,340] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 20:26:01,448] {scheduler_job.py:155} INFO - Started process (PID=22535) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:01,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:26:01,453] {logging_mixin.py:112} INFO - [2020-11-01 20:26:01,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:01,552] {logging_mixin.py:112} INFO - [2020-11-01 20:26:01,550] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:26:01,552] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:01,582] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-01 20:26:14,704] {scheduler_job.py:155} INFO - Started process (PID=22600) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:14,709] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:26:14,710] {logging_mixin.py:112} INFO - [2020-11-01 20:26:14,709] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:14,830] {logging_mixin.py:112} INFO - [2020-11-01 20:26:14,829] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:26:14,830] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:14,858] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-01 20:26:28,120] {scheduler_job.py:155} INFO - Started process (PID=22661) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:28,126] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:26:28,127] {logging_mixin.py:112} INFO - [2020-11-01 20:26:28,126] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:28,254] {logging_mixin.py:112} INFO - [2020-11-01 20:26:28,253] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:26:28,255] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:28,286] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-01 20:26:41,334] {scheduler_job.py:155} INFO - Started process (PID=22725) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:41,340] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:26:41,341] {logging_mixin.py:112} INFO - [2020-11-01 20:26:41,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:41,456] {logging_mixin.py:112} INFO - [2020-11-01 20:26:41,454] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:26:41,457] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:41,488] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-01 20:26:54,736] {scheduler_job.py:155} INFO - Started process (PID=22787) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:54,740] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:26:54,741] {logging_mixin.py:112} INFO - [2020-11-01 20:26:54,741] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:54,852] {logging_mixin.py:112} INFO - [2020-11-01 20:26:54,851] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:26:54,852] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:26:54,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-01 20:27:07,961] {scheduler_job.py:155} INFO - Started process (PID=22853) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:07,965] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:27:07,966] {logging_mixin.py:112} INFO - [2020-11-01 20:27:07,966] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:08,097] {logging_mixin.py:112} INFO - [2020-11-01 20:27:08,096] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:27:08,098] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:08,125] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 20:27:21,188] {scheduler_job.py:155} INFO - Started process (PID=22915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:21,193] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:27:21,193] {logging_mixin.py:112} INFO - [2020-11-01 20:27:21,193] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:21,290] {logging_mixin.py:112} INFO - [2020-11-01 20:27:21,289] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:27:21,290] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:21,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-01 20:27:34,440] {scheduler_job.py:155} INFO - Started process (PID=22977) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:34,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:27:34,453] {logging_mixin.py:112} INFO - [2020-11-01 20:27:34,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:34,642] {logging_mixin.py:112} INFO - [2020-11-01 20:27:34,640] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:27:34,642] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:34,679] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-01 20:27:47,718] {scheduler_job.py:155} INFO - Started process (PID=23044) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:47,725] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:27:47,725] {logging_mixin.py:112} INFO - [2020-11-01 20:27:47,725] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:47,846] {logging_mixin.py:112} INFO - [2020-11-01 20:27:47,845] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:27:47,847] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:27:47,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-01 20:28:01,036] {scheduler_job.py:155} INFO - Started process (PID=23126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:01,042] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:28:01,043] {logging_mixin.py:112} INFO - [2020-11-01 20:28:01,043] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:01,150] {logging_mixin.py:112} INFO - [2020-11-01 20:28:01,149] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:28:01,151] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:01,175] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-01 20:28:14,300] {scheduler_job.py:155} INFO - Started process (PID=23204) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:14,308] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:28:14,309] {logging_mixin.py:112} INFO - [2020-11-01 20:28:14,308] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:14,412] {logging_mixin.py:112} INFO - [2020-11-01 20:28:14,411] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:28:14,412] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:14,436] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-01 20:28:27,668] {scheduler_job.py:155} INFO - Started process (PID=23269) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:27,672] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:28:27,673] {logging_mixin.py:112} INFO - [2020-11-01 20:28:27,673] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:27,790] {logging_mixin.py:112} INFO - [2020-11-01 20:28:27,789] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:28:27,790] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:27,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:28:40,960] {scheduler_job.py:155} INFO - Started process (PID=23332) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:40,965] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:28:40,966] {logging_mixin.py:112} INFO - [2020-11-01 20:28:40,966] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:41,176] {logging_mixin.py:112} INFO - [2020-11-01 20:28:41,175] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:28:41,178] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:41,215] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.254 seconds
[2020-11-01 20:28:54,230] {scheduler_job.py:155} INFO - Started process (PID=23392) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:54,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:28:54,234] {logging_mixin.py:112} INFO - [2020-11-01 20:28:54,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:54,351] {logging_mixin.py:112} INFO - [2020-11-01 20:28:54,350] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:28:54,352] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:28:54,377] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-01 20:29:07,529] {scheduler_job.py:155} INFO - Started process (PID=23459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:07,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:29:07,537] {logging_mixin.py:112} INFO - [2020-11-01 20:29:07,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:07,700] {logging_mixin.py:112} INFO - [2020-11-01 20:29:07,698] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:29:07,700] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:07,729] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.200 seconds
[2020-11-01 20:29:20,785] {scheduler_job.py:155} INFO - Started process (PID=23525) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:20,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:29:20,792] {logging_mixin.py:112} INFO - [2020-11-01 20:29:20,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:20,931] {logging_mixin.py:112} INFO - [2020-11-01 20:29:20,930] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:29:20,931] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:20,965] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-01 20:29:33,964] {scheduler_job.py:155} INFO - Started process (PID=23584) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:33,973] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:29:33,975] {logging_mixin.py:112} INFO - [2020-11-01 20:29:33,974] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:34,137] {logging_mixin.py:112} INFO - [2020-11-01 20:29:34,135] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:29:34,138] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:34,171] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.206 seconds
[2020-11-01 20:29:47,195] {scheduler_job.py:155} INFO - Started process (PID=23650) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:47,199] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:29:47,199] {logging_mixin.py:112} INFO - [2020-11-01 20:29:47,199] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:47,347] {logging_mixin.py:112} INFO - [2020-11-01 20:29:47,346] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:29:47,347] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:29:47,375] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-01 20:30:00,519] {scheduler_job.py:155} INFO - Started process (PID=23715) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:00,525] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:30:00,528] {logging_mixin.py:112} INFO - [2020-11-01 20:30:00,526] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:00,712] {logging_mixin.py:112} INFO - [2020-11-01 20:30:00,711] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:30:00,712] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:00,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.226 seconds
[2020-11-01 20:30:13,861] {scheduler_job.py:155} INFO - Started process (PID=23778) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:13,866] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:30:13,867] {logging_mixin.py:112} INFO - [2020-11-01 20:30:13,867] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:13,997] {logging_mixin.py:112} INFO - [2020-11-01 20:30:13,996] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:30:13,998] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:14,026] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-01 20:30:27,118] {scheduler_job.py:155} INFO - Started process (PID=23838) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:27,121] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:30:27,122] {logging_mixin.py:112} INFO - [2020-11-01 20:30:27,122] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:27,245] {logging_mixin.py:112} INFO - [2020-11-01 20:30:27,243] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:30:27,245] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:27,276] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-01 20:30:40,451] {scheduler_job.py:155} INFO - Started process (PID=23900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:40,474] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:30:40,475] {logging_mixin.py:112} INFO - [2020-11-01 20:30:40,475] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:40,785] {logging_mixin.py:112} INFO - [2020-11-01 20:30:40,784] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:30:40,786] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:40,820] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.368 seconds
[2020-11-01 20:30:53,736] {scheduler_job.py:155} INFO - Started process (PID=23956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:53,740] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:30:53,741] {logging_mixin.py:112} INFO - [2020-11-01 20:30:53,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:53,840] {logging_mixin.py:112} INFO - [2020-11-01 20:30:53,839] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:30:53,840] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:30:53,863] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-01 20:31:07,023] {scheduler_job.py:155} INFO - Started process (PID=24017) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:07,027] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:31:07,028] {logging_mixin.py:112} INFO - [2020-11-01 20:31:07,028] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:07,124] {logging_mixin.py:112} INFO - [2020-11-01 20:31:07,123] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:31:07,124] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:07,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-01 20:31:20,256] {scheduler_job.py:155} INFO - Started process (PID=24082) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:20,261] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:31:20,261] {logging_mixin.py:112} INFO - [2020-11-01 20:31:20,261] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:20,357] {logging_mixin.py:112} INFO - [2020-11-01 20:31:20,356] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:31:20,358] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:20,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-01 20:31:33,654] {scheduler_job.py:155} INFO - Started process (PID=24141) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:33,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:31:33,660] {logging_mixin.py:112} INFO - [2020-11-01 20:31:33,660] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:33,755] {logging_mixin.py:112} INFO - [2020-11-01 20:31:33,754] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:31:33,756] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:33,784] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-01 20:31:46,940] {scheduler_job.py:155} INFO - Started process (PID=24208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:46,947] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:31:46,948] {logging_mixin.py:112} INFO - [2020-11-01 20:31:46,947] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:47,051] {logging_mixin.py:112} INFO - [2020-11-01 20:31:47,049] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:31:47,051] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:31:47,073] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:32:00,242] {scheduler_job.py:155} INFO - Started process (PID=24271) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:00,247] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:32:00,247] {logging_mixin.py:112} INFO - [2020-11-01 20:32:00,247] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:00,359] {logging_mixin.py:112} INFO - [2020-11-01 20:32:00,358] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:32:00,359] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:00,384] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 20:32:13,505] {scheduler_job.py:155} INFO - Started process (PID=24337) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:13,511] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:32:13,517] {logging_mixin.py:112} INFO - [2020-11-01 20:32:13,516] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:13,643] {logging_mixin.py:112} INFO - [2020-11-01 20:32:13,642] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:32:13,643] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:13,667] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-01 20:32:26,830] {scheduler_job.py:155} INFO - Started process (PID=24403) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:26,834] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:32:26,835] {logging_mixin.py:112} INFO - [2020-11-01 20:32:26,834] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:26,942] {logging_mixin.py:112} INFO - [2020-11-01 20:32:26,941] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:32:26,942] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:26,967] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 20:32:40,088] {scheduler_job.py:155} INFO - Started process (PID=24467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:40,092] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:32:40,093] {logging_mixin.py:112} INFO - [2020-11-01 20:32:40,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:40,192] {logging_mixin.py:112} INFO - [2020-11-01 20:32:40,191] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:32:40,192] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:40,215] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-01 20:32:53,275] {scheduler_job.py:155} INFO - Started process (PID=24536) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:53,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:32:53,280] {logging_mixin.py:112} INFO - [2020-11-01 20:32:53,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:53,405] {logging_mixin.py:112} INFO - [2020-11-01 20:32:53,404] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:32:53,405] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:32:53,439] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 20:33:06,551] {scheduler_job.py:155} INFO - Started process (PID=24607) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:06,558] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:33:06,559] {logging_mixin.py:112} INFO - [2020-11-01 20:33:06,559] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:06,679] {logging_mixin.py:112} INFO - [2020-11-01 20:33:06,678] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:33:06,680] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:06,705] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-01 20:33:19,752] {scheduler_job.py:155} INFO - Started process (PID=24681) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:19,756] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:33:19,757] {logging_mixin.py:112} INFO - [2020-11-01 20:33:19,757] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:19,882] {logging_mixin.py:112} INFO - [2020-11-01 20:33:19,881] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:33:19,882] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:19,909] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-01 20:33:32,961] {scheduler_job.py:155} INFO - Started process (PID=24744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:32,965] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:33:32,965] {logging_mixin.py:112} INFO - [2020-11-01 20:33:32,965] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:33,074] {logging_mixin.py:112} INFO - [2020-11-01 20:33:33,073] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:33:33,075] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:33,099] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-01 20:33:46,142] {scheduler_job.py:155} INFO - Started process (PID=24808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:46,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:33:46,147] {logging_mixin.py:112} INFO - [2020-11-01 20:33:46,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:46,259] {logging_mixin.py:112} INFO - [2020-11-01 20:33:46,258] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:33:46,259] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:46,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-01 20:33:59,429] {scheduler_job.py:155} INFO - Started process (PID=24871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:59,434] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:33:59,434] {logging_mixin.py:112} INFO - [2020-11-01 20:33:59,434] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:59,544] {logging_mixin.py:112} INFO - [2020-11-01 20:33:59,543] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:33:59,544] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:33:59,571] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 20:34:12,644] {scheduler_job.py:155} INFO - Started process (PID=24934) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:12,657] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:34:12,665] {logging_mixin.py:112} INFO - [2020-11-01 20:34:12,665] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:12,896] {logging_mixin.py:112} INFO - [2020-11-01 20:34:12,895] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:34:12,896] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:12,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.291 seconds
[2020-11-01 20:34:25,948] {scheduler_job.py:155} INFO - Started process (PID=24998) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:25,952] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:34:25,953] {logging_mixin.py:112} INFO - [2020-11-01 20:34:25,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:26,061] {logging_mixin.py:112} INFO - [2020-11-01 20:34:26,060] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:34:26,062] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:26,090] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 20:34:39,241] {scheduler_job.py:155} INFO - Started process (PID=25064) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:39,245] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:34:39,245] {logging_mixin.py:112} INFO - [2020-11-01 20:34:39,245] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:39,340] {logging_mixin.py:112} INFO - [2020-11-01 20:34:39,339] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:34:39,340] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:39,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-01 20:34:52,441] {scheduler_job.py:155} INFO - Started process (PID=25130) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:52,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:34:52,445] {logging_mixin.py:112} INFO - [2020-11-01 20:34:52,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:52,558] {logging_mixin.py:112} INFO - [2020-11-01 20:34:52,557] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:34:52,559] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:34:52,585] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 20:35:05,707] {scheduler_job.py:155} INFO - Started process (PID=25194) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:05,711] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:35:05,712] {logging_mixin.py:112} INFO - [2020-11-01 20:35:05,712] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:05,850] {logging_mixin.py:112} INFO - [2020-11-01 20:35:05,848] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:35:05,850] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:05,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-01 20:35:18,955] {scheduler_job.py:155} INFO - Started process (PID=25260) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:18,958] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:35:18,959] {logging_mixin.py:112} INFO - [2020-11-01 20:35:18,959] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:19,077] {logging_mixin.py:112} INFO - [2020-11-01 20:35:19,076] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:35:19,078] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:19,108] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-01 20:35:32,253] {scheduler_job.py:155} INFO - Started process (PID=25323) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:32,256] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:35:32,257] {logging_mixin.py:112} INFO - [2020-11-01 20:35:32,257] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:32,359] {logging_mixin.py:112} INFO - [2020-11-01 20:35:32,357] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:35:32,359] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:32,384] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-01 20:35:45,470] {scheduler_job.py:155} INFO - Started process (PID=25390) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:45,477] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:35:45,483] {logging_mixin.py:112} INFO - [2020-11-01 20:35:45,480] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:45,664] {logging_mixin.py:112} INFO - [2020-11-01 20:35:45,663] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:35:45,665] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:45,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.220 seconds
[2020-11-01 20:35:58,702] {scheduler_job.py:155} INFO - Started process (PID=25451) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:58,705] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:35:58,706] {logging_mixin.py:112} INFO - [2020-11-01 20:35:58,706] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:58,811] {logging_mixin.py:112} INFO - [2020-11-01 20:35:58,810] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:35:58,812] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:35:58,839] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 20:36:11,938] {scheduler_job.py:155} INFO - Started process (PID=25513) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:11,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:36:11,943] {logging_mixin.py:112} INFO - [2020-11-01 20:36:11,942] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:12,067] {logging_mixin.py:112} INFO - [2020-11-01 20:36:12,066] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:36:12,067] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:12,093] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-01 20:36:25,127] {scheduler_job.py:155} INFO - Started process (PID=25577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:25,131] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:36:25,132] {logging_mixin.py:112} INFO - [2020-11-01 20:36:25,132] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:25,226] {logging_mixin.py:112} INFO - [2020-11-01 20:36:25,225] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:36:25,226] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:25,253] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-01 20:36:38,470] {scheduler_job.py:155} INFO - Started process (PID=25639) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:38,474] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:36:38,474] {logging_mixin.py:112} INFO - [2020-11-01 20:36:38,474] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:38,624] {logging_mixin.py:112} INFO - [2020-11-01 20:36:38,623] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:36:38,624] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:38,652] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-01 20:36:51,750] {scheduler_job.py:155} INFO - Started process (PID=25704) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:51,757] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:36:51,758] {logging_mixin.py:112} INFO - [2020-11-01 20:36:51,758] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:51,947] {logging_mixin.py:112} INFO - [2020-11-01 20:36:51,946] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:36:51,948] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:36:51,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-01 20:37:04,955] {scheduler_job.py:155} INFO - Started process (PID=25766) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:04,959] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:37:04,959] {logging_mixin.py:112} INFO - [2020-11-01 20:37:04,959] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:05,079] {logging_mixin.py:112} INFO - [2020-11-01 20:37:05,078] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:37:05,080] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:05,105] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:37:18,214] {scheduler_job.py:155} INFO - Started process (PID=25832) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:18,227] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:37:18,228] {logging_mixin.py:112} INFO - [2020-11-01 20:37:18,227] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:18,421] {logging_mixin.py:112} INFO - [2020-11-01 20:37:18,419] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:37:18,421] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:18,462] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.248 seconds
[2020-11-01 20:37:31,533] {scheduler_job.py:155} INFO - Started process (PID=25891) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:31,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:37:31,537] {logging_mixin.py:112} INFO - [2020-11-01 20:37:31,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:31,643] {logging_mixin.py:112} INFO - [2020-11-01 20:37:31,642] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:37:31,644] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:31,673] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-01 20:37:44,840] {scheduler_job.py:155} INFO - Started process (PID=25956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:44,845] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:37:44,846] {logging_mixin.py:112} INFO - [2020-11-01 20:37:44,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:44,954] {logging_mixin.py:112} INFO - [2020-11-01 20:37:44,953] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:37:44,955] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:44,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 20:37:58,072] {scheduler_job.py:155} INFO - Started process (PID=26024) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:58,076] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:37:58,076] {logging_mixin.py:112} INFO - [2020-11-01 20:37:58,076] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:58,180] {logging_mixin.py:112} INFO - [2020-11-01 20:37:58,179] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:37:58,180] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:37:58,205] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:38:11,397] {scheduler_job.py:155} INFO - Started process (PID=26086) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:11,409] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:38:11,410] {logging_mixin.py:112} INFO - [2020-11-01 20:38:11,409] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:11,566] {logging_mixin.py:112} INFO - [2020-11-01 20:38:11,565] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:38:11,566] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:11,589] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-01 20:38:24,732] {scheduler_job.py:155} INFO - Started process (PID=26149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:24,737] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:38:24,738] {logging_mixin.py:112} INFO - [2020-11-01 20:38:24,738] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:24,911] {logging_mixin.py:112} INFO - [2020-11-01 20:38:24,909] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:38:24,912] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:24,941] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.209 seconds
[2020-11-01 20:38:37,969] {scheduler_job.py:155} INFO - Started process (PID=26212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:37,972] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:38:37,973] {logging_mixin.py:112} INFO - [2020-11-01 20:38:37,973] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:38,076] {logging_mixin.py:112} INFO - [2020-11-01 20:38:38,074] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:38:38,076] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:38,101] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:38:51,226] {scheduler_job.py:155} INFO - Started process (PID=26281) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:51,232] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:38:51,233] {logging_mixin.py:112} INFO - [2020-11-01 20:38:51,233] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:51,339] {logging_mixin.py:112} INFO - [2020-11-01 20:38:51,338] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:38:51,339] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:38:51,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-01 20:39:04,557] {scheduler_job.py:155} INFO - Started process (PID=26342) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:04,561] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:39:04,561] {logging_mixin.py:112} INFO - [2020-11-01 20:39:04,561] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:04,663] {logging_mixin.py:112} INFO - [2020-11-01 20:39:04,662] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:39:04,664] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:04,689] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 20:39:17,942] {scheduler_job.py:155} INFO - Started process (PID=26408) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:17,947] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:39:17,952] {logging_mixin.py:112} INFO - [2020-11-01 20:39:17,951] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:18,142] {logging_mixin.py:112} INFO - [2020-11-01 20:39:18,141] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:39:18,143] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:18,166] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-01 20:39:31,229] {scheduler_job.py:155} INFO - Started process (PID=26472) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:31,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:39:31,234] {logging_mixin.py:112} INFO - [2020-11-01 20:39:31,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:31,337] {logging_mixin.py:112} INFO - [2020-11-01 20:39:31,336] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:39:31,337] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:31,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-01 20:39:44,581] {scheduler_job.py:155} INFO - Started process (PID=26535) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:44,585] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:39:44,585] {logging_mixin.py:112} INFO - [2020-11-01 20:39:44,585] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:44,768] {logging_mixin.py:112} INFO - [2020-11-01 20:39:44,766] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:39:44,768] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:44,814] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-01 20:39:57,827] {scheduler_job.py:155} INFO - Started process (PID=26594) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:57,832] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:39:57,833] {logging_mixin.py:112} INFO - [2020-11-01 20:39:57,832] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:57,947] {logging_mixin.py:112} INFO - [2020-11-01 20:39:57,946] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:39:57,947] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:39:57,974] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-01 20:40:11,122] {scheduler_job.py:155} INFO - Started process (PID=26654) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:11,128] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:40:11,128] {logging_mixin.py:112} INFO - [2020-11-01 20:40:11,128] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:11,278] {logging_mixin.py:112} INFO - [2020-11-01 20:40:11,277] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:40:11,278] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:11,303] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-01 20:40:24,399] {scheduler_job.py:155} INFO - Started process (PID=26718) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:24,403] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:40:24,404] {logging_mixin.py:112} INFO - [2020-11-01 20:40:24,403] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:24,646] {logging_mixin.py:112} INFO - [2020-11-01 20:40:24,645] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:40:24,647] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:24,680] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.281 seconds
[2020-11-01 20:40:37,679] {scheduler_job.py:155} INFO - Started process (PID=26775) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:37,684] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:40:37,684] {logging_mixin.py:112} INFO - [2020-11-01 20:40:37,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:37,793] {logging_mixin.py:112} INFO - [2020-11-01 20:40:37,792] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:40:37,793] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:37,820] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-01 20:40:50,926] {scheduler_job.py:155} INFO - Started process (PID=26845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:50,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:40:50,942] {logging_mixin.py:112} INFO - [2020-11-01 20:40:50,942] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:51,088] {logging_mixin.py:112} INFO - [2020-11-01 20:40:51,087] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:40:51,088] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:40:51,111] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-01 20:41:04,153] {scheduler_job.py:155} INFO - Started process (PID=26905) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:04,157] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:41:04,158] {logging_mixin.py:112} INFO - [2020-11-01 20:41:04,158] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:04,265] {logging_mixin.py:112} INFO - [2020-11-01 20:41:04,264] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:41:04,265] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:04,298] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-01 20:41:17,422] {scheduler_job.py:155} INFO - Started process (PID=26966) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:17,427] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:41:17,428] {logging_mixin.py:112} INFO - [2020-11-01 20:41:17,428] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:17,550] {logging_mixin.py:112} INFO - [2020-11-01 20:41:17,549] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:41:17,551] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:17,586] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 20:41:30,743] {scheduler_job.py:155} INFO - Started process (PID=27028) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:30,750] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:41:30,751] {logging_mixin.py:112} INFO - [2020-11-01 20:41:30,750] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:30,906] {logging_mixin.py:112} INFO - [2020-11-01 20:41:30,905] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:41:30,907] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:30,958] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.215 seconds
[2020-11-01 20:41:44,027] {scheduler_job.py:155} INFO - Started process (PID=27086) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:44,031] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:41:44,034] {logging_mixin.py:112} INFO - [2020-11-01 20:41:44,034] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:44,148] {logging_mixin.py:112} INFO - [2020-11-01 20:41:44,147] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:41:44,148] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:44,173] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-01 20:41:57,228] {scheduler_job.py:155} INFO - Started process (PID=27150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:57,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:41:57,234] {logging_mixin.py:112} INFO - [2020-11-01 20:41:57,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:57,340] {logging_mixin.py:112} INFO - [2020-11-01 20:41:57,339] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:41:57,341] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:41:57,370] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 20:42:10,485] {scheduler_job.py:155} INFO - Started process (PID=27210) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:10,488] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:42:10,489] {logging_mixin.py:112} INFO - [2020-11-01 20:42:10,489] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:10,594] {logging_mixin.py:112} INFO - [2020-11-01 20:42:10,593] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:42:10,594] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:10,618] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-01 20:42:23,799] {scheduler_job.py:155} INFO - Started process (PID=27276) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:23,811] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:42:23,812] {logging_mixin.py:112} INFO - [2020-11-01 20:42:23,811] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:24,020] {logging_mixin.py:112} INFO - [2020-11-01 20:42:24,019] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:42:24,021] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:24,047] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-01 20:42:37,009] {scheduler_job.py:155} INFO - Started process (PID=27336) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:37,013] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:42:37,014] {logging_mixin.py:112} INFO - [2020-11-01 20:42:37,014] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:37,128] {logging_mixin.py:112} INFO - [2020-11-01 20:42:37,126] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:42:37,128] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:37,154] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-01 20:42:50,258] {scheduler_job.py:155} INFO - Started process (PID=27398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:50,263] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:42:50,264] {logging_mixin.py:112} INFO - [2020-11-01 20:42:50,264] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:50,383] {logging_mixin.py:112} INFO - [2020-11-01 20:42:50,382] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:42:50,383] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:42:50,410] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 20:43:03,514] {scheduler_job.py:155} INFO - Started process (PID=27464) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:03,528] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:43:03,529] {logging_mixin.py:112} INFO - [2020-11-01 20:43:03,529] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:03,651] {logging_mixin.py:112} INFO - [2020-11-01 20:43:03,650] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:43:03,651] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:03,679] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-01 20:43:16,803] {scheduler_job.py:155} INFO - Started process (PID=27526) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:16,807] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:43:16,808] {logging_mixin.py:112} INFO - [2020-11-01 20:43:16,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:16,924] {logging_mixin.py:112} INFO - [2020-11-01 20:43:16,923] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:43:16,924] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:16,951] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-01 20:43:30,079] {scheduler_job.py:155} INFO - Started process (PID=27588) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:30,082] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:43:30,083] {logging_mixin.py:112} INFO - [2020-11-01 20:43:30,083] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:30,230] {logging_mixin.py:112} INFO - [2020-11-01 20:43:30,229] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:43:30,230] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:30,261] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.183 seconds
[2020-11-01 20:43:43,335] {scheduler_job.py:155} INFO - Started process (PID=27652) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:43,340] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:43:43,341] {logging_mixin.py:112} INFO - [2020-11-01 20:43:43,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:43,447] {logging_mixin.py:112} INFO - [2020-11-01 20:43:43,445] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:43:43,447] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:43,471] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-01 20:43:56,593] {scheduler_job.py:155} INFO - Started process (PID=27717) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:56,605] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:43:56,605] {logging_mixin.py:112} INFO - [2020-11-01 20:43:56,605] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:56,856] {logging_mixin.py:112} INFO - [2020-11-01 20:43:56,854] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:43:56,856] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:43:56,884] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.291 seconds
[2020-11-01 20:44:09,912] {scheduler_job.py:155} INFO - Started process (PID=27779) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:09,918] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:44:09,919] {logging_mixin.py:112} INFO - [2020-11-01 20:44:09,919] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:10,039] {logging_mixin.py:112} INFO - [2020-11-01 20:44:10,037] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:44:10,039] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:10,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-01 20:44:23,185] {scheduler_job.py:155} INFO - Started process (PID=27841) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:23,192] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:44:23,193] {logging_mixin.py:112} INFO - [2020-11-01 20:44:23,192] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:23,332] {logging_mixin.py:112} INFO - [2020-11-01 20:44:23,331] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:44:23,333] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:23,364] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-01 20:44:36,366] {scheduler_job.py:155} INFO - Started process (PID=27905) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:36,370] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:44:36,370] {logging_mixin.py:112} INFO - [2020-11-01 20:44:36,370] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:36,471] {logging_mixin.py:112} INFO - [2020-11-01 20:44:36,469] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:44:36,471] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:36,499] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-01 20:44:49,664] {scheduler_job.py:155} INFO - Started process (PID=27968) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:49,669] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:44:49,672] {logging_mixin.py:112} INFO - [2020-11-01 20:44:49,670] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:49,825] {logging_mixin.py:112} INFO - [2020-11-01 20:44:49,824] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:44:49,826] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:44:49,859] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.195 seconds
[2020-11-01 20:45:02,936] {scheduler_job.py:155} INFO - Started process (PID=28031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:02,948] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:45:02,949] {logging_mixin.py:112} INFO - [2020-11-01 20:45:02,949] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:03,112] {logging_mixin.py:112} INFO - [2020-11-01 20:45:03,110] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:45:03,112] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:03,150] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-01 20:45:16,184] {scheduler_job.py:155} INFO - Started process (PID=28092) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:16,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:45:16,188] {logging_mixin.py:112} INFO - [2020-11-01 20:45:16,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:16,317] {logging_mixin.py:112} INFO - [2020-11-01 20:45:16,316] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:45:16,318] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:16,346] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-01 20:45:29,403] {scheduler_job.py:155} INFO - Started process (PID=28156) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:29,407] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:45:29,407] {logging_mixin.py:112} INFO - [2020-11-01 20:45:29,407] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:29,530] {logging_mixin.py:112} INFO - [2020-11-01 20:45:29,528] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:45:29,530] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:29,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 20:45:42,706] {scheduler_job.py:155} INFO - Started process (PID=28215) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:42,712] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:45:42,713] {logging_mixin.py:112} INFO - [2020-11-01 20:45:42,713] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:42,841] {logging_mixin.py:112} INFO - [2020-11-01 20:45:42,839] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:45:42,841] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:42,871] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-01 20:45:55,903] {scheduler_job.py:155} INFO - Started process (PID=28278) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:55,916] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:45:55,916] {logging_mixin.py:112} INFO - [2020-11-01 20:45:55,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:56,078] {logging_mixin.py:112} INFO - [2020-11-01 20:45:56,076] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:45:56,078] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:45:56,108] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-01 20:46:09,217] {scheduler_job.py:155} INFO - Started process (PID=28335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:09,227] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:46:09,227] {logging_mixin.py:112} INFO - [2020-11-01 20:46:09,227] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:09,389] {logging_mixin.py:112} INFO - [2020-11-01 20:46:09,387] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:46:09,389] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:09,422] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-01 20:46:22,531] {scheduler_job.py:155} INFO - Started process (PID=28397) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:22,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:46:22,537] {logging_mixin.py:112} INFO - [2020-11-01 20:46:22,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:22,679] {logging_mixin.py:112} INFO - [2020-11-01 20:46:22,678] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:46:22,679] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:22,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-01 20:46:35,756] {scheduler_job.py:155} INFO - Started process (PID=28461) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:35,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:46:35,763] {logging_mixin.py:112} INFO - [2020-11-01 20:46:35,763] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:35,974] {logging_mixin.py:112} INFO - [2020-11-01 20:46:35,973] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:46:35,975] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:36,012] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.256 seconds
[2020-11-01 20:46:49,010] {scheduler_job.py:155} INFO - Started process (PID=28522) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:49,014] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:46:49,014] {logging_mixin.py:112} INFO - [2020-11-01 20:46:49,014] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:49,121] {logging_mixin.py:112} INFO - [2020-11-01 20:46:49,120] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:46:49,122] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:46:49,147] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-01 20:47:02,221] {scheduler_job.py:155} INFO - Started process (PID=28585) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:02,238] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:47:02,239] {logging_mixin.py:112} INFO - [2020-11-01 20:47:02,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:02,387] {logging_mixin.py:112} INFO - [2020-11-01 20:47:02,386] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:47:02,388] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:02,410] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.189 seconds
[2020-11-01 20:47:15,612] {scheduler_job.py:155} INFO - Started process (PID=28651) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:15,618] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:47:15,620] {logging_mixin.py:112} INFO - [2020-11-01 20:47:15,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:15,739] {logging_mixin.py:112} INFO - [2020-11-01 20:47:15,738] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:47:15,739] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:15,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 20:47:28,921] {scheduler_job.py:155} INFO - Started process (PID=28725) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:28,929] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:47:28,930] {logging_mixin.py:112} INFO - [2020-11-01 20:47:28,930] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:29,308] {logging_mixin.py:112} INFO - [2020-11-01 20:47:29,306] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:47:29,309] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:29,341] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.420 seconds
[2020-11-01 20:47:42,189] {scheduler_job.py:155} INFO - Started process (PID=28791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:42,201] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:47:42,202] {logging_mixin.py:112} INFO - [2020-11-01 20:47:42,202] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:42,329] {logging_mixin.py:112} INFO - [2020-11-01 20:47:42,327] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:47:42,329] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:42,356] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-01 20:47:55,447] {scheduler_job.py:155} INFO - Started process (PID=28862) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:55,450] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:47:55,451] {logging_mixin.py:112} INFO - [2020-11-01 20:47:55,451] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:55,559] {logging_mixin.py:112} INFO - [2020-11-01 20:47:55,558] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:47:55,560] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:47:55,584] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 20:48:08,754] {scheduler_job.py:155} INFO - Started process (PID=28927) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:08,760] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:48:08,763] {logging_mixin.py:112} INFO - [2020-11-01 20:48:08,761] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:08,991] {logging_mixin.py:112} INFO - [2020-11-01 20:48:08,989] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:48:08,991] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:09,026] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.271 seconds
[2020-11-01 20:48:22,044] {scheduler_job.py:155} INFO - Started process (PID=29000) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:22,058] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:48:22,067] {logging_mixin.py:112} INFO - [2020-11-01 20:48:22,067] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:22,201] {logging_mixin.py:112} INFO - [2020-11-01 20:48:22,200] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:48:22,202] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:22,226] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-01 20:48:35,372] {scheduler_job.py:155} INFO - Started process (PID=29064) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:35,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:48:35,378] {logging_mixin.py:112} INFO - [2020-11-01 20:48:35,378] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:35,559] {logging_mixin.py:112} INFO - [2020-11-01 20:48:35,557] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:48:35,561] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:35,597] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.225 seconds
[2020-11-01 20:48:48,728] {scheduler_job.py:155} INFO - Started process (PID=29135) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:48,732] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:48:48,732] {logging_mixin.py:112} INFO - [2020-11-01 20:48:48,732] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:48,846] {logging_mixin.py:112} INFO - [2020-11-01 20:48:48,845] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:48:48,846] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:48:48,872] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-01 20:49:01,999] {scheduler_job.py:155} INFO - Started process (PID=29208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:02,009] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:49:02,010] {logging_mixin.py:112} INFO - [2020-11-01 20:49:02,009] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:02,259] {logging_mixin.py:112} INFO - [2020-11-01 20:49:02,250] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:49:02,260] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:02,310] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.311 seconds
[2020-11-01 20:49:15,217] {scheduler_job.py:155} INFO - Started process (PID=29272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:15,221] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:49:15,222] {logging_mixin.py:112} INFO - [2020-11-01 20:49:15,222] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:15,329] {logging_mixin.py:112} INFO - [2020-11-01 20:49:15,328] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:49:15,330] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:15,356] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-01 20:49:28,514] {scheduler_job.py:155} INFO - Started process (PID=29342) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:28,523] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:49:28,525] {logging_mixin.py:112} INFO - [2020-11-01 20:49:28,525] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:28,666] {logging_mixin.py:112} INFO - [2020-11-01 20:49:28,665] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:49:28,667] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:28,697] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.183 seconds
[2020-11-01 20:49:41,705] {scheduler_job.py:155} INFO - Started process (PID=29409) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:41,709] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:49:41,710] {logging_mixin.py:112} INFO - [2020-11-01 20:49:41,710] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:41,857] {logging_mixin.py:112} INFO - [2020-11-01 20:49:41,855] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:49:41,858] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:41,886] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-01 20:49:54,985] {scheduler_job.py:155} INFO - Started process (PID=29479) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:54,990] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:49:54,991] {logging_mixin.py:112} INFO - [2020-11-01 20:49:54,990] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:55,113] {logging_mixin.py:112} INFO - [2020-11-01 20:49:55,112] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:49:55,113] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:49:55,144] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 20:50:08,232] {scheduler_job.py:155} INFO - Started process (PID=29547) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:08,243] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:50:08,244] {logging_mixin.py:112} INFO - [2020-11-01 20:50:08,244] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:08,411] {logging_mixin.py:112} INFO - [2020-11-01 20:50:08,410] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:50:08,412] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:08,453] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.221 seconds
[2020-11-01 20:50:21,542] {scheduler_job.py:155} INFO - Started process (PID=29613) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:21,547] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:50:21,547] {logging_mixin.py:112} INFO - [2020-11-01 20:50:21,547] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:21,687] {logging_mixin.py:112} INFO - [2020-11-01 20:50:21,685] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:50:21,687] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:21,713] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-01 20:50:34,749] {scheduler_job.py:155} INFO - Started process (PID=29680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:34,758] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:50:34,762] {logging_mixin.py:112} INFO - [2020-11-01 20:50:34,760] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:34,971] {logging_mixin.py:112} INFO - [2020-11-01 20:50:34,970] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:50:34,972] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:35,001] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.252 seconds
[2020-11-01 20:50:47,964] {scheduler_job.py:155} INFO - Started process (PID=29744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:47,969] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:50:47,969] {logging_mixin.py:112} INFO - [2020-11-01 20:50:47,969] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:48,133] {logging_mixin.py:112} INFO - [2020-11-01 20:50:48,132] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:50:48,133] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:50:48,171] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.207 seconds
[2020-11-01 20:51:01,232] {scheduler_job.py:155} INFO - Started process (PID=29804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:01,242] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:51:01,248] {logging_mixin.py:112} INFO - [2020-11-01 20:51:01,247] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:01,688] {logging_mixin.py:112} INFO - [2020-11-01 20:51:01,687] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:51:01,688] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:01,739] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.507 seconds
[2020-11-01 20:51:14,511] {scheduler_job.py:155} INFO - Started process (PID=29873) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:14,516] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:51:14,518] {logging_mixin.py:112} INFO - [2020-11-01 20:51:14,517] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:14,743] {logging_mixin.py:112} INFO - [2020-11-01 20:51:14,742] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:51:14,744] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:14,778] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.266 seconds
[2020-11-01 20:51:27,824] {scheduler_job.py:155} INFO - Started process (PID=29945) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:27,828] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:51:27,829] {logging_mixin.py:112} INFO - [2020-11-01 20:51:27,828] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:27,949] {logging_mixin.py:112} INFO - [2020-11-01 20:51:27,948] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:51:27,950] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:27,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-01 20:51:41,000] {scheduler_job.py:155} INFO - Started process (PID=30013) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:41,003] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:51:41,004] {logging_mixin.py:112} INFO - [2020-11-01 20:51:41,004] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:41,107] {logging_mixin.py:112} INFO - [2020-11-01 20:51:41,106] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:51:41,107] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:41,143] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 20:51:54,255] {scheduler_job.py:155} INFO - Started process (PID=30087) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:54,259] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:51:54,259] {logging_mixin.py:112} INFO - [2020-11-01 20:51:54,259] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:54,367] {logging_mixin.py:112} INFO - [2020-11-01 20:51:54,366] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:51:54,367] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:51:54,396] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-01 20:52:07,466] {scheduler_job.py:155} INFO - Started process (PID=30156) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:07,469] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:52:07,470] {logging_mixin.py:112} INFO - [2020-11-01 20:52:07,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:07,582] {logging_mixin.py:112} INFO - [2020-11-01 20:52:07,581] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:52:07,583] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:07,609] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 20:52:20,789] {scheduler_job.py:155} INFO - Started process (PID=30226) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:20,792] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:52:20,794] {logging_mixin.py:112} INFO - [2020-11-01 20:52:20,794] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:20,894] {logging_mixin.py:112} INFO - [2020-11-01 20:52:20,893] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:52:20,894] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:20,919] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-01 20:52:34,068] {scheduler_job.py:155} INFO - Started process (PID=30302) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:34,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:52:34,072] {logging_mixin.py:112} INFO - [2020-11-01 20:52:34,072] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:34,253] {logging_mixin.py:112} INFO - [2020-11-01 20:52:34,252] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:52:34,254] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:34,279] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-01 20:52:47,396] {scheduler_job.py:155} INFO - Started process (PID=30369) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:47,401] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:52:47,401] {logging_mixin.py:112} INFO - [2020-11-01 20:52:47,401] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:47,509] {logging_mixin.py:112} INFO - [2020-11-01 20:52:47,508] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:52:47,509] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:52:47,538] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 20:53:00,605] {scheduler_job.py:155} INFO - Started process (PID=30445) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:00,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:53:00,610] {logging_mixin.py:112} INFO - [2020-11-01 20:53:00,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:00,715] {logging_mixin.py:112} INFO - [2020-11-01 20:53:00,714] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:53:00,716] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:00,741] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-01 20:53:13,858] {scheduler_job.py:155} INFO - Started process (PID=30514) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:13,864] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:53:13,864] {logging_mixin.py:112} INFO - [2020-11-01 20:53:13,864] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:13,983] {logging_mixin.py:112} INFO - [2020-11-01 20:53:13,982] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:53:13,984] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:14,011] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-01 20:53:27,094] {scheduler_job.py:155} INFO - Started process (PID=30587) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:27,107] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:53:27,107] {logging_mixin.py:112} INFO - [2020-11-01 20:53:27,107] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:27,314] {logging_mixin.py:112} INFO - [2020-11-01 20:53:27,313] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:53:27,315] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:27,353] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.259 seconds
[2020-11-01 20:53:40,355] {scheduler_job.py:155} INFO - Started process (PID=30656) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:40,359] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:53:40,360] {logging_mixin.py:112} INFO - [2020-11-01 20:53:40,360] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:40,468] {logging_mixin.py:112} INFO - [2020-11-01 20:53:40,467] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:53:40,468] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:40,493] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-01 20:53:53,607] {scheduler_job.py:155} INFO - Started process (PID=30726) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:53,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:53:53,612] {logging_mixin.py:112} INFO - [2020-11-01 20:53:53,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:53,722] {logging_mixin.py:112} INFO - [2020-11-01 20:53:53,721] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:53:53,723] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:53:53,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 20:54:06,878] {scheduler_job.py:155} INFO - Started process (PID=30798) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:06,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:54:06,905] {logging_mixin.py:112} INFO - [2020-11-01 20:54:06,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:07,172] {logging_mixin.py:112} INFO - [2020-11-01 20:54:07,171] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:54:07,173] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:07,205] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.326 seconds
[2020-11-01 20:54:20,138] {scheduler_job.py:155} INFO - Started process (PID=30857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:20,148] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:54:20,150] {logging_mixin.py:112} INFO - [2020-11-01 20:54:20,149] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:20,298] {logging_mixin.py:112} INFO - [2020-11-01 20:54:20,296] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:54:20,299] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:20,333] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.196 seconds
[2020-11-01 20:54:33,391] {scheduler_job.py:155} INFO - Started process (PID=30926) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:33,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:54:33,397] {logging_mixin.py:112} INFO - [2020-11-01 20:54:33,397] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:33,591] {logging_mixin.py:112} INFO - [2020-11-01 20:54:33,589] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:54:33,592] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:33,649] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.258 seconds
[2020-11-01 20:54:46,708] {scheduler_job.py:155} INFO - Started process (PID=30987) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:46,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:54:46,715] {logging_mixin.py:112} INFO - [2020-11-01 20:54:46,715] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:46,835] {logging_mixin.py:112} INFO - [2020-11-01 20:54:46,834] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:54:46,835] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:46,866] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-01 20:54:59,990] {scheduler_job.py:155} INFO - Started process (PID=31058) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:54:59,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:54:59,995] {logging_mixin.py:112} INFO - [2020-11-01 20:54:59,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:00,171] {logging_mixin.py:112} INFO - [2020-11-01 20:55:00,169] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:55:00,171] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:00,198] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.208 seconds
[2020-11-01 20:55:13,193] {scheduler_job.py:155} INFO - Started process (PID=31126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:13,201] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:55:13,202] {logging_mixin.py:112} INFO - [2020-11-01 20:55:13,202] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:13,384] {logging_mixin.py:112} INFO - [2020-11-01 20:55:13,381] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:55:13,384] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:13,429] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-01 20:55:26,456] {scheduler_job.py:155} INFO - Started process (PID=31199) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:26,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:55:26,460] {logging_mixin.py:112} INFO - [2020-11-01 20:55:26,460] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:26,585] {logging_mixin.py:112} INFO - [2020-11-01 20:55:26,584] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:55:26,585] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:26,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 20:55:39,770] {scheduler_job.py:155} INFO - Started process (PID=31268) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:39,786] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:55:39,786] {logging_mixin.py:112} INFO - [2020-11-01 20:55:39,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:39,972] {logging_mixin.py:112} INFO - [2020-11-01 20:55:39,970] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:55:39,972] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:40,006] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-01 20:55:53,020] {scheduler_job.py:155} INFO - Started process (PID=31330) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:53,024] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:55:53,025] {logging_mixin.py:112} INFO - [2020-11-01 20:55:53,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:53,142] {logging_mixin.py:112} INFO - [2020-11-01 20:55:53,141] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:55:53,142] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:55:53,171] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-01 20:56:06,256] {scheduler_job.py:155} INFO - Started process (PID=31398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:06,262] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:56:06,264] {logging_mixin.py:112} INFO - [2020-11-01 20:56:06,263] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:06,394] {logging_mixin.py:112} INFO - [2020-11-01 20:56:06,393] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:56:06,394] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:06,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-01 20:56:19,457] {scheduler_job.py:155} INFO - Started process (PID=31469) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:19,463] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:56:19,464] {logging_mixin.py:112} INFO - [2020-11-01 20:56:19,464] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:19,575] {logging_mixin.py:112} INFO - [2020-11-01 20:56:19,574] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:56:19,575] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:19,604] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-01 20:56:32,904] {scheduler_job.py:155} INFO - Started process (PID=31535) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:32,910] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:56:32,911] {logging_mixin.py:112} INFO - [2020-11-01 20:56:32,910] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:33,041] {logging_mixin.py:112} INFO - [2020-11-01 20:56:33,040] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:56:33,042] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:33,069] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-01 20:56:46,124] {scheduler_job.py:155} INFO - Started process (PID=31602) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:46,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:56:46,130] {logging_mixin.py:112} INFO - [2020-11-01 20:56:46,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:46,243] {logging_mixin.py:112} INFO - [2020-11-01 20:56:46,241] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:56:46,243] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:46,273] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 20:56:59,408] {scheduler_job.py:155} INFO - Started process (PID=31676) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:59,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:56:59,414] {logging_mixin.py:112} INFO - [2020-11-01 20:56:59,413] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:59,533] {logging_mixin.py:112} INFO - [2020-11-01 20:56:59,532] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:56:59,534] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:56:59,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-01 20:57:12,626] {scheduler_job.py:155} INFO - Started process (PID=31745) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:12,631] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:57:12,631] {logging_mixin.py:112} INFO - [2020-11-01 20:57:12,631] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:12,750] {logging_mixin.py:112} INFO - [2020-11-01 20:57:12,749] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:57:12,750] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:12,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-01 20:57:25,951] {scheduler_job.py:155} INFO - Started process (PID=31808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:25,955] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:57:25,956] {logging_mixin.py:112} INFO - [2020-11-01 20:57:25,955] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:26,076] {logging_mixin.py:112} INFO - [2020-11-01 20:57:26,075] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:57:26,076] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:26,104] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-01 20:57:39,198] {scheduler_job.py:155} INFO - Started process (PID=31884) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:39,223] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:57:39,224] {logging_mixin.py:112} INFO - [2020-11-01 20:57:39,223] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:39,572] {logging_mixin.py:112} INFO - [2020-11-01 20:57:39,571] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:57:39,573] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:39,605] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.407 seconds
[2020-11-01 20:57:52,394] {scheduler_job.py:155} INFO - Started process (PID=31950) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:52,400] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:57:52,401] {logging_mixin.py:112} INFO - [2020-11-01 20:57:52,401] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:52,529] {logging_mixin.py:112} INFO - [2020-11-01 20:57:52,528] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:57:52,529] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:57:52,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 20:58:05,774] {scheduler_job.py:155} INFO - Started process (PID=32022) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:05,778] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:58:05,779] {logging_mixin.py:112} INFO - [2020-11-01 20:58:05,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:05,893] {logging_mixin.py:112} INFO - [2020-11-01 20:58:05,892] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:58:05,893] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:05,923] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:58:19,011] {scheduler_job.py:155} INFO - Started process (PID=32087) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:19,020] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:58:19,021] {logging_mixin.py:112} INFO - [2020-11-01 20:58:19,021] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:19,184] {logging_mixin.py:112} INFO - [2020-11-01 20:58:19,183] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:58:19,185] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:19,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.216 seconds
[2020-11-01 20:58:32,272] {scheduler_job.py:155} INFO - Started process (PID=32150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:32,287] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:58:32,288] {logging_mixin.py:112} INFO - [2020-11-01 20:58:32,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:32,471] {logging_mixin.py:112} INFO - [2020-11-01 20:58:32,470] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:58:32,471] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:32,496] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.225 seconds
[2020-11-01 20:58:45,517] {scheduler_job.py:155} INFO - Started process (PID=32209) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:45,524] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:58:45,524] {logging_mixin.py:112} INFO - [2020-11-01 20:58:45,524] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:45,644] {logging_mixin.py:112} INFO - [2020-11-01 20:58:45,643] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:58:45,644] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:45,668] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:58:58,803] {scheduler_job.py:155} INFO - Started process (PID=32272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:58,808] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:58:58,809] {logging_mixin.py:112} INFO - [2020-11-01 20:58:58,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:58,925] {logging_mixin.py:112} INFO - [2020-11-01 20:58:58,924] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:58:58,925] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:58:58,951] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-01 20:59:12,068] {scheduler_job.py:155} INFO - Started process (PID=32333) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:12,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:59:12,072] {logging_mixin.py:112} INFO - [2020-11-01 20:59:12,072] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:12,211] {logging_mixin.py:112} INFO - [2020-11-01 20:59:12,202] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:59:12,211] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:12,243] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-01 20:59:25,292] {scheduler_job.py:155} INFO - Started process (PID=32394) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:25,296] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:59:25,297] {logging_mixin.py:112} INFO - [2020-11-01 20:59:25,297] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:25,413] {logging_mixin.py:112} INFO - [2020-11-01 20:59:25,412] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:59:25,413] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:25,442] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 20:59:38,573] {scheduler_job.py:155} INFO - Started process (PID=32455) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:38,579] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:59:38,580] {logging_mixin.py:112} INFO - [2020-11-01 20:59:38,580] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:38,783] {logging_mixin.py:112} INFO - [2020-11-01 20:59:38,782] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:59:38,783] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:38,808] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-01 20:59:51,851] {scheduler_job.py:155} INFO - Started process (PID=32519) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:51,856] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 20:59:51,857] {logging_mixin.py:112} INFO - [2020-11-01 20:59:51,857] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:51,986] {logging_mixin.py:112} INFO - [2020-11-01 20:59:51,985] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 20:59:51,986] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 20:59:52,013] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-01 21:00:05,107] {scheduler_job.py:155} INFO - Started process (PID=32580) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:05,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:00:05,112] {logging_mixin.py:112} INFO - [2020-11-01 21:00:05,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:05,264] {logging_mixin.py:112} INFO - [2020-11-01 21:00:05,263] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:00:05,264] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:05,294] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-01 21:00:18,360] {scheduler_job.py:155} INFO - Started process (PID=32641) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:18,364] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:00:18,366] {logging_mixin.py:112} INFO - [2020-11-01 21:00:18,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:18,480] {logging_mixin.py:112} INFO - [2020-11-01 21:00:18,479] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:00:18,480] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:18,509] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 21:00:31,631] {scheduler_job.py:155} INFO - Started process (PID=32708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:31,635] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:00:31,636] {logging_mixin.py:112} INFO - [2020-11-01 21:00:31,636] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:31,740] {logging_mixin.py:112} INFO - [2020-11-01 21:00:31,739] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:00:31,740] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:31,767] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 21:00:44,876] {scheduler_job.py:155} INFO - Started process (PID=304) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:44,881] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:00:44,882] {logging_mixin.py:112} INFO - [2020-11-01 21:00:44,882] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:45,122] {logging_mixin.py:112} INFO - [2020-11-01 21:00:45,120] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:00:45,122] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:45,155] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.279 seconds
[2020-11-01 21:00:58,097] {scheduler_job.py:155} INFO - Started process (PID=369) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:58,104] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:00:58,104] {logging_mixin.py:112} INFO - [2020-11-01 21:00:58,104] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:58,232] {logging_mixin.py:112} INFO - [2020-11-01 21:00:58,231] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:00:58,233] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:00:58,256] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 21:01:11,359] {scheduler_job.py:155} INFO - Started process (PID=429) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:11,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:01:11,367] {logging_mixin.py:112} INFO - [2020-11-01 21:01:11,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:11,464] {logging_mixin.py:112} INFO - [2020-11-01 21:01:11,463] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:01:11,465] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:11,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-01 21:01:24,553] {scheduler_job.py:155} INFO - Started process (PID=509) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:24,558] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:01:24,561] {logging_mixin.py:112} INFO - [2020-11-01 21:01:24,561] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:24,689] {logging_mixin.py:112} INFO - [2020-11-01 21:01:24,688] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:01:24,690] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:24,722] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-01 21:01:37,777] {scheduler_job.py:155} INFO - Started process (PID=583) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:37,781] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:01:37,782] {logging_mixin.py:112} INFO - [2020-11-01 21:01:37,782] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:37,899] {logging_mixin.py:112} INFO - [2020-11-01 21:01:37,898] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:01:37,900] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:37,929] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:01:51,016] {scheduler_job.py:155} INFO - Started process (PID=663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:51,021] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:01:51,021] {logging_mixin.py:112} INFO - [2020-11-01 21:01:51,021] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:51,135] {logging_mixin.py:112} INFO - [2020-11-01 21:01:51,134] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:01:51,135] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:01:51,163] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-01 21:02:04,295] {scheduler_job.py:155} INFO - Started process (PID=726) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:04,299] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:02:04,300] {logging_mixin.py:112} INFO - [2020-11-01 21:02:04,300] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:04,411] {logging_mixin.py:112} INFO - [2020-11-01 21:02:04,409] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:02:04,411] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:04,456] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-01 21:02:17,521] {scheduler_job.py:155} INFO - Started process (PID=793) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:17,531] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:02:17,535] {logging_mixin.py:112} INFO - [2020-11-01 21:02:17,535] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:17,758] {logging_mixin.py:112} INFO - [2020-11-01 21:02:17,757] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:02:17,759] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:17,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.272 seconds
[2020-11-01 21:02:30,831] {scheduler_job.py:155} INFO - Started process (PID=857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:30,835] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:02:30,836] {logging_mixin.py:112} INFO - [2020-11-01 21:02:30,836] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:30,951] {logging_mixin.py:112} INFO - [2020-11-01 21:02:30,949] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:02:30,951] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:30,980] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 21:02:44,059] {scheduler_job.py:155} INFO - Started process (PID=916) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:44,063] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:02:44,064] {logging_mixin.py:112} INFO - [2020-11-01 21:02:44,064] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:44,207] {logging_mixin.py:112} INFO - [2020-11-01 21:02:44,206] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:02:44,208] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:44,236] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-01 21:02:57,264] {scheduler_job.py:155} INFO - Started process (PID=985) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:57,267] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:02:57,268] {logging_mixin.py:112} INFO - [2020-11-01 21:02:57,268] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:57,378] {logging_mixin.py:112} INFO - [2020-11-01 21:02:57,377] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:02:57,379] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:02:57,422] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-01 21:03:10,529] {scheduler_job.py:155} INFO - Started process (PID=1054) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:10,533] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:03:10,534] {logging_mixin.py:112} INFO - [2020-11-01 21:03:10,533] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:10,631] {logging_mixin.py:112} INFO - [2020-11-01 21:03:10,630] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:03:10,632] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:10,655] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-01 21:03:23,773] {scheduler_job.py:155} INFO - Started process (PID=1144) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:23,778] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:03:23,778] {logging_mixin.py:112} INFO - [2020-11-01 21:03:23,778] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:23,895] {logging_mixin.py:112} INFO - [2020-11-01 21:03:23,894] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:03:23,896] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:23,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 21:03:37,077] {scheduler_job.py:155} INFO - Started process (PID=1206) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:37,081] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:03:37,082] {logging_mixin.py:112} INFO - [2020-11-01 21:03:37,082] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:37,224] {logging_mixin.py:112} INFO - [2020-11-01 21:03:37,222] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:03:37,224] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:37,263] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-01 21:03:50,282] {scheduler_job.py:155} INFO - Started process (PID=1335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:50,286] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:03:50,287] {logging_mixin.py:112} INFO - [2020-11-01 21:03:50,287] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:50,406] {logging_mixin.py:112} INFO - [2020-11-01 21:03:50,405] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:03:50,407] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:03:50,443] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-01 21:04:03,570] {scheduler_job.py:155} INFO - Started process (PID=1522) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:03,577] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:04:03,578] {logging_mixin.py:112} INFO - [2020-11-01 21:04:03,577] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:03,738] {logging_mixin.py:112} INFO - [2020-11-01 21:04:03,737] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:04:03,739] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:03,772] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-01 21:04:16,910] {scheduler_job.py:155} INFO - Started process (PID=1606) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:16,931] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:04:16,931] {logging_mixin.py:112} INFO - [2020-11-01 21:04:16,931] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:17,155] {logging_mixin.py:112} INFO - [2020-11-01 21:04:17,153] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:04:17,155] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:17,218] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.308 seconds
[2020-11-01 21:04:30,118] {scheduler_job.py:155} INFO - Started process (PID=1697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:30,123] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:04:30,124] {logging_mixin.py:112} INFO - [2020-11-01 21:04:30,124] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:30,326] {logging_mixin.py:112} INFO - [2020-11-01 21:04:30,325] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:04:30,327] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:30,355] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-01 21:04:43,356] {scheduler_job.py:155} INFO - Started process (PID=1823) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:43,361] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:04:43,362] {logging_mixin.py:112} INFO - [2020-11-01 21:04:43,362] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:43,496] {logging_mixin.py:112} INFO - [2020-11-01 21:04:43,495] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:04:43,496] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:43,532] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-01 21:04:56,546] {scheduler_job.py:155} INFO - Started process (PID=1911) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:56,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:04:56,552] {logging_mixin.py:112} INFO - [2020-11-01 21:04:56,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:56,703] {logging_mixin.py:112} INFO - [2020-11-01 21:04:56,702] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:04:56,704] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:04:56,733] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-01 21:05:09,925] {scheduler_job.py:155} INFO - Started process (PID=1980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:09,931] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:05:09,932] {logging_mixin.py:112} INFO - [2020-11-01 21:05:09,931] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:10,145] {logging_mixin.py:112} INFO - [2020-11-01 21:05:10,143] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:05:10,145] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:10,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.256 seconds
[2020-11-01 21:05:23,176] {scheduler_job.py:155} INFO - Started process (PID=2040) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:23,182] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:05:23,195] {logging_mixin.py:112} INFO - [2020-11-01 21:05:23,191] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:23,446] {logging_mixin.py:112} INFO - [2020-11-01 21:05:23,445] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:05:23,447] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:23,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.306 seconds
[2020-11-01 21:05:36,383] {scheduler_job.py:155} INFO - Started process (PID=2100) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:36,387] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:05:36,388] {logging_mixin.py:112} INFO - [2020-11-01 21:05:36,388] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:36,511] {logging_mixin.py:112} INFO - [2020-11-01 21:05:36,510] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:05:36,511] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:36,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 21:05:49,567] {scheduler_job.py:155} INFO - Started process (PID=2178) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:49,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:05:49,593] {logging_mixin.py:112} INFO - [2020-11-01 21:05:49,593] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:49,869] {logging_mixin.py:112} INFO - [2020-11-01 21:05:49,868] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:05:49,870] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:05:49,914] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.347 seconds
[2020-11-01 21:06:02,874] {scheduler_job.py:155} INFO - Started process (PID=2242) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:02,879] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:06:02,880] {logging_mixin.py:112} INFO - [2020-11-01 21:06:02,880] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:02,988] {logging_mixin.py:112} INFO - [2020-11-01 21:06:02,987] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:06:02,988] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:03,023] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 21:06:16,200] {scheduler_job.py:155} INFO - Started process (PID=2304) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:16,206] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:06:16,207] {logging_mixin.py:112} INFO - [2020-11-01 21:06:16,207] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:16,353] {logging_mixin.py:112} INFO - [2020-11-01 21:06:16,352] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:06:16,353] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:16,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-01 21:06:29,405] {scheduler_job.py:155} INFO - Started process (PID=2368) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:29,411] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:06:29,412] {logging_mixin.py:112} INFO - [2020-11-01 21:06:29,412] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:29,521] {logging_mixin.py:112} INFO - [2020-11-01 21:06:29,520] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:06:29,522] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:29,548] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:06:42,676] {scheduler_job.py:155} INFO - Started process (PID=2431) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:42,680] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:06:42,680] {logging_mixin.py:112} INFO - [2020-11-01 21:06:42,680] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:42,784] {logging_mixin.py:112} INFO - [2020-11-01 21:06:42,783] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:06:42,784] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:42,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-01 21:06:55,953] {scheduler_job.py:155} INFO - Started process (PID=2492) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:55,958] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:06:55,965] {logging_mixin.py:112} INFO - [2020-11-01 21:06:55,965] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:56,249] {logging_mixin.py:112} INFO - [2020-11-01 21:06:56,237] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:06:56,250] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:06:56,311] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.358 seconds
[2020-11-01 21:07:09,210] {scheduler_job.py:155} INFO - Started process (PID=2548) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:09,216] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:07:09,218] {logging_mixin.py:112} INFO - [2020-11-01 21:07:09,217] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:09,392] {logging_mixin.py:112} INFO - [2020-11-01 21:07:09,390] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:07:09,392] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:09,427] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.216 seconds
[2020-11-01 21:07:22,406] {scheduler_job.py:155} INFO - Started process (PID=2613) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:22,419] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:07:22,420] {logging_mixin.py:112} INFO - [2020-11-01 21:07:22,420] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:22,632] {logging_mixin.py:112} INFO - [2020-11-01 21:07:22,630] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:07:22,633] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:22,667] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.262 seconds
[2020-11-01 21:07:35,641] {scheduler_job.py:155} INFO - Started process (PID=2675) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:35,653] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:07:35,655] {logging_mixin.py:112} INFO - [2020-11-01 21:07:35,655] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:35,762] {logging_mixin.py:112} INFO - [2020-11-01 21:07:35,761] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:07:35,762] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:35,785] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:07:48,910] {scheduler_job.py:155} INFO - Started process (PID=2732) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:48,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:07:48,916] {logging_mixin.py:112} INFO - [2020-11-01 21:07:48,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:49,097] {logging_mixin.py:112} INFO - [2020-11-01 21:07:49,095] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:07:49,098] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:07:49,133] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.223 seconds
[2020-11-01 21:08:02,213] {scheduler_job.py:155} INFO - Started process (PID=2797) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:02,218] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:08:02,219] {logging_mixin.py:112} INFO - [2020-11-01 21:08:02,219] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:02,421] {logging_mixin.py:112} INFO - [2020-11-01 21:08:02,420] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:08:02,422] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:02,460] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-01 21:08:15,496] {scheduler_job.py:155} INFO - Started process (PID=2851) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:15,500] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:08:15,501] {logging_mixin.py:112} INFO - [2020-11-01 21:08:15,501] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:15,663] {logging_mixin.py:112} INFO - [2020-11-01 21:08:15,661] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:08:15,664] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:15,696] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.201 seconds
[2020-11-01 21:08:28,800] {scheduler_job.py:155} INFO - Started process (PID=2917) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:28,804] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:08:28,804] {logging_mixin.py:112} INFO - [2020-11-01 21:08:28,804] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:28,908] {logging_mixin.py:112} INFO - [2020-11-01 21:08:28,907] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:08:28,908] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:28,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-01 21:08:42,148] {scheduler_job.py:155} INFO - Started process (PID=2979) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:42,153] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:08:42,153] {logging_mixin.py:112} INFO - [2020-11-01 21:08:42,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:42,306] {logging_mixin.py:112} INFO - [2020-11-01 21:08:42,304] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:08:42,307] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:42,340] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-01 21:08:54,382] {scheduler_job.py:155} INFO - Started process (PID=3041) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:54,388] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:08:54,395] {logging_mixin.py:112} INFO - [2020-11-01 21:08:54,394] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:54,661] {logging_mixin.py:112} INFO - [2020-11-01 21:08:54,659] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:08:54,661] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:08:54,694] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.316 seconds
[2020-11-01 21:09:07,649] {scheduler_job.py:155} INFO - Started process (PID=3101) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:07,653] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:09:07,654] {logging_mixin.py:112} INFO - [2020-11-01 21:09:07,653] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:07,756] {logging_mixin.py:112} INFO - [2020-11-01 21:09:07,755] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:09:07,757] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:07,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-01 21:09:20,910] {scheduler_job.py:155} INFO - Started process (PID=3162) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:20,914] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:09:20,915] {logging_mixin.py:112} INFO - [2020-11-01 21:09:20,914] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:21,040] {logging_mixin.py:112} INFO - [2020-11-01 21:09:21,039] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:09:21,040] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:21,069] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 21:09:34,067] {scheduler_job.py:155} INFO - Started process (PID=3224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:34,073] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:09:34,074] {logging_mixin.py:112} INFO - [2020-11-01 21:09:34,073] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:34,217] {logging_mixin.py:112} INFO - [2020-11-01 21:09:34,216] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:09:34,218] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:34,254] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-01 21:09:47,362] {scheduler_job.py:155} INFO - Started process (PID=3284) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:47,367] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:09:47,368] {logging_mixin.py:112} INFO - [2020-11-01 21:09:47,367] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:47,478] {logging_mixin.py:112} INFO - [2020-11-01 21:09:47,476] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:09:47,478] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:09:47,513] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:10:00,588] {scheduler_job.py:155} INFO - Started process (PID=3346) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:00,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:10:00,597] {logging_mixin.py:112} INFO - [2020-11-01 21:10:00,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:00,739] {logging_mixin.py:112} INFO - [2020-11-01 21:10:00,737] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:10:00,739] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:00,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-01 21:10:13,811] {scheduler_job.py:155} INFO - Started process (PID=3405) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:13,815] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:10:13,816] {logging_mixin.py:112} INFO - [2020-11-01 21:10:13,816] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:13,937] {logging_mixin.py:112} INFO - [2020-11-01 21:10:13,935] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:10:13,937] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:13,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.172 seconds
[2020-11-01 21:10:27,091] {scheduler_job.py:155} INFO - Started process (PID=3468) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:27,095] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:10:27,102] {logging_mixin.py:112} INFO - [2020-11-01 21:10:27,102] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:27,299] {logging_mixin.py:112} INFO - [2020-11-01 21:10:27,297] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:10:27,299] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:27,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.248 seconds
[2020-11-01 21:10:40,314] {scheduler_job.py:155} INFO - Started process (PID=3522) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:40,318] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:10:40,319] {logging_mixin.py:112} INFO - [2020-11-01 21:10:40,319] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:40,436] {logging_mixin.py:112} INFO - [2020-11-01 21:10:40,434] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:10:40,436] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:40,463] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 21:10:53,556] {scheduler_job.py:155} INFO - Started process (PID=3582) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:53,560] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:10:53,561] {logging_mixin.py:112} INFO - [2020-11-01 21:10:53,561] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:53,682] {logging_mixin.py:112} INFO - [2020-11-01 21:10:53,681] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:10:53,683] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:10:53,717] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-01 21:11:06,785] {scheduler_job.py:155} INFO - Started process (PID=3645) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:06,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:11:06,790] {logging_mixin.py:112} INFO - [2020-11-01 21:11:06,790] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:06,937] {logging_mixin.py:112} INFO - [2020-11-01 21:11:06,936] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:11:06,938] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:06,969] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.184 seconds
[2020-11-01 21:11:20,023] {scheduler_job.py:155} INFO - Started process (PID=3703) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:20,027] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:11:20,029] {logging_mixin.py:112} INFO - [2020-11-01 21:11:20,028] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:20,157] {logging_mixin.py:112} INFO - [2020-11-01 21:11:20,156] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:11:20,158] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:20,194] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-01 21:11:33,291] {scheduler_job.py:155} INFO - Started process (PID=3766) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:33,296] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:11:33,297] {logging_mixin.py:112} INFO - [2020-11-01 21:11:33,297] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:33,426] {logging_mixin.py:112} INFO - [2020-11-01 21:11:33,425] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:11:33,426] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:33,454] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 21:11:46,505] {scheduler_job.py:155} INFO - Started process (PID=3826) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:46,510] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:11:46,511] {logging_mixin.py:112} INFO - [2020-11-01 21:11:46,511] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:46,678] {logging_mixin.py:112} INFO - [2020-11-01 21:11:46,676] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:11:46,679] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:46,722] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.217 seconds
[2020-11-01 21:11:59,776] {scheduler_job.py:155} INFO - Started process (PID=3893) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:59,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:11:59,786] {logging_mixin.py:112} INFO - [2020-11-01 21:11:59,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:59,899] {logging_mixin.py:112} INFO - [2020-11-01 21:11:59,898] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:11:59,899] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:11:59,929] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:12:13,042] {scheduler_job.py:155} INFO - Started process (PID=3953) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:13,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:12:13,047] {logging_mixin.py:112} INFO - [2020-11-01 21:12:13,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:13,159] {logging_mixin.py:112} INFO - [2020-11-01 21:12:13,158] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:12:13,159] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:13,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-01 21:12:26,284] {scheduler_job.py:155} INFO - Started process (PID=4021) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:26,302] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:12:26,312] {logging_mixin.py:112} INFO - [2020-11-01 21:12:26,311] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:26,548] {logging_mixin.py:112} INFO - [2020-11-01 21:12:26,547] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:12:26,549] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:26,583] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.299 seconds
[2020-11-01 21:12:39,468] {scheduler_job.py:155} INFO - Started process (PID=4095) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:39,475] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:12:39,476] {logging_mixin.py:112} INFO - [2020-11-01 21:12:39,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:39,624] {logging_mixin.py:112} INFO - [2020-11-01 21:12:39,623] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:12:39,625] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:39,653] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-01 21:12:52,726] {scheduler_job.py:155} INFO - Started process (PID=4158) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:52,731] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:12:52,732] {logging_mixin.py:112} INFO - [2020-11-01 21:12:52,732] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:52,924] {logging_mixin.py:112} INFO - [2020-11-01 21:12:52,922] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:12:52,924] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:12:52,970] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.244 seconds
[2020-11-01 21:13:05,974] {scheduler_job.py:155} INFO - Started process (PID=4246) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:05,978] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:13:05,979] {logging_mixin.py:112} INFO - [2020-11-01 21:13:05,978] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:06,095] {logging_mixin.py:112} INFO - [2020-11-01 21:13:06,094] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:13:06,096] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:06,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-01 21:13:19,231] {scheduler_job.py:155} INFO - Started process (PID=4313) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:19,237] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:13:19,238] {logging_mixin.py:112} INFO - [2020-11-01 21:13:19,238] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:19,401] {logging_mixin.py:112} INFO - [2020-11-01 21:13:19,399] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:13:19,401] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:19,441] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.210 seconds
[2020-11-01 21:13:32,452] {scheduler_job.py:155} INFO - Started process (PID=4376) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:32,457] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:13:32,458] {logging_mixin.py:112} INFO - [2020-11-01 21:13:32,457] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:32,565] {logging_mixin.py:112} INFO - [2020-11-01 21:13:32,563] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:13:32,565] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:32,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 21:13:45,701] {scheduler_job.py:155} INFO - Started process (PID=4440) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:45,705] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:13:45,706] {logging_mixin.py:112} INFO - [2020-11-01 21:13:45,705] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:45,840] {logging_mixin.py:112} INFO - [2020-11-01 21:13:45,839] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:13:45,840] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:45,871] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-01 21:13:58,963] {scheduler_job.py:155} INFO - Started process (PID=4511) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:58,978] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:13:58,984] {logging_mixin.py:112} INFO - [2020-11-01 21:13:58,983] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:59,142] {logging_mixin.py:112} INFO - [2020-11-01 21:13:59,141] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:13:59,143] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:13:59,168] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.205 seconds
[2020-11-01 21:14:12,146] {scheduler_job.py:155} INFO - Started process (PID=4570) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:12,154] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:14:12,155] {logging_mixin.py:112} INFO - [2020-11-01 21:14:12,154] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:12,288] {logging_mixin.py:112} INFO - [2020-11-01 21:14:12,287] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:14:12,288] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:12,319] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-01 21:14:25,413] {scheduler_job.py:155} INFO - Started process (PID=4630) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:25,416] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:14:25,417] {logging_mixin.py:112} INFO - [2020-11-01 21:14:25,417] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:25,633] {logging_mixin.py:112} INFO - [2020-11-01 21:14:25,631] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:14:25,633] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:25,669] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.256 seconds
[2020-11-01 21:14:38,653] {scheduler_job.py:155} INFO - Started process (PID=4693) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:38,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:14:38,660] {logging_mixin.py:112} INFO - [2020-11-01 21:14:38,660] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:38,801] {logging_mixin.py:112} INFO - [2020-11-01 21:14:38,799] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:14:38,802] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:38,832] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-01 21:14:51,875] {scheduler_job.py:155} INFO - Started process (PID=4753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:51,880] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:14:51,880] {logging_mixin.py:112} INFO - [2020-11-01 21:14:51,880] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:52,028] {logging_mixin.py:112} INFO - [2020-11-01 21:14:52,024] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:14:52,028] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:14:52,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-01 21:15:05,056] {scheduler_job.py:155} INFO - Started process (PID=4817) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:05,062] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:15:05,063] {logging_mixin.py:112} INFO - [2020-11-01 21:15:05,063] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:05,247] {logging_mixin.py:112} INFO - [2020-11-01 21:15:05,246] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:15:05,248] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:05,289] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-01 21:15:18,318] {scheduler_job.py:155} INFO - Started process (PID=4875) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:18,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:15:18,326] {logging_mixin.py:112} INFO - [2020-11-01 21:15:18,326] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:18,483] {logging_mixin.py:112} INFO - [2020-11-01 21:15:18,481] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:15:18,484] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:18,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-01 21:15:31,537] {scheduler_job.py:155} INFO - Started process (PID=4941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:31,562] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:15:31,562] {logging_mixin.py:112} INFO - [2020-11-01 21:15:31,562] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:31,704] {logging_mixin.py:112} INFO - [2020-11-01 21:15:31,703] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:15:31,704] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:31,734] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-01 21:15:44,723] {scheduler_job.py:155} INFO - Started process (PID=4999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:44,727] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:15:44,728] {logging_mixin.py:112} INFO - [2020-11-01 21:15:44,728] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:44,845] {logging_mixin.py:112} INFO - [2020-11-01 21:15:44,844] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:15:44,845] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:44,873] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 21:15:57,962] {scheduler_job.py:155} INFO - Started process (PID=5061) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:57,967] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:15:57,968] {logging_mixin.py:112} INFO - [2020-11-01 21:15:57,968] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:58,129] {logging_mixin.py:112} INFO - [2020-11-01 21:15:58,128] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:15:58,129] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:15:58,160] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-01 21:16:11,177] {scheduler_job.py:155} INFO - Started process (PID=5123) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:11,190] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:16:11,193] {logging_mixin.py:112} INFO - [2020-11-01 21:16:11,192] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:11,413] {logging_mixin.py:112} INFO - [2020-11-01 21:16:11,412] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:16:11,414] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:11,464] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.287 seconds
[2020-11-01 21:16:24,425] {scheduler_job.py:155} INFO - Started process (PID=5185) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:24,429] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:16:24,430] {logging_mixin.py:112} INFO - [2020-11-01 21:16:24,429] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:24,538] {logging_mixin.py:112} INFO - [2020-11-01 21:16:24,537] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:16:24,539] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:24,569] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-01 21:16:37,723] {scheduler_job.py:155} INFO - Started process (PID=5246) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:37,726] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:16:37,727] {logging_mixin.py:112} INFO - [2020-11-01 21:16:37,727] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:37,841] {logging_mixin.py:112} INFO - [2020-11-01 21:16:37,834] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:16:37,842] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:37,869] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-01 21:16:51,007] {scheduler_job.py:155} INFO - Started process (PID=5308) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:51,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:16:51,019] {logging_mixin.py:112} INFO - [2020-11-01 21:16:51,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:51,241] {logging_mixin.py:112} INFO - [2020-11-01 21:16:51,240] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:16:51,241] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:16:51,273] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.266 seconds
[2020-11-01 21:17:04,227] {scheduler_job.py:155} INFO - Started process (PID=5375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:04,237] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:17:04,243] {logging_mixin.py:112} INFO - [2020-11-01 21:17:04,243] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:04,426] {logging_mixin.py:112} INFO - [2020-11-01 21:17:04,425] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:17:04,427] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:04,458] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.230 seconds
[2020-11-01 21:17:17,487] {scheduler_job.py:155} INFO - Started process (PID=5436) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:17,492] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:17:17,492] {logging_mixin.py:112} INFO - [2020-11-01 21:17:17,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:17,610] {logging_mixin.py:112} INFO - [2020-11-01 21:17:17,609] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:17:17,610] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:17,639] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:17:30,932] {scheduler_job.py:155} INFO - Started process (PID=5503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:30,935] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:17:30,936] {logging_mixin.py:112} INFO - [2020-11-01 21:17:30,936] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:31,136] {logging_mixin.py:112} INFO - [2020-11-01 21:17:31,134] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:17:31,136] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:31,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-01 21:17:44,236] {scheduler_job.py:155} INFO - Started process (PID=5563) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:44,241] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:17:44,243] {logging_mixin.py:112} INFO - [2020-11-01 21:17:44,242] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:44,380] {logging_mixin.py:112} INFO - [2020-11-01 21:17:44,379] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:17:44,381] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:44,406] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-01 21:17:57,480] {scheduler_job.py:155} INFO - Started process (PID=5632) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:57,484] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:17:57,485] {logging_mixin.py:112} INFO - [2020-11-01 21:17:57,485] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:57,597] {logging_mixin.py:112} INFO - [2020-11-01 21:17:57,596] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:17:57,598] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:17:57,626] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-01 21:18:10,745] {scheduler_job.py:155} INFO - Started process (PID=5702) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:10,748] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:18:10,749] {logging_mixin.py:112} INFO - [2020-11-01 21:18:10,749] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:10,901] {logging_mixin.py:112} INFO - [2020-11-01 21:18:10,900] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:18:10,901] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:10,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-01 21:18:24,022] {scheduler_job.py:155} INFO - Started process (PID=5765) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:24,031] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:18:24,032] {logging_mixin.py:112} INFO - [2020-11-01 21:18:24,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:24,174] {logging_mixin.py:112} INFO - [2020-11-01 21:18:24,173] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:18:24,174] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:24,203] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-01 21:18:37,227] {scheduler_job.py:155} INFO - Started process (PID=5827) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:37,232] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:18:37,233] {logging_mixin.py:112} INFO - [2020-11-01 21:18:37,232] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:37,445] {logging_mixin.py:112} INFO - [2020-11-01 21:18:37,444] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:18:37,446] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:37,478] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-01 21:18:50,489] {scheduler_job.py:155} INFO - Started process (PID=5888) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:50,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:18:50,494] {logging_mixin.py:112} INFO - [2020-11-01 21:18:50,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:50,630] {logging_mixin.py:112} INFO - [2020-11-01 21:18:50,629] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:18:50,630] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:18:50,659] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-01 21:19:03,750] {scheduler_job.py:155} INFO - Started process (PID=5952) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:03,764] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:19:03,764] {logging_mixin.py:112} INFO - [2020-11-01 21:19:03,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:03,960] {logging_mixin.py:112} INFO - [2020-11-01 21:19:03,959] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:19:03,961] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:04,076] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.326 seconds
[2020-11-01 21:19:17,005] {scheduler_job.py:155} INFO - Started process (PID=6012) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:17,025] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:19:17,027] {logging_mixin.py:112} INFO - [2020-11-01 21:19:17,027] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:17,541] {logging_mixin.py:112} INFO - [2020-11-01 21:19:17,538] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:19:17,541] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:17,657] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.652 seconds
[2020-11-01 21:19:31,065] {scheduler_job.py:155} INFO - Started process (PID=6067) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:31,070] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:19:31,071] {logging_mixin.py:112} INFO - [2020-11-01 21:19:31,071] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:31,211] {logging_mixin.py:112} INFO - [2020-11-01 21:19:31,210] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:19:31,212] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:31,236] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-01 21:19:43,299] {scheduler_job.py:155} INFO - Started process (PID=6123) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:43,303] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:19:43,304] {logging_mixin.py:112} INFO - [2020-11-01 21:19:43,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:43,499] {logging_mixin.py:112} INFO - [2020-11-01 21:19:43,497] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:19:43,499] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:43,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.230 seconds
[2020-11-01 21:19:56,583] {scheduler_job.py:155} INFO - Started process (PID=6183) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:56,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:19:56,587] {logging_mixin.py:112} INFO - [2020-11-01 21:19:56,587] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:56,699] {logging_mixin.py:112} INFO - [2020-11-01 21:19:56,698] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:19:56,700] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:19:56,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 21:20:09,821] {scheduler_job.py:155} INFO - Started process (PID=6252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:09,827] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:20:09,828] {logging_mixin.py:112} INFO - [2020-11-01 21:20:09,828] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:09,942] {logging_mixin.py:112} INFO - [2020-11-01 21:20:09,941] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:20:09,943] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:09,970] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 21:20:23,072] {scheduler_job.py:155} INFO - Started process (PID=6313) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:23,075] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:20:23,076] {logging_mixin.py:112} INFO - [2020-11-01 21:20:23,076] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:23,221] {logging_mixin.py:112} INFO - [2020-11-01 21:20:23,220] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:20:23,221] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:23,251] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-01 21:20:36,304] {scheduler_job.py:155} INFO - Started process (PID=6377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:36,313] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:20:36,314] {logging_mixin.py:112} INFO - [2020-11-01 21:20:36,313] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:36,524] {logging_mixin.py:112} INFO - [2020-11-01 21:20:36,522] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:20:36,524] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:36,557] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.253 seconds
[2020-11-01 21:20:49,514] {scheduler_job.py:155} INFO - Started process (PID=6435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:49,519] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:20:49,519] {logging_mixin.py:112} INFO - [2020-11-01 21:20:49,519] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:49,673] {logging_mixin.py:112} INFO - [2020-11-01 21:20:49,672] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:20:49,673] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:20:49,698] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.183 seconds
[2020-11-01 21:21:02,797] {scheduler_job.py:155} INFO - Started process (PID=6496) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:02,803] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:21:02,804] {logging_mixin.py:112} INFO - [2020-11-01 21:21:02,804] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:03,085] {logging_mixin.py:112} INFO - [2020-11-01 21:21:03,084] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:21:03,087] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:03,192] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.395 seconds
[2020-11-01 21:21:16,445] {scheduler_job.py:155} INFO - Started process (PID=6554) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:16,486] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:21:16,489] {logging_mixin.py:112} INFO - [2020-11-01 21:21:16,487] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:16,666] {logging_mixin.py:112} INFO - [2020-11-01 21:21:16,664] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:21:16,666] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:16,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.262 seconds
[2020-11-01 21:21:29,688] {scheduler_job.py:155} INFO - Started process (PID=6616) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:29,695] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:21:29,696] {logging_mixin.py:112} INFO - [2020-11-01 21:21:29,695] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:29,811] {logging_mixin.py:112} INFO - [2020-11-01 21:21:29,810] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:21:29,811] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:29,846] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-01 21:21:42,981] {scheduler_job.py:155} INFO - Started process (PID=6676) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:42,985] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:21:42,985] {logging_mixin.py:112} INFO - [2020-11-01 21:21:42,985] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:43,098] {logging_mixin.py:112} INFO - [2020-11-01 21:21:43,096] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:21:43,098] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:43,128] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-01 21:21:56,313] {scheduler_job.py:155} INFO - Started process (PID=6732) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:56,332] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:21:56,335] {logging_mixin.py:112} INFO - [2020-11-01 21:21:56,335] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:56,655] {logging_mixin.py:112} INFO - [2020-11-01 21:21:56,650] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:21:56,665] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:21:56,729] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.416 seconds
[2020-11-01 21:22:09,520] {scheduler_job.py:155} INFO - Started process (PID=6795) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:09,524] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:22:09,525] {logging_mixin.py:112} INFO - [2020-11-01 21:22:09,525] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:09,673] {logging_mixin.py:112} INFO - [2020-11-01 21:22:09,672] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:22:09,674] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:09,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.205 seconds
[2020-11-01 21:22:22,781] {scheduler_job.py:155} INFO - Started process (PID=6855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:22,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:22:22,787] {logging_mixin.py:112} INFO - [2020-11-01 21:22:22,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:22,910] {logging_mixin.py:112} INFO - [2020-11-01 21:22:22,909] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:22:22,910] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:22,938] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-01 21:22:36,034] {scheduler_job.py:155} INFO - Started process (PID=6919) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:36,045] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:22:36,045] {logging_mixin.py:112} INFO - [2020-11-01 21:22:36,045] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:36,237] {logging_mixin.py:112} INFO - [2020-11-01 21:22:36,236] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:22:36,238] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:36,274] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.240 seconds
[2020-11-01 21:22:49,227] {scheduler_job.py:155} INFO - Started process (PID=6980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:49,230] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:22:49,231] {logging_mixin.py:112} INFO - [2020-11-01 21:22:49,231] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:49,341] {logging_mixin.py:112} INFO - [2020-11-01 21:22:49,340] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:22:49,341] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:22:49,370] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 21:23:02,457] {scheduler_job.py:155} INFO - Started process (PID=7045) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:02,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:23:02,462] {logging_mixin.py:112} INFO - [2020-11-01 21:23:02,461] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:02,578] {logging_mixin.py:112} INFO - [2020-11-01 21:23:02,576] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:23:02,578] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:02,608] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-01 21:23:15,670] {scheduler_job.py:155} INFO - Started process (PID=7107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:15,674] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:23:15,675] {logging_mixin.py:112} INFO - [2020-11-01 21:23:15,675] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:15,788] {logging_mixin.py:112} INFO - [2020-11-01 21:23:15,787] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:23:15,789] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:15,817] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-01 21:23:28,990] {scheduler_job.py:155} INFO - Started process (PID=7167) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:28,994] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:23:28,995] {logging_mixin.py:112} INFO - [2020-11-01 21:23:28,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:29,123] {logging_mixin.py:112} INFO - [2020-11-01 21:23:29,122] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:23:29,123] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:29,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 21:23:42,196] {scheduler_job.py:155} INFO - Started process (PID=7232) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:42,201] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:23:42,201] {logging_mixin.py:112} INFO - [2020-11-01 21:23:42,201] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:42,355] {logging_mixin.py:112} INFO - [2020-11-01 21:23:42,354] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:23:42,355] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:42,384] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-01 21:23:55,439] {scheduler_job.py:155} INFO - Started process (PID=7295) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:55,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:23:55,445] {logging_mixin.py:112} INFO - [2020-11-01 21:23:55,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:55,546] {logging_mixin.py:112} INFO - [2020-11-01 21:23:55,545] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:23:55,546] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:23:55,568] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.129 seconds
[2020-11-01 21:24:08,669] {scheduler_job.py:155} INFO - Started process (PID=7360) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:08,675] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:24:08,676] {logging_mixin.py:112} INFO - [2020-11-01 21:24:08,676] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:08,921] {logging_mixin.py:112} INFO - [2020-11-01 21:24:08,917] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:24:08,928] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:08,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.306 seconds
[2020-11-01 21:24:21,868] {scheduler_job.py:155} INFO - Started process (PID=7420) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:21,872] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:24:21,873] {logging_mixin.py:112} INFO - [2020-11-01 21:24:21,873] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:21,990] {logging_mixin.py:112} INFO - [2020-11-01 21:24:21,989] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:24:21,991] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:22,019] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:24:35,144] {scheduler_job.py:155} INFO - Started process (PID=7480) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:35,153] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:24:35,153] {logging_mixin.py:112} INFO - [2020-11-01 21:24:35,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:35,275] {logging_mixin.py:112} INFO - [2020-11-01 21:24:35,274] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:24:35,275] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:35,308] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 21:24:48,348] {scheduler_job.py:155} INFO - Started process (PID=7544) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:48,352] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:24:48,353] {logging_mixin.py:112} INFO - [2020-11-01 21:24:48,353] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:48,456] {logging_mixin.py:112} INFO - [2020-11-01 21:24:48,454] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:24:48,456] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:24:48,486] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 21:25:01,578] {scheduler_job.py:155} INFO - Started process (PID=7608) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:01,593] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:25:01,594] {logging_mixin.py:112} INFO - [2020-11-01 21:25:01,594] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:01,804] {logging_mixin.py:112} INFO - [2020-11-01 21:25:01,801] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:25:01,805] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:01,848] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-01 21:25:14,760] {scheduler_job.py:155} INFO - Started process (PID=7668) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:14,764] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:25:14,764] {logging_mixin.py:112} INFO - [2020-11-01 21:25:14,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:14,874] {logging_mixin.py:112} INFO - [2020-11-01 21:25:14,872] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:25:14,874] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:14,900] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-01 21:25:27,994] {scheduler_job.py:155} INFO - Started process (PID=7729) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:27,998] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:25:27,999] {logging_mixin.py:112} INFO - [2020-11-01 21:25:27,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:28,116] {logging_mixin.py:112} INFO - [2020-11-01 21:25:28,115] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:25:28,117] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:28,146] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:25:41,226] {scheduler_job.py:155} INFO - Started process (PID=7792) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:41,237] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:25:41,238] {logging_mixin.py:112} INFO - [2020-11-01 21:25:41,238] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:41,476] {logging_mixin.py:112} INFO - [2020-11-01 21:25:41,473] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:25:41,476] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:41,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.292 seconds
[2020-11-01 21:25:54,444] {scheduler_job.py:155} INFO - Started process (PID=7851) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:54,449] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:25:54,450] {logging_mixin.py:112} INFO - [2020-11-01 21:25:54,450] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:54,579] {logging_mixin.py:112} INFO - [2020-11-01 21:25:54,578] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:25:54,579] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:25:54,609] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-01 21:26:07,710] {scheduler_job.py:155} INFO - Started process (PID=7912) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:07,713] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:26:07,714] {logging_mixin.py:112} INFO - [2020-11-01 21:26:07,714] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:07,845] {logging_mixin.py:112} INFO - [2020-11-01 21:26:07,843] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:26:07,845] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:07,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-01 21:26:20,925] {scheduler_job.py:155} INFO - Started process (PID=7971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:20,930] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:26:20,930] {logging_mixin.py:112} INFO - [2020-11-01 21:26:20,930] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:21,038] {logging_mixin.py:112} INFO - [2020-11-01 21:26:21,037] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:26:21,038] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:21,068] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 21:26:34,457] {scheduler_job.py:155} INFO - Started process (PID=8027) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:34,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:26:34,471] {logging_mixin.py:112} INFO - [2020-11-01 21:26:34,471] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:34,674] {logging_mixin.py:112} INFO - [2020-11-01 21:26:34,672] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:26:34,674] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:34,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.254 seconds
[2020-11-01 21:26:47,917] {scheduler_job.py:155} INFO - Started process (PID=8090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:47,922] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:26:47,931] {logging_mixin.py:112} INFO - [2020-11-01 21:26:47,931] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:48,280] {logging_mixin.py:112} INFO - [2020-11-01 21:26:48,278] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:26:48,280] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:26:48,349] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.433 seconds
[2020-11-01 21:27:01,133] {scheduler_job.py:155} INFO - Started process (PID=8145) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:01,137] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:27:01,138] {logging_mixin.py:112} INFO - [2020-11-01 21:27:01,138] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:01,320] {logging_mixin.py:112} INFO - [2020-11-01 21:27:01,319] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:27:01,321] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:01,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.222 seconds
[2020-11-01 21:27:14,392] {scheduler_job.py:155} INFO - Started process (PID=8204) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:14,408] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:27:14,408] {logging_mixin.py:112} INFO - [2020-11-01 21:27:14,408] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:14,651] {logging_mixin.py:112} INFO - [2020-11-01 21:27:14,649] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:27:14,651] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:14,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.292 seconds
[2020-11-01 21:27:27,591] {scheduler_job.py:155} INFO - Started process (PID=8258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:27,597] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:27:27,597] {logging_mixin.py:112} INFO - [2020-11-01 21:27:27,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:27,745] {logging_mixin.py:112} INFO - [2020-11-01 21:27:27,742] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:27:27,746] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:27,780] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-01 21:27:40,912] {scheduler_job.py:155} INFO - Started process (PID=8317) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:40,918] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:27:40,919] {logging_mixin.py:112} INFO - [2020-11-01 21:27:40,919] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:41,090] {logging_mixin.py:112} INFO - [2020-11-01 21:27:41,089] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:27:41,090] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:41,124] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.212 seconds
[2020-11-01 21:27:54,124] {scheduler_job.py:155} INFO - Started process (PID=8377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:54,128] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:27:54,129] {logging_mixin.py:112} INFO - [2020-11-01 21:27:54,129] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:54,244] {logging_mixin.py:112} INFO - [2020-11-01 21:27:54,242] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:27:54,244] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:27:54,272] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-01 21:28:07,407] {scheduler_job.py:155} INFO - Started process (PID=8443) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:07,412] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:28:07,412] {logging_mixin.py:112} INFO - [2020-11-01 21:28:07,412] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:07,556] {logging_mixin.py:112} INFO - [2020-11-01 21:28:07,555] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:28:07,557] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:07,583] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-01 21:28:20,656] {scheduler_job.py:155} INFO - Started process (PID=8505) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:20,661] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:28:20,661] {logging_mixin.py:112} INFO - [2020-11-01 21:28:20,661] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:20,766] {logging_mixin.py:112} INFO - [2020-11-01 21:28:20,765] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:28:20,767] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:20,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 21:28:33,961] {scheduler_job.py:155} INFO - Started process (PID=8566) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:33,964] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:28:33,965] {logging_mixin.py:112} INFO - [2020-11-01 21:28:33,965] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:34,083] {logging_mixin.py:112} INFO - [2020-11-01 21:28:34,082] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:28:34,084] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:34,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 21:28:47,213] {scheduler_job.py:155} INFO - Started process (PID=8628) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:47,220] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:28:47,221] {logging_mixin.py:112} INFO - [2020-11-01 21:28:47,221] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:47,405] {logging_mixin.py:112} INFO - [2020-11-01 21:28:47,403] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:28:47,405] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:28:47,445] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.232 seconds
[2020-11-01 21:29:00,456] {scheduler_job.py:155} INFO - Started process (PID=8691) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:00,471] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:29:00,473] {logging_mixin.py:112} INFO - [2020-11-01 21:29:00,473] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:00,639] {logging_mixin.py:112} INFO - [2020-11-01 21:29:00,637] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:29:00,639] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:00,670] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-01 21:29:13,692] {scheduler_job.py:155} INFO - Started process (PID=8750) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:13,697] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:29:13,698] {logging_mixin.py:112} INFO - [2020-11-01 21:29:13,697] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:13,925] {logging_mixin.py:112} INFO - [2020-11-01 21:29:13,924] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:29:13,926] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:13,961] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-01 21:29:26,947] {scheduler_job.py:155} INFO - Started process (PID=8809) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:26,952] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:29:26,952] {logging_mixin.py:112} INFO - [2020-11-01 21:29:26,952] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:27,058] {logging_mixin.py:112} INFO - [2020-11-01 21:29:27,057] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:29:27,058] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:27,083] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-01 21:29:40,195] {scheduler_job.py:155} INFO - Started process (PID=8870) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:40,201] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:29:40,202] {logging_mixin.py:112} INFO - [2020-11-01 21:29:40,202] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:40,341] {logging_mixin.py:112} INFO - [2020-11-01 21:29:40,339] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:29:40,341] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:40,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-01 21:29:53,439] {scheduler_job.py:155} INFO - Started process (PID=8933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:53,444] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:29:53,445] {logging_mixin.py:112} INFO - [2020-11-01 21:29:53,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:53,621] {logging_mixin.py:112} INFO - [2020-11-01 21:29:53,619] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:29:53,621] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:29:53,653] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-01 21:30:06,696] {scheduler_job.py:155} INFO - Started process (PID=8993) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:06,703] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:30:06,704] {logging_mixin.py:112} INFO - [2020-11-01 21:30:06,704] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:06,907] {logging_mixin.py:112} INFO - [2020-11-01 21:30:06,905] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:30:06,908] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:06,947] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-01 21:30:18,954] {scheduler_job.py:155} INFO - Started process (PID=9053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:18,957] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:30:18,958] {logging_mixin.py:112} INFO - [2020-11-01 21:30:18,958] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:19,131] {logging_mixin.py:112} INFO - [2020-11-01 21:30:19,130] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:30:19,131] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:19,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-01 21:30:32,251] {scheduler_job.py:155} INFO - Started process (PID=9116) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:32,264] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:30:32,265] {logging_mixin.py:112} INFO - [2020-11-01 21:30:32,265] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:32,399] {logging_mixin.py:112} INFO - [2020-11-01 21:30:32,398] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:30:32,399] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:32,427] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-01 21:30:45,672] {scheduler_job.py:155} INFO - Started process (PID=9171) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:45,678] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:30:45,687] {logging_mixin.py:112} INFO - [2020-11-01 21:30:45,687] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:45,955] {logging_mixin.py:112} INFO - [2020-11-01 21:30:45,954] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:30:45,955] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:45,995] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.329 seconds
[2020-11-01 21:30:58,910] {scheduler_job.py:155} INFO - Started process (PID=9228) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:58,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:30:58,916] {logging_mixin.py:112} INFO - [2020-11-01 21:30:58,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:59,093] {logging_mixin.py:112} INFO - [2020-11-01 21:30:59,091] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:30:59,093] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:30:59,133] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.223 seconds
[2020-11-01 21:31:12,173] {scheduler_job.py:155} INFO - Started process (PID=9281) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:12,178] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:31:12,197] {logging_mixin.py:112} INFO - [2020-11-01 21:31:12,196] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:12,483] {logging_mixin.py:112} INFO - [2020-11-01 21:31:12,480] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:31:12,483] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:12,544] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.371 seconds
[2020-11-01 21:31:25,552] {scheduler_job.py:155} INFO - Started process (PID=9340) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:25,557] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:31:25,580] {logging_mixin.py:112} INFO - [2020-11-01 21:31:25,558] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:25,770] {logging_mixin.py:112} INFO - [2020-11-01 21:31:25,769] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:31:25,771] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:25,811] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.259 seconds
[2020-11-01 21:31:38,897] {scheduler_job.py:155} INFO - Started process (PID=9400) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:38,903] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:31:38,904] {logging_mixin.py:112} INFO - [2020-11-01 21:31:38,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:39,245] {logging_mixin.py:112} INFO - [2020-11-01 21:31:39,244] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:31:39,246] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:39,279] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.382 seconds
[2020-11-01 21:31:52,181] {scheduler_job.py:155} INFO - Started process (PID=9463) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:52,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:31:52,197] {logging_mixin.py:112} INFO - [2020-11-01 21:31:52,196] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:52,592] {logging_mixin.py:112} INFO - [2020-11-01 21:31:52,590] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:31:52,592] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:31:52,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.502 seconds
[2020-11-01 21:32:05,403] {scheduler_job.py:155} INFO - Started process (PID=9521) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:05,408] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:32:05,409] {logging_mixin.py:112} INFO - [2020-11-01 21:32:05,409] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:05,552] {logging_mixin.py:112} INFO - [2020-11-01 21:32:05,550] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:32:05,552] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:05,580] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-01 21:32:18,615] {scheduler_job.py:155} INFO - Started process (PID=9584) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:18,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:32:18,619] {logging_mixin.py:112} INFO - [2020-11-01 21:32:18,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:18,757] {logging_mixin.py:112} INFO - [2020-11-01 21:32:18,756] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:32:18,757] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:18,784] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-01 21:32:31,983] {scheduler_job.py:155} INFO - Started process (PID=9649) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:31,986] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:32:31,987] {logging_mixin.py:112} INFO - [2020-11-01 21:32:31,987] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:32,097] {logging_mixin.py:112} INFO - [2020-11-01 21:32:32,096] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:32:32,097] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:32,124] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-01 21:32:45,386] {scheduler_job.py:155} INFO - Started process (PID=9704) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:45,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:32:45,392] {logging_mixin.py:112} INFO - [2020-11-01 21:32:45,392] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:45,526] {logging_mixin.py:112} INFO - [2020-11-01 21:32:45,524] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:32:45,526] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:45,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.172 seconds
[2020-11-01 21:32:58,620] {scheduler_job.py:155} INFO - Started process (PID=9765) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:58,624] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:32:58,625] {logging_mixin.py:112} INFO - [2020-11-01 21:32:58,625] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:58,751] {logging_mixin.py:112} INFO - [2020-11-01 21:32:58,749] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:32:58,751] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:32:58,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-01 21:33:11,861] {scheduler_job.py:155} INFO - Started process (PID=9828) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:11,866] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:33:11,867] {logging_mixin.py:112} INFO - [2020-11-01 21:33:11,867] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:11,986] {logging_mixin.py:112} INFO - [2020-11-01 21:33:11,985] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:33:11,986] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:12,017] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-01 21:33:25,288] {scheduler_job.py:155} INFO - Started process (PID=9891) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:25,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:33:25,293] {logging_mixin.py:112} INFO - [2020-11-01 21:33:25,293] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:25,782] {logging_mixin.py:112} INFO - [2020-11-01 21:33:25,781] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:33:25,783] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:25,816] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.529 seconds
[2020-11-01 21:33:38,387] {scheduler_job.py:155} INFO - Started process (PID=9950) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:38,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:33:38,392] {logging_mixin.py:112} INFO - [2020-11-01 21:33:38,392] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:38,501] {logging_mixin.py:112} INFO - [2020-11-01 21:33:38,500] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:33:38,502] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:38,530] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 21:33:51,586] {scheduler_job.py:155} INFO - Started process (PID=10021) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:51,606] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:33:51,606] {logging_mixin.py:112} INFO - [2020-11-01 21:33:51,606] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:51,758] {logging_mixin.py:112} INFO - [2020-11-01 21:33:51,757] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:33:51,758] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:33:51,782] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.196 seconds
[2020-11-01 21:34:04,872] {scheduler_job.py:155} INFO - Started process (PID=10090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:04,878] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:34:04,879] {logging_mixin.py:112} INFO - [2020-11-01 21:34:04,879] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:04,992] {logging_mixin.py:112} INFO - [2020-11-01 21:34:04,991] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:34:04,992] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:05,021] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-01 21:34:18,129] {scheduler_job.py:155} INFO - Started process (PID=10150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:18,133] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:34:18,134] {logging_mixin.py:112} INFO - [2020-11-01 21:34:18,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:18,265] {logging_mixin.py:112} INFO - [2020-11-01 21:34:18,264] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:34:18,266] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:18,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-01 21:34:31,430] {scheduler_job.py:155} INFO - Started process (PID=10211) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:31,437] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:34:31,438] {logging_mixin.py:112} INFO - [2020-11-01 21:34:31,438] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:31,538] {logging_mixin.py:112} INFO - [2020-11-01 21:34:31,537] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:34:31,538] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:31,563] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-01 21:34:44,852] {scheduler_job.py:155} INFO - Started process (PID=10275) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:44,857] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:34:44,858] {logging_mixin.py:112} INFO - [2020-11-01 21:34:44,858] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:44,966] {logging_mixin.py:112} INFO - [2020-11-01 21:34:44,965] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:34:44,966] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:44,992] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-01 21:34:58,136] {scheduler_job.py:155} INFO - Started process (PID=10339) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:58,140] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:34:58,141] {logging_mixin.py:112} INFO - [2020-11-01 21:34:58,141] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:58,267] {logging_mixin.py:112} INFO - [2020-11-01 21:34:58,266] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:34:58,268] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:34:58,297] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-01 21:35:11,412] {scheduler_job.py:155} INFO - Started process (PID=10403) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:11,416] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:35:11,417] {logging_mixin.py:112} INFO - [2020-11-01 21:35:11,417] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:11,567] {logging_mixin.py:112} INFO - [2020-11-01 21:35:11,565] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:35:11,567] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:11,604] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.193 seconds
[2020-11-01 21:35:24,620] {scheduler_job.py:155} INFO - Started process (PID=10469) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:24,627] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:35:24,635] {logging_mixin.py:112} INFO - [2020-11-01 21:35:24,635] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:24,820] {logging_mixin.py:112} INFO - [2020-11-01 21:35:24,818] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:35:24,820] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:24,851] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-01 21:35:37,816] {scheduler_job.py:155} INFO - Started process (PID=10528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:37,830] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:35:37,831] {logging_mixin.py:112} INFO - [2020-11-01 21:35:37,831] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:37,949] {logging_mixin.py:112} INFO - [2020-11-01 21:35:37,948] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:35:37,950] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:37,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 21:35:51,074] {scheduler_job.py:155} INFO - Started process (PID=10589) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:51,078] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:35:51,079] {logging_mixin.py:112} INFO - [2020-11-01 21:35:51,079] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:51,200] {logging_mixin.py:112} INFO - [2020-11-01 21:35:51,198] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:35:51,200] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:35:51,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-01 21:36:04,336] {scheduler_job.py:155} INFO - Started process (PID=10651) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:04,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:36:04,342] {logging_mixin.py:112} INFO - [2020-11-01 21:36:04,342] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:04,438] {logging_mixin.py:112} INFO - [2020-11-01 21:36:04,437] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:36:04,438] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:04,463] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-01 21:36:17,535] {scheduler_job.py:155} INFO - Started process (PID=10711) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:17,539] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:36:17,541] {logging_mixin.py:112} INFO - [2020-11-01 21:36:17,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:17,651] {logging_mixin.py:112} INFO - [2020-11-01 21:36:17,650] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:36:17,651] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:17,679] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:36:30,733] {scheduler_job.py:155} INFO - Started process (PID=10775) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:30,741] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:36:30,742] {logging_mixin.py:112} INFO - [2020-11-01 21:36:30,742] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:30,936] {logging_mixin.py:112} INFO - [2020-11-01 21:36:30,935] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:36:30,937] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:30,974] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.241 seconds
[2020-11-01 21:36:43,953] {scheduler_job.py:155} INFO - Started process (PID=10835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:43,957] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:36:43,957] {logging_mixin.py:112} INFO - [2020-11-01 21:36:43,957] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:44,073] {logging_mixin.py:112} INFO - [2020-11-01 21:36:44,072] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:36:44,073] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:44,097] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:36:57,156] {scheduler_job.py:155} INFO - Started process (PID=10899) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:57,166] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:36:57,167] {logging_mixin.py:112} INFO - [2020-11-01 21:36:57,167] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:57,389] {logging_mixin.py:112} INFO - [2020-11-01 21:36:57,388] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:36:57,389] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:36:57,434] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.278 seconds
[2020-11-01 21:37:10,386] {scheduler_job.py:155} INFO - Started process (PID=10957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:10,390] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:37:10,393] {logging_mixin.py:112} INFO - [2020-11-01 21:37:10,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:10,506] {logging_mixin.py:112} INFO - [2020-11-01 21:37:10,505] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:37:10,507] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:10,535] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-01 21:37:23,733] {scheduler_job.py:155} INFO - Started process (PID=11031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:23,736] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:37:23,737] {logging_mixin.py:112} INFO - [2020-11-01 21:37:23,737] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:23,839] {logging_mixin.py:112} INFO - [2020-11-01 21:37:23,837] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:37:23,839] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:23,864] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-01 21:37:37,004] {scheduler_job.py:155} INFO - Started process (PID=11094) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:37,008] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:37:37,009] {logging_mixin.py:112} INFO - [2020-11-01 21:37:37,009] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:37,155] {logging_mixin.py:112} INFO - [2020-11-01 21:37:37,154] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:37:37,156] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:37,184] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-01 21:37:50,322] {scheduler_job.py:155} INFO - Started process (PID=11155) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:50,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:37:50,327] {logging_mixin.py:112} INFO - [2020-11-01 21:37:50,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:50,450] {logging_mixin.py:112} INFO - [2020-11-01 21:37:50,449] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:37:50,451] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:37:50,480] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-01 21:38:03,517] {scheduler_job.py:155} INFO - Started process (PID=11225) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:03,522] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:38:03,523] {logging_mixin.py:112} INFO - [2020-11-01 21:38:03,522] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:03,634] {logging_mixin.py:112} INFO - [2020-11-01 21:38:03,633] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:38:03,634] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:03,661] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:38:16,789] {scheduler_job.py:155} INFO - Started process (PID=11284) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:16,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:38:16,795] {logging_mixin.py:112} INFO - [2020-11-01 21:38:16,795] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:16,940] {logging_mixin.py:112} INFO - [2020-11-01 21:38:16,939] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:38:16,941] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:16,966] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-01 21:38:29,974] {scheduler_job.py:155} INFO - Started process (PID=11350) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:29,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:38:29,984] {logging_mixin.py:112} INFO - [2020-11-01 21:38:29,984] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:30,106] {logging_mixin.py:112} INFO - [2020-11-01 21:38:30,105] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:38:30,106] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:30,139] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-01 21:38:43,234] {scheduler_job.py:155} INFO - Started process (PID=11411) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:43,239] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:38:43,239] {logging_mixin.py:112} INFO - [2020-11-01 21:38:43,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:43,380] {logging_mixin.py:112} INFO - [2020-11-01 21:38:43,378] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:38:43,380] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:43,406] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-01 21:38:56,462] {scheduler_job.py:155} INFO - Started process (PID=11476) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:56,466] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:38:56,466] {logging_mixin.py:112} INFO - [2020-11-01 21:38:56,466] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:56,699] {logging_mixin.py:112} INFO - [2020-11-01 21:38:56,698] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:38:56,700] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:38:56,730] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-01 21:39:09,654] {scheduler_job.py:155} INFO - Started process (PID=11537) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:09,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:39:09,659] {logging_mixin.py:112} INFO - [2020-11-01 21:39:09,658] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:09,762] {logging_mixin.py:112} INFO - [2020-11-01 21:39:09,760] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:39:09,762] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:09,789] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-01 21:39:22,981] {scheduler_job.py:155} INFO - Started process (PID=11600) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:22,985] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:39:22,985] {logging_mixin.py:112} INFO - [2020-11-01 21:39:22,985] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:23,103] {logging_mixin.py:112} INFO - [2020-11-01 21:39:23,102] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:39:23,103] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:23,132] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-01 21:39:36,236] {scheduler_job.py:155} INFO - Started process (PID=11662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:36,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:39:36,241] {logging_mixin.py:112} INFO - [2020-11-01 21:39:36,241] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:36,356] {logging_mixin.py:112} INFO - [2020-11-01 21:39:36,355] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:39:36,357] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:36,381] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-01 21:39:49,546] {scheduler_job.py:155} INFO - Started process (PID=11727) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:49,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:39:49,552] {logging_mixin.py:112} INFO - [2020-11-01 21:39:49,551] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:49,671] {logging_mixin.py:112} INFO - [2020-11-01 21:39:49,670] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:39:49,671] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:39:49,697] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:40:02,859] {scheduler_job.py:155} INFO - Started process (PID=11791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:02,864] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:40:02,864] {logging_mixin.py:112} INFO - [2020-11-01 21:40:02,864] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:02,980] {logging_mixin.py:112} INFO - [2020-11-01 21:40:02,979] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:40:02,980] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:03,004] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-01 21:40:16,204] {scheduler_job.py:155} INFO - Started process (PID=11850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:16,208] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:40:16,209] {logging_mixin.py:112} INFO - [2020-11-01 21:40:16,209] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:16,311] {logging_mixin.py:112} INFO - [2020-11-01 21:40:16,310] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:40:16,311] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:16,347] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-01 21:40:29,447] {scheduler_job.py:155} INFO - Started process (PID=11917) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:29,457] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:40:29,461] {logging_mixin.py:112} INFO - [2020-11-01 21:40:29,461] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:29,617] {logging_mixin.py:112} INFO - [2020-11-01 21:40:29,616] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:40:29,617] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:29,639] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-01 21:40:42,735] {scheduler_job.py:155} INFO - Started process (PID=11976) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:42,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:40:42,739] {logging_mixin.py:112} INFO - [2020-11-01 21:40:42,739] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:42,844] {logging_mixin.py:112} INFO - [2020-11-01 21:40:42,843] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:40:42,845] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:42,868] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-01 21:40:56,092] {scheduler_job.py:155} INFO - Started process (PID=12038) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:56,095] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:40:56,096] {logging_mixin.py:112} INFO - [2020-11-01 21:40:56,095] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:56,206] {logging_mixin.py:112} INFO - [2020-11-01 21:40:56,205] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:40:56,206] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:40:56,231] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-01 21:41:09,319] {scheduler_job.py:155} INFO - Started process (PID=12099) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:09,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:41:09,323] {logging_mixin.py:112} INFO - [2020-11-01 21:41:09,323] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:09,418] {logging_mixin.py:112} INFO - [2020-11-01 21:41:09,417] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:41:09,419] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:09,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-01 21:41:22,571] {scheduler_job.py:155} INFO - Started process (PID=12160) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:22,577] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:41:22,579] {logging_mixin.py:112} INFO - [2020-11-01 21:41:22,578] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:22,702] {logging_mixin.py:112} INFO - [2020-11-01 21:41:22,701] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:41:22,702] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:22,734] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-01 21:41:35,864] {scheduler_job.py:155} INFO - Started process (PID=12225) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:35,877] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:41:35,878] {logging_mixin.py:112} INFO - [2020-11-01 21:41:35,877] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:36,089] {logging_mixin.py:112} INFO - [2020-11-01 21:41:36,087] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:41:36,089] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:36,124] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-01 21:41:49,122] {scheduler_job.py:155} INFO - Started process (PID=12287) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:49,127] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:41:49,128] {logging_mixin.py:112} INFO - [2020-11-01 21:41:49,128] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:49,240] {logging_mixin.py:112} INFO - [2020-11-01 21:41:49,239] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:41:49,240] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:41:49,266] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:42:02,366] {scheduler_job.py:155} INFO - Started process (PID=12353) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:02,383] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:42:02,384] {logging_mixin.py:112} INFO - [2020-11-01 21:42:02,384] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:02,797] {logging_mixin.py:112} INFO - [2020-11-01 21:42:02,796] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:42:02,798] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:02,836] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.470 seconds
[2020-11-01 21:42:15,575] {scheduler_job.py:155} INFO - Started process (PID=12412) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:15,580] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:42:15,580] {logging_mixin.py:112} INFO - [2020-11-01 21:42:15,580] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:15,752] {logging_mixin.py:112} INFO - [2020-11-01 21:42:15,751] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:42:15,752] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:15,779] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-01 21:42:28,859] {scheduler_job.py:155} INFO - Started process (PID=12475) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:28,868] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:42:28,869] {logging_mixin.py:112} INFO - [2020-11-01 21:42:28,868] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:29,114] {logging_mixin.py:112} INFO - [2020-11-01 21:42:29,112] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:42:29,114] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:29,148] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.289 seconds
[2020-11-01 21:42:42,097] {scheduler_job.py:155} INFO - Started process (PID=12538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:42,105] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:42:42,106] {logging_mixin.py:112} INFO - [2020-11-01 21:42:42,105] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:42,294] {logging_mixin.py:112} INFO - [2020-11-01 21:42:42,293] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:42:42,295] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:42,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-01 21:42:55,360] {scheduler_job.py:155} INFO - Started process (PID=12600) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:55,374] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:42:55,375] {logging_mixin.py:112} INFO - [2020-11-01 21:42:55,375] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:55,640] {logging_mixin.py:112} INFO - [2020-11-01 21:42:55,638] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:42:55,640] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:42:55,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.323 seconds
[2020-11-01 21:43:08,596] {scheduler_job.py:155} INFO - Started process (PID=12672) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:08,600] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:43:08,600] {logging_mixin.py:112} INFO - [2020-11-01 21:43:08,600] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:08,711] {logging_mixin.py:112} INFO - [2020-11-01 21:43:08,710] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:43:08,711] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:08,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-01 21:43:21,930] {scheduler_job.py:155} INFO - Started process (PID=12731) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:21,934] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:43:21,935] {logging_mixin.py:112} INFO - [2020-11-01 21:43:21,935] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:22,056] {logging_mixin.py:112} INFO - [2020-11-01 21:43:22,054] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:43:22,056] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:22,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-01 21:43:35,253] {scheduler_job.py:155} INFO - Started process (PID=12797) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:35,265] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:43:35,265] {logging_mixin.py:112} INFO - [2020-11-01 21:43:35,265] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:35,443] {logging_mixin.py:112} INFO - [2020-11-01 21:43:35,442] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:43:35,444] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:35,471] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.218 seconds
[2020-11-01 21:43:48,430] {scheduler_job.py:155} INFO - Started process (PID=12860) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:48,442] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:43:48,444] {logging_mixin.py:112} INFO - [2020-11-01 21:43:48,443] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:48,551] {logging_mixin.py:112} INFO - [2020-11-01 21:43:48,550] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:43:48,552] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:43:48,582] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-01 21:44:01,690] {scheduler_job.py:155} INFO - Started process (PID=12922) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:01,694] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:44:01,694] {logging_mixin.py:112} INFO - [2020-11-01 21:44:01,694] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:01,953] {logging_mixin.py:112} INFO - [2020-11-01 21:44:01,952] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:44:01,953] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:01,979] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.289 seconds
[2020-11-01 21:44:14,923] {scheduler_job.py:155} INFO - Started process (PID=12985) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:14,929] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:44:14,930] {logging_mixin.py:112} INFO - [2020-11-01 21:44:14,930] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:15,062] {logging_mixin.py:112} INFO - [2020-11-01 21:44:15,060] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:44:15,063] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:15,099] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-01 21:44:28,195] {scheduler_job.py:155} INFO - Started process (PID=13048) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:28,199] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:44:28,200] {logging_mixin.py:112} INFO - [2020-11-01 21:44:28,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:28,311] {logging_mixin.py:112} INFO - [2020-11-01 21:44:28,309] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:44:28,311] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:28,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-01 21:44:41,472] {scheduler_job.py:155} INFO - Started process (PID=13116) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:41,490] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:44:41,490] {logging_mixin.py:112} INFO - [2020-11-01 21:44:41,490] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:41,669] {logging_mixin.py:112} INFO - [2020-11-01 21:44:41,667] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:44:41,669] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:44:41,703] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-01 21:45:02,984] {scheduler_job.py:155} INFO - Started process (PID=13229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:02,989] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:45:02,990] {logging_mixin.py:112} INFO - [2020-11-01 21:45:02,990] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:03,282] {logging_mixin.py:112} INFO - [2020-11-01 21:45:03,281] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:45:03,283] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:03,317] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.334 seconds
[2020-11-01 21:45:16,329] {scheduler_job.py:155} INFO - Started process (PID=13297) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:16,335] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:45:16,347] {logging_mixin.py:112} INFO - [2020-11-01 21:45:16,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:16,666] {logging_mixin.py:112} INFO - [2020-11-01 21:45:16,665] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_Key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_Key_sensor'
[2020-11-01 21:45:16,666] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:16,699] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.371 seconds
[2020-11-01 21:45:29,562] {scheduler_job.py:155} INFO - Started process (PID=13357) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:29,568] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:45:29,569] {logging_mixin.py:112} INFO - [2020-11-01 21:45:29,568] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:29,770] {logging_mixin.py:112} INFO - [2020-11-01 21:45:29,769] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:45:29,770] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:29,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-01 21:45:43,006] {scheduler_job.py:155} INFO - Started process (PID=13427) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:43,010] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:45:43,019] {logging_mixin.py:112} INFO - [2020-11-01 21:45:43,019] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:43,233] {logging_mixin.py:112} INFO - [2020-11-01 21:45:43,232] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:45:43,234] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:43,263] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-01 21:45:56,306] {scheduler_job.py:155} INFO - Started process (PID=13508) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:56,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:45:56,327] {logging_mixin.py:112} INFO - [2020-11-01 21:45:56,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:56,751] {logging_mixin.py:112} INFO - [2020-11-01 21:45:56,746] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:45:56,752] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:45:56,790] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.485 seconds
[2020-11-01 21:46:09,609] {scheduler_job.py:155} INFO - Started process (PID=13594) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:09,625] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:46:09,625] {logging_mixin.py:112} INFO - [2020-11-01 21:46:09,625] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:10,091] {logging_mixin.py:112} INFO - [2020-11-01 21:46:10,086] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:46:10,091] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:10,126] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.518 seconds
[2020-11-01 21:46:22,880] {scheduler_job.py:155} INFO - Started process (PID=13664) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:22,892] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:46:22,893] {logging_mixin.py:112} INFO - [2020-11-01 21:46:22,892] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:23,254] {logging_mixin.py:112} INFO - [2020-11-01 21:46:23,251] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:46:23,254] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:23,289] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.409 seconds
[2020-11-01 21:46:36,113] {scheduler_job.py:155} INFO - Started process (PID=13727) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:36,124] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:46:36,125] {logging_mixin.py:112} INFO - [2020-11-01 21:46:36,125] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:36,423] {logging_mixin.py:112} INFO - [2020-11-01 21:46:36,421] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:46:36,424] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:36,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.342 seconds
[2020-11-01 21:46:49,350] {scheduler_job.py:155} INFO - Started process (PID=13786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:49,389] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:46:49,390] {logging_mixin.py:112} INFO - [2020-11-01 21:46:49,390] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:49,858] {logging_mixin.py:112} INFO - [2020-11-01 21:46:49,857] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:46:49,859] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:46:49,917] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.567 seconds
[2020-11-01 21:47:02,685] {scheduler_job.py:155} INFO - Started process (PID=13847) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:02,700] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:47:02,701] {logging_mixin.py:112} INFO - [2020-11-01 21:47:02,701] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:02,909] {logging_mixin.py:112} INFO - [2020-11-01 21:47:02,908] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:47:02,909] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:02,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-01 21:47:15,982] {scheduler_job.py:155} INFO - Started process (PID=13908) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:15,986] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:47:15,987] {logging_mixin.py:112} INFO - [2020-11-01 21:47:15,987] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:16,188] {logging_mixin.py:112} INFO - [2020-11-01 21:47:16,187] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:47:16,188] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:16,215] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-01 21:47:29,249] {scheduler_job.py:155} INFO - Started process (PID=13969) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:29,253] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:47:29,253] {logging_mixin.py:112} INFO - [2020-11-01 21:47:29,253] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:29,455] {logging_mixin.py:112} INFO - [2020-11-01 21:47:29,454] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:47:29,455] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:29,478] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-01 21:47:42,443] {scheduler_job.py:155} INFO - Started process (PID=14028) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:42,448] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:47:42,448] {logging_mixin.py:112} INFO - [2020-11-01 21:47:42,448] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:42,650] {logging_mixin.py:112} INFO - [2020-11-01 21:47:42,649] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:47:42,650] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:42,673] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-01 21:47:55,683] {scheduler_job.py:155} INFO - Started process (PID=14088) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:55,689] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:47:55,690] {logging_mixin.py:112} INFO - [2020-11-01 21:47:55,690] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:55,993] {logging_mixin.py:112} INFO - [2020-11-01 21:47:55,992] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:47:55,993] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:47:56,028] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.345 seconds
[2020-11-01 21:48:08,997] {scheduler_job.py:155} INFO - Started process (PID=14147) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:09,001] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:48:09,002] {logging_mixin.py:112} INFO - [2020-11-01 21:48:09,002] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:09,284] {logging_mixin.py:112} INFO - [2020-11-01 21:48:09,282] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 10, in <module>
    from airflow.operators.s3_key_sensor import S3KeySensor
ModuleNotFoundError: No module named 'airflow.operators.s3_key_sensor'
[2020-11-01 21:48:09,284] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:09,305] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.309 seconds
[2020-11-01 21:48:22,209] {scheduler_job.py:155} INFO - Started process (PID=14212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:22,213] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:48:22,214] {logging_mixin.py:112} INFO - [2020-11-01 21:48:22,213] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:22,463] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:48:23,194] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:48:23,253] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:23,366] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:48:23,449] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:48:23,456] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:48:23,515] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:48:23,524] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 19:00:00+00:00 [success]> in ORM
[2020-11-01 21:48:23,533] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:48:23,543] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:48:23,559] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.350 seconds
[2020-11-01 21:48:35,628] {scheduler_job.py:155} INFO - Started process (PID=14296) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:35,631] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:48:35,632] {logging_mixin.py:112} INFO - [2020-11-01 21:48:35,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:35,901] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:48:36,850] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:48:36,976] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:37,148] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:48:37,224] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:48:37,470] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:48:37,481] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.853 seconds
[2020-11-01 21:48:48,926] {scheduler_job.py:155} INFO - Started process (PID=14402) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:48,931] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:48:48,931] {logging_mixin.py:112} INFO - [2020-11-01 21:48:48,931] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:49,150] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:48:49,852] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:48:49,895] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:48:49,995] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:48:50,035] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:48:50,129] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:48:50,138] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.212 seconds
[2020-11-01 21:49:02,159] {scheduler_job.py:155} INFO - Started process (PID=14466) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:02,163] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:49:02,164] {logging_mixin.py:112} INFO - [2020-11-01 21:49:02,164] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:02,357] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:49:03,004] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:49:03,039] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:03,105] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:49:03,140] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:49:03,276] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:49:03,283] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.124 seconds
[2020-11-01 21:49:15,474] {scheduler_job.py:155} INFO - Started process (PID=14524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:15,478] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:49:15,478] {logging_mixin.py:112} INFO - [2020-11-01 21:49:15,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:15,788] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:49:16,724] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:49:16,761] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:16,820] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:49:16,847] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:49:16,925] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:49:16,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.459 seconds
[2020-11-01 21:49:28,768] {scheduler_job.py:155} INFO - Started process (PID=14586) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:28,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:49:28,788] {logging_mixin.py:112} INFO - [2020-11-01 21:49:28,788] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:29,195] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:49:30,123] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:49:30,166] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:30,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:49:30,245] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:49:30,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:49:30,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.548 seconds
[2020-11-01 21:49:42,067] {scheduler_job.py:155} INFO - Started process (PID=14647) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:42,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:49:42,072] {logging_mixin.py:112} INFO - [2020-11-01 21:49:42,072] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:42,270] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:49:42,851] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:49:42,879] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:42,934] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:49:42,954] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:49:43,014] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:49:43,019] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.951 seconds
[2020-11-01 21:49:55,265] {scheduler_job.py:155} INFO - Started process (PID=14713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:55,269] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:49:55,270] {logging_mixin.py:112} INFO - [2020-11-01 21:49:55,270] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:55,484] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:49:56,136] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:49:56,173] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:49:56,233] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:49:56,256] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:49:56,375] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:49:56,381] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-01 21:50:08,551] {scheduler_job.py:155} INFO - Started process (PID=14773) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:08,555] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:50:08,561] {logging_mixin.py:112} INFO - [2020-11-01 21:50:08,556] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:08,772] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:50:09,380] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:50:09,409] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:09,452] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:50:09,473] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:50:09,527] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:50:09,533] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-01 21:50:21,858] {scheduler_job.py:155} INFO - Started process (PID=14836) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:21,875] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:50:21,876] {logging_mixin.py:112} INFO - [2020-11-01 21:50:21,876] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:22,070] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:50:22,663] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:50:22,691] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:22,731] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:50:22,750] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:50:22,808] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:50:22,812] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.954 seconds
[2020-11-01 21:50:35,178] {scheduler_job.py:155} INFO - Started process (PID=14907) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:35,187] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:50:35,188] {logging_mixin.py:112} INFO - [2020-11-01 21:50:35,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:35,384] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:50:35,992] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:50:36,023] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:36,063] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:50:36,081] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:50:36,138] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:50:36,146] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:50:36,157] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-01 21:50:48,483] {scheduler_job.py:155} INFO - Started process (PID=14993) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:48,526] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:50:48,531] {logging_mixin.py:112} INFO - [2020-11-01 21:50:48,526] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:48,887] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:50:49,890] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:50:49,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:50:49,986] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:50:50,018] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:50:50,105] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:50:50,117] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:50:50,132] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.648 seconds
[2020-11-01 21:51:01,898] {scheduler_job.py:155} INFO - Started process (PID=15080) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:01,934] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:51:01,940] {logging_mixin.py:112} INFO - [2020-11-01 21:51:01,940] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:02,221] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:51:02,982] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:51:03,035] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:03,109] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:51:03,141] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:51:03,228] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:51:03,234] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.336 seconds
[2020-11-01 21:51:15,167] {scheduler_job.py:155} INFO - Started process (PID=15135) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:15,170] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:51:15,171] {logging_mixin.py:112} INFO - [2020-11-01 21:51:15,171] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:15,404] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:51:16,338] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:51:16,368] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:16,409] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:51:16,428] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:51:16,492] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:51:16,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.330 seconds
[2020-11-01 21:51:28,446] {scheduler_job.py:155} INFO - Started process (PID=15198) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:28,449] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:51:28,450] {logging_mixin.py:112} INFO - [2020-11-01 21:51:28,450] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:28,833] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:51:29,889] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:51:29,934] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:30,004] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:51:30,040] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:51:30,149] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:51:30,159] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.713 seconds
[2020-11-01 21:51:41,800] {scheduler_job.py:155} INFO - Started process (PID=15255) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:41,805] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:51:41,806] {logging_mixin.py:112} INFO - [2020-11-01 21:51:41,806] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:42,020] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:51:42,909] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:51:42,942] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:42,992] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:51:43,016] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:51:43,119] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:51:43,126] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.327 seconds
[2020-11-01 21:51:55,072] {scheduler_job.py:155} INFO - Started process (PID=15309) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:55,076] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:51:55,076] {logging_mixin.py:112} INFO - [2020-11-01 21:51:55,076] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:55,272] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:51:56,040] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:51:56,090] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:51:56,163] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:51:56,186] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:51:56,255] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:51:56,260] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.188 seconds
[2020-11-01 21:52:08,366] {scheduler_job.py:155} INFO - Started process (PID=15371) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:08,373] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:52:08,377] {logging_mixin.py:112} INFO - [2020-11-01 21:52:08,377] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:08,704] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:52:09,462] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:52:09,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:09,537] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:52:09,559] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:52:09,594] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:52:09,665] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:52:09,672] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 20:52:01.950143+00:00 [success]> in ORM
[2020-11-01 21:52:09,679] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:52:09,687] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:52:09,699] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.333 seconds
[2020-11-01 21:52:21,594] {scheduler_job.py:155} INFO - Started process (PID=15458) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:21,606] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:52:21,607] {logging_mixin.py:112} INFO - [2020-11-01 21:52:21,607] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:21,835] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:52:22,571] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:52:22,618] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:22,688] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:52:22,708] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:52:22,745] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:52:22,823] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:52:22,827] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-01 21:52:34,931] {scheduler_job.py:155} INFO - Started process (PID=15542) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:34,953] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:52:34,954] {logging_mixin.py:112} INFO - [2020-11-01 21:52:34,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:35,151] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:52:35,728] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:52:35,756] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:35,799] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:52:35,817] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:52:35,851] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:52:35,915] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:52:35,925] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.994 seconds
[2020-11-01 21:52:48,176] {scheduler_job.py:155} INFO - Started process (PID=15600) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:48,180] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:52:48,181] {logging_mixin.py:112} INFO - [2020-11-01 21:52:48,180] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:48,372] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:52:48,944] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:52:48,973] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:52:49,008] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:52:49,032] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:52:49,070] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:52:49,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:52:49,150] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:52:49,162] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.986 seconds
[2020-11-01 21:53:00,542] {scheduler_job.py:155} INFO - Started process (PID=15688) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:00,549] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:53:00,550] {logging_mixin.py:112} INFO - [2020-11-01 21:53:00,550] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:00,790] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:53:01,513] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:53:01,564] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:01,628] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:53:01,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:53:01,686] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:53:01,753] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:53:01,760] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:53:01,770] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.228 seconds
[2020-11-01 21:53:14,843] {scheduler_job.py:155} INFO - Started process (PID=15773) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:14,849] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:53:14,850] {logging_mixin.py:112} INFO - [2020-11-01 21:53:14,850] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:15,042] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:53:15,653] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:53:15,681] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:15,722] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:53:15,741] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:53:15,762] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:53:15,829] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:53:15,833] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.991 seconds
[2020-11-01 21:53:28,125] {scheduler_job.py:155} INFO - Started process (PID=15840) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:28,135] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:53:28,136] {logging_mixin.py:112} INFO - [2020-11-01 21:53:28,136] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:28,376] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:53:29,040] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:53:29,071] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:29,139] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:53:29,175] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:53:29,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:53:29,369] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:53:29,373] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.248 seconds
[2020-11-01 21:53:41,367] {scheduler_job.py:155} INFO - Started process (PID=15902) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:41,371] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:53:41,372] {logging_mixin.py:112} INFO - [2020-11-01 21:53:41,372] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:41,564] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:53:42,148] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:53:42,178] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:42,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:53:42,243] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:53:42,264] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:53:42,327] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:53:42,331] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.964 seconds
[2020-11-01 21:53:54,607] {scheduler_job.py:155} INFO - Started process (PID=15966) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:54,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:53:54,611] {logging_mixin.py:112} INFO - [2020-11-01 21:53:54,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:54,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:53:55,396] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:53:55,426] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:53:55,467] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:53:55,486] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:53:55,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:53:55,571] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:53:55,576] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.969 seconds
[2020-11-01 21:54:08,002] {scheduler_job.py:155} INFO - Started process (PID=16035) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:08,007] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:54:08,008] {logging_mixin.py:112} INFO - [2020-11-01 21:54:08,008] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:08,202] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:54:08,852] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:54:08,877] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:08,913] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:54:08,931] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:54:08,950] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:54:09,012] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:54:09,017] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.015 seconds
[2020-11-01 21:54:21,321] {scheduler_job.py:155} INFO - Started process (PID=16107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:21,331] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:54:21,332] {logging_mixin.py:112} INFO - [2020-11-01 21:54:21,331] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:21,547] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:54:22,127] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:54:22,159] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:22,207] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:54:22,229] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:54:22,250] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:54:22,324] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:54:22,332] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:54:22,346] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.025 seconds
[2020-11-01 21:54:34,506] {scheduler_job.py:155} INFO - Started process (PID=16194) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:34,511] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:54:34,512] {logging_mixin.py:112} INFO - [2020-11-01 21:54:34,512] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:34,778] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:54:35,468] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:54:35,500] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:35,542] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:54:35,565] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:54:35,585] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:54:35,659] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:54:35,666] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:54:35,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.170 seconds
[2020-11-01 21:54:47,954] {scheduler_job.py:155} INFO - Started process (PID=16283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:47,959] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:54:47,960] {logging_mixin.py:112} INFO - [2020-11-01 21:54:47,960] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:48,195] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:54:48,809] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:54:48,837] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:54:48,877] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:54:48,896] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:54:48,916] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:54:48,995] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:54:48,999] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.045 seconds
[2020-11-01 21:55:01,266] {scheduler_job.py:155} INFO - Started process (PID=16351) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:01,277] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:55:01,279] {logging_mixin.py:112} INFO - [2020-11-01 21:55:01,278] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:01,629] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:55:02,363] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:55:02,400] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:02,451] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:55:02,475] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:55:02,501] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:55:02,575] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:55:02,582] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:55:02,591] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.325 seconds
[2020-11-01 21:55:14,535] {scheduler_job.py:155} INFO - Started process (PID=16434) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:14,539] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:55:14,540] {logging_mixin.py:112} INFO - [2020-11-01 21:55:14,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:14,733] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:55:15,336] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:55:15,364] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:15,407] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:55:15,426] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:55:15,453] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:55:15,531] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:55:15,540] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 19:00:00+00:00 [scheduled]> in ORM
[2020-11-01 21:55:15,550] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.016 seconds
[2020-11-01 21:55:27,794] {scheduler_job.py:155} INFO - Started process (PID=16518) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:27,797] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:55:27,798] {logging_mixin.py:112} INFO - [2020-11-01 21:55:27,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:28,037] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:55:28,704] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:55:28,733] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:28,773] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:55:28,793] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False>
[2020-11-01 21:55:28,810] {logging_mixin.py:112} INFO - [2020-11-01 21:55:28,810] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 19:00:00+00:00: scheduled__2020-11-01T19:00:00+00:00, externally triggered: False> failed
[2020-11-01 21:55:28,814] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:55:28,862] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:55:28,866] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.072 seconds
[2020-11-01 21:55:41,103] {scheduler_job.py:155} INFO - Started process (PID=16586) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:41,106] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:55:41,107] {logging_mixin.py:112} INFO - [2020-11-01 21:55:41,107] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:41,319] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:55:41,918] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:55:41,946] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:41,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:55:42,006] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:55:42,053] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:55:42,057] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.955 seconds
[2020-11-01 21:55:54,316] {scheduler_job.py:155} INFO - Started process (PID=16657) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:54,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:55:54,322] {logging_mixin.py:112} INFO - [2020-11-01 21:55:54,321] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:54,508] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:55:55,233] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:55:55,261] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:55:55,301] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:55:55,325] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:55:55,377] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:55:55,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.066 seconds
[2020-11-01 21:56:07,577] {scheduler_job.py:155} INFO - Started process (PID=16722) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:07,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:56:07,583] {logging_mixin.py:112} INFO - [2020-11-01 21:56:07,583] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:07,791] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:56:08,362] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:56:08,390] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:08,432] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:56:08,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:56:08,500] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:56:08,505] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.928 seconds
[2020-11-01 21:56:20,826] {scheduler_job.py:155} INFO - Started process (PID=16789) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:20,842] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:56:20,846] {logging_mixin.py:112} INFO - [2020-11-01 21:56:20,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:21,056] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:56:21,775] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:56:21,818] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:21,910] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:56:21,938] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:56:21,995] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:56:21,999] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.173 seconds
[2020-11-01 21:56:34,151] {scheduler_job.py:155} INFO - Started process (PID=16855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:34,157] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:56:34,160] {logging_mixin.py:112} INFO - [2020-11-01 21:56:34,159] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:34,447] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:56:35,161] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:56:35,240] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:35,284] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:56:35,309] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:56:35,380] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:56:35,387] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:56:35,400] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.248 seconds
[2020-11-01 21:56:47,396] {scheduler_job.py:155} INFO - Started process (PID=16936) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:47,402] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:56:47,402] {logging_mixin.py:112} INFO - [2020-11-01 21:56:47,402] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:47,589] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:56:48,175] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:56:48,208] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:56:48,255] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:56:48,272] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:56:48,317] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:56:48,324] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:56:48,337] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.941 seconds
[2020-11-01 21:57:00,676] {scheduler_job.py:155} INFO - Started process (PID=17019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:00,687] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:57:00,691] {logging_mixin.py:112} INFO - [2020-11-01 21:57:00,690] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:00,868] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:57:01,478] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:57:01,507] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:01,547] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:57:01,566] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:57:01,610] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:57:01,614] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.938 seconds
[2020-11-01 21:57:14,015] {scheduler_job.py:155} INFO - Started process (PID=17084) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:14,020] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:57:14,021] {logging_mixin.py:112} INFO - [2020-11-01 21:57:14,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:14,218] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:57:14,826] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:57:14,855] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:14,895] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:57:14,914] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:57:14,959] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:57:14,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.949 seconds
[2020-11-01 21:57:27,312] {scheduler_job.py:155} INFO - Started process (PID=17146) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:27,317] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:57:27,318] {logging_mixin.py:112} INFO - [2020-11-01 21:57:27,318] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:27,499] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:57:28,071] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:57:28,101] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:28,142] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:57:28,164] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:57:28,217] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:57:28,224] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.912 seconds
[2020-11-01 21:57:40,563] {scheduler_job.py:155} INFO - Started process (PID=17212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:40,567] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:57:40,567] {logging_mixin.py:112} INFO - [2020-11-01 21:57:40,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:40,744] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:57:41,337] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:57:41,366] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:41,406] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:57:41,425] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:57:41,470] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:57:41,475] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.912 seconds
[2020-11-01 21:57:54,013] {scheduler_job.py:155} INFO - Started process (PID=17276) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:54,024] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:57:54,025] {logging_mixin.py:112} INFO - [2020-11-01 21:57:54,025] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:54,221] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:57:54,818] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:57:54,847] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:57:54,888] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:57:54,907] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:57:54,960] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:57:54,964] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.951 seconds
[2020-11-01 21:58:07,291] {scheduler_job.py:155} INFO - Started process (PID=17348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:07,295] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:58:07,307] {logging_mixin.py:112} INFO - [2020-11-01 21:58:07,307] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:07,488] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:58:08,066] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:58:08,100] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:08,151] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:58:08,176] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:58:08,227] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:58:08,232] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.940 seconds
[2020-11-01 21:58:20,614] {scheduler_job.py:155} INFO - Started process (PID=17410) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:20,617] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:58:20,618] {logging_mixin.py:112} INFO - [2020-11-01 21:58:20,618] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:20,803] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:58:21,388] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:58:21,417] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:21,459] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:58:21,483] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:58:21,542] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:58:21,547] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.933 seconds
[2020-11-01 21:58:33,921] {scheduler_job.py:155} INFO - Started process (PID=17476) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:33,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:58:33,927] {logging_mixin.py:112} INFO - [2020-11-01 21:58:33,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:34,157] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:58:34,836] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:58:34,870] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:34,911] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:58:34,937] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:58:34,987] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:58:34,990] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.070 seconds
[2020-11-01 21:58:47,249] {scheduler_job.py:155} INFO - Started process (PID=17538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:47,257] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:58:47,258] {logging_mixin.py:112} INFO - [2020-11-01 21:58:47,258] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:47,583] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:58:48,552] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:58:48,592] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:58:48,660] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:58:48,691] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:58:48,767] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:58:48,780] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:58:48,797] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.548 seconds
[2020-11-01 21:59:00,517] {scheduler_job.py:155} INFO - Started process (PID=17623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:00,523] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:59:00,524] {logging_mixin.py:112} INFO - [2020-11-01 21:59:00,523] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:00,793] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:59:01,452] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:59:01,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:01,536] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:59:01,561] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:59:01,626] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:59:01,633] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:52:01.950143+00:00 [scheduled]> in ORM
[2020-11-01 21:59:01,644] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.127 seconds
[2020-11-01 21:59:13,924] {scheduler_job.py:155} INFO - Started process (PID=17707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:13,928] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:59:13,929] {logging_mixin.py:112} INFO - [2020-11-01 21:59:13,928] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:14,243] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:59:15,063] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:59:15,096] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:15,138] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:59:15,164] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True>
[2020-11-01 21:59:15,185] {logging_mixin.py:112} INFO - [2020-11-01 21:59:15,185] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 20:52:01.950143+00:00: manual__2020-11-01T20:52:01.950143+00:00, externally triggered: True> failed
[2020-11-01 21:59:15,190] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:59:15,200] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.272 seconds
[2020-11-01 21:59:27,153] {scheduler_job.py:155} INFO - Started process (PID=17765) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:27,161] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:59:27,162] {logging_mixin.py:112} INFO - [2020-11-01 21:59:27,162] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:27,378] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:59:27,958] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:59:27,986] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:28,047] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:59:28,067] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:59:28,070] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.917 seconds
[2020-11-01 21:59:40,362] {scheduler_job.py:155} INFO - Started process (PID=17850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:40,369] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:59:40,370] {logging_mixin.py:112} INFO - [2020-11-01 21:59:40,370] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:40,559] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:59:41,136] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:59:41,167] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:41,208] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:59:41,228] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:59:41,232] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.870 seconds
[2020-11-01 21:59:53,707] {scheduler_job.py:155} INFO - Started process (PID=17915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:53,710] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 21:59:53,711] {logging_mixin.py:112} INFO - [2020-11-01 21:59:53,711] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:53,892] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 21:59:54,482] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 21:59:54,510] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 21:59:54,552] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 21:59:54,574] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 21:59:54,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.872 seconds
[2020-11-01 22:00:07,082] {scheduler_job.py:155} INFO - Started process (PID=17984) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:07,086] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:00:07,091] {logging_mixin.py:112} INFO - [2020-11-01 22:00:07,091] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:07,327] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:00:07,999] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:00:08,033] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:08,137] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:00:08,264] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:00:08,279] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:00:08,385] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:00:08,396] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 20:00:00+00:00 [success]> in ORM
[2020-11-01 22:00:08,407] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:00:08,417] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:00:08,437] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.363 seconds
[2020-11-01 22:00:20,314] {scheduler_job.py:155} INFO - Started process (PID=18071) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:20,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:00:20,326] {logging_mixin.py:112} INFO - [2020-11-01 22:00:20,326] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:20,544] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:00:21,224] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:00:21,256] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:21,303] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:00:21,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:00:21,397] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:00:21,401] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.086 seconds
[2020-11-01 22:00:33,695] {scheduler_job.py:155} INFO - Started process (PID=18150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:33,699] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:00:33,700] {logging_mixin.py:112} INFO - [2020-11-01 22:00:33,699] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:33,879] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:00:34,496] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:00:34,524] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:34,564] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:00:34,583] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:00:34,640] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:00:34,644] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.949 seconds
[2020-11-01 22:00:46,944] {scheduler_job.py:155} INFO - Started process (PID=18213) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:46,948] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:00:46,949] {logging_mixin.py:112} INFO - [2020-11-01 22:00:46,949] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:47,150] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:00:47,734] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:00:47,762] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:00:47,804] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:00:47,824] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:00:47,882] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:00:47,886] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.941 seconds
[2020-11-01 22:01:00,181] {scheduler_job.py:155} INFO - Started process (PID=18290) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:00,187] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:01:00,188] {logging_mixin.py:112} INFO - [2020-11-01 22:01:00,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:00,568] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:01:01,407] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:01:01,447] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:01,510] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:01:01,534] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:01:01,610] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:01:01,616] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.435 seconds
[2020-11-01 22:01:13,407] {scheduler_job.py:155} INFO - Started process (PID=18353) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:13,411] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:01:13,412] {logging_mixin.py:112} INFO - [2020-11-01 22:01:13,412] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:13,648] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:01:14,237] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:01:14,265] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:14,305] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:01:14,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:01:14,379] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:01:14,383] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.977 seconds
[2020-11-01 22:01:26,706] {scheduler_job.py:155} INFO - Started process (PID=18413) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:26,717] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:01:26,719] {logging_mixin.py:112} INFO - [2020-11-01 22:01:26,718] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:26,937] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:01:27,551] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:01:27,588] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:27,652] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:01:27,674] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:01:27,740] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:01:27,744] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.039 seconds
[2020-11-01 22:01:40,014] {scheduler_job.py:155} INFO - Started process (PID=18478) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:40,030] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:01:40,031] {logging_mixin.py:112} INFO - [2020-11-01 22:01:40,031] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:40,285] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:01:40,940] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:01:40,969] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:41,012] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:01:41,032] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:01:41,085] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:01:41,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.086 seconds
[2020-11-01 22:01:53,260] {scheduler_job.py:155} INFO - Started process (PID=18540) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:53,265] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:01:53,266] {logging_mixin.py:112} INFO - [2020-11-01 22:01:53,266] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:53,586] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:01:54,474] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:01:54,516] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:01:54,562] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:01:54,585] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:01:54,658] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:01:54,664] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.403 seconds
[2020-11-01 22:02:06,562] {scheduler_job.py:155} INFO - Started process (PID=18598) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:06,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:02:06,576] {logging_mixin.py:112} INFO - [2020-11-01 22:02:06,576] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:06,802] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:02:07,716] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:02:07,750] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:07,795] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:02:07,817] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:02:07,890] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:02:07,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.333 seconds
[2020-11-01 22:02:19,809] {scheduler_job.py:155} INFO - Started process (PID=18661) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:19,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:02:19,814] {logging_mixin.py:112} INFO - [2020-11-01 22:02:19,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:19,996] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:02:20,588] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:02:20,624] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:20,668] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:02:20,688] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:02:20,746] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:02:20,753] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:02:20,764] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.955 seconds
[2020-11-01 22:02:33,042] {scheduler_job.py:155} INFO - Started process (PID=18748) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:33,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:02:33,046] {logging_mixin.py:112} INFO - [2020-11-01 22:02:33,046] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:33,239] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:02:33,828] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:02:33,858] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:33,919] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:02:33,950] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:02:34,045] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:02:34,057] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:02:34,072] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.030 seconds
[2020-11-01 22:02:46,417] {scheduler_job.py:155} INFO - Started process (PID=18837) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:46,420] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:02:46,421] {logging_mixin.py:112} INFO - [2020-11-01 22:02:46,421] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:46,608] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:02:47,177] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:02:47,205] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:47,250] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:02:47,270] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:02:47,326] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:02:47,333] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.916 seconds
[2020-11-01 22:02:59,668] {scheduler_job.py:155} INFO - Started process (PID=18899) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:59,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:02:59,676] {logging_mixin.py:112} INFO - [2020-11-01 22:02:59,676] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:02:59,935] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:03:00,857] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:03:00,899] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:00,970] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:03:01,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:03:01,121] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:03:01,136] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.467 seconds
[2020-11-01 22:03:12,887] {scheduler_job.py:155} INFO - Started process (PID=18965) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:12,891] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:03:12,892] {logging_mixin.py:112} INFO - [2020-11-01 22:03:12,892] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:13,082] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:03:13,670] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:03:13,700] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:13,741] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:03:13,760] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:03:13,817] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:03:13,822] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.935 seconds
[2020-11-01 22:03:26,162] {scheduler_job.py:155} INFO - Started process (PID=19023) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:26,178] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:03:26,178] {logging_mixin.py:112} INFO - [2020-11-01 22:03:26,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:26,448] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:03:27,151] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:03:27,186] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:27,229] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:03:27,253] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:03:27,291] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:03:27,375] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:03:27,385] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 21:03:14.918435+00:00 [success]> in ORM
[2020-11-01 22:03:27,392] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:03:27,398] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:03:27,415] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.254 seconds
[2020-11-01 22:03:39,592] {scheduler_job.py:155} INFO - Started process (PID=19164) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:39,620] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:03:39,621] {logging_mixin.py:112} INFO - [2020-11-01 22:03:39,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:39,993] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:03:41,659] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:03:41,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:41,833] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:03:41,870] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:03:41,949] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:03:42,097] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:03:42,103] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.511 seconds
[2020-11-01 22:03:53,871] {scheduler_job.py:155} INFO - Started process (PID=19246) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:53,876] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:03:53,877] {logging_mixin.py:112} INFO - [2020-11-01 22:03:53,876] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:54,068] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:03:54,687] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:03:54,717] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:03:54,758] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:03:54,777] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:03:54,811] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:03:54,898] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:03:54,903] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.032 seconds
[2020-11-01 22:04:07,103] {scheduler_job.py:155} INFO - Started process (PID=19308) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:07,107] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:04:07,108] {logging_mixin.py:112} INFO - [2020-11-01 22:04:07,107] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:07,389] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:04:08,125] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:04:08,159] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:08,202] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:04:08,223] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:04:08,257] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:04:08,346] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:04:08,351] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.248 seconds
[2020-11-01 22:04:20,377] {scheduler_job.py:155} INFO - Started process (PID=19371) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:20,381] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:04:20,385] {logging_mixin.py:112} INFO - [2020-11-01 22:04:20,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:20,609] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:04:21,190] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:04:21,221] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:21,262] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:04:21,290] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:04:21,329] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:04:21,431] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:04:21,438] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.061 seconds
[2020-11-01 22:04:33,671] {scheduler_job.py:155} INFO - Started process (PID=19432) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:33,675] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:04:33,687] {logging_mixin.py:112} INFO - [2020-11-01 22:04:33,686] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:34,066] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:04:35,100] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:04:35,140] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:35,202] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:04:35,233] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:04:35,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:04:35,407] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:04:35,416] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:04:35,431] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.761 seconds
[2020-11-01 22:04:46,947] {scheduler_job.py:155} INFO - Started process (PID=19547) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:46,961] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:04:46,965] {logging_mixin.py:112} INFO - [2020-11-01 22:04:46,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:47,284] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:04:48,005] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:04:48,034] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:04:48,089] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:04:48,108] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:04:48,142] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:04:48,222] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:04:48,228] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:04:48,238] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.291 seconds
[2020-11-01 22:05:00,217] {scheduler_job.py:155} INFO - Started process (PID=19630) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:00,222] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:05:00,222] {logging_mixin.py:112} INFO - [2020-11-01 22:05:00,222] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:00,497] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:05:01,093] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:05:01,122] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:01,162] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:05:01,183] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:05:01,207] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:05:01,289] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:05:01,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.076 seconds
[2020-11-01 22:05:13,475] {scheduler_job.py:155} INFO - Started process (PID=19692) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:13,478] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:05:13,479] {logging_mixin.py:112} INFO - [2020-11-01 22:05:13,479] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:13,664] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:05:14,324] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:05:14,359] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:14,407] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:05:14,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:05:14,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:05:14,587] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:05:14,591] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-01 22:05:26,772] {scheduler_job.py:155} INFO - Started process (PID=19749) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:26,780] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:05:26,781] {logging_mixin.py:112} INFO - [2020-11-01 22:05:26,781] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:27,094] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:05:28,065] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:05:28,100] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:28,142] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:05:28,162] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:05:28,186] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:05:28,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:05:28,292] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.520 seconds
[2020-11-01 22:05:40,043] {scheduler_job.py:155} INFO - Started process (PID=19810) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:40,051] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:05:40,052] {logging_mixin.py:112} INFO - [2020-11-01 22:05:40,052] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:40,251] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:05:40,941] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:05:40,980] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:41,035] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:05:41,057] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:05:41,086] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:05:41,177] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:05:41,185] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:05:41,195] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-01 22:05:53,317] {scheduler_job.py:155} INFO - Started process (PID=19906) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:53,320] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:05:53,321] {logging_mixin.py:112} INFO - [2020-11-01 22:05:53,321] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:53,524] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:05:54,217] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:05:54,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:05:54,323] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:05:54,367] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:05:54,390] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:05:54,450] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:05:54,458] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:05:54,469] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.152 seconds
[2020-11-01 22:06:06,536] {scheduler_job.py:155} INFO - Started process (PID=19991) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:06,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:06:06,540] {logging_mixin.py:112} INFO - [2020-11-01 22:06:06,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:06,726] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:06:07,308] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:06:07,336] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:07,377] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:06:07,396] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:06:07,416] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:06:07,481] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:06:07,485] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.949 seconds
[2020-11-01 22:06:19,786] {scheduler_job.py:155} INFO - Started process (PID=20054) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:19,792] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:06:19,793] {logging_mixin.py:112} INFO - [2020-11-01 22:06:19,793] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:19,997] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:06:20,625] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:06:20,678] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:20,763] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:06:20,833] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:06:20,874] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:06:21,081] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:06:21,087] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.302 seconds
[2020-11-01 22:06:33,059] {scheduler_job.py:155} INFO - Started process (PID=20119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:33,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:06:33,083] {logging_mixin.py:112} INFO - [2020-11-01 22:06:33,083] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:33,343] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:06:33,988] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:06:34,019] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:34,074] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:06:34,131] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:06:34,158] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:06:34,240] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:06:34,244] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.186 seconds
[2020-11-01 22:06:46,274] {scheduler_job.py:155} INFO - Started process (PID=20176) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:46,280] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:06:46,280] {logging_mixin.py:112} INFO - [2020-11-01 22:06:46,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:46,460] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:06:47,034] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:06:47,062] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:47,102] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:06:47,121] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:06:47,141] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:06:47,211] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:06:47,219] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:06:47,229] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.955 seconds
[2020-11-01 22:06:59,528] {scheduler_job.py:155} INFO - Started process (PID=20266) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:59,532] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:06:59,533] {logging_mixin.py:112} INFO - [2020-11-01 22:06:59,533] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:06:59,716] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:07:00,319] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:07:00,347] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:00,390] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:07:00,410] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:07:00,430] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:07:00,500] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:07:00,507] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 20:00:00+00:00 [scheduled]> in ORM
[2020-11-01 22:07:00,516] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.988 seconds
[2020-11-01 22:07:12,872] {scheduler_job.py:155} INFO - Started process (PID=20359) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:12,876] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:07:12,877] {logging_mixin.py:112} INFO - [2020-11-01 22:07:12,877] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:13,080] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:07:13,691] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:07:13,719] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:13,762] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:07:13,781] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False>
[2020-11-01 22:07:13,799] {logging_mixin.py:112} INFO - [2020-11-01 22:07:13,799] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 20:00:00+00:00: scheduled__2020-11-01T20:00:00+00:00, externally triggered: False> failed
[2020-11-01 22:07:13,803] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:07:13,854] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:07:13,859] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.987 seconds
[2020-11-01 22:07:26,158] {scheduler_job.py:155} INFO - Started process (PID=20419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:26,163] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:07:26,163] {logging_mixin.py:112} INFO - [2020-11-01 22:07:26,163] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:26,392] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:07:27,014] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:07:27,046] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:27,089] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:07:27,107] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:07:27,157] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:07:27,163] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.005 seconds
[2020-11-01 22:07:39,358] {scheduler_job.py:155} INFO - Started process (PID=20488) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:39,362] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:07:39,363] {logging_mixin.py:112} INFO - [2020-11-01 22:07:39,362] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:39,551] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:07:40,112] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:07:40,140] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:40,181] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:07:40,201] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:07:40,245] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:07:40,249] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.892 seconds
[2020-11-01 22:07:52,735] {scheduler_job.py:155} INFO - Started process (PID=20553) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:52,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:07:52,740] {logging_mixin.py:112} INFO - [2020-11-01 22:07:52,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:52,932] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:07:53,570] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:07:53,605] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:07:53,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:07:53,663] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:07:53,709] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:07:53,716] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:07:53,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.992 seconds
[2020-11-01 22:08:06,063] {scheduler_job.py:155} INFO - Started process (PID=20636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:06,070] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:08:06,071] {logging_mixin.py:112} INFO - [2020-11-01 22:08:06,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:06,271] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:08:06,841] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:08:06,869] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:06,909] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:08:06,928] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:08:06,985] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:08:06,995] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:08:07,006] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.943 seconds
[2020-11-01 22:08:19,283] {scheduler_job.py:155} INFO - Started process (PID=20721) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:19,293] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:08:19,294] {logging_mixin.py:112} INFO - [2020-11-01 22:08:19,293] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:19,492] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:08:20,166] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:08:20,198] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:20,242] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:08:20,261] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:08:20,336] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:08:20,341] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.058 seconds
[2020-11-01 22:08:32,550] {scheduler_job.py:155} INFO - Started process (PID=20786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:32,554] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:08:32,554] {logging_mixin.py:112} INFO - [2020-11-01 22:08:32,554] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:32,746] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:08:33,365] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:08:33,396] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:33,442] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:08:33,463] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:08:33,524] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:08:33,530] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-01 22:08:45,833] {scheduler_job.py:155} INFO - Started process (PID=20850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:45,840] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:08:45,841] {logging_mixin.py:112} INFO - [2020-11-01 22:08:45,841] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:46,025] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:08:46,629] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:08:46,658] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:46,697] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:08:46,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:08:46,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:08:46,779] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.946 seconds
[2020-11-01 22:08:59,073] {scheduler_job.py:155} INFO - Started process (PID=20914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:59,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:08:59,078] {logging_mixin.py:112} INFO - [2020-11-01 22:08:59,078] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:08:59,307] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:09:00,048] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:09:00,081] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:00,132] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:09:00,179] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:09:00,275] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:09:00,280] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.207 seconds
[2020-11-01 22:09:12,318] {scheduler_job.py:155} INFO - Started process (PID=20988) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:12,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:09:12,323] {logging_mixin.py:112} INFO - [2020-11-01 22:09:12,322] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:12,510] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:09:13,079] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:09:13,107] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:13,150] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:09:13,171] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:09:13,235] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:09:13,239] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.921 seconds
[2020-11-01 22:09:25,647] {scheduler_job.py:155} INFO - Started process (PID=21052) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:25,652] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:09:25,653] {logging_mixin.py:112} INFO - [2020-11-01 22:09:25,653] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:25,849] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:09:26,411] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:09:26,439] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:26,481] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:09:26,500] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:09:26,558] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:09:26,562] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.916 seconds
[2020-11-01 22:09:38,906] {scheduler_job.py:155} INFO - Started process (PID=21128) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:38,910] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:09:38,910] {logging_mixin.py:112} INFO - [2020-11-01 22:09:38,910] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:39,107] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:09:39,730] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:09:39,758] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:39,798] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:09:39,817] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:09:39,873] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:09:39,878] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.972 seconds
[2020-11-01 22:09:52,153] {scheduler_job.py:155} INFO - Started process (PID=21192) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:52,160] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:09:52,161] {logging_mixin.py:112} INFO - [2020-11-01 22:09:52,160] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:52,350] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:09:52,988] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:09:53,017] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:09:53,058] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:09:53,077] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:09:53,137] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:09:53,144] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.990 seconds
[2020-11-01 22:10:05,409] {scheduler_job.py:155} INFO - Started process (PID=21259) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:05,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:10:05,415] {logging_mixin.py:112} INFO - [2020-11-01 22:10:05,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:05,645] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:10:06,223] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:10:06,251] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:06,292] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:10:06,311] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:10:06,370] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:10:06,378] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:10:06,389] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-01 22:10:18,664] {scheduler_job.py:155} INFO - Started process (PID=21343) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:18,668] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:10:18,669] {logging_mixin.py:112} INFO - [2020-11-01 22:10:18,669] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:18,868] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:10:19,510] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:10:19,542] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:19,591] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:10:19,614] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:10:19,671] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:10:19,681] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:03:14.918435+00:00 [scheduled]> in ORM
[2020-11-01 22:10:19,693] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.029 seconds
[2020-11-01 22:10:31,930] {scheduler_job.py:155} INFO - Started process (PID=21429) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:31,947] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:10:31,949] {logging_mixin.py:112} INFO - [2020-11-01 22:10:31,948] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:32,169] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:10:32,811] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:10:32,842] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:32,887] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:10:32,917] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True>
[2020-11-01 22:10:32,943] {logging_mixin.py:112} INFO - [2020-11-01 22:10:32,942] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 21:03:14.918435+00:00: manual__2020-11-01T21:03:14.918435+00:00, externally triggered: True> failed
[2020-11-01 22:10:32,948] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:10:32,954] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.024 seconds
[2020-11-01 22:10:45,182] {scheduler_job.py:155} INFO - Started process (PID=21489) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:45,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:10:45,187] {logging_mixin.py:112} INFO - [2020-11-01 22:10:45,186] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:45,392] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:10:46,124] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:10:46,154] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:46,207] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:10:46,260] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:10:46,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.089 seconds
[2020-11-01 22:10:58,500] {scheduler_job.py:155} INFO - Started process (PID=21561) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:58,509] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:10:58,510] {logging_mixin.py:112} INFO - [2020-11-01 22:10:58,510] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:58,966] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:10:59,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:10:59,824] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:10:59,864] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:10:59,883] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:10:59,887] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.387 seconds
[2020-11-01 22:11:11,744] {scheduler_job.py:155} INFO - Started process (PID=21622) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:11,747] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:11:11,748] {logging_mixin.py:112} INFO - [2020-11-01 22:11:11,747] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:11,939] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:11:12,508] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:11:12,537] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:12,577] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:11:12,601] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:11:12,609] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.865 seconds
[2020-11-01 22:11:25,093] {scheduler_job.py:155} INFO - Started process (PID=21684) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:25,098] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:11:25,098] {logging_mixin.py:112} INFO - [2020-11-01 22:11:25,098] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:25,643] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:11:26,730] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:11:26,768] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:26,819] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:11:26,857] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:11:26,872] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.780 seconds
[2020-11-01 22:11:38,359] {scheduler_job.py:155} INFO - Started process (PID=21744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:38,364] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:11:38,364] {logging_mixin.py:112} INFO - [2020-11-01 22:11:38,364] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:38,585] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:11:39,256] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:11:39,288] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:39,337] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:11:39,361] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:11:39,365] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.006 seconds
[2020-11-01 22:11:51,634] {scheduler_job.py:155} INFO - Started process (PID=21806) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:51,639] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:11:51,639] {logging_mixin.py:112} INFO - [2020-11-01 22:11:51,639] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:51,851] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:11:52,444] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:11:52,472] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:11:52,517] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:11:52,538] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:11:52,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.908 seconds
[2020-11-01 22:12:04,916] {scheduler_job.py:155} INFO - Started process (PID=21870) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:04,923] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:12:04,924] {logging_mixin.py:112} INFO - [2020-11-01 22:12:04,923] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:05,124] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:12:05,724] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:12:05,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:05,796] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:12:05,820] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:12:05,825] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.908 seconds
[2020-11-01 22:12:18,176] {scheduler_job.py:155} INFO - Started process (PID=21933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:18,183] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:12:18,183] {logging_mixin.py:112} INFO - [2020-11-01 22:12:18,183] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:18,396] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:12:18,961] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:12:18,989] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:19,030] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:12:19,049] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:12:19,053] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.877 seconds
[2020-11-01 22:12:31,425] {scheduler_job.py:155} INFO - Started process (PID=22005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:31,429] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:12:31,430] {logging_mixin.py:112} INFO - [2020-11-01 22:12:31,430] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:31,618] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:12:32,193] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:12:32,226] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:32,271] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:12:32,293] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:12:32,298] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.874 seconds
[2020-11-01 22:12:44,741] {scheduler_job.py:155} INFO - Started process (PID=22069) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:44,746] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:12:44,747] {logging_mixin.py:112} INFO - [2020-11-01 22:12:44,747] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:44,933] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:12:45,501] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:12:45,530] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:45,576] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:12:45,597] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:12:45,602] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.861 seconds
[2020-11-01 22:12:58,008] {scheduler_job.py:155} INFO - Started process (PID=22133) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:58,012] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:12:58,013] {logging_mixin.py:112} INFO - [2020-11-01 22:12:58,013] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:58,211] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:12:58,777] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:12:58,805] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:12:58,846] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:12:58,867] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:12:58,871] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.863 seconds
[2020-11-01 22:13:11,373] {scheduler_job.py:155} INFO - Started process (PID=22196) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:11,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:13:11,377] {logging_mixin.py:112} INFO - [2020-11-01 22:13:11,377] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:11,582] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:13:12,304] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:13:12,337] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:12,385] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:13:12,409] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:13:12,413] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.040 seconds
[2020-11-01 22:13:24,605] {scheduler_job.py:155} INFO - Started process (PID=22258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:24,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:13:24,609] {logging_mixin.py:112} INFO - [2020-11-01 22:13:24,609] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:24,836] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:13:25,482] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:13:25,517] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:25,572] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:13:25,596] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:13:25,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.999 seconds
[2020-11-01 22:13:37,927] {scheduler_job.py:155} INFO - Started process (PID=22319) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:37,936] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:13:37,938] {logging_mixin.py:112} INFO - [2020-11-01 22:13:37,937] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:38,134] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:13:38,887] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:13:38,922] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:39,008] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:13:39,031] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:13:39,035] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.108 seconds
[2020-11-01 22:13:51,206] {scheduler_job.py:155} INFO - Started process (PID=22380) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:51,210] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:13:51,210] {logging_mixin.py:112} INFO - [2020-11-01 22:13:51,210] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:51,438] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:13:52,038] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:13:52,068] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:13:52,107] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:13:52,127] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:13:52,131] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.926 seconds
[2020-11-01 22:14:04,475] {scheduler_job.py:155} INFO - Started process (PID=22441) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:04,480] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:14:04,480] {logging_mixin.py:112} INFO - [2020-11-01 22:14:04,480] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:04,685] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:14:05,292] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:14:05,321] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:05,362] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:14:05,381] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:14:05,385] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.910 seconds
[2020-11-01 22:14:17,772] {scheduler_job.py:155} INFO - Started process (PID=22504) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:17,777] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:14:17,778] {logging_mixin.py:112} INFO - [2020-11-01 22:14:17,777] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:18,061] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:14:18,723] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:14:18,752] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:18,798] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:14:18,822] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:14:18,826] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.055 seconds
[2020-11-01 22:14:31,178] {scheduler_job.py:155} INFO - Started process (PID=22569) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:31,182] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:14:31,183] {logging_mixin.py:112} INFO - [2020-11-01 22:14:31,183] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:31,407] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:14:32,046] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:14:32,077] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:32,122] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:14:32,144] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:14:32,148] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.970 seconds
[2020-11-01 22:14:43,387] {scheduler_job.py:155} INFO - Started process (PID=22625) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:43,392] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:14:43,393] {logging_mixin.py:112} INFO - [2020-11-01 22:14:43,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:43,589] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:14:44,224] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:14:44,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:44,303] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:14:44,325] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:14:44,330] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.943 seconds
[2020-11-01 22:14:56,682] {scheduler_job.py:155} INFO - Started process (PID=22694) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:56,688] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:14:56,688] {logging_mixin.py:112} INFO - [2020-11-01 22:14:56,688] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:56,910] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:14:57,520] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:14:57,549] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:14:57,589] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:14:57,608] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:14:57,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.930 seconds
[2020-11-01 22:15:09,947] {scheduler_job.py:155} INFO - Started process (PID=22768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:09,952] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:15:09,953] {logging_mixin.py:112} INFO - [2020-11-01 22:15:09,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:10,179] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:15:10,859] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:15:10,900] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:10,942] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:15:10,964] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:15:10,968] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.021 seconds
[2020-11-01 22:15:23,315] {scheduler_job.py:155} INFO - Started process (PID=22828) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:23,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:15:23,320] {logging_mixin.py:112} INFO - [2020-11-01 22:15:23,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:23,519] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:15:24,102] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:15:24,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:24,173] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:15:24,198] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:15:24,202] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.887 seconds
[2020-11-01 22:15:36,599] {scheduler_job.py:155} INFO - Started process (PID=22889) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:36,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:15:36,610] {logging_mixin.py:112} INFO - [2020-11-01 22:15:36,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:36,807] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:15:37,440] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:15:37,468] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:37,514] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:15:37,536] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:15:37,540] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.941 seconds
[2020-11-01 22:15:49,882] {scheduler_job.py:155} INFO - Started process (PID=22955) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:49,891] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:15:49,894] {logging_mixin.py:112} INFO - [2020-11-01 22:15:49,894] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:50,164] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:15:50,892] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:15:50,931] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:15:50,976] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:15:50,996] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:15:51,000] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.126 seconds
[2020-11-01 22:16:03,164] {scheduler_job.py:155} INFO - Started process (PID=23019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:03,168] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:16:03,168] {logging_mixin.py:112} INFO - [2020-11-01 22:16:03,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:03,411] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:16:04,012] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:16:04,042] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:04,099] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:16:04,122] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:16:04,126] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.962 seconds
[2020-11-01 22:16:16,470] {scheduler_job.py:155} INFO - Started process (PID=23078) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:16,477] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:16:16,478] {logging_mixin.py:112} INFO - [2020-11-01 22:16:16,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:16,730] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:16:17,436] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:16:17,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:17,519] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:16:17,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:16:17,545] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.075 seconds
[2020-11-01 22:16:29,706] {scheduler_job.py:155} INFO - Started process (PID=23175) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:29,710] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:16:29,710] {logging_mixin.py:112} INFO - [2020-11-01 22:16:29,710] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:29,904] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:16:30,529] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:16:30,560] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:30,601] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:16:30,620] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:16:30,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.919 seconds
[2020-11-01 22:16:42,979] {scheduler_job.py:155} INFO - Started process (PID=23236) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:42,988] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:16:42,990] {logging_mixin.py:112} INFO - [2020-11-01 22:16:42,989] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:43,200] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:16:43,976] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:16:44,007] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:44,051] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:16:44,073] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:16:44,078] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.098 seconds
[2020-11-01 22:16:56,228] {scheduler_job.py:155} INFO - Started process (PID=23307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:56,231] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:16:56,232] {logging_mixin.py:112} INFO - [2020-11-01 22:16:56,231] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:56,424] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:16:57,091] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:16:57,119] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:16:57,160] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:16:57,180] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:16:57,185] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.958 seconds
[2020-11-01 22:17:09,487] {scheduler_job.py:155} INFO - Started process (PID=23374) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:09,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:17:09,494] {logging_mixin.py:112} INFO - [2020-11-01 22:17:09,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:09,717] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:17:10,417] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:17:10,459] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:10,522] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:17:10,548] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:17:10,556] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.069 seconds
[2020-11-01 22:17:22,746] {scheduler_job.py:155} INFO - Started process (PID=23451) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:22,749] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:17:22,750] {logging_mixin.py:112} INFO - [2020-11-01 22:17:22,750] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:22,934] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:17:23,523] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:17:23,552] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:23,593] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:17:23,620] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:17:23,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.879 seconds
[2020-11-01 22:17:36,106] {scheduler_job.py:155} INFO - Started process (PID=23518) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:36,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:17:36,112] {logging_mixin.py:112} INFO - [2020-11-01 22:17:36,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:36,302] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:17:36,899] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:17:36,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:37,000] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:17:37,032] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:17:37,039] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.933 seconds
[2020-11-01 22:17:49,342] {scheduler_job.py:155} INFO - Started process (PID=23582) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:49,347] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:17:49,347] {logging_mixin.py:112} INFO - [2020-11-01 22:17:49,347] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:49,541] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:17:50,131] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:17:50,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:17:50,209] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:17:50,232] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:17:50,237] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.895 seconds
[2020-11-01 22:18:02,591] {scheduler_job.py:155} INFO - Started process (PID=23646) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:02,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:18:02,596] {logging_mixin.py:112} INFO - [2020-11-01 22:18:02,595] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:02,805] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:18:03,437] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:18:03,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:03,522] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:18:03,553] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:18:03,560] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.970 seconds
[2020-11-01 22:18:15,850] {scheduler_job.py:155} INFO - Started process (PID=23706) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:15,855] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:18:15,856] {logging_mixin.py:112} INFO - [2020-11-01 22:18:15,856] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:16,065] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:18:16,759] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:18:16,793] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:16,842] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:18:16,872] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:18:16,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.026 seconds
[2020-11-01 22:18:29,101] {scheduler_job.py:155} INFO - Started process (PID=23768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:29,105] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:18:29,106] {logging_mixin.py:112} INFO - [2020-11-01 22:18:29,106] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:29,328] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:18:30,019] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:18:30,050] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:30,091] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:18:30,115] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:18:30,119] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.018 seconds
[2020-11-01 22:18:42,413] {scheduler_job.py:155} INFO - Started process (PID=23827) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:42,417] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:18:42,418] {logging_mixin.py:112} INFO - [2020-11-01 22:18:42,418] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:42,640] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:18:43,323] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:18:43,360] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:43,442] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:18:43,465] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:18:43,474] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.061 seconds
[2020-11-01 22:18:55,657] {scheduler_job.py:155} INFO - Started process (PID=23889) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:55,661] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:18:55,661] {logging_mixin.py:112} INFO - [2020-11-01 22:18:55,661] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:55,844] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:18:56,423] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:18:56,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:18:56,494] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:18:56,514] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:18:56,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.861 seconds
[2020-11-01 22:19:09,039] {scheduler_job.py:155} INFO - Started process (PID=23962) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:09,043] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:19:09,044] {logging_mixin.py:112} INFO - [2020-11-01 22:19:09,043] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:09,226] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:19:09,785] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:19:09,813] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:09,851] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:19:09,870] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:19:09,875] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.836 seconds
[2020-11-01 22:19:22,229] {scheduler_job.py:155} INFO - Started process (PID=24025) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:22,234] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:19:22,235] {logging_mixin.py:112} INFO - [2020-11-01 22:19:22,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:22,448] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:19:23,090] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:19:23,121] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:23,168] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:19:23,191] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:19:23,195] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.967 seconds
[2020-11-01 22:19:35,448] {scheduler_job.py:155} INFO - Started process (PID=24089) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:35,453] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:19:35,454] {logging_mixin.py:112} INFO - [2020-11-01 22:19:35,454] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:35,667] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:19:36,270] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:19:36,300] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:36,344] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:19:36,366] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:19:36,370] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.922 seconds
[2020-11-01 22:19:48,915] {scheduler_job.py:155} INFO - Started process (PID=24153) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:48,923] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:19:48,926] {logging_mixin.py:112} INFO - [2020-11-01 22:19:48,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:49,200] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:19:49,806] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:19:49,838] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:19:49,884] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:19:49,908] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:19:49,914] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.999 seconds
[2020-11-01 22:20:02,220] {scheduler_job.py:155} INFO - Started process (PID=24222) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:02,226] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:20:02,227] {logging_mixin.py:112} INFO - [2020-11-01 22:20:02,226] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:02,427] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:20:03,094] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:20:03,122] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:03,163] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:20:03,191] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:20:03,196] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.976 seconds
[2020-11-01 22:20:15,472] {scheduler_job.py:155} INFO - Started process (PID=24283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:15,476] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:20:15,477] {logging_mixin.py:112} INFO - [2020-11-01 22:20:15,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:15,669] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:20:16,257] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:20:16,284] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:16,324] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:20:16,344] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:20:16,348] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.876 seconds
[2020-11-01 22:20:28,741] {scheduler_job.py:155} INFO - Started process (PID=24347) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:28,745] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:20:28,746] {logging_mixin.py:112} INFO - [2020-11-01 22:20:28,746] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:28,924] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:20:29,488] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:20:29,516] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:29,557] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:20:29,576] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:20:29,580] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.839 seconds
[2020-11-01 22:20:42,085] {scheduler_job.py:155} INFO - Started process (PID=24410) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:42,093] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:20:42,094] {logging_mixin.py:112} INFO - [2020-11-01 22:20:42,093] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:42,293] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:20:42,845] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:20:42,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:42,914] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:20:42,933] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:20:42,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.852 seconds
[2020-11-01 22:20:55,361] {scheduler_job.py:155} INFO - Started process (PID=24478) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:55,368] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:20:55,370] {logging_mixin.py:112} INFO - [2020-11-01 22:20:55,370] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:55,631] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:20:56,228] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:20:56,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:20:56,301] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:20:56,320] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:20:56,324] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.963 seconds
[2020-11-01 22:21:08,645] {scheduler_job.py:155} INFO - Started process (PID=24543) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:08,649] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:21:08,650] {logging_mixin.py:112} INFO - [2020-11-01 22:21:08,650] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:08,835] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:21:09,549] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:21:09,580] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:09,622] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:21:09,642] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:21:09,649] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.004 seconds
[2020-11-01 22:21:21,942] {scheduler_job.py:155} INFO - Started process (PID=24607) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:21,949] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:21:21,950] {logging_mixin.py:112} INFO - [2020-11-01 22:21:21,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:22,159] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:21:22,769] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:21:22,802] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:22,847] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:21:22,869] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:21:22,874] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.932 seconds
[2020-11-01 22:21:35,196] {scheduler_job.py:155} INFO - Started process (PID=24671) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:35,202] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:21:35,203] {logging_mixin.py:112} INFO - [2020-11-01 22:21:35,202] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:35,425] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:21:36,088] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:21:36,120] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:36,160] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:21:36,186] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:21:36,192] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.995 seconds
[2020-11-01 22:21:48,494] {scheduler_job.py:155} INFO - Started process (PID=24734) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:48,498] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:21:48,502] {logging_mixin.py:112} INFO - [2020-11-01 22:21:48,502] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:48,726] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:21:49,426] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:21:49,455] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:21:49,519] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:21:49,567] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:21:49,572] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.077 seconds
[2020-11-01 22:22:01,765] {scheduler_job.py:155} INFO - Started process (PID=24802) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:01,771] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:22:01,772] {logging_mixin.py:112} INFO - [2020-11-01 22:22:01,772] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:01,955] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:22:02,525] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:22:02,552] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:02,592] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:22:02,611] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:22:02,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.851 seconds
[2020-11-01 22:22:15,212] {scheduler_job.py:155} INFO - Started process (PID=24869) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:15,219] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:22:15,220] {logging_mixin.py:112} INFO - [2020-11-01 22:22:15,219] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:15,432] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:22:16,019] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:22:16,048] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:16,088] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:22:16,107] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:22:16,112] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.899 seconds
[2020-11-01 22:22:28,460] {scheduler_job.py:155} INFO - Started process (PID=24934) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:28,469] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:22:28,470] {logging_mixin.py:112} INFO - [2020-11-01 22:22:28,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:28,653] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:22:29,207] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:22:29,234] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:29,276] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:22:29,295] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:22:29,299] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.839 seconds
[2020-11-01 22:22:41,720] {scheduler_job.py:155} INFO - Started process (PID=24997) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:41,731] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:22:41,732] {logging_mixin.py:112} INFO - [2020-11-01 22:22:41,732] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:41,934] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:22:42,503] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:22:42,531] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:42,572] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:22:42,590] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:22:42,595] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.875 seconds
[2020-11-01 22:22:55,080] {scheduler_job.py:155} INFO - Started process (PID=25066) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:55,085] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:22:55,086] {logging_mixin.py:112} INFO - [2020-11-01 22:22:55,086] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:55,283] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:22:55,854] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:22:55,882] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:22:55,923] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:22:55,942] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:22:55,947] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 22:23:08,319] {scheduler_job.py:155} INFO - Started process (PID=25130) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:08,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:23:08,327] {logging_mixin.py:112} INFO - [2020-11-01 22:23:08,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:08,529] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:23:09,092] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:23:09,121] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:09,162] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:23:09,183] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:23:09,187] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.868 seconds
[2020-11-01 22:23:21,635] {scheduler_job.py:155} INFO - Started process (PID=25194) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:21,639] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:23:21,640] {logging_mixin.py:112} INFO - [2020-11-01 22:23:21,640] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:21,853] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:23:22,481] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:23:22,512] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:22,552] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:23:22,571] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:23:22,575] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.940 seconds
[2020-11-01 22:23:34,944] {scheduler_job.py:155} INFO - Started process (PID=25258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:34,949] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:23:34,950] {logging_mixin.py:112} INFO - [2020-11-01 22:23:34,949] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:35,137] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:23:35,751] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:23:35,779] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:35,815] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:23:35,834] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:23:35,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.894 seconds
[2020-11-01 22:23:48,261] {scheduler_job.py:155} INFO - Started process (PID=25322) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:48,265] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:23:48,265] {logging_mixin.py:112} INFO - [2020-11-01 22:23:48,265] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:48,448] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:23:49,011] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:23:49,040] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:23:49,082] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:23:49,101] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:23:49,105] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.844 seconds
[2020-11-01 22:24:01,558] {scheduler_job.py:155} INFO - Started process (PID=25387) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:01,565] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:24:01,566] {logging_mixin.py:112} INFO - [2020-11-01 22:24:01,566] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:01,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:24:02,404] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:24:02,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:02,486] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:24:02,507] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:24:02,512] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.954 seconds
[2020-11-01 22:24:14,866] {scheduler_job.py:155} INFO - Started process (PID=25446) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:14,871] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:24:14,872] {logging_mixin.py:112} INFO - [2020-11-01 22:24:14,871] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:15,066] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:24:15,701] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:24:15,731] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:15,775] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:24:15,797] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:24:15,801] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.935 seconds
[2020-11-01 22:24:28,156] {scheduler_job.py:155} INFO - Started process (PID=25516) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:28,161] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:24:28,162] {logging_mixin.py:112} INFO - [2020-11-01 22:24:28,162] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:28,452] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:24:29,217] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:24:29,251] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:29,288] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:24:29,306] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:24:29,309] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-01 22:24:41,411] {scheduler_job.py:155} INFO - Started process (PID=25575) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:41,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:24:41,418] {logging_mixin.py:112} INFO - [2020-11-01 22:24:41,418] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:41,624] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:24:42,273] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:24:42,308] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:42,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:24:42,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:24:42,388] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.977 seconds
[2020-11-01 22:24:54,711] {scheduler_job.py:155} INFO - Started process (PID=25643) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:54,716] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:24:54,717] {logging_mixin.py:112} INFO - [2020-11-01 22:24:54,717] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:54,912] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:24:55,642] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:24:55,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:24:55,710] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:24:55,740] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:24:55,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.033 seconds
[2020-11-01 22:25:08,012] {scheduler_job.py:155} INFO - Started process (PID=25705) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:08,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:25:08,018] {logging_mixin.py:112} INFO - [2020-11-01 22:25:08,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:08,216] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:25:08,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:25:08,827] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:08,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:25:08,904] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:25:08,909] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.897 seconds
[2020-11-01 22:25:21,312] {scheduler_job.py:155} INFO - Started process (PID=25768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:21,318] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:25:21,319] {logging_mixin.py:112} INFO - [2020-11-01 22:25:21,318] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:21,528] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:25:22,260] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:25:22,290] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:22,335] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:25:22,356] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:25:22,361] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.049 seconds
[2020-11-01 22:25:34,655] {scheduler_job.py:155} INFO - Started process (PID=25828) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:34,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:25:34,660] {logging_mixin.py:112} INFO - [2020-11-01 22:25:34,660] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:34,854] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:25:35,445] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:25:35,473] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:35,513] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:25:35,535] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:25:35,539] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.884 seconds
[2020-11-01 22:25:47,940] {scheduler_job.py:155} INFO - Started process (PID=25890) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:47,945] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:25:47,946] {logging_mixin.py:112} INFO - [2020-11-01 22:25:47,946] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:48,129] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:25:48,860] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:25:48,902] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:25:48,949] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:25:48,973] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:25:48,979] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.040 seconds
[2020-11-01 22:26:01,272] {scheduler_job.py:155} INFO - Started process (PID=25956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:01,276] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:26:01,277] {logging_mixin.py:112} INFO - [2020-11-01 22:26:01,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:01,470] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:26:02,101] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:26:02,131] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:02,175] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:26:02,206] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:26:02,210] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.938 seconds
[2020-11-01 22:26:14,524] {scheduler_job.py:155} INFO - Started process (PID=26017) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:14,531] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:26:14,532] {logging_mixin.py:112} INFO - [2020-11-01 22:26:14,532] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:14,736] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:26:15,381] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:26:15,418] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:15,480] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:26:15,506] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:26:15,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.991 seconds
[2020-11-01 22:26:27,852] {scheduler_job.py:155} INFO - Started process (PID=26081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:27,859] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:26:27,860] {logging_mixin.py:112} INFO - [2020-11-01 22:26:27,860] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:28,055] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:26:28,661] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:26:28,692] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:28,736] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:26:28,754] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:26:28,758] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.906 seconds
[2020-11-01 22:26:41,161] {scheduler_job.py:155} INFO - Started process (PID=26143) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:41,165] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:26:41,166] {logging_mixin.py:112} INFO - [2020-11-01 22:26:41,165] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:41,369] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:26:41,976] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:26:42,004] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:42,046] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:26:42,065] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:26:42,070] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.909 seconds
[2020-11-01 22:26:54,457] {scheduler_job.py:155} INFO - Started process (PID=26208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:54,461] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:26:54,462] {logging_mixin.py:112} INFO - [2020-11-01 22:26:54,462] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:54,666] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:26:55,260] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:26:55,288] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:26:55,329] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:26:55,348] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:26:55,352] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.895 seconds
[2020-11-01 22:27:07,790] {scheduler_job.py:155} INFO - Started process (PID=26271) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:07,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:27:07,794] {logging_mixin.py:112} INFO - [2020-11-01 22:27:07,794] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:07,991] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:27:08,604] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:27:08,632] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:08,683] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:27:08,703] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:27:08,718] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.928 seconds
[2020-11-01 22:27:21,093] {scheduler_job.py:155} INFO - Started process (PID=26334) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:21,097] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:27:21,098] {logging_mixin.py:112} INFO - [2020-11-01 22:27:21,098] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:21,289] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:27:21,883] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:27:21,911] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:21,950] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:27:21,970] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:27:21,974] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.881 seconds
[2020-11-01 22:27:34,368] {scheduler_job.py:155} INFO - Started process (PID=26399) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:34,373] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:27:34,374] {logging_mixin.py:112} INFO - [2020-11-01 22:27:34,373] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:34,572] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:27:35,183] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:27:35,221] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:35,284] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:27:35,316] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:27:35,321] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.953 seconds
[2020-11-01 22:27:47,661] {scheduler_job.py:155} INFO - Started process (PID=26461) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:47,665] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:27:47,666] {logging_mixin.py:112} INFO - [2020-11-01 22:27:47,666] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:47,857] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:27:48,444] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:27:48,477] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:27:48,519] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:27:48,541] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:27:48,545] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.884 seconds
[2020-11-01 22:28:00,994] {scheduler_job.py:155} INFO - Started process (PID=26524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:00,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:28:00,998] {logging_mixin.py:112} INFO - [2020-11-01 22:28:00,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:01,204] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:28:01,784] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:28:01,814] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:01,857] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:28:01,876] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:28:01,881] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.887 seconds
[2020-11-01 22:28:14,232] {scheduler_job.py:155} INFO - Started process (PID=26586) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:14,237] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:28:14,237] {logging_mixin.py:112} INFO - [2020-11-01 22:28:14,237] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:14,443] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:28:15,032] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:28:15,060] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:15,096] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:28:15,113] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:28:15,117] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.885 seconds
[2020-11-01 22:28:27,538] {scheduler_job.py:155} INFO - Started process (PID=26655) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:27,542] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:28:27,543] {logging_mixin.py:112} INFO - [2020-11-01 22:28:27,543] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:27,726] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:28:28,335] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:28:28,364] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:28,407] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:28:28,427] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:28:28,431] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.894 seconds
[2020-11-01 22:28:40,877] {scheduler_job.py:155} INFO - Started process (PID=26718) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:40,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:28:40,883] {logging_mixin.py:112} INFO - [2020-11-01 22:28:40,883] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:41,060] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:28:41,657] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:28:41,685] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:41,754] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:28:41,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:28:41,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.906 seconds
[2020-11-01 22:28:54,344] {scheduler_job.py:155} INFO - Started process (PID=26781) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:54,347] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:28:54,348] {logging_mixin.py:112} INFO - [2020-11-01 22:28:54,348] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:54,545] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:28:55,173] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:28:55,215] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:28:55,277] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:28:55,308] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:28:55,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.971 seconds
[2020-11-01 22:29:07,601] {scheduler_job.py:155} INFO - Started process (PID=26845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:07,608] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:29:07,609] {logging_mixin.py:112} INFO - [2020-11-01 22:29:07,608] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:07,818] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:29:08,439] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:29:08,467] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:08,508] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:29:08,527] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:29:08,531] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.930 seconds
[2020-11-01 22:29:20,857] {scheduler_job.py:155} INFO - Started process (PID=26909) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:20,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:29:20,866] {logging_mixin.py:112} INFO - [2020-11-01 22:29:20,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:21,067] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:29:21,657] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:29:21,684] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:21,724] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:29:21,744] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:29:21,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.892 seconds
[2020-11-01 22:29:34,373] {scheduler_job.py:155} INFO - Started process (PID=26974) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:34,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:29:34,378] {logging_mixin.py:112} INFO - [2020-11-01 22:29:34,378] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:34,616] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:29:35,209] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:29:35,245] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:35,297] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:29:35,318] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:29:35,323] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.950 seconds
[2020-11-01 22:29:47,706] {scheduler_job.py:155} INFO - Started process (PID=27042) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:47,711] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:29:47,712] {logging_mixin.py:112} INFO - [2020-11-01 22:29:47,711] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:47,921] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:29:48,588] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:29:48,630] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:29:48,689] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:29:48,732] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:29:48,738] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.033 seconds
[2020-11-01 22:30:01,025] {scheduler_job.py:155} INFO - Started process (PID=27104) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:01,030] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:30:01,031] {logging_mixin.py:112} INFO - [2020-11-01 22:30:01,031] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:01,244] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:30:01,828] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:30:01,857] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:01,899] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:30:01,920] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:30:01,924] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.898 seconds
[2020-11-01 22:30:14,380] {scheduler_job.py:155} INFO - Started process (PID=27167) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:14,385] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:30:14,386] {logging_mixin.py:112} INFO - [2020-11-01 22:30:14,386] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:14,567] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:30:15,218] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:30:15,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:15,302] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:30:15,319] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:30:15,323] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.943 seconds
[2020-11-01 22:30:27,839] {scheduler_job.py:155} INFO - Started process (PID=27229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:27,844] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:30:27,845] {logging_mixin.py:112} INFO - [2020-11-01 22:30:27,845] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:28,028] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:30:28,601] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:30:28,629] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:28,669] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:30:28,688] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:30:28,692] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.853 seconds
[2020-11-01 22:30:41,174] {scheduler_job.py:155} INFO - Started process (PID=27292) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:41,184] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:30:41,185] {logging_mixin.py:112} INFO - [2020-11-01 22:30:41,185] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:41,387] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:30:42,134] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:30:42,163] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:42,209] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:30:42,229] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:30:42,234] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.060 seconds
[2020-11-01 22:30:54,599] {scheduler_job.py:155} INFO - Started process (PID=27354) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:54,606] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:30:54,607] {logging_mixin.py:112} INFO - [2020-11-01 22:30:54,606] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:54,784] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:30:55,333] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:30:55,362] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:30:55,401] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:30:55,421] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:30:55,425] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.826 seconds
[2020-11-01 22:31:08,038] {scheduler_job.py:155} INFO - Started process (PID=27420) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:08,044] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:31:08,045] {logging_mixin.py:112} INFO - [2020-11-01 22:31:08,045] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:08,230] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:31:08,925] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:31:08,953] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:08,994] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:31:09,014] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:31:09,018] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.981 seconds
[2020-11-01 22:31:21,347] {scheduler_job.py:155} INFO - Started process (PID=27485) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:21,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:31:21,358] {logging_mixin.py:112} INFO - [2020-11-01 22:31:21,358] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:21,536] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:31:22,092] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:31:22,120] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:22,161] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:31:22,182] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:31:22,186] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.840 seconds
[2020-11-01 22:31:34,809] {scheduler_job.py:155} INFO - Started process (PID=27547) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:34,823] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:31:34,824] {logging_mixin.py:112} INFO - [2020-11-01 22:31:34,824] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:35,111] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:31:35,702] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:31:35,730] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:35,781] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:31:35,811] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:31:35,815] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.007 seconds
[2020-11-01 22:31:48,171] {scheduler_job.py:155} INFO - Started process (PID=27611) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:48,176] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:31:48,177] {logging_mixin.py:112} INFO - [2020-11-01 22:31:48,177] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:48,353] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:31:48,908] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:31:48,937] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:31:48,979] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:31:48,998] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:31:49,003] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.832 seconds
[2020-11-01 22:32:01,566] {scheduler_job.py:155} INFO - Started process (PID=27676) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:01,571] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:32:01,572] {logging_mixin.py:112} INFO - [2020-11-01 22:32:01,571] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:01,798] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:32:02,392] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:32:02,420] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:02,460] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:32:02,479] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:32:02,484] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.918 seconds
[2020-11-01 22:32:14,915] {scheduler_job.py:155} INFO - Started process (PID=27740) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:14,922] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:32:14,924] {logging_mixin.py:112} INFO - [2020-11-01 22:32:14,923] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:15,116] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:32:15,702] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:32:15,730] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:15,769] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:32:15,788] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:32:15,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.877 seconds
[2020-11-01 22:32:28,273] {scheduler_job.py:155} INFO - Started process (PID=27804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:28,277] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:32:28,278] {logging_mixin.py:112} INFO - [2020-11-01 22:32:28,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:28,482] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:32:29,047] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:32:29,075] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:29,119] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:32:29,141] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:32:29,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.872 seconds
[2020-11-01 22:32:41,652] {scheduler_job.py:155} INFO - Started process (PID=27868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:41,661] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:32:41,662] {logging_mixin.py:112} INFO - [2020-11-01 22:32:41,662] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:41,854] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:32:42,435] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:32:42,463] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:42,504] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:32:42,524] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:32:42,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.877 seconds
[2020-11-01 22:32:55,106] {scheduler_job.py:155} INFO - Started process (PID=27934) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:55,110] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:32:55,111] {logging_mixin.py:112} INFO - [2020-11-01 22:32:55,110] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:55,302] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:32:55,867] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:32:55,894] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:32:55,936] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:32:55,955] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:32:55,959] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.853 seconds
[2020-11-01 22:33:08,415] {scheduler_job.py:155} INFO - Started process (PID=27997) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:08,421] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:33:08,422] {logging_mixin.py:112} INFO - [2020-11-01 22:33:08,422] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:08,605] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:33:09,165] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:33:09,198] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:09,243] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:33:09,263] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:33:09,267] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.852 seconds
[2020-11-01 22:33:21,808] {scheduler_job.py:155} INFO - Started process (PID=28062) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:21,816] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:33:21,817] {logging_mixin.py:112} INFO - [2020-11-01 22:33:21,817] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:22,007] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:33:22,592] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:33:22,620] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:22,663] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:33:22,682] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:33:22,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.878 seconds
[2020-11-01 22:33:35,175] {scheduler_job.py:155} INFO - Started process (PID=28126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:35,183] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:33:35,184] {logging_mixin.py:112} INFO - [2020-11-01 22:33:35,184] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:35,372] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:33:35,932] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:33:35,960] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:36,002] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:33:36,022] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:33:36,026] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.851 seconds
[2020-11-01 22:33:48,393] {scheduler_job.py:155} INFO - Started process (PID=28189) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:48,402] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:33:48,403] {logging_mixin.py:112} INFO - [2020-11-01 22:33:48,403] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:48,588] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:33:49,144] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:33:49,181] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:33:49,232] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:33:49,259] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:33:49,265] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.872 seconds
[2020-11-01 22:34:01,654] {scheduler_job.py:155} INFO - Started process (PID=28254) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:01,660] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:34:01,661] {logging_mixin.py:112} INFO - [2020-11-01 22:34:01,661] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:01,844] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:34:02,481] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:34:02,509] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:02,550] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:34:02,569] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:34:02,574] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.920 seconds
[2020-11-01 22:34:14,976] {scheduler_job.py:155} INFO - Started process (PID=28317) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:14,981] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:34:14,982] {logging_mixin.py:112} INFO - [2020-11-01 22:34:14,982] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:15,173] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:34:15,737] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:34:15,765] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:15,808] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:34:15,828] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:34:15,832] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.856 seconds
[2020-11-01 22:34:28,415] {scheduler_job.py:155} INFO - Started process (PID=28385) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:28,426] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:34:28,427] {logging_mixin.py:112} INFO - [2020-11-01 22:34:28,427] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:28,622] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:34:29,168] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:34:29,199] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:29,243] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:34:29,262] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:34:29,266] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.851 seconds
[2020-11-01 22:34:41,828] {scheduler_job.py:155} INFO - Started process (PID=28448) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:41,836] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:34:41,837] {logging_mixin.py:112} INFO - [2020-11-01 22:34:41,837] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:42,027] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:34:42,618] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:34:42,646] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:42,689] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:34:42,709] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:34:42,714] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.886 seconds
[2020-11-01 22:34:55,291] {scheduler_job.py:155} INFO - Started process (PID=28514) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:55,296] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:34:55,297] {logging_mixin.py:112} INFO - [2020-11-01 22:34:55,296] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:55,479] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:34:56,055] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:34:56,083] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:34:56,124] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:34:56,143] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:34:56,147] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.857 seconds
[2020-11-01 22:35:08,743] {scheduler_job.py:155} INFO - Started process (PID=28582) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:08,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:35:08,753] {logging_mixin.py:112} INFO - [2020-11-01 22:35:08,752] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:08,946] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:35:09,528] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:35:09,557] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:09,597] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:35:09,619] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:35:09,622] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.879 seconds
[2020-11-01 22:35:22,116] {scheduler_job.py:155} INFO - Started process (PID=28647) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:22,120] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:35:22,121] {logging_mixin.py:112} INFO - [2020-11-01 22:35:22,120] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:22,325] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:35:22,899] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:35:22,926] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:22,969] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:35:22,988] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:35:22,992] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.876 seconds
[2020-11-01 22:35:35,404] {scheduler_job.py:155} INFO - Started process (PID=28711) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:35,409] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:35:35,409] {logging_mixin.py:112} INFO - [2020-11-01 22:35:35,409] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:35,590] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:35:36,158] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:35:36,189] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:36,231] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:35:36,250] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:35:36,254] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.850 seconds
[2020-11-01 22:35:48,823] {scheduler_job.py:155} INFO - Started process (PID=28775) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:48,833] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:35:48,834] {logging_mixin.py:112} INFO - [2020-11-01 22:35:48,834] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:49,013] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:35:49,613] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:35:49,650] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:35:49,711] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:35:49,746] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:35:49,752] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.929 seconds
[2020-11-01 22:36:02,263] {scheduler_job.py:155} INFO - Started process (PID=28844) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:02,268] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:36:02,269] {logging_mixin.py:112} INFO - [2020-11-01 22:36:02,269] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:02,459] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:36:03,040] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:36:03,068] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:03,110] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:36:03,131] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:36:03,135] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.872 seconds
[2020-11-01 22:36:15,598] {scheduler_job.py:155} INFO - Started process (PID=28912) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:15,602] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:36:15,603] {logging_mixin.py:112} INFO - [2020-11-01 22:36:15,602] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:15,780] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:36:16,442] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:36:16,483] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:16,545] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:36:16,575] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:36:16,581] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.984 seconds
[2020-11-01 22:36:29,054] {scheduler_job.py:155} INFO - Started process (PID=28974) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:29,061] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:36:29,062] {logging_mixin.py:112} INFO - [2020-11-01 22:36:29,061] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:29,247] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:36:29,804] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:36:29,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:29,881] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:36:29,901] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:36:29,907] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.853 seconds
[2020-11-01 22:36:42,493] {scheduler_job.py:155} INFO - Started process (PID=29037) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:42,502] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:36:42,506] {logging_mixin.py:112} INFO - [2020-11-01 22:36:42,505] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:42,697] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:36:43,437] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:36:43,465] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:43,507] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:36:43,527] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:36:43,532] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.039 seconds
[2020-11-01 22:36:55,838] {scheduler_job.py:155} INFO - Started process (PID=29099) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:55,841] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:36:55,842] {logging_mixin.py:112} INFO - [2020-11-01 22:36:55,842] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:56,026] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:36:56,589] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:36:56,617] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:36:56,665] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:36:56,686] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:36:56,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.852 seconds
[2020-11-01 22:37:09,313] {scheduler_job.py:155} INFO - Started process (PID=29158) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:09,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:37:09,323] {logging_mixin.py:112} INFO - [2020-11-01 22:37:09,322] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:09,570] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:37:10,212] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:37:10,243] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:10,284] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:37:10,304] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:37:10,307] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.995 seconds
[2020-11-01 22:37:22,780] {scheduler_job.py:155} INFO - Started process (PID=29224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:22,788] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:37:22,789] {logging_mixin.py:112} INFO - [2020-11-01 22:37:22,789] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:22,982] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:37:23,557] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:37:23,589] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:23,661] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:37:23,686] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:37:23,691] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.911 seconds
[2020-11-01 22:37:36,258] {scheduler_job.py:155} INFO - Started process (PID=29286) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:36,262] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:37:36,263] {logging_mixin.py:112} INFO - [2020-11-01 22:37:36,263] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:36,455] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:37:37,010] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:37:37,041] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:37,082] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:37:37,103] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:37:37,107] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.849 seconds
[2020-11-01 22:37:49,534] {scheduler_job.py:155} INFO - Started process (PID=29351) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:49,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:37:49,540] {logging_mixin.py:112} INFO - [2020-11-01 22:37:49,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:49,736] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:37:50,304] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:37:50,334] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:37:50,377] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:37:50,396] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:37:50,401] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 22:38:02,977] {scheduler_job.py:155} INFO - Started process (PID=29416) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:02,985] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:38:02,986] {logging_mixin.py:112} INFO - [2020-11-01 22:38:02,986] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:03,179] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:38:03,775] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:38:03,803] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:03,844] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:38:03,864] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:38:03,868] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.891 seconds
[2020-11-01 22:38:16,298] {scheduler_job.py:155} INFO - Started process (PID=29481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:16,304] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:38:16,304] {logging_mixin.py:112} INFO - [2020-11-01 22:38:16,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:16,484] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:38:17,044] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:38:17,072] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:17,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:38:17,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:38:17,144] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.846 seconds
[2020-11-01 22:38:29,675] {scheduler_job.py:155} INFO - Started process (PID=29545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:29,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:38:29,684] {logging_mixin.py:112} INFO - [2020-11-01 22:38:29,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:29,882] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:38:30,466] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:38:30,494] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:30,548] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:38:30,568] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:38:30,572] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.898 seconds
[2020-11-01 22:38:43,099] {scheduler_job.py:155} INFO - Started process (PID=29608) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:43,109] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:38:43,111] {logging_mixin.py:112} INFO - [2020-11-01 22:38:43,111] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:43,295] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:38:43,861] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:38:43,889] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:43,932] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:38:43,951] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:38:43,956] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.857 seconds
[2020-11-01 22:38:56,490] {scheduler_job.py:155} INFO - Started process (PID=29673) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:56,498] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:38:56,499] {logging_mixin.py:112} INFO - [2020-11-01 22:38:56,499] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:56,687] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:38:57,249] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:38:57,278] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:38:57,324] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:38:57,344] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:38:57,348] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.859 seconds
[2020-11-01 22:39:09,936] {scheduler_job.py:155} INFO - Started process (PID=29739) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:09,943] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:39:09,944] {logging_mixin.py:112} INFO - [2020-11-01 22:39:09,944] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:10,143] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:39:10,698] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:39:10,726] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:10,769] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:39:10,788] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:39:10,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.857 seconds
[2020-11-01 22:39:23,293] {scheduler_job.py:155} INFO - Started process (PID=29802) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:23,297] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:39:23,297] {logging_mixin.py:112} INFO - [2020-11-01 22:39:23,297] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:23,483] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:39:24,046] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:39:24,078] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:24,119] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:39:24,155] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:39:24,160] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 22:39:36,618] {scheduler_job.py:155} INFO - Started process (PID=29865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:36,627] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:39:36,628] {logging_mixin.py:112} INFO - [2020-11-01 22:39:36,628] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:36,802] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:39:37,361] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:39:37,389] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:37,430] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:39:37,451] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:39:37,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.836 seconds
[2020-11-01 22:39:50,150] {scheduler_job.py:155} INFO - Started process (PID=29933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:50,159] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:39:50,160] {logging_mixin.py:112} INFO - [2020-11-01 22:39:50,160] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:50,340] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:39:50,900] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:39:50,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:39:50,971] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:39:50,990] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:39:50,994] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.844 seconds
[2020-11-01 22:40:03,527] {scheduler_job.py:155} INFO - Started process (PID=29999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:03,531] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:40:03,532] {logging_mixin.py:112} INFO - [2020-11-01 22:40:03,532] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:03,716] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:40:04,288] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:40:04,316] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:04,375] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:40:04,407] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:40:04,414] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.887 seconds
[2020-11-01 22:40:16,955] {scheduler_job.py:155} INFO - Started process (PID=30061) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:16,960] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:40:16,960] {logging_mixin.py:112} INFO - [2020-11-01 22:40:16,960] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:17,146] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:40:17,745] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:40:17,773] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:17,816] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:40:17,835] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:40:17,839] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.885 seconds
[2020-11-01 22:40:30,317] {scheduler_job.py:155} INFO - Started process (PID=30126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:30,320] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:40:30,321] {logging_mixin.py:112} INFO - [2020-11-01 22:40:30,321] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:30,497] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:40:31,166] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:40:31,206] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:31,257] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:40:31,277] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:40:31,282] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.964 seconds
[2020-11-01 22:40:43,654] {scheduler_job.py:155} INFO - Started process (PID=30188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:43,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:40:43,660] {logging_mixin.py:112} INFO - [2020-11-01 22:40:43,660] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:43,834] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:40:44,394] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:40:44,422] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:44,466] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:40:44,484] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:40:44,488] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.834 seconds
[2020-11-01 22:40:57,085] {scheduler_job.py:155} INFO - Started process (PID=30253) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:57,092] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:40:57,093] {logging_mixin.py:112} INFO - [2020-11-01 22:40:57,093] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:57,267] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:40:57,973] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:40:58,002] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:40:58,052] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:40:58,072] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:40:58,077] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.991 seconds
[2020-11-01 22:41:10,414] {scheduler_job.py:155} INFO - Started process (PID=30323) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:10,417] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:41:10,418] {logging_mixin.py:112} INFO - [2020-11-01 22:41:10,418] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:10,597] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:41:11,174] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:41:11,202] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:11,247] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:41:11,266] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:41:11,270] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.856 seconds
[2020-11-01 22:41:23,793] {scheduler_job.py:155} INFO - Started process (PID=30386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:23,800] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:41:23,801] {logging_mixin.py:112} INFO - [2020-11-01 22:41:23,801] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:23,988] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:41:24,674] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:41:24,702] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:24,749] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:41:24,768] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:41:24,772] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.979 seconds
[2020-11-01 22:41:37,267] {scheduler_job.py:155} INFO - Started process (PID=30448) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:37,271] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:41:37,272] {logging_mixin.py:112} INFO - [2020-11-01 22:41:37,272] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:37,452] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:41:38,013] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:41:38,041] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:38,082] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:41:38,101] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:41:38,105] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.838 seconds
[2020-11-01 22:41:50,651] {scheduler_job.py:155} INFO - Started process (PID=30513) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:50,656] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:41:50,657] {logging_mixin.py:112} INFO - [2020-11-01 22:41:50,657] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:50,922] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:41:51,478] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:41:51,506] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:41:51,546] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:41:51,565] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:41:51,569] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.919 seconds
[2020-11-01 22:42:04,168] {scheduler_job.py:155} INFO - Started process (PID=30579) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:04,176] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:42:04,176] {logging_mixin.py:112} INFO - [2020-11-01 22:42:04,176] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:04,366] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:42:04,954] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:42:04,981] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:05,020] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:42:05,038] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:42:05,042] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.874 seconds
[2020-11-01 22:42:17,500] {scheduler_job.py:155} INFO - Started process (PID=30644) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:17,504] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:42:17,504] {logging_mixin.py:112} INFO - [2020-11-01 22:42:17,504] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:17,692] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:42:18,275] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:42:18,305] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:18,348] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:42:18,367] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:42:18,371] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.871 seconds
[2020-11-01 22:42:31,085] {scheduler_job.py:155} INFO - Started process (PID=30711) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:31,089] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:42:31,090] {logging_mixin.py:112} INFO - [2020-11-01 22:42:31,090] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:31,279] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:42:31,861] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:42:31,891] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:31,934] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:42:31,954] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:42:31,958] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.873 seconds
[2020-11-01 22:42:44,481] {scheduler_job.py:155} INFO - Started process (PID=30773) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:44,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:42:44,492] {logging_mixin.py:112} INFO - [2020-11-01 22:42:44,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:44,677] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:42:45,249] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:42:45,277] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:45,320] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:42:45,339] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:42:45,344] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.863 seconds
[2020-11-01 22:42:57,878] {scheduler_job.py:155} INFO - Started process (PID=30836) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:57,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:42:57,883] {logging_mixin.py:112} INFO - [2020-11-01 22:42:57,883] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:58,070] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:42:58,657] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:42:58,686] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:42:58,727] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:42:58,745] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:42:58,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.872 seconds
[2020-11-01 22:43:11,271] {scheduler_job.py:155} INFO - Started process (PID=30900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:11,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:43:11,275] {logging_mixin.py:112} INFO - [2020-11-01 22:43:11,275] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:11,451] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:43:12,018] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:43:12,047] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:12,088] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:43:12,107] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:43:12,111] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.840 seconds
[2020-11-01 22:43:24,842] {scheduler_job.py:155} INFO - Started process (PID=30967) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:24,848] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:43:24,849] {logging_mixin.py:112} INFO - [2020-11-01 22:43:24,849] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:25,045] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:43:25,594] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:43:25,624] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:25,664] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:43:25,684] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:43:25,689] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.847 seconds
[2020-11-01 22:43:38,172] {scheduler_job.py:155} INFO - Started process (PID=31030) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:38,180] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:43:38,181] {logging_mixin.py:112} INFO - [2020-11-01 22:43:38,181] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:38,385] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:43:38,953] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:43:38,982] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:39,023] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:43:39,044] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:43:39,049] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.876 seconds
[2020-11-01 22:43:51,585] {scheduler_job.py:155} INFO - Started process (PID=31099) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:51,596] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:43:51,597] {logging_mixin.py:112} INFO - [2020-11-01 22:43:51,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:51,790] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:43:52,407] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:43:52,450] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:43:52,507] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:43:52,537] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:43:52,543] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.958 seconds
[2020-11-01 22:44:04,983] {scheduler_job.py:155} INFO - Started process (PID=31165) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:04,987] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:44:04,988] {logging_mixin.py:112} INFO - [2020-11-01 22:44:04,988] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:05,165] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:44:05,767] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:44:05,796] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:05,838] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:44:05,857] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:44:05,861] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.878 seconds
[2020-11-01 22:44:18,385] {scheduler_job.py:155} INFO - Started process (PID=31229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:18,393] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:44:18,394] {logging_mixin.py:112} INFO - [2020-11-01 22:44:18,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:18,577] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:44:19,265] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:44:19,294] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:19,332] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:44:19,351] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:44:19,355] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.971 seconds
[2020-11-01 22:44:31,790] {scheduler_job.py:155} INFO - Started process (PID=31292) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:31,801] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:44:31,802] {logging_mixin.py:112} INFO - [2020-11-01 22:44:31,802] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:31,985] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:44:32,592] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:44:32,621] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:32,662] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:44:32,681] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:44:32,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.896 seconds
[2020-11-01 22:44:45,191] {scheduler_job.py:155} INFO - Started process (PID=31355) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:45,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:44:45,198] {logging_mixin.py:112} INFO - [2020-11-01 22:44:45,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:45,379] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:44:46,083] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:44:46,115] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:46,155] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:44:46,174] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:44:46,179] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.988 seconds
[2020-11-01 22:44:58,595] {scheduler_job.py:155} INFO - Started process (PID=31422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:58,600] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:44:58,600] {logging_mixin.py:112} INFO - [2020-11-01 22:44:58,600] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:58,772] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:44:59,320] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:44:59,349] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:44:59,391] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:44:59,411] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:44:59,415] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.820 seconds
[2020-11-01 22:45:12,023] {scheduler_job.py:155} INFO - Started process (PID=31482) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:12,033] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:45:12,034] {logging_mixin.py:112} INFO - [2020-11-01 22:45:12,034] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:12,284] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:45:12,864] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:45:12,893] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:12,933] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:45:12,952] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:45:12,957] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.936 seconds
[2020-11-01 22:45:25,391] {scheduler_job.py:155} INFO - Started process (PID=31542) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:25,394] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:45:25,403] {logging_mixin.py:112} INFO - [2020-11-01 22:45:25,403] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:25,604] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:45:26,185] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:45:26,217] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:26,262] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:45:26,283] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:45:26,288] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.897 seconds
[2020-11-01 22:45:38,702] {scheduler_job.py:155} INFO - Started process (PID=31597) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:38,707] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:45:38,708] {logging_mixin.py:112} INFO - [2020-11-01 22:45:38,708] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:38,903] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:45:39,482] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:45:39,509] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:39,552] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:45:39,572] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:45:39,576] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.874 seconds
[2020-11-01 22:45:52,205] {scheduler_job.py:155} INFO - Started process (PID=31653) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:52,215] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:45:52,216] {logging_mixin.py:112} INFO - [2020-11-01 22:45:52,216] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:52,404] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:45:52,960] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:45:52,987] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:45:53,031] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:45:53,050] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:45:53,054] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.849 seconds
[2020-11-01 22:46:05,492] {scheduler_job.py:155} INFO - Started process (PID=31708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:05,496] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:46:05,497] {logging_mixin.py:112} INFO - [2020-11-01 22:46:05,497] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:05,680] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:46:06,340] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:46:06,374] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:06,418] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:46:06,440] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:46:06,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.952 seconds
[2020-11-01 22:46:18,881] {scheduler_job.py:155} INFO - Started process (PID=31766) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:18,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:46:18,888] {logging_mixin.py:112} INFO - [2020-11-01 22:46:18,888] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:19,077] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:46:19,641] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:46:19,669] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:19,710] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:46:19,729] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:46:19,733] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.852 seconds
[2020-11-01 22:46:32,233] {scheduler_job.py:155} INFO - Started process (PID=31821) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:32,236] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:46:32,237] {logging_mixin.py:112} INFO - [2020-11-01 22:46:32,237] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:32,417] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:46:32,999] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:46:33,030] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:33,072] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:46:33,091] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:46:33,096] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.863 seconds
[2020-11-01 22:46:45,502] {scheduler_job.py:155} INFO - Started process (PID=31875) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:45,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:46:45,507] {logging_mixin.py:112} INFO - [2020-11-01 22:46:45,506] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:45,689] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:46:46,270] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:46:46,299] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:46,339] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:46:46,359] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:46:46,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.861 seconds
[2020-11-01 22:46:59,101] {scheduler_job.py:155} INFO - Started process (PID=31931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:59,107] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:46:59,108] {logging_mixin.py:112} INFO - [2020-11-01 22:46:59,108] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:59,294] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:46:59,877] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:46:59,904] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:46:59,946] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:46:59,965] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:46:59,971] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.870 seconds
[2020-11-01 22:47:12,376] {scheduler_job.py:155} INFO - Started process (PID=31984) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:12,380] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:47:12,381] {logging_mixin.py:112} INFO - [2020-11-01 22:47:12,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:12,558] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:47:13,132] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:47:13,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:13,207] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:47:13,226] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:47:13,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.855 seconds
[2020-11-01 22:47:25,836] {scheduler_job.py:155} INFO - Started process (PID=32038) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:25,840] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:47:25,841] {logging_mixin.py:112} INFO - [2020-11-01 22:47:25,841] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:26,026] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:47:26,591] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:47:26,619] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:26,660] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:47:26,679] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:47:26,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.848 seconds
[2020-11-01 22:47:39,273] {scheduler_job.py:155} INFO - Started process (PID=32091) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:39,277] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:47:39,277] {logging_mixin.py:112} INFO - [2020-11-01 22:47:39,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:39,475] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:47:40,047] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:47:40,076] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:40,118] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:47:40,138] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:47:40,142] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.869 seconds
[2020-11-01 22:47:52,652] {scheduler_job.py:155} INFO - Started process (PID=32145) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:52,662] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:47:52,664] {logging_mixin.py:112} INFO - [2020-11-01 22:47:52,663] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:52,849] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:47:53,435] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:47:53,462] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:47:53,499] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:47:53,518] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:47:53,522] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.871 seconds
[2020-11-01 22:48:06,086] {scheduler_job.py:155} INFO - Started process (PID=32198) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:06,095] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:48:06,096] {logging_mixin.py:112} INFO - [2020-11-01 22:48:06,096] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:06,276] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:48:06,832] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:48:06,860] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:06,901] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:48:06,921] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:48:06,926] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.846 seconds
[2020-11-01 22:48:19,370] {scheduler_job.py:155} INFO - Started process (PID=32251) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:19,382] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:48:19,383] {logging_mixin.py:112} INFO - [2020-11-01 22:48:19,383] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:19,564] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:48:20,143] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:48:20,171] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:20,214] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:48:20,242] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:48:20,246] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.876 seconds
[2020-11-01 22:48:32,807] {scheduler_job.py:155} INFO - Started process (PID=32305) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:32,816] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:48:32,818] {logging_mixin.py:112} INFO - [2020-11-01 22:48:32,818] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:32,994] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:48:33,573] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:48:33,602] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:33,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:48:33,683] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:48:33,691] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.884 seconds
[2020-11-01 22:48:46,206] {scheduler_job.py:155} INFO - Started process (PID=32358) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:46,211] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:48:46,211] {logging_mixin.py:112} INFO - [2020-11-01 22:48:46,211] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:46,392] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:48:46,957] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:48:46,985] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:47,028] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:48:47,047] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:48:47,052] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.846 seconds
[2020-11-01 22:48:59,483] {scheduler_job.py:155} INFO - Started process (PID=32414) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:59,486] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:48:59,487] {logging_mixin.py:112} INFO - [2020-11-01 22:48:59,487] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:48:59,661] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:49:00,249] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:49:00,278] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:00,338] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:49:00,368] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:49:00,375] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.892 seconds
[2020-11-01 22:49:12,869] {scheduler_job.py:155} INFO - Started process (PID=32467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:12,874] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:49:12,875] {logging_mixin.py:112} INFO - [2020-11-01 22:49:12,875] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:13,053] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:49:13,648] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:49:13,676] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:13,717] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:49:13,736] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:49:13,740] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.872 seconds
[2020-11-01 22:49:26,224] {scheduler_job.py:155} INFO - Started process (PID=32521) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:26,229] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:49:26,230] {logging_mixin.py:112} INFO - [2020-11-01 22:49:26,229] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:26,472] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:49:27,344] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:49:27,388] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:27,447] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:49:27,476] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:49:27,481] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.257 seconds
[2020-11-01 22:49:39,502] {scheduler_job.py:155} INFO - Started process (PID=32574) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:39,505] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:49:39,506] {logging_mixin.py:112} INFO - [2020-11-01 22:49:39,506] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:39,710] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:49:40,386] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:49:40,417] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:40,459] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:49:40,479] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:49:40,484] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.982 seconds
[2020-11-01 22:49:52,899] {scheduler_job.py:155} INFO - Started process (PID=32629) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:52,910] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:49:52,912] {logging_mixin.py:112} INFO - [2020-11-01 22:49:52,911] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:53,103] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:49:53,706] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:49:53,747] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:49:53,807] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:49:53,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:49:53,845] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.946 seconds
[2020-11-01 22:50:06,405] {scheduler_job.py:155} INFO - Started process (PID=32683) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:06,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:50:06,415] {logging_mixin.py:112} INFO - [2020-11-01 22:50:06,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:06,593] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:50:07,146] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:50:07,174] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:07,217] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:50:07,237] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:50:07,242] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.837 seconds
[2020-11-01 22:50:19,882] {scheduler_job.py:155} INFO - Started process (PID=32736) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:19,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:50:19,888] {logging_mixin.py:112} INFO - [2020-11-01 22:50:19,888] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:20,063] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:50:20,803] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:50:20,831] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:20,873] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:50:20,903] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:50:20,908] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.026 seconds
[2020-11-01 22:50:33,281] {scheduler_job.py:155} INFO - Started process (PID=325) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:33,286] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:50:33,287] {logging_mixin.py:112} INFO - [2020-11-01 22:50:33,286] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:33,488] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:50:34,049] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:50:34,076] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:34,114] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:50:34,133] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:50:34,138] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.856 seconds
[2020-11-01 22:50:46,674] {scheduler_job.py:155} INFO - Started process (PID=382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:46,684] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:50:46,684] {logging_mixin.py:112} INFO - [2020-11-01 22:50:46,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:46,856] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:50:47,556] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:50:47,585] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:50:47,624] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:50:47,643] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:50:47,647] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.974 seconds
[2020-11-01 22:51:00,076] {scheduler_job.py:155} INFO - Started process (PID=437) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:00,081] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:51:00,082] {logging_mixin.py:112} INFO - [2020-11-01 22:51:00,082] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:00,266] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:51:00,829] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:51:00,856] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:00,896] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:51:00,916] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:51:00,920] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.844 seconds
[2020-11-01 22:51:13,449] {scheduler_job.py:155} INFO - Started process (PID=512) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:13,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:51:13,456] {logging_mixin.py:112} INFO - [2020-11-01 22:51:13,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:13,648] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:51:14,373] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:51:14,410] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:14,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:51:14,482] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:51:14,489] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.040 seconds
[2020-11-01 22:51:26,819] {scheduler_job.py:155} INFO - Started process (PID=578) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:26,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:51:26,825] {logging_mixin.py:112} INFO - [2020-11-01 22:51:26,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:27,012] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:51:27,592] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:51:27,620] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:27,662] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:51:27,682] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:51:27,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.867 seconds
[2020-11-01 22:51:40,238] {scheduler_job.py:155} INFO - Started process (PID=639) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:40,242] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:51:40,243] {logging_mixin.py:112} INFO - [2020-11-01 22:51:40,242] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:40,421] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:51:41,135] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:51:41,164] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:41,208] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:51:41,227] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:51:41,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.993 seconds
[2020-11-01 22:51:53,673] {scheduler_job.py:155} INFO - Started process (PID=701) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:53,680] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:51:53,681] {logging_mixin.py:112} INFO - [2020-11-01 22:51:53,681] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:53,861] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:51:54,434] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:51:54,462] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:51:54,504] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:51:54,523] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:51:54,527] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.854 seconds
[2020-11-01 22:52:07,063] {scheduler_job.py:155} INFO - Started process (PID=754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:07,068] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:52:07,069] {logging_mixin.py:112} INFO - [2020-11-01 22:52:07,069] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:07,318] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:52:07,876] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:52:07,903] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:07,945] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:52:07,964] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:52:07,969] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.906 seconds
[2020-11-01 22:52:20,490] {scheduler_job.py:155} INFO - Started process (PID=807) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:20,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:52:20,494] {logging_mixin.py:112} INFO - [2020-11-01 22:52:20,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:20,667] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:52:21,234] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:52:21,262] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:21,303] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:52:21,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:52:21,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.837 seconds
[2020-11-01 22:52:33,882] {scheduler_job.py:155} INFO - Started process (PID=861) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:33,885] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:52:33,886] {logging_mixin.py:112} INFO - [2020-11-01 22:52:33,886] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:34,069] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:52:34,635] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:52:34,662] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:34,704] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:52:34,724] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:52:34,728] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.846 seconds
[2020-11-01 22:52:47,237] {scheduler_job.py:155} INFO - Started process (PID=914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:47,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:52:47,248] {logging_mixin.py:112} INFO - [2020-11-01 22:52:47,247] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:47,452] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:52:48,017] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:52:48,045] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:52:48,085] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:52:48,106] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:52:48,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.873 seconds
[2020-11-01 22:53:00,654] {scheduler_job.py:155} INFO - Started process (PID=968) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:00,662] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:53:00,663] {logging_mixin.py:112} INFO - [2020-11-01 22:53:00,663] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:00,857] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:53:01,452] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:53:01,482] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:01,525] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:53:01,544] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:53:01,548] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.895 seconds
[2020-11-01 22:53:14,101] {scheduler_job.py:155} INFO - Started process (PID=1021) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:14,108] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:53:14,109] {logging_mixin.py:112} INFO - [2020-11-01 22:53:14,108] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:14,305] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:53:14,866] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:53:14,893] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:14,937] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:53:14,957] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:53:14,961] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.860 seconds
[2020-11-01 22:53:27,356] {scheduler_job.py:155} INFO - Started process (PID=1111) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:27,360] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:53:27,361] {logging_mixin.py:112} INFO - [2020-11-01 22:53:27,361] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:27,565] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:53:28,141] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:53:28,170] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:28,210] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:53:28,230] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:53:28,235] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.879 seconds
[2020-11-01 22:53:40,742] {scheduler_job.py:155} INFO - Started process (PID=1164) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:40,748] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:53:40,749] {logging_mixin.py:112} INFO - [2020-11-01 22:53:40,749] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:40,951] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:53:41,533] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:53:41,561] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:41,602] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:53:41,621] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:53:41,626] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.883 seconds
[2020-11-01 22:53:54,070] {scheduler_job.py:155} INFO - Started process (PID=1229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:54,075] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:53:54,076] {logging_mixin.py:112} INFO - [2020-11-01 22:53:54,076] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:54,291] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:53:54,863] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:53:54,890] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:53:54,930] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:53:54,949] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:53:54,953] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.884 seconds
[2020-11-01 22:54:07,357] {scheduler_job.py:155} INFO - Started process (PID=1343) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:07,361] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:54:07,361] {logging_mixin.py:112} INFO - [2020-11-01 22:54:07,361] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:07,547] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:54:08,121] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:54:08,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:08,191] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:54:08,213] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:54:08,217] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.860 seconds
[2020-11-01 22:54:20,654] {scheduler_job.py:155} INFO - Started process (PID=1503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:20,670] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:54:20,672] {logging_mixin.py:112} INFO - [2020-11-01 22:54:20,672] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:20,859] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:54:21,439] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:54:21,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:21,513] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:54:21,532] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:54:21,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.882 seconds
[2020-11-01 22:54:34,104] {scheduler_job.py:155} INFO - Started process (PID=1577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:34,110] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:54:34,111] {logging_mixin.py:112} INFO - [2020-11-01 22:54:34,111] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:34,336] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:54:35,011] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:54:35,043] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:35,083] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:54:35,103] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:54:35,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.008 seconds
[2020-11-01 22:54:47,458] {scheduler_job.py:155} INFO - Started process (PID=1663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:47,465] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:54:47,466] {logging_mixin.py:112} INFO - [2020-11-01 22:54:47,466] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:47,659] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:54:48,257] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:54:48,285] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:54:48,325] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:54:48,345] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:54:48,349] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.891 seconds
[2020-11-01 22:55:00,878] {scheduler_job.py:155} INFO - Started process (PID=1775) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:55:00,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:55:00,980] {logging_mixin.py:112} INFO - [2020-11-01 22:55:00,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:55:01,071] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 22:55:01,646] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 22:55:01,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:55:01,715] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 22:55:01,739] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 22:55:01,743] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.865 seconds
[2020-11-01 22:55:14,311] {scheduler_job.py:155} INFO - Started process (PID=1862) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:55:14,316] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 22:55:14,317] {logging_mixin.py:112} INFO - [2020-11-01 22:55:14,317] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 22:55:14,536] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:26:24,017] {scheduler_job.py:155} INFO - Started process (PID=1943) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:24,021] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:26:24,022] {logging_mixin.py:112} INFO - [2020-11-01 23:26:24,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:24,174] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:26:25,164] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:26:25,215] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:25,280] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:26:25,357] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:26:25,365] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:26:25,446] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:26:25,456] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 21:00:00+00:00 [success]> in ORM
[2020-11-01 23:26:25,466] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:26:25,476] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:26:25,492] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.476 seconds
[2020-11-01 23:26:37,263] {scheduler_job.py:155} INFO - Started process (PID=2036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:37,274] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:26:37,275] {logging_mixin.py:112} INFO - [2020-11-01 23:26:37,275] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:37,390] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:26:38,069] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:26:38,107] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:38,158] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:26:38,185] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:26:38,255] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:26:38,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.997 seconds
[2020-11-01 23:26:50,447] {scheduler_job.py:155} INFO - Started process (PID=2125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:50,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:26:50,453] {logging_mixin.py:112} INFO - [2020-11-01 23:26:50,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:50,554] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:26:51,105] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:26:51,133] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:26:51,171] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:26:51,189] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:26:51,239] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:26:51,243] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.796 seconds
[2020-11-01 23:27:03,631] {scheduler_job.py:155} INFO - Started process (PID=2206) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:03,636] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:27:03,637] {logging_mixin.py:112} INFO - [2020-11-01 23:27:03,636] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:03,740] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:27:04,314] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:27:04,339] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:04,376] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:27:04,393] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:27:04,443] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:27:04,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.817 seconds
[2020-11-01 23:27:16,846] {scheduler_job.py:155} INFO - Started process (PID=2270) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:16,851] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:27:16,852] {logging_mixin.py:112} INFO - [2020-11-01 23:27:16,852] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:16,975] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:27:17,559] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:27:17,581] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:17,623] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:27:17,640] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:27:17,693] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:27:17,697] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.850 seconds
[2020-11-01 23:27:30,046] {scheduler_job.py:155} INFO - Started process (PID=2334) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:30,051] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:27:30,052] {logging_mixin.py:112} INFO - [2020-11-01 23:27:30,052] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:30,149] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:27:30,677] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:27:30,704] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:30,750] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:27:30,769] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:27:30,828] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:27:30,832] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.787 seconds
[2020-11-01 23:27:43,405] {scheduler_job.py:155} INFO - Started process (PID=2407) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:43,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:27:43,415] {logging_mixin.py:112} INFO - [2020-11-01 23:27:43,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:43,499] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:27:44,021] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:27:44,045] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:44,084] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:27:44,101] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:27:44,151] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:27:44,156] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.751 seconds
[2020-11-01 23:27:56,903] {scheduler_job.py:155} INFO - Started process (PID=2478) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:56,906] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:27:56,907] {logging_mixin.py:112} INFO - [2020-11-01 23:27:56,907] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:57,006] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:27:57,575] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:27:57,603] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:27:57,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:27:57,662] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:27:57,720] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:27:57,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.823 seconds
[2020-11-01 23:28:10,204] {scheduler_job.py:155} INFO - Started process (PID=2546) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:10,209] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:28:10,209] {logging_mixin.py:112} INFO - [2020-11-01 23:28:10,209] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:10,306] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:28:10,908] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:28:10,944] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:10,993] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:28:11,016] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:28:11,081] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:28:11,086] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.882 seconds
[2020-11-01 23:28:23,627] {scheduler_job.py:155} INFO - Started process (PID=2618) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:23,630] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:28:23,631] {logging_mixin.py:112} INFO - [2020-11-01 23:28:23,631] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:23,738] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:28:24,359] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:28:24,395] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:24,436] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:28:24,455] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:28:24,516] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:28:24,521] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.894 seconds
[2020-11-01 23:28:36,859] {scheduler_job.py:155} INFO - Started process (PID=2687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:36,863] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:28:36,865] {logging_mixin.py:112} INFO - [2020-11-01 23:28:36,865] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:36,979] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:28:37,555] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:28:37,585] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:37,629] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:28:37,651] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:28:37,713] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:28:37,723] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:28:37,738] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.879 seconds
[2020-11-01 23:28:50,161] {scheduler_job.py:155} INFO - Started process (PID=2777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:50,169] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:28:50,172] {logging_mixin.py:112} INFO - [2020-11-01 23:28:50,172] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:50,282] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:28:50,828] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:28:50,857] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:28:50,918] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:28:50,941] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:28:51,016] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:28:51,024] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:28:51,041] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.880 seconds
[2020-11-01 23:29:03,436] {scheduler_job.py:155} INFO - Started process (PID=2865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:03,444] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:29:03,445] {logging_mixin.py:112} INFO - [2020-11-01 23:29:03,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:03,548] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:29:04,141] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:29:04,172] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:04,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:29:04,249] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:29:04,303] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:29:04,306] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.871 seconds
[2020-11-01 23:29:16,799] {scheduler_job.py:155} INFO - Started process (PID=2929) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:16,802] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:29:16,804] {logging_mixin.py:112} INFO - [2020-11-01 23:29:16,803] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:16,909] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:29:17,452] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:29:17,478] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:17,516] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:29:17,534] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:29:17,597] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:29:17,601] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.809 seconds
[2020-11-01 23:29:30,121] {scheduler_job.py:155} INFO - Started process (PID=2999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:30,133] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:29:30,134] {logging_mixin.py:112} INFO - [2020-11-01 23:29:30,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:30,241] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:29:30,766] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:29:30,801] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:30,841] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:29:30,858] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:29:30,914] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:29:30,921] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.800 seconds
[2020-11-01 23:29:43,479] {scheduler_job.py:155} INFO - Started process (PID=3063) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:43,483] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:29:43,484] {logging_mixin.py:112} INFO - [2020-11-01 23:29:43,484] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:43,572] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:29:44,113] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:29:44,142] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:44,183] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:29:44,205] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:29:44,263] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:29:44,272] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.793 seconds
[2020-11-01 23:29:56,933] {scheduler_job.py:155} INFO - Started process (PID=3129) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:56,942] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:29:56,944] {logging_mixin.py:112} INFO - [2020-11-01 23:29:56,943] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:57,046] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:29:57,581] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:29:57,607] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:29:57,652] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:29:57,668] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:29:57,722] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:29:57,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.793 seconds
[2020-11-01 23:30:10,290] {scheduler_job.py:155} INFO - Started process (PID=3193) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:10,298] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:30:10,299] {logging_mixin.py:112} INFO - [2020-11-01 23:30:10,299] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:10,407] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:30:10,963] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:30:10,991] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:11,030] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:30:11,049] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:30:11,102] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:30:11,106] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.816 seconds
[2020-11-01 23:30:23,795] {scheduler_job.py:155} INFO - Started process (PID=3258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:23,805] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:30:23,806] {logging_mixin.py:112} INFO - [2020-11-01 23:30:23,806] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:23,908] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:30:24,494] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:30:24,521] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:24,559] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:30:24,579] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:30:24,631] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:30:24,635] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.840 seconds
[2020-11-01 23:30:37,047] {scheduler_job.py:155} INFO - Started process (PID=3324) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:37,053] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:30:37,054] {logging_mixin.py:112} INFO - [2020-11-01 23:30:37,053] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:37,146] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:30:37,689] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:30:37,719] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:37,761] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:30:37,780] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:30:37,841] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:30:37,845] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.798 seconds
[2020-11-01 23:30:50,510] {scheduler_job.py:155} INFO - Started process (PID=3388) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:50,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:30:50,514] {logging_mixin.py:112} INFO - [2020-11-01 23:30:50,513] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:50,605] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:30:51,180] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:30:51,212] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:30:51,257] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:30:51,275] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:30:51,336] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:30:51,342] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:30:51,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.844 seconds
[2020-11-01 23:31:03,771] {scheduler_job.py:155} INFO - Started process (PID=3478) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:03,776] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:31:03,776] {logging_mixin.py:112} INFO - [2020-11-01 23:31:03,776] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:03,871] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:31:04,411] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:31:04,438] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:04,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:31:04,500] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:31:04,557] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:31:04,563] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:31:04,574] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.802 seconds
[2020-11-01 23:31:17,052] {scheduler_job.py:155} INFO - Started process (PID=3566) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:17,057] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:31:17,057] {logging_mixin.py:112} INFO - [2020-11-01 23:31:17,057] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:17,185] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:31:17,753] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:31:17,787] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:17,828] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:31:17,849] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:31:17,928] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:31:17,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.881 seconds
[2020-11-01 23:31:30,447] {scheduler_job.py:155} INFO - Started process (PID=3631) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:30,453] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:31:30,454] {logging_mixin.py:112} INFO - [2020-11-01 23:31:30,454] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:30,559] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:31:31,201] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:31:31,229] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:31,270] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:31:31,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:31:31,342] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:31:31,351] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.904 seconds
[2020-11-01 23:31:43,806] {scheduler_job.py:155} INFO - Started process (PID=3699) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:43,810] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:31:43,810] {logging_mixin.py:112} INFO - [2020-11-01 23:31:43,810] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:43,899] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:31:44,439] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:31:44,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:44,505] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:31:44,527] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:31:44,593] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:31:44,599] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.793 seconds
[2020-11-01 23:31:57,205] {scheduler_job.py:155} INFO - Started process (PID=3763) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:57,213] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:31:57,214] {logging_mixin.py:112} INFO - [2020-11-01 23:31:57,214] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:57,319] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:31:57,891] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:31:57,924] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:31:57,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:31:58,011] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:31:58,085] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:31:58,091] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.886 seconds
[2020-11-01 23:32:10,555] {scheduler_job.py:155} INFO - Started process (PID=3831) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:10,561] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:32:10,562] {logging_mixin.py:112} INFO - [2020-11-01 23:32:10,562] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:10,677] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:32:11,255] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:32:11,281] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:11,320] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:32:11,337] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:32:11,397] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:32:11,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.847 seconds
[2020-11-01 23:32:23,912] {scheduler_job.py:155} INFO - Started process (PID=3894) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:23,917] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:32:23,917] {logging_mixin.py:112} INFO - [2020-11-01 23:32:23,917] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:24,037] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:32:24,789] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:32:24,823] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:24,881] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:32:24,902] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:32:24,984] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:32:24,988] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.076 seconds
[2020-11-01 23:32:37,194] {scheduler_job.py:155} INFO - Started process (PID=3954) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:37,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:32:37,197] {logging_mixin.py:112} INFO - [2020-11-01 23:32:37,197] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:37,290] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:32:37,859] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:32:37,891] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:37,941] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:32:37,964] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:32:38,041] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:32:38,047] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.853 seconds
[2020-11-01 23:32:50,433] {scheduler_job.py:155} INFO - Started process (PID=4018) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:50,439] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:32:50,439] {logging_mixin.py:112} INFO - [2020-11-01 23:32:50,439] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:50,531] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:32:51,153] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:32:51,189] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:32:51,237] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:32:51,260] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:32:51,325] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:32:51,330] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.896 seconds
[2020-11-01 23:33:03,763] {scheduler_job.py:155} INFO - Started process (PID=4080) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:03,771] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:33:03,773] {logging_mixin.py:112} INFO - [2020-11-01 23:33:03,773] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:03,888] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:33:04,406] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:33:04,433] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:04,470] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:33:04,490] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:33:04,551] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:33:04,561] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:33:04,574] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.811 seconds
[2020-11-01 23:33:17,017] {scheduler_job.py:155} INFO - Started process (PID=4170) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:17,022] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:33:17,022] {logging_mixin.py:112} INFO - [2020-11-01 23:33:17,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:17,113] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:33:17,630] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:33:17,659] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:17,700] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:33:17,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:33:17,759] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:33:17,765] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 21:00:00+00:00 [scheduled]> in ORM
[2020-11-01 23:33:17,774] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.757 seconds
[2020-11-01 23:33:30,346] {scheduler_job.py:155} INFO - Started process (PID=4281) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:30,351] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:33:30,352] {logging_mixin.py:112} INFO - [2020-11-01 23:33:30,352] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:30,441] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:33:31,012] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:33:31,038] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:31,081] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:33:31,099] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False>
[2020-11-01 23:33:31,113] {logging_mixin.py:112} INFO - [2020-11-01 23:33:31,113] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 21:00:00+00:00: scheduled__2020-11-01T21:00:00+00:00, externally triggered: False> failed
[2020-11-01 23:33:31,117] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:33:31,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.775 seconds
[2020-11-01 23:33:43,655] {scheduler_job.py:155} INFO - Started process (PID=4348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:43,665] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:33:43,667] {logging_mixin.py:112} INFO - [2020-11-01 23:33:43,666] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:43,765] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:33:44,416] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:33:44,443] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:44,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:33:44,495] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:33:44,498] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.843 seconds
[2020-11-01 23:33:57,049] {scheduler_job.py:155} INFO - Started process (PID=4414) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:57,056] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:33:57,057] {logging_mixin.py:112} INFO - [2020-11-01 23:33:57,057] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:57,153] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:33:57,694] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:33:57,718] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:33:57,767] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:33:57,785] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:33:57,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.742 seconds
[2020-11-01 23:34:10,367] {scheduler_job.py:155} INFO - Started process (PID=4486) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:10,384] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:34:10,385] {logging_mixin.py:112} INFO - [2020-11-01 23:34:10,385] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:10,488] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:34:11,061] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:34:11,092] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:11,132] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:34:11,151] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:34:11,155] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.787 seconds
[2020-11-01 23:34:23,719] {scheduler_job.py:155} INFO - Started process (PID=4550) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:23,723] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:34:23,724] {logging_mixin.py:112} INFO - [2020-11-01 23:34:23,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:23,824] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:34:24,424] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:34:24,457] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:24,507] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:34:24,531] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:34:24,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.817 seconds
[2020-11-01 23:34:37,112] {scheduler_job.py:155} INFO - Started process (PID=4612) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:37,120] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:34:37,122] {logging_mixin.py:112} INFO - [2020-11-01 23:34:37,121] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:37,221] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:34:37,756] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:34:37,784] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:37,824] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:34:37,842] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:34:37,846] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.735 seconds
[2020-11-01 23:34:50,586] {scheduler_job.py:155} INFO - Started process (PID=4677) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:50,594] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:34:50,595] {logging_mixin.py:112} INFO - [2020-11-01 23:34:50,594] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:50,678] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:34:51,183] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:34:51,209] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:34:51,247] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:34:51,266] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:34:51,272] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.686 seconds
[2020-11-01 23:35:03,978] {scheduler_job.py:155} INFO - Started process (PID=4745) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:03,985] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:35:03,986] {logging_mixin.py:112} INFO - [2020-11-01 23:35:03,985] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:04,085] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:35:04,591] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:35:04,619] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:04,658] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:35:04,676] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:35:04,680] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.702 seconds
[2020-11-01 23:35:17,447] {scheduler_job.py:155} INFO - Started process (PID=4810) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:17,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:35:17,452] {logging_mixin.py:112} INFO - [2020-11-01 23:35:17,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:17,564] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:35:18,348] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:35:18,385] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:18,424] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:35:18,446] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:35:18,458] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.011 seconds
[2020-11-01 23:35:30,866] {scheduler_job.py:155} INFO - Started process (PID=4871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:30,869] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:35:30,870] {logging_mixin.py:112} INFO - [2020-11-01 23:35:30,870] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:30,950] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:35:31,459] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:35:31,486] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:31,527] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:35:31,545] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:35:31,549] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.682 seconds
[2020-11-01 23:35:44,316] {scheduler_job.py:155} INFO - Started process (PID=4937) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:44,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:35:44,327] {logging_mixin.py:112} INFO - [2020-11-01 23:35:44,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:44,413] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:35:45,072] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:35:45,098] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:45,134] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:35:45,153] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:35:45,158] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.842 seconds
[2020-11-01 23:35:57,803] {scheduler_job.py:155} INFO - Started process (PID=5002) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:57,807] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:35:57,808] {logging_mixin.py:112} INFO - [2020-11-01 23:35:57,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:57,893] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:35:58,416] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:35:58,442] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:35:58,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:35:58,494] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:35:58,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.694 seconds
[2020-11-01 23:36:11,221] {scheduler_job.py:155} INFO - Started process (PID=5069) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:11,227] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:36:11,230] {logging_mixin.py:112} INFO - [2020-11-01 23:36:11,230] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:11,360] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:36:11,869] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:36:11,895] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:11,934] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:36:11,950] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:36:11,954] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.732 seconds
[2020-11-01 23:36:24,672] {scheduler_job.py:155} INFO - Started process (PID=5132) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:24,677] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:36:24,678] {logging_mixin.py:112} INFO - [2020-11-01 23:36:24,678] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:24,788] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:36:25,399] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:36:25,426] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:25,477] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:36:25,496] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:36:25,500] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.827 seconds
[2020-11-01 23:36:38,070] {scheduler_job.py:155} INFO - Started process (PID=5199) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:38,074] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:36:38,081] {logging_mixin.py:112} INFO - [2020-11-01 23:36:38,075] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:38,205] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:36:38,749] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:36:38,777] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:38,822] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:36:38,842] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:36:38,846] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.776 seconds
[2020-11-01 23:36:51,649] {scheduler_job.py:155} INFO - Started process (PID=5261) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:51,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:36:51,659] {logging_mixin.py:112} INFO - [2020-11-01 23:36:51,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:51,749] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:36:52,288] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:36:52,315] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:36:52,354] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:36:52,372] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:36:52,377] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.727 seconds
[2020-11-01 23:37:04,977] {scheduler_job.py:155} INFO - Started process (PID=5331) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:04,981] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:37:04,982] {logging_mixin.py:112} INFO - [2020-11-01 23:37:04,982] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:05,089] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:37:05,711] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:37:05,738] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:05,777] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:37:05,797] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:37:05,801] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.824 seconds
[2020-11-01 23:37:18,482] {scheduler_job.py:155} INFO - Started process (PID=5396) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:18,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:37:18,495] {logging_mixin.py:112} INFO - [2020-11-01 23:37:18,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:18,596] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:37:19,126] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:37:19,152] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:19,192] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:37:19,210] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:37:19,214] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.732 seconds
[2020-11-01 23:37:32,001] {scheduler_job.py:155} INFO - Started process (PID=5459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:32,009] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:37:32,010] {logging_mixin.py:112} INFO - [2020-11-01 23:37:32,010] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:32,113] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:37:32,620] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:37:32,645] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:32,685] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:37:32,703] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:37:32,706] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.705 seconds
[2020-11-01 23:37:45,479] {scheduler_job.py:155} INFO - Started process (PID=5526) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:45,486] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:37:45,487] {logging_mixin.py:112} INFO - [2020-11-01 23:37:45,486] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:45,575] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:37:46,091] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:37:46,117] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:46,156] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:37:46,174] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:37:46,178] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.699 seconds
[2020-11-01 23:37:58,864] {scheduler_job.py:155} INFO - Started process (PID=5588) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:58,868] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:37:58,869] {logging_mixin.py:112} INFO - [2020-11-01 23:37:58,869] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:58,952] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:37:59,466] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:37:59,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:37:59,532] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:37:59,550] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:37:59,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.703 seconds
[2020-11-01 23:38:12,299] {scheduler_job.py:155} INFO - Started process (PID=5653) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:12,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:38:12,306] {logging_mixin.py:112} INFO - [2020-11-01 23:38:12,306] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:12,397] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:38:12,923] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:38:12,963] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:13,020] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:38:13,048] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:38:13,054] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.755 seconds
[2020-11-01 23:38:25,823] {scheduler_job.py:155} INFO - Started process (PID=5718) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:25,826] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:38:25,827] {logging_mixin.py:112} INFO - [2020-11-01 23:38:25,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:25,917] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:38:26,441] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:38:26,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:26,503] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:38:26,521] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:38:26,525] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.703 seconds
[2020-11-01 23:38:39,353] {scheduler_job.py:155} INFO - Started process (PID=5784) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:39,361] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:38:39,363] {logging_mixin.py:112} INFO - [2020-11-01 23:38:39,362] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:39,477] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:38:40,085] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:38:40,112] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:40,152] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:38:40,170] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:38:40,174] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.820 seconds
[2020-11-01 23:38:52,991] {scheduler_job.py:155} INFO - Started process (PID=5853) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:52,998] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:38:52,999] {logging_mixin.py:112} INFO - [2020-11-01 23:38:52,999] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:53,093] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:38:53,596] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:38:53,622] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:38:53,665] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:38:53,682] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:38:53,686] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.697 seconds
[2020-11-01 23:39:06,559] {scheduler_job.py:155} INFO - Started process (PID=5917) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:06,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:39:06,567] {logging_mixin.py:112} INFO - [2020-11-01 23:39:06,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:06,663] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:39:07,192] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:39:07,218] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:07,262] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:39:07,280] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:39:07,284] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.725 seconds
[2020-11-01 23:39:20,000] {scheduler_job.py:155} INFO - Started process (PID=5983) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:20,009] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:39:20,015] {logging_mixin.py:112} INFO - [2020-11-01 23:39:20,015] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:20,103] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:39:20,606] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:39:20,632] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:20,674] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:39:20,692] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:39:20,696] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.696 seconds
[2020-11-01 23:39:33,535] {scheduler_job.py:155} INFO - Started process (PID=6049) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:33,542] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:39:33,543] {logging_mixin.py:112} INFO - [2020-11-01 23:39:33,543] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:33,671] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:39:34,398] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:39:34,429] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:34,476] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:39:34,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:39:34,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.963 seconds
[2020-11-01 23:39:46,913] {scheduler_job.py:155} INFO - Started process (PID=6116) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:46,919] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:39:46,920] {logging_mixin.py:112} INFO - [2020-11-01 23:39:46,919] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:47,011] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:39:47,532] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:39:47,558] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:39:47,596] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:39:47,614] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:39:47,618] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.705 seconds
[2020-11-01 23:40:00,250] {scheduler_job.py:155} INFO - Started process (PID=6185) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:00,254] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:40:00,254] {logging_mixin.py:112} INFO - [2020-11-01 23:40:00,254] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:00,410] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:40:01,092] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:40:01,119] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:01,156] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:40:01,173] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:40:01,177] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.928 seconds
[2020-11-01 23:40:13,705] {scheduler_job.py:155} INFO - Started process (PID=6254) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:13,711] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:40:13,712] {logging_mixin.py:112} INFO - [2020-11-01 23:40:13,712] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:13,807] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:40:14,317] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:40:14,343] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:14,379] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:40:14,397] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:40:14,401] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.697 seconds
[2020-11-01 23:40:26,988] {scheduler_job.py:155} INFO - Started process (PID=6319) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:26,991] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:40:26,992] {logging_mixin.py:112} INFO - [2020-11-01 23:40:26,992] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:27,084] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:40:27,590] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:40:27,616] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:27,656] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:40:27,675] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:40:27,679] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.691 seconds
[2020-11-01 23:40:40,511] {scheduler_job.py:155} INFO - Started process (PID=6385) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:40,520] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:40:40,521] {logging_mixin.py:112} INFO - [2020-11-01 23:40:40,521] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:40,604] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:40:41,106] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:40:41,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:41,171] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:40:41,189] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:40:41,193] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.682 seconds
[2020-11-01 23:40:54,052] {scheduler_job.py:155} INFO - Started process (PID=6456) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:54,060] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:40:54,061] {logging_mixin.py:112} INFO - [2020-11-01 23:40:54,061] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:54,156] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:40:54,681] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:40:54,707] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:40:54,749] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:40:54,769] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:40:54,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.722 seconds
[2020-11-01 23:41:07,620] {scheduler_job.py:155} INFO - Started process (PID=6519) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:07,629] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:41:07,630] {logging_mixin.py:112} INFO - [2020-11-01 23:41:07,630] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:07,742] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:41:08,460] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:41:08,487] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:08,523] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:41:08,538] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:41:08,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.922 seconds
[2020-11-01 23:41:21,146] {scheduler_job.py:155} INFO - Started process (PID=6583) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:21,151] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:41:21,152] {logging_mixin.py:112} INFO - [2020-11-01 23:41:21,152] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:21,242] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:41:21,751] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:41:21,784] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:21,826] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:41:21,846] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:41:21,850] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.705 seconds
[2020-11-01 23:41:34,646] {scheduler_job.py:155} INFO - Started process (PID=6653) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:34,650] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:41:34,651] {logging_mixin.py:112} INFO - [2020-11-01 23:41:34,651] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:34,759] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:41:35,266] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:41:35,292] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:35,330] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:41:35,349] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:41:35,353] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.707 seconds
[2020-11-01 23:41:48,057] {scheduler_job.py:155} INFO - Started process (PID=6719) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:48,069] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:41:48,071] {logging_mixin.py:112} INFO - [2020-11-01 23:41:48,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:48,161] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:41:48,676] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:41:48,702] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:41:48,744] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:41:48,764] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:41:48,769] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.711 seconds
[2020-11-01 23:42:01,673] {scheduler_job.py:155} INFO - Started process (PID=6782) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:01,682] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:42:01,684] {logging_mixin.py:112} INFO - [2020-11-01 23:42:01,683] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:01,779] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:42:02,296] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:42:02,321] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:02,360] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:42:02,378] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:42:02,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.709 seconds
[2020-11-01 23:42:15,317] {scheduler_job.py:155} INFO - Started process (PID=6857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:15,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:42:15,323] {logging_mixin.py:112} INFO - [2020-11-01 23:42:15,323] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:15,415] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:42:15,925] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:42:15,951] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:15,992] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:42:16,010] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:42:16,014] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.697 seconds
[2020-11-01 23:42:28,775] {scheduler_job.py:155} INFO - Started process (PID=6923) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:28,783] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:42:28,784] {logging_mixin.py:112} INFO - [2020-11-01 23:42:28,783] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:28,881] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:42:29,390] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:42:29,415] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:29,457] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:42:29,474] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:42:29,479] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.705 seconds
[2020-11-01 23:42:42,217] {scheduler_job.py:155} INFO - Started process (PID=6988) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:42,222] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:42:42,223] {logging_mixin.py:112} INFO - [2020-11-01 23:42:42,223] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:42,306] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:42:42,826] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:42:42,852] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:42,892] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:42:42,911] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:42:42,915] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.698 seconds
[2020-11-01 23:42:55,789] {scheduler_job.py:155} INFO - Started process (PID=7052) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:55,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:42:55,795] {logging_mixin.py:112} INFO - [2020-11-01 23:42:55,794] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:55,889] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:42:56,400] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:42:56,428] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:42:56,467] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:42:56,485] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:42:56,489] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.700 seconds
[2020-11-01 23:43:09,320] {scheduler_job.py:155} INFO - Started process (PID=7120) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:09,324] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:43:09,325] {logging_mixin.py:112} INFO - [2020-11-01 23:43:09,325] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:09,428] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:43:10,188] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:43:10,224] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:10,278] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:43:10,300] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:43:10,305] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.985 seconds
[2020-11-01 23:43:22,813] {scheduler_job.py:155} INFO - Started process (PID=7184) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:22,816] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:43:22,817] {logging_mixin.py:112} INFO - [2020-11-01 23:43:22,816] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:22,901] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:43:23,408] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:43:23,434] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:23,474] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:43:23,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:43:23,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.684 seconds
[2020-11-01 23:43:36,268] {scheduler_job.py:155} INFO - Started process (PID=7249) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:36,272] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:43:36,273] {logging_mixin.py:112} INFO - [2020-11-01 23:43:36,273] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:36,408] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:43:36,972] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:43:36,999] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:37,038] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:43:37,056] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:43:37,060] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.792 seconds
[2020-11-01 23:43:49,881] {scheduler_job.py:155} INFO - Started process (PID=7317) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:49,888] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:43:49,889] {logging_mixin.py:112} INFO - [2020-11-01 23:43:49,889] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:49,991] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:43:50,495] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:43:50,521] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:43:50,559] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:43:50,577] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:43:50,581] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.700 seconds
[2020-11-01 23:44:03,443] {scheduler_job.py:155} INFO - Started process (PID=7382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:03,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:44:03,453] {logging_mixin.py:112} INFO - [2020-11-01 23:44:03,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:03,542] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:44:04,049] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:44:04,081] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:04,127] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:44:04,145] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:44:04,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.706 seconds
[2020-11-01 23:44:16,984] {scheduler_job.py:155} INFO - Started process (PID=7446) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:16,993] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:44:16,995] {logging_mixin.py:112} INFO - [2020-11-01 23:44:16,994] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:17,081] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:44:17,585] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:44:17,612] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:17,650] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:44:17,674] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:44:17,678] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.694 seconds
[2020-11-01 23:44:30,559] {scheduler_job.py:155} INFO - Started process (PID=7510) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:30,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:44:30,567] {logging_mixin.py:112} INFO - [2020-11-01 23:44:30,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:30,660] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:44:31,172] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:44:31,198] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:31,236] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:44:31,255] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:44:31,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.700 seconds
[2020-11-01 23:44:43,943] {scheduler_job.py:155} INFO - Started process (PID=7577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:43,951] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:44:43,967] {logging_mixin.py:112} INFO - [2020-11-01 23:44:43,966] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:44,053] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:44:44,584] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:44:44,610] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:44,650] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:44:44,668] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:44:44,672] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.729 seconds
[2020-11-01 23:44:57,515] {scheduler_job.py:155} INFO - Started process (PID=7658) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:57,522] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:44:57,523] {logging_mixin.py:112} INFO - [2020-11-01 23:44:57,523] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:57,610] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:44:58,179] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:44:58,205] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:44:58,246] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:44:58,261] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:44:58,264] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.750 seconds
[2020-11-01 23:45:10,793] {scheduler_job.py:155} INFO - Started process (PID=7735) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:10,796] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:45:10,797] {logging_mixin.py:112} INFO - [2020-11-01 23:45:10,797] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:10,926] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:45:11,481] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:45:11,507] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:11,550] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:45:11,586] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:45:11,590] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.798 seconds
[2020-11-01 23:45:24,215] {scheduler_job.py:155} INFO - Started process (PID=7807) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:24,225] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:45:24,227] {logging_mixin.py:112} INFO - [2020-11-01 23:45:24,226] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:24,314] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:45:24,845] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:45:24,876] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:24,918] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:45:24,936] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:45:24,940] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.726 seconds
[2020-11-01 23:45:37,789] {scheduler_job.py:155} INFO - Started process (PID=7882) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:37,797] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:45:37,799] {logging_mixin.py:112} INFO - [2020-11-01 23:45:37,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:37,891] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:45:38,652] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:45:38,683] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:38,728] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:45:38,751] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:45:38,756] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.967 seconds
[2020-11-01 23:45:51,382] {scheduler_job.py:155} INFO - Started process (PID=7955) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:51,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:45:51,397] {logging_mixin.py:112} INFO - [2020-11-01 23:45:51,397] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:51,487] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:45:51,995] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:45:52,020] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:45:52,053] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:45:52,069] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:45:52,072] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.691 seconds
[2020-11-01 23:46:04,843] {scheduler_job.py:155} INFO - Started process (PID=8031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:04,848] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:46:04,849] {logging_mixin.py:112} INFO - [2020-11-01 23:46:04,849] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:04,982] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:46:05,526] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:46:05,551] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:05,590] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:46:05,613] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:46:05,624] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.781 seconds
[2020-11-01 23:46:18,323] {scheduler_job.py:155} INFO - Started process (PID=8105) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:18,333] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:46:18,334] {logging_mixin.py:112} INFO - [2020-11-01 23:46:18,334] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:18,433] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:46:18,990] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:46:19,024] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:19,067] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:46:19,094] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:46:19,098] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.775 seconds
[2020-11-01 23:46:31,852] {scheduler_job.py:155} INFO - Started process (PID=8177) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:31,859] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:46:31,860] {logging_mixin.py:112} INFO - [2020-11-01 23:46:31,859] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:31,952] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:46:32,463] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:46:32,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:32,546] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:46:32,564] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:46:32,568] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.716 seconds
[2020-11-01 23:46:45,294] {scheduler_job.py:155} INFO - Started process (PID=8252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:45,302] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:46:45,303] {logging_mixin.py:112} INFO - [2020-11-01 23:46:45,302] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:45,392] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:46:45,907] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:46:45,932] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:45,987] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:46:46,007] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:46:46,011] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.717 seconds
[2020-11-01 23:46:58,779] {scheduler_job.py:155} INFO - Started process (PID=8321) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:58,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:46:58,788] {logging_mixin.py:112} INFO - [2020-11-01 23:46:58,787] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:58,876] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:46:59,439] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:46:59,465] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:46:59,508] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:46:59,526] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:46:59,530] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.751 seconds
[2020-11-01 23:47:12,257] {scheduler_job.py:155} INFO - Started process (PID=8402) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:12,261] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:47:12,261] {logging_mixin.py:112} INFO - [2020-11-01 23:47:12,261] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:12,343] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:47:12,875] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:47:12,900] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:12,944] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:47:12,962] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:47:12,966] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.710 seconds
[2020-11-01 23:47:25,831] {scheduler_job.py:155} INFO - Started process (PID=8474) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:25,845] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:47:25,847] {logging_mixin.py:112} INFO - [2020-11-01 23:47:25,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:25,930] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:47:26,434] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:47:26,460] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:26,499] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:47:26,516] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:47:26,520] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.690 seconds
[2020-11-01 23:47:39,234] {scheduler_job.py:155} INFO - Started process (PID=8547) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:39,241] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:47:39,242] {logging_mixin.py:112} INFO - [2020-11-01 23:47:39,242] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:39,357] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:47:39,859] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:47:39,885] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:39,923] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:47:39,941] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:47:39,945] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.711 seconds
[2020-11-01 23:47:52,734] {scheduler_job.py:155} INFO - Started process (PID=8619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:52,743] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:47:52,744] {logging_mixin.py:112} INFO - [2020-11-01 23:47:52,744] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:52,839] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:47:53,424] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:47:53,449] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:47:53,485] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:47:53,501] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:47:53,504] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.770 seconds
[2020-11-01 23:48:06,176] {scheduler_job.py:155} INFO - Started process (PID=8688) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:06,185] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:48:06,187] {logging_mixin.py:112} INFO - [2020-11-01 23:48:06,186] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:06,273] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:48:06,887] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:48:06,914] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:06,963] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:48:06,984] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:48:06,988] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.812 seconds
[2020-11-01 23:48:19,795] {scheduler_job.py:155} INFO - Started process (PID=8764) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:19,802] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:48:19,803] {logging_mixin.py:112} INFO - [2020-11-01 23:48:19,803] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:19,903] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:48:20,406] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:48:20,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:20,472] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:48:20,490] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:48:20,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.700 seconds
[2020-11-01 23:48:33,235] {scheduler_job.py:155} INFO - Started process (PID=8837) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:33,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:48:33,240] {logging_mixin.py:112} INFO - [2020-11-01 23:48:33,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:33,356] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:48:33,869] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:48:33,895] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:33,935] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:48:33,953] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:48:33,957] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.722 seconds
[2020-11-01 23:48:46,953] {scheduler_job.py:155} INFO - Started process (PID=8911) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:46,964] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:48:46,966] {logging_mixin.py:112} INFO - [2020-11-01 23:48:46,965] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:47,053] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:48:47,566] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:48:47,592] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:48:47,630] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:48:47,648] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:48:47,652] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.699 seconds
[2020-11-01 23:49:00,563] {scheduler_job.py:155} INFO - Started process (PID=8983) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:00,572] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:49:00,573] {logging_mixin.py:112} INFO - [2020-11-01 23:49:00,573] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:00,664] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:49:01,205] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:49:01,231] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:01,271] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:49:01,289] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:49:01,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.730 seconds
[2020-11-01 23:49:14,063] {scheduler_job.py:155} INFO - Started process (PID=9058) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:14,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:49:14,072] {logging_mixin.py:112} INFO - [2020-11-01 23:49:14,071] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:14,168] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:49:14,680] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:49:14,705] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:14,746] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:49:14,765] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:49:14,769] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.706 seconds
[2020-11-01 23:49:27,642] {scheduler_job.py:155} INFO - Started process (PID=9133) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:27,649] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:49:27,649] {logging_mixin.py:112} INFO - [2020-11-01 23:49:27,649] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:27,732] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:49:28,247] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:49:28,272] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:28,309] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:49:28,326] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:49:28,330] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.688 seconds
[2020-11-01 23:49:41,001] {scheduler_job.py:155} INFO - Started process (PID=9207) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:41,008] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:49:41,009] {logging_mixin.py:112} INFO - [2020-11-01 23:49:41,009] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:41,108] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:49:41,607] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:49:41,633] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:41,671] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:49:41,689] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:49:41,693] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.693 seconds
[2020-11-01 23:49:54,541] {scheduler_job.py:155} INFO - Started process (PID=9280) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:54,548] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:49:54,549] {logging_mixin.py:112} INFO - [2020-11-01 23:49:54,548] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:54,653] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:49:55,293] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:49:55,319] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:49:55,355] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:49:55,374] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:49:55,378] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.837 seconds
[2020-11-01 23:50:08,028] {scheduler_job.py:155} INFO - Started process (PID=9356) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:08,031] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:50:08,032] {logging_mixin.py:112} INFO - [2020-11-01 23:50:08,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:08,117] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:50:08,747] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:50:08,773] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:08,814] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:50:08,833] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:50:08,837] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.809 seconds
[2020-11-01 23:50:21,564] {scheduler_job.py:155} INFO - Started process (PID=9428) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:21,574] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:50:21,575] {logging_mixin.py:112} INFO - [2020-11-01 23:50:21,575] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:21,676] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:50:22,187] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:50:22,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:22,253] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:50:22,270] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:50:22,274] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.711 seconds
[2020-11-01 23:50:34,885] {scheduler_job.py:155} INFO - Started process (PID=9504) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:34,890] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:50:34,890] {logging_mixin.py:112} INFO - [2020-11-01 23:50:34,890] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:35,024] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:50:35,549] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:50:35,575] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:35,619] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:50:35,638] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:50:35,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.756 seconds
[2020-11-01 23:50:48,518] {scheduler_job.py:155} INFO - Started process (PID=9582) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:48,528] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:50:48,529] {logging_mixin.py:112} INFO - [2020-11-01 23:50:48,529] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:48,625] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:50:49,136] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:50:49,163] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:50:49,205] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:50:49,224] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:50:49,228] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.710 seconds
[2020-11-01 23:51:02,161] {scheduler_job.py:155} INFO - Started process (PID=9655) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:02,170] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:51:02,172] {logging_mixin.py:112} INFO - [2020-11-01 23:51:02,171] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:02,259] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:51:02,773] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:51:02,798] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:02,831] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:51:02,846] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:51:02,850] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.689 seconds
[2020-11-01 23:51:15,775] {scheduler_job.py:155} INFO - Started process (PID=9729) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:15,779] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:51:15,780] {logging_mixin.py:112} INFO - [2020-11-01 23:51:15,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:15,861] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:51:16,376] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:51:16,401] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:16,438] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:51:16,456] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:51:16,461] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.686 seconds
[2020-11-01 23:51:29,152] {scheduler_job.py:155} INFO - Started process (PID=9802) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:29,160] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:51:29,161] {logging_mixin.py:112} INFO - [2020-11-01 23:51:29,161] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:29,259] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:51:29,766] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:51:29,795] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:29,834] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:51:29,852] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:51:29,856] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.705 seconds
[2020-11-01 23:51:42,784] {scheduler_job.py:155} INFO - Started process (PID=9873) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:42,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:51:42,790] {logging_mixin.py:112} INFO - [2020-11-01 23:51:42,790] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:42,887] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:51:43,477] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:51:43,518] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:43,573] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:51:43,590] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:51:43,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.811 seconds
[2020-11-01 23:51:56,270] {scheduler_job.py:155} INFO - Started process (PID=9946) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:56,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:51:56,280] {logging_mixin.py:112} INFO - [2020-11-01 23:51:56,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:56,393] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:51:56,902] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:51:56,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:51:56,968] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:51:56,988] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:51:56,992] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.722 seconds
[2020-11-01 23:52:09,725] {scheduler_job.py:155} INFO - Started process (PID=10030) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:09,736] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:52:09,737] {logging_mixin.py:112} INFO - [2020-11-01 23:52:09,737] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:09,872] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:52:10,464] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:52:10,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:10,535] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:52:10,553] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:52:10,557] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.833 seconds
[2020-11-01 23:52:23,187] {scheduler_job.py:155} INFO - Started process (PID=10108) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:23,196] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:52:23,197] {logging_mixin.py:112} INFO - [2020-11-01 23:52:23,197] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:23,286] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:52:23,819] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:52:23,844] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:23,891] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:52:23,910] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:52:23,914] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.726 seconds
[2020-11-01 23:52:36,777] {scheduler_job.py:155} INFO - Started process (PID=10181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:36,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:52:36,786] {logging_mixin.py:112} INFO - [2020-11-01 23:52:36,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:36,877] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:52:37,377] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:52:37,402] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:37,443] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:52:37,462] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:52:37,466] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.690 seconds
[2020-11-01 23:52:50,363] {scheduler_job.py:155} INFO - Started process (PID=10257) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:50,370] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:52:50,372] {logging_mixin.py:112} INFO - [2020-11-01 23:52:50,371] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:50,471] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:52:50,981] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:52:51,006] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:52:51,045] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:52:51,062] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:52:51,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.704 seconds
[2020-11-01 23:53:03,807] {scheduler_job.py:155} INFO - Started process (PID=10332) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:03,811] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:53:03,811] {logging_mixin.py:112} INFO - [2020-11-01 23:53:03,811] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:03,892] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:53:04,395] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:53:04,421] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:04,459] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:53:04,477] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:53:04,481] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.674 seconds
[2020-11-01 23:53:17,365] {scheduler_job.py:155} INFO - Started process (PID=10406) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:17,374] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:53:17,375] {logging_mixin.py:112} INFO - [2020-11-01 23:53:17,375] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:17,472] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:53:17,978] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:53:18,005] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:18,045] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:53:18,063] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:53:18,067] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.702 seconds
[2020-11-01 23:53:30,816] {scheduler_job.py:155} INFO - Started process (PID=10479) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:30,819] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:53:30,820] {logging_mixin.py:112} INFO - [2020-11-01 23:53:30,820] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:30,908] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:53:31,412] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:53:31,438] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:31,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:53:31,496] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:53:31,501] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.685 seconds
[2020-11-01 23:53:44,405] {scheduler_job.py:155} INFO - Started process (PID=10553) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:44,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:53:44,417] {logging_mixin.py:112} INFO - [2020-11-01 23:53:44,417] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:44,500] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:53:45,126] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:53:45,153] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:45,188] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:53:45,205] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:53:45,209] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.804 seconds
[2020-11-01 23:53:57,908] {scheduler_job.py:155} INFO - Started process (PID=10628) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:57,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:53:57,916] {logging_mixin.py:112} INFO - [2020-11-01 23:53:57,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:57,999] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:53:58,512] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:53:58,538] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:53:58,575] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:53:58,593] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:53:58,597] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.689 seconds
[2020-11-01 23:54:11,486] {scheduler_job.py:155} INFO - Started process (PID=10701) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:11,492] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:54:11,492] {logging_mixin.py:112} INFO - [2020-11-01 23:54:11,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:11,617] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:54:12,136] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:54:12,163] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:12,202] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:54:12,219] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:54:12,223] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.737 seconds
[2020-11-01 23:54:25,013] {scheduler_job.py:155} INFO - Started process (PID=10777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:25,019] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:54:25,020] {logging_mixin.py:112} INFO - [2020-11-01 23:54:25,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:25,115] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:54:25,618] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:54:25,644] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:25,685] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:54:25,703] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:54:25,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.694 seconds
[2020-11-01 23:54:38,519] {scheduler_job.py:155} INFO - Started process (PID=10850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:38,527] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:54:38,528] {logging_mixin.py:112} INFO - [2020-11-01 23:54:38,528] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:38,616] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:54:39,135] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:54:39,161] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:39,201] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:54:39,219] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:54:39,223] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.703 seconds
[2020-11-01 23:54:52,127] {scheduler_job.py:155} INFO - Started process (PID=10924) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:52,133] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:54:52,134] {logging_mixin.py:112} INFO - [2020-11-01 23:54:52,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:52,234] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:54:52,734] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:54:52,760] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:54:52,799] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:54:52,817] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:54:52,821] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.694 seconds
[2020-11-01 23:55:05,688] {scheduler_job.py:155} INFO - Started process (PID=11000) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:05,695] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:55:05,696] {logging_mixin.py:112} INFO - [2020-11-01 23:55:05,695] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:05,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:55:06,304] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:55:06,330] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:06,370] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:55:06,388] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:55:06,392] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.704 seconds
[2020-11-01 23:55:19,380] {scheduler_job.py:155} INFO - Started process (PID=11075) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:19,388] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:55:19,389] {logging_mixin.py:112} INFO - [2020-11-01 23:55:19,388] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:19,473] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:55:19,995] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:55:20,033] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:20,088] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:55:20,116] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:55:20,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.741 seconds
[2020-11-01 23:55:32,820] {scheduler_job.py:155} INFO - Started process (PID=11148) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:32,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:55:32,825] {logging_mixin.py:112} INFO - [2020-11-01 23:55:32,824] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:32,907] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:55:33,438] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:55:33,467] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:33,506] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:55:33,523] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:55:33,527] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.707 seconds
[2020-11-01 23:55:46,408] {scheduler_job.py:155} INFO - Started process (PID=11224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:46,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:55:46,416] {logging_mixin.py:112} INFO - [2020-11-01 23:55:46,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:46,558] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:55:47,140] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:55:47,166] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:47,205] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:55:47,222] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:55:47,226] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.819 seconds
[2020-11-01 23:55:59,828] {scheduler_job.py:155} INFO - Started process (PID=11298) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:59,831] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:55:59,832] {logging_mixin.py:112} INFO - [2020-11-01 23:55:59,832] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:55:59,923] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:56:00,444] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:56:00,470] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:00,504] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:56:00,521] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:56:00,524] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.696 seconds
[2020-11-01 23:56:13,328] {scheduler_job.py:155} INFO - Started process (PID=11378) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:13,338] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:56:13,339] {logging_mixin.py:112} INFO - [2020-11-01 23:56:13,339] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:13,420] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:56:13,952] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:56:13,978] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:14,017] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:56:14,035] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:56:14,039] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.711 seconds
[2020-11-01 23:56:26,822] {scheduler_job.py:155} INFO - Started process (PID=11448) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:26,826] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:56:26,827] {logging_mixin.py:112} INFO - [2020-11-01 23:56:26,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:26,926] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:56:27,439] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:56:27,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:27,506] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:56:27,523] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:56:27,527] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.705 seconds
[2020-11-01 23:56:40,196] {scheduler_job.py:155} INFO - Started process (PID=11520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:40,203] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:56:40,204] {logging_mixin.py:112} INFO - [2020-11-01 23:56:40,204] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:40,292] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:56:40,801] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:56:40,827] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:40,866] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:56:40,884] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:56:40,887] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.692 seconds
[2020-11-01 23:56:53,799] {scheduler_job.py:155} INFO - Started process (PID=11594) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:53,809] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:56:53,809] {logging_mixin.py:112} INFO - [2020-11-01 23:56:53,809] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:53,893] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:56:54,426] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:56:54,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:56:54,492] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:56:54,510] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:56:54,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.715 seconds
[2020-11-01 23:57:07,135] {scheduler_job.py:155} INFO - Started process (PID=11672) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:07,145] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:57:07,146] {logging_mixin.py:112} INFO - [2020-11-01 23:57:07,146] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:07,257] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:57:07,795] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:57:07,831] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:07,867] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:57:07,889] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:57:07,894] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.759 seconds
[2020-11-01 23:57:20,694] {scheduler_job.py:155} INFO - Started process (PID=11746) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:20,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:57:20,703] {logging_mixin.py:112} INFO - [2020-11-01 23:57:20,702] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:20,801] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:57:21,307] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:57:21,332] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:21,368] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:57:21,386] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:57:21,390] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.696 seconds
[2020-11-01 23:57:33,938] {scheduler_job.py:155} INFO - Started process (PID=11821) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:33,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:57:33,941] {logging_mixin.py:112} INFO - [2020-11-01 23:57:33,941] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:34,021] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:57:34,533] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:57:34,559] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:34,596] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:57:34,616] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:57:34,620] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.682 seconds
[2020-11-01 23:57:47,455] {scheduler_job.py:155} INFO - Started process (PID=11897) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:47,465] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:57:47,467] {logging_mixin.py:112} INFO - [2020-11-01 23:57:47,466] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:47,556] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:57:48,066] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:57:48,093] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:57:48,132] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:57:48,149] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:57:48,153] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.698 seconds
[2020-11-01 23:58:00,882] {scheduler_job.py:155} INFO - Started process (PID=11971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:00,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:58:00,887] {logging_mixin.py:112} INFO - [2020-11-01 23:58:00,887] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:01,017] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:58:01,639] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:58:01,669] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:01,712] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:58:01,731] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:58:01,736] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.854 seconds
[2020-11-01 23:58:14,157] {scheduler_job.py:155} INFO - Started process (PID=12052) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:14,161] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:58:14,161] {logging_mixin.py:112} INFO - [2020-11-01 23:58:14,161] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:14,258] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:58:14,806] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:58:14,836] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:14,877] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:58:14,896] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:58:14,899] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.743 seconds
[2020-11-01 23:58:28,053] {scheduler_job.py:155} INFO - Started process (PID=12130) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:28,217] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:58:28,227] {logging_mixin.py:112} INFO - [2020-11-01 23:58:28,224] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:30,526] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:58:35,737] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:58:35,882] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:36,119] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:58:36,213] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:58:36,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 8.178 seconds
[2020-11-01 23:58:58,894] {scheduler_job.py:155} INFO - Started process (PID=12236) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:58,902] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:58:58,903] {logging_mixin.py:112} INFO - [2020-11-01 23:58:58,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:58:59,157] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:59:00,308] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:59:00,354] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:00,414] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:59:00,442] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:59:00,448] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.554 seconds
[2020-11-01 23:59:12,344] {scheduler_job.py:155} INFO - Started process (PID=12299) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:12,354] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:59:12,357] {logging_mixin.py:112} INFO - [2020-11-01 23:59:12,356] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:12,562] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:59:13,276] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:59:13,307] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:13,350] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:59:13,371] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:59:13,376] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.032 seconds
[2020-11-01 23:59:25,914] {scheduler_job.py:155} INFO - Started process (PID=12355) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:25,923] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:59:25,924] {logging_mixin.py:112} INFO - [2020-11-01 23:59:25,924] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:26,063] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:59:26,890] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:59:26,935] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:26,989] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:59:27,011] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:59:27,016] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.102 seconds
[2020-11-01 23:59:39,498] {scheduler_job.py:155} INFO - Started process (PID=12421) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:39,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:59:39,507] {logging_mixin.py:112} INFO - [2020-11-01 23:59:39,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:39,699] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:59:40,334] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:59:40,367] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:40,409] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:59:40,431] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:59:40,436] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.938 seconds
[2020-11-01 23:59:52,874] {scheduler_job.py:155} INFO - Started process (PID=12487) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:52,878] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-01 23:59:52,879] {logging_mixin.py:112} INFO - [2020-11-01 23:59:52,879] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:52,973] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-01 23:59:53,703] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-01 23:59:53,746] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-01 23:59:53,799] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-01 23:59:53,836] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-01 23:59:53,840] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.966 seconds
[2020-11-02 00:00:06,334] {scheduler_job.py:155} INFO - Started process (PID=12554) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:06,351] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:00:06,353] {logging_mixin.py:112} INFO - [2020-11-02 00:00:06,352] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:06,476] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:00:07,119] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:00:07,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:07,201] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:00:07,249] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-01T22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:00:07,253] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:00:07,395] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:00:07,403] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-01 22:00:00+00:00 [success]> in ORM
[2020-11-02 00:00:07,412] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:00:07,419] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:00:07,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.097 seconds
[2020-11-02 00:00:19,779] {scheduler_job.py:155} INFO - Started process (PID=12643) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:19,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:00:19,791] {logging_mixin.py:112} INFO - [2020-11-02 00:00:19,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:19,945] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:00:20,736] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:00:20,790] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:20,907] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:00:20,936] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:00:21,118] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:00:21,122] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.343 seconds
[2020-11-02 00:00:33,214] {scheduler_job.py:155} INFO - Started process (PID=12730) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:33,224] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:00:33,225] {logging_mixin.py:112} INFO - [2020-11-02 00:00:33,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:33,343] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:00:34,130] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:00:34,171] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:34,222] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:00:34,251] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:00:34,414] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:00:34,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.205 seconds
[2020-11-02 00:00:46,748] {scheduler_job.py:155} INFO - Started process (PID=12791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:46,758] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:00:46,759] {logging_mixin.py:112} INFO - [2020-11-02 00:00:46,758] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:46,894] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:00:47,550] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:00:47,596] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:00:47,667] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:00:47,697] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:00:47,879] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:00:47,883] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.135 seconds
[2020-11-02 00:01:00,279] {scheduler_job.py:155} INFO - Started process (PID=12846) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:00,284] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:01:00,285] {logging_mixin.py:112} INFO - [2020-11-02 00:01:00,285] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:00,388] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:01:01,055] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:01:01,088] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:01,141] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:01:01,176] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:01:01,338] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:01:01,342] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.064 seconds
[2020-11-02 00:01:13,805] {scheduler_job.py:155} INFO - Started process (PID=12901) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:13,812] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:01:13,813] {logging_mixin.py:112} INFO - [2020-11-02 00:01:13,813] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:13,928] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:01:14,829] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:01:14,862] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:14,904] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:01:14,928] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:01:15,099] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:01:15,104] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.299 seconds
[2020-11-02 00:01:27,285] {scheduler_job.py:155} INFO - Started process (PID=12971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:27,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:01:27,293] {logging_mixin.py:112} INFO - [2020-11-02 00:01:27,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:27,400] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:01:28,112] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:01:28,152] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:28,203] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:01:28,227] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:01:28,397] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:01:28,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-02 00:01:40,795] {scheduler_job.py:155} INFO - Started process (PID=13028) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:40,800] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:01:40,801] {logging_mixin.py:112} INFO - [2020-11-02 00:01:40,800] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:40,911] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:01:41,580] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:01:41,626] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:41,694] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:01:41,736] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:01:41,913] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:01:41,917] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.122 seconds
[2020-11-02 00:01:54,228] {scheduler_job.py:155} INFO - Started process (PID=13092) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:54,237] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:01:54,239] {logging_mixin.py:112} INFO - [2020-11-02 00:01:54,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:54,376] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:01:54,994] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:01:55,030] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:01:55,079] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:01:55,099] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:01:55,279] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:01:55,284] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.056 seconds
[2020-11-02 00:02:07,791] {scheduler_job.py:155} INFO - Started process (PID=13149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:07,798] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:02:07,798] {logging_mixin.py:112} INFO - [2020-11-02 00:02:07,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:07,943] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:02:08,724] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:02:08,764] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:08,821] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:02:08,855] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:02:09,039] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:02:09,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.253 seconds
[2020-11-02 00:02:21,315] {scheduler_job.py:155} INFO - Started process (PID=13224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:21,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:02:21,323] {logging_mixin.py:112} INFO - [2020-11-02 00:02:21,322] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:21,428] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:02:22,086] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:02:22,158] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:22,260] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:02:22,300] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:02:22,513] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:02:22,520] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:02:22,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.214 seconds
[2020-11-02 00:02:34,705] {scheduler_job.py:155} INFO - Started process (PID=13309) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:34,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:02:34,716] {logging_mixin.py:112} INFO - [2020-11-02 00:02:34,715] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:34,873] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:02:35,546] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:02:35,600] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:35,673] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:02:35,713] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:02:35,948] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:02:35,956] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:02:35,971] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.266 seconds
[2020-11-02 00:02:48,039] {scheduler_job.py:155} INFO - Started process (PID=13394) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:48,047] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:02:48,047] {logging_mixin.py:112} INFO - [2020-11-02 00:02:48,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:48,179] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:02:48,797] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:02:48,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:02:48,878] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:02:48,900] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:02:49,069] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:02:49,075] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.035 seconds
[2020-11-02 00:03:01,478] {scheduler_job.py:155} INFO - Started process (PID=13457) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:01,482] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:03:01,482] {logging_mixin.py:112} INFO - [2020-11-02 00:03:01,482] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:01,578] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:03:02,210] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:03:02,245] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:02,292] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:03:02,313] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:03:02,471] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:03:02,476] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.998 seconds
[2020-11-02 00:03:14,954] {scheduler_job.py:155} INFO - Started process (PID=13524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:14,965] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:03:14,966] {logging_mixin.py:112} INFO - [2020-11-02 00:03:14,966] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:15,203] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:03:16,009] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:03:16,052] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:16,102] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:03:16,122] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:03:16,272] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:03:16,279] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.325 seconds
[2020-11-02 00:03:28,629] {scheduler_job.py:155} INFO - Started process (PID=13606) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:28,635] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:03:28,637] {logging_mixin.py:112} INFO - [2020-11-02 00:03:28,636] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:28,758] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:03:29,589] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:03:29,635] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:29,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:03:29,737] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:03:29,960] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:03:29,967] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.338 seconds
[2020-11-02 00:03:42,135] {scheduler_job.py:155} INFO - Started process (PID=13671) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:42,142] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:03:42,143] {logging_mixin.py:112} INFO - [2020-11-02 00:03:42,143] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:42,259] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:03:42,924] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:03:42,958] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:43,010] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:03:43,035] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:03:43,213] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:03:43,217] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.082 seconds
[2020-11-02 00:03:55,805] {scheduler_job.py:155} INFO - Started process (PID=13733) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:55,815] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:03:55,817] {logging_mixin.py:112} INFO - [2020-11-02 00:03:55,816] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:55,927] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:03:56,537] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:03:56,569] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:03:56,616] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:03:56,635] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:03:56,843] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:03:56,849] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.044 seconds
[2020-11-02 00:04:09,313] {scheduler_job.py:155} INFO - Started process (PID=13795) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:09,323] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:04:09,324] {logging_mixin.py:112} INFO - [2020-11-02 00:04:09,324] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:09,449] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:04:10,115] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:04:10,147] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:10,193] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:04:10,212] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:04:10,381] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:04:10,386] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.073 seconds
[2020-11-02 00:04:22,940] {scheduler_job.py:155} INFO - Started process (PID=13859) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:22,947] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:04:22,948] {logging_mixin.py:112} INFO - [2020-11-02 00:04:22,948] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:23,122] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:04:23,780] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:04:23,836] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:23,894] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:04:23,921] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:04:24,142] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:04:24,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.208 seconds
[2020-11-02 00:04:36,442] {scheduler_job.py:155} INFO - Started process (PID=13922) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:36,449] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:04:36,450] {logging_mixin.py:112} INFO - [2020-11-02 00:04:36,450] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:36,573] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:04:37,205] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:04:37,236] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:37,281] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:04:37,300] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:04:37,502] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:04:37,510] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:04:37,520] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-02 00:04:49,865] {scheduler_job.py:155} INFO - Started process (PID=14012) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:49,869] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:04:49,870] {logging_mixin.py:112} INFO - [2020-11-02 00:04:49,870] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:49,979] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:04:50,682] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:04:50,713] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:04:50,764] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:04:50,798] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:04:51,023] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:04:51,034] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:04:51,048] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.184 seconds
[2020-11-02 00:05:03,237] {scheduler_job.py:155} INFO - Started process (PID=14097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:03,244] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:05:03,246] {logging_mixin.py:112} INFO - [2020-11-02 00:05:03,245] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:03,411] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:05:04,111] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:05:04,143] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:04,225] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:05:04,257] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:05:04,413] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:05:04,416] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.180 seconds
[2020-11-02 00:05:16,803] {scheduler_job.py:155} INFO - Started process (PID=14158) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:16,814] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:05:16,815] {logging_mixin.py:112} INFO - [2020-11-02 00:05:16,815] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:16,934] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:05:17,513] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:05:17,543] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:17,679] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:05:17,699] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:05:17,757] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:05:17,764] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.961 seconds
[2020-11-02 00:05:30,064] {scheduler_job.py:155} INFO - Started process (PID=14230) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:30,068] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:05:30,068] {logging_mixin.py:112} INFO - [2020-11-02 00:05:30,068] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:30,197] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:05:30,921] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:05:30,954] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:31,098] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:05:31,120] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:05:31,181] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:05:31,184] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.120 seconds
[2020-11-02 00:05:43,658] {scheduler_job.py:155} INFO - Started process (PID=14297) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:43,669] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:05:43,670] {logging_mixin.py:112} INFO - [2020-11-02 00:05:43,669] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:43,775] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:05:44,572] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:05:44,603] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:44,751] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:05:44,784] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:05:44,888] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:05:44,894] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.236 seconds
[2020-11-02 00:05:57,118] {scheduler_job.py:155} INFO - Started process (PID=14383) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:57,128] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:05:57,130] {logging_mixin.py:112} INFO - [2020-11-02 00:05:57,129] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:57,240] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:05:57,865] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:05:57,895] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:05:58,029] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:05:58,048] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:05:58,112] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:05:58,116] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.999 seconds
[2020-11-02 00:06:10,638] {scheduler_job.py:155} INFO - Started process (PID=14444) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:10,650] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:06:10,656] {logging_mixin.py:112} INFO - [2020-11-02 00:06:10,651] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:10,857] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:06:11,451] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:06:11,482] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:11,623] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:06:11,643] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:06:11,700] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:06:11,704] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.066 seconds
[2020-11-02 00:06:23,997] {scheduler_job.py:155} INFO - Started process (PID=14509) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:24,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:06:24,003] {logging_mixin.py:112} INFO - [2020-11-02 00:06:24,003] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:24,094] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:06:24,701] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:06:24,733] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:24,930] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:06:24,951] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:06:25,020] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:06:25,024] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.027 seconds
[2020-11-02 00:06:37,493] {scheduler_job.py:155} INFO - Started process (PID=14576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:37,515] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:06:37,518] {logging_mixin.py:112} INFO - [2020-11-02 00:06:37,517] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:37,714] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:06:38,381] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:06:38,411] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:38,630] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:06:38,651] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:06:38,743] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:06:38,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.257 seconds
[2020-11-02 00:06:51,054] {scheduler_job.py:155} INFO - Started process (PID=14639) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:51,079] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:06:51,085] {logging_mixin.py:112} INFO - [2020-11-02 00:06:51,080] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:51,210] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:06:51,860] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:06:51,906] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:06:52,061] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:06:52,081] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:06:52,149] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:06:52,156] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:06:52,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.113 seconds
[2020-11-02 00:07:04,623] {scheduler_job.py:155} INFO - Started process (PID=14724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:04,634] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:07:04,635] {logging_mixin.py:112} INFO - [2020-11-02 00:07:04,635] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:04,767] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:07:05,373] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:07:05,425] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:05,647] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:07:05,678] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:07:05,755] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:07:05,765] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-01 22:00:00+00:00 [scheduled]> in ORM
[2020-11-02 00:07:05,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-02 00:07:17,979] {scheduler_job.py:155} INFO - Started process (PID=14811) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:17,989] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:07:17,990] {logging_mixin.py:112} INFO - [2020-11-02 00:07:17,990] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:18,115] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:07:18,723] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:07:18,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:18,883] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:07:18,904] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False>
[2020-11-02 00:07:18,920] {logging_mixin.py:112} INFO - [2020-11-02 00:07:18,920] {dagrun.py:311} INFO - Marking run <DagRun S3_task @ 2020-11-01 22:00:00+00:00: scheduled__2020-11-01T22:00:00+00:00, externally triggered: False> failed
[2020-11-02 00:07:18,926] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:07:18,931] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.952 seconds
[2020-11-02 00:07:31,554] {scheduler_job.py:155} INFO - Started process (PID=14875) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:31,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:07:31,568] {logging_mixin.py:112} INFO - [2020-11-02 00:07:31,568] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:31,800] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:07:32,438] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:07:32,468] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:32,607] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:07:32,649] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:07:32,655] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.101 seconds
[2020-11-02 00:07:45,160] {scheduler_job.py:155} INFO - Started process (PID=14940) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:45,168] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:07:45,169] {logging_mixin.py:112} INFO - [2020-11-02 00:07:45,169] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:45,275] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:07:45,877] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:07:45,907] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:46,046] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:07:46,067] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:07:46,072] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.912 seconds
[2020-11-02 00:07:58,840] {scheduler_job.py:155} INFO - Started process (PID=15002) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:58,852] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:07:58,854] {logging_mixin.py:112} INFO - [2020-11-02 00:07:58,853] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:58,975] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:07:59,581] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:07:59,627] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:07:59,815] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:07:59,856] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:07:59,866] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.026 seconds
[2020-11-02 00:08:12,231] {scheduler_job.py:155} INFO - Started process (PID=15068) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:12,239] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:08:12,240] {logging_mixin.py:112} INFO - [2020-11-02 00:08:12,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:12,405] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:08:13,036] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:08:13,067] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:13,201] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:08:13,223] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:08:13,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.995 seconds
[2020-11-02 00:08:25,879] {scheduler_job.py:155} INFO - Started process (PID=15129) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:25,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:08:25,884] {logging_mixin.py:112} INFO - [2020-11-02 00:08:25,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:25,989] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:08:26,580] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:08:26,615] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:26,803] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:08:26,828] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:08:26,834] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.955 seconds
[2020-11-02 00:08:39,455] {scheduler_job.py:155} INFO - Started process (PID=15193) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:39,465] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:08:39,467] {logging_mixin.py:112} INFO - [2020-11-02 00:08:39,467] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:39,596] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:08:40,187] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:08:40,218] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:40,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:08:40,399] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:08:40,404] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.949 seconds
[2020-11-02 00:08:52,965] {scheduler_job.py:155} INFO - Started process (PID=15253) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:52,973] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:08:52,974] {logging_mixin.py:112} INFO - [2020-11-02 00:08:52,974] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:53,089] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:08:53,753] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:08:53,799] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:08:53,974] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:08:53,995] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:08:53,999] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.035 seconds
[2020-11-02 00:09:06,607] {scheduler_job.py:155} INFO - Started process (PID=15316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:06,624] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:09:06,626] {logging_mixin.py:112} INFO - [2020-11-02 00:09:06,626] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:06,774] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:09:07,343] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:09:07,380] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:07,571] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:09:07,608] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:09:07,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.011 seconds
[2020-11-02 00:09:20,164] {scheduler_job.py:155} INFO - Started process (PID=15381) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:20,181] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:09:20,182] {logging_mixin.py:112} INFO - [2020-11-02 00:09:20,182] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:20,309] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:09:21,010] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:09:21,067] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:21,286] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:09:21,326] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:09:21,334] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.170 seconds
[2020-11-02 00:09:33,877] {scheduler_job.py:155} INFO - Started process (PID=15436) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:33,885] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:09:33,886] {logging_mixin.py:112} INFO - [2020-11-02 00:09:33,885] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:34,018] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:09:34,675] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:09:34,712] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:34,848] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:09:34,872] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:09:34,879] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.002 seconds
[2020-11-02 00:09:47,335] {scheduler_job.py:155} INFO - Started process (PID=15498) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:47,339] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:09:47,340] {logging_mixin.py:112} INFO - [2020-11-02 00:09:47,340] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:47,450] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:09:48,197] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:09:48,231] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:09:48,370] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:09:48,388] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:09:48,391] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.057 seconds
[2020-11-02 00:10:01,153] {scheduler_job.py:155} INFO - Started process (PID=15558) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:01,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:10:01,166] {logging_mixin.py:112} INFO - [2020-11-02 00:10:01,166] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:01,280] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:10:01,852] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:10:01,890] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:02,024] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:10:02,045] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:10:02,049] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.896 seconds
[2020-11-02 00:10:14,697] {scheduler_job.py:155} INFO - Started process (PID=15622) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:14,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:10:14,703] {logging_mixin.py:112} INFO - [2020-11-02 00:10:14,703] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:14,858] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:10:15,573] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:10:15,604] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:15,829] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:10:15,851] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:10:15,856] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.159 seconds
[2020-11-02 00:10:28,407] {scheduler_job.py:155} INFO - Started process (PID=15685) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:28,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:10:28,414] {logging_mixin.py:112} INFO - [2020-11-02 00:10:28,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:28,514] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:10:29,139] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:10:29,171] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:29,327] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:10:29,347] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:10:29,351] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.944 seconds
[2020-11-02 00:10:41,897] {scheduler_job.py:155} INFO - Started process (PID=15745) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:41,902] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:10:41,903] {logging_mixin.py:112} INFO - [2020-11-02 00:10:41,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:42,008] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:10:42,665] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:10:42,697] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:42,842] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:10:42,863] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:10:42,867] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.970 seconds
[2020-11-02 00:10:55,619] {scheduler_job.py:155} INFO - Started process (PID=15807) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:55,631] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:10:55,633] {logging_mixin.py:112} INFO - [2020-11-02 00:10:55,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:55,798] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:10:56,385] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:10:56,419] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:10:56,578] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:10:56,598] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:10:56,602] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.983 seconds
[2020-11-02 00:11:09,099] {scheduler_job.py:155} INFO - Started process (PID=15868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:09,112] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:11:09,114] {logging_mixin.py:112} INFO - [2020-11-02 00:11:09,114] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:09,245] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:11:09,837] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:11:09,871] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:10,005] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:11:10,024] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:11:10,028] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.929 seconds
[2020-11-02 00:11:22,693] {scheduler_job.py:155} INFO - Started process (PID=15931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:22,715] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:11:22,718] {logging_mixin.py:112} INFO - [2020-11-02 00:11:22,717] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:22,872] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:11:23,448] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:11:23,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:23,669] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:11:23,691] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:11:23,696] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.003 seconds
[2020-11-02 00:11:36,025] {scheduler_job.py:155} INFO - Started process (PID=15998) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:36,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:11:36,030] {logging_mixin.py:112} INFO - [2020-11-02 00:11:36,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:36,131] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:11:36,744] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:11:36,789] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:36,919] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:11:36,940] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:11:36,948] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.923 seconds
[2020-11-02 00:11:49,655] {scheduler_job.py:155} INFO - Started process (PID=16066) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:49,668] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:11:49,670] {logging_mixin.py:112} INFO - [2020-11-02 00:11:49,670] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:49,833] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:11:50,414] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:11:50,449] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:11:50,580] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:11:50,600] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:11:50,605] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.950 seconds
[2020-11-02 00:12:03,286] {scheduler_job.py:155} INFO - Started process (PID=16140) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:03,298] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:12:03,300] {logging_mixin.py:112} INFO - [2020-11-02 00:12:03,299] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:03,498] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:12:04,120] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:12:04,167] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:04,307] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:12:04,327] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:12:04,331] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.046 seconds
[2020-11-02 00:12:16,894] {scheduler_job.py:155} INFO - Started process (PID=16209) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:16,899] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:12:16,900] {logging_mixin.py:112} INFO - [2020-11-02 00:12:16,900] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:17,009] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:12:17,692] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:12:17,725] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:17,855] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:12:17,875] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:12:17,879] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.985 seconds
[2020-11-02 00:12:30,698] {scheduler_job.py:155} INFO - Started process (PID=16268) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:30,713] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:12:30,715] {logging_mixin.py:112} INFO - [2020-11-02 00:12:30,714] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:30,868] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:12:31,458] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:12:31,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:31,622] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:12:31,643] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:12:31,648] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.951 seconds
[2020-11-02 00:12:44,142] {scheduler_job.py:155} INFO - Started process (PID=16335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:44,151] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:12:44,153] {logging_mixin.py:112} INFO - [2020-11-02 00:12:44,152] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:44,270] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:12:44,948] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:12:44,981] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:45,139] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:12:45,164] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:12:45,169] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.028 seconds
[2020-11-02 00:12:57,889] {scheduler_job.py:155} INFO - Started process (PID=16404) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:57,895] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:12:57,896] {logging_mixin.py:112} INFO - [2020-11-02 00:12:57,896] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:58,013] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:12:58,661] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:12:58,693] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:12:58,823] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:12:58,844] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:12:58,848] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.960 seconds
[2020-11-02 00:13:11,494] {scheduler_job.py:155} INFO - Started process (PID=16462) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:11,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:13:11,515] {logging_mixin.py:112} INFO - [2020-11-02 00:13:11,514] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:11,716] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:13:12,343] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:13:12,375] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:12,555] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:13:12,585] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:13:12,591] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.098 seconds
[2020-11-02 00:13:24,975] {scheduler_job.py:155} INFO - Started process (PID=16522) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:24,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:13:24,980] {logging_mixin.py:112} INFO - [2020-11-02 00:13:24,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:25,090] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:13:25,658] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:13:25,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:25,839] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:13:25,864] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:13:25,868] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.893 seconds
[2020-11-02 00:13:47,707] {scheduler_job.py:155} INFO - Started process (PID=16589) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:47,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:13:47,744] {logging_mixin.py:112} INFO - [2020-11-02 00:13:47,744] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:48,210] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:13:50,032] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:13:50,127] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:50,422] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:13:50,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:13:50,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.775 seconds
[2020-11-02 00:13:56,100] {scheduler_job.py:155} INFO - Started process (PID=16643) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:56,106] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:13:56,107] {logging_mixin.py:112} INFO - [2020-11-02 00:13:56,107] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:56,280] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:13:57,492] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:13:57,555] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:13:57,742] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:13:57,775] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:13:57,782] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.681 seconds
[2020-11-02 00:14:03,287] {scheduler_job.py:155} INFO - Started process (PID=16697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:03,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:03,291] {logging_mixin.py:112} INFO - [2020-11-02 00:14:03,291] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:03,388] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:04,129] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:04,161] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:04,289] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:04,309] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:04,313] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.026 seconds
[2020-11-02 00:14:11,086] {scheduler_job.py:155} INFO - Started process (PID=16752) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:11,090] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:11,091] {logging_mixin.py:112} INFO - [2020-11-02 00:14:11,090] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:11,209] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:12,187] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:12,226] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:12,366] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:12,387] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:12,392] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.306 seconds
[2020-11-02 00:14:18,515] {scheduler_job.py:155} INFO - Started process (PID=16826) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:18,519] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:18,519] {logging_mixin.py:112} INFO - [2020-11-02 00:14:18,519] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:18,715] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:19,586] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:19,645] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:19,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:19,920] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:19,927] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.412 seconds
[2020-11-02 00:14:26,332] {scheduler_job.py:155} INFO - Started process (PID=16881) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:26,338] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:26,338] {logging_mixin.py:112} INFO - [2020-11-02 00:14:26,338] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:26,458] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:27,246] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:27,293] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:27,464] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:27,489] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:27,495] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.163 seconds
[2020-11-02 00:14:33,764] {scheduler_job.py:155} INFO - Started process (PID=16936) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:33,768] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:33,768] {logging_mixin.py:112} INFO - [2020-11-02 00:14:33,768] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:33,893] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:34,834] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:34,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:35,036] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:35,061] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:35,065] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.302 seconds
[2020-11-02 00:14:41,654] {scheduler_job.py:155} INFO - Started process (PID=16991) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:41,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:41,659] {logging_mixin.py:112} INFO - [2020-11-02 00:14:41,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:41,821] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:42,736] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:42,781] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:42,987] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:43,023] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:43,028] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.374 seconds
[2020-11-02 00:14:49,012] {scheduler_job.py:155} INFO - Started process (PID=17047) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:49,027] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:49,028] {logging_mixin.py:112} INFO - [2020-11-02 00:14:49,027] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:49,138] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:49,753] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:49,795] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:49,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:49,950] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:49,955] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.943 seconds
[2020-11-02 00:14:56,136] {scheduler_job.py:155} INFO - Started process (PID=17104) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:56,139] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:14:56,140] {logging_mixin.py:112} INFO - [2020-11-02 00:14:56,140] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:56,234] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:14:56,834] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:14:56,880] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:14:57,069] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:14:57,099] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:14:57,105] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.969 seconds
[2020-11-02 00:15:04,098] {scheduler_job.py:155} INFO - Started process (PID=17172) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:04,121] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:15:04,121] {logging_mixin.py:112} INFO - [2020-11-02 00:15:04,121] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:04,235] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:15:04,901] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:15:04,969] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:05,152] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:15:05,173] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:15:05,177] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.079 seconds
[2020-11-02 00:15:11,567] {scheduler_job.py:155} INFO - Started process (PID=17227) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:11,575] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:15:11,576] {logging_mixin.py:112} INFO - [2020-11-02 00:15:11,575] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:11,779] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:15:12,463] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:15:12,499] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:12,656] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:15:12,676] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:15:12,681] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.114 seconds
[2020-11-02 00:15:19,665] {scheduler_job.py:155} INFO - Started process (PID=17285) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:19,677] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:15:19,679] {logging_mixin.py:112} INFO - [2020-11-02 00:15:19,678] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:19,800] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:15:20,556] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:15:20,693] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:20,737] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:15:20,769] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:15:20,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.109 seconds
[2020-11-02 00:15:34,171] {scheduler_job.py:155} INFO - Started process (PID=17347) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:34,174] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:15:34,175] {logging_mixin.py:112} INFO - [2020-11-02 00:15:34,175] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:34,288] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:15:34,897] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:15:35,021] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:35,071] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:15:35,093] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:15:35,098] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.927 seconds
[2020-11-02 00:15:47,464] {scheduler_job.py:155} INFO - Started process (PID=17422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:47,479] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:15:47,480] {logging_mixin.py:112} INFO - [2020-11-02 00:15:47,479] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:47,696] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:15:48,505] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:15:48,708] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:15:48,775] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:15:48,803] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:15:48,807] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.343 seconds
[2020-11-02 00:16:01,124] {scheduler_job.py:155} INFO - Started process (PID=17479) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:01,128] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:16:01,130] {logging_mixin.py:112} INFO - [2020-11-02 00:16:01,129] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:01,239] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:16:01,854] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:16:01,979] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:02,023] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:16:02,048] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:16:02,054] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.930 seconds
[2020-11-02 00:16:14,563] {scheduler_job.py:155} INFO - Started process (PID=17538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:14,569] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:16:14,570] {logging_mixin.py:112} INFO - [2020-11-02 00:16:14,570] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:14,736] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:16:15,417] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:16:15,556] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:15,601] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:16:15,620] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:16:15,624] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-02 00:16:28,131] {scheduler_job.py:155} INFO - Started process (PID=17598) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:28,136] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:16:28,137] {logging_mixin.py:112} INFO - [2020-11-02 00:16:28,137] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:28,266] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:16:28,909] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:16:29,040] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:29,084] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:16:29,109] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:16:29,114] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.983 seconds
[2020-11-02 00:16:41,418] {scheduler_job.py:155} INFO - Started process (PID=17658) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:41,447] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:16:41,449] {logging_mixin.py:112} INFO - [2020-11-02 00:16:41,449] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:41,593] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:16:42,420] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:16:42,556] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:42,601] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:16:42,622] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:16:42,626] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.209 seconds
[2020-11-02 00:16:54,838] {scheduler_job.py:155} INFO - Started process (PID=17715) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:54,842] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:16:54,843] {logging_mixin.py:112} INFO - [2020-11-02 00:16:54,843] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:54,962] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:16:55,598] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:16:55,783] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:16:55,854] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:16:55,890] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:16:55,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.057 seconds
[2020-11-02 00:17:08,244] {scheduler_job.py:155} INFO - Started process (PID=17773) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:08,248] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:17:08,248] {logging_mixin.py:112} INFO - [2020-11-02 00:17:08,248] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:08,357] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:17:09,099] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:17:09,254] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:09,307] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:17:09,328] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:17:09,332] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.088 seconds
[2020-11-02 00:17:21,512] {scheduler_job.py:155} INFO - Started process (PID=17856) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:21,521] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:17:21,522] {logging_mixin.py:112} INFO - [2020-11-02 00:17:21,522] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:21,665] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:17:22,505] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:17:22,637] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:22,678] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:17:22,696] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:17:22,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.188 seconds
[2020-11-02 00:17:35,058] {scheduler_job.py:155} INFO - Started process (PID=17914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:35,064] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:17:35,069] {logging_mixin.py:112} INFO - [2020-11-02 00:17:35,069] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:35,354] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:17:36,322] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:17:36,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:36,526] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:17:36,558] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:17:36,564] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.506 seconds
[2020-11-02 00:17:48,386] {scheduler_job.py:155} INFO - Started process (PID=17990) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:48,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:17:48,392] {logging_mixin.py:112} INFO - [2020-11-02 00:17:48,391] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:48,551] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:17:49,238] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:17:49,363] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:17:49,422] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:17:49,455] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:17:49,464] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-02 00:18:01,786] {scheduler_job.py:155} INFO - Started process (PID=18044) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:01,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:18:01,796] {logging_mixin.py:112} INFO - [2020-11-02 00:18:01,796] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:01,973] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:18:02,693] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:18:02,820] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:02,880] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:18:02,917] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:18:02,930] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.144 seconds
[2020-11-02 00:18:15,196] {scheduler_job.py:155} INFO - Started process (PID=18112) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:15,199] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:18:15,200] {logging_mixin.py:112} INFO - [2020-11-02 00:18:15,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:15,309] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:18:16,013] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:18:16,172] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:16,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:18:16,246] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:18:16,250] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.054 seconds
[2020-11-02 00:18:28,600] {scheduler_job.py:155} INFO - Started process (PID=18173) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:28,603] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:18:28,604] {logging_mixin.py:112} INFO - [2020-11-02 00:18:28,603] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:28,712] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:18:29,459] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:18:29,586] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:29,634] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:18:29,674] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:18:29,681] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.081 seconds
[2020-11-02 00:18:41,987] {scheduler_job.py:155} INFO - Started process (PID=18237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:42,031] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:18:42,037] {logging_mixin.py:112} INFO - [2020-11-02 00:18:42,037] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:42,217] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:18:43,163] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:18:43,399] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:43,444] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:18:43,468] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:18:43,473] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.487 seconds
[2020-11-02 00:18:55,314] {scheduler_job.py:155} INFO - Started process (PID=18296) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:55,317] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:18:55,318] {logging_mixin.py:112} INFO - [2020-11-02 00:18:55,317] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:55,491] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:18:56,451] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:18:56,593] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:18:56,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:18:56,668] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:18:56,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.363 seconds
[2020-11-02 00:19:08,703] {scheduler_job.py:155} INFO - Started process (PID=18359) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:08,710] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:19:08,712] {logging_mixin.py:112} INFO - [2020-11-02 00:19:08,711] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:08,867] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:19:09,563] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:19:09,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:09,735] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:19:09,757] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:19:09,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.059 seconds
[2020-11-02 00:19:22,239] {scheduler_job.py:155} INFO - Started process (PID=18419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:22,245] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:19:22,246] {logging_mixin.py:112} INFO - [2020-11-02 00:19:22,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:22,465] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:19:23,476] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:19:23,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:23,738] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:19:23,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:19:23,782] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.543 seconds
[2020-11-02 00:19:35,698] {scheduler_job.py:155} INFO - Started process (PID=18477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:35,709] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:19:35,711] {logging_mixin.py:112} INFO - [2020-11-02 00:19:35,710] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:35,844] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:19:36,588] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:19:36,619] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:36,659] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:19:36,677] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:19:36,681] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.983 seconds
[2020-11-02 00:19:49,044] {scheduler_job.py:155} INFO - Started process (PID=18536) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:49,049] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:19:49,050] {logging_mixin.py:112} INFO - [2020-11-02 00:19:49,050] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:49,185] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:19:49,953] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:19:49,985] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:19:50,029] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:19:50,056] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:19:50,061] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.018 seconds
[2020-11-02 00:20:02,307] {scheduler_job.py:155} INFO - Started process (PID=18595) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:02,311] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:20:02,311] {logging_mixin.py:112} INFO - [2020-11-02 00:20:02,311] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:02,446] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:20:03,350] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:20:03,381] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:03,434] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:20:03,458] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:20:03,467] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.159 seconds
[2020-11-02 00:20:15,787] {scheduler_job.py:155} INFO - Started process (PID=18654) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:15,797] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:20:15,798] {logging_mixin.py:112} INFO - [2020-11-02 00:20:15,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:15,957] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:20:16,736] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:20:16,771] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:16,819] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:20:16,841] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:20:16,845] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.058 seconds
[2020-11-02 00:20:29,103] {scheduler_job.py:155} INFO - Started process (PID=18720) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:29,112] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:20:29,119] {logging_mixin.py:112} INFO - [2020-11-02 00:20:29,119] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:29,457] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:20:30,657] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:20:30,686] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:30,737] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:20:30,755] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:20:30,760] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.657 seconds
[2020-11-02 00:20:42,472] {scheduler_job.py:155} INFO - Started process (PID=18784) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:42,481] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:20:42,482] {logging_mixin.py:112} INFO - [2020-11-02 00:20:42,482] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:42,593] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:20:43,435] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:20:43,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:43,513] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:20:43,532] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:20:43,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.064 seconds
[2020-11-02 00:20:55,968] {scheduler_job.py:155} INFO - Started process (PID=18843) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:55,975] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:20:55,977] {logging_mixin.py:112} INFO - [2020-11-02 00:20:55,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:56,147] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:20:56,931] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:20:56,965] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:20:57,008] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:20:57,025] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:20:57,029] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-02 00:21:09,371] {scheduler_job.py:155} INFO - Started process (PID=18902) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:09,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:21:09,378] {logging_mixin.py:112} INFO - [2020-11-02 00:21:09,378] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:09,504] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:21:10,268] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:21:10,300] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:10,345] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:21:10,367] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:21:10,372] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.001 seconds
[2020-11-02 00:21:22,945] {scheduler_job.py:155} INFO - Started process (PID=18962) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:22,953] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:21:22,955] {logging_mixin.py:112} INFO - [2020-11-02 00:21:22,954] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:23,121] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:21:23,932] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:21:23,964] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:24,018] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:21:24,043] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:21:24,047] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.102 seconds
[2020-11-02 00:21:36,282] {scheduler_job.py:155} INFO - Started process (PID=19022) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:36,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:21:36,292] {logging_mixin.py:112} INFO - [2020-11-02 00:21:36,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:36,453] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:21:37,211] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:21:37,250] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:37,340] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:21:37,380] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:21:37,385] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-02 00:21:49,718] {scheduler_job.py:155} INFO - Started process (PID=19088) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:49,745] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:21:49,747] {logging_mixin.py:112} INFO - [2020-11-02 00:21:49,746] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:49,962] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:21:50,962] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:21:51,002] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:21:51,054] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:21:51,077] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:21:51,083] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.365 seconds
[2020-11-02 00:22:02,986] {scheduler_job.py:155} INFO - Started process (PID=19141) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:02,993] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:22:02,994] {logging_mixin.py:112} INFO - [2020-11-02 00:22:02,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:03,188] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:22:04,390] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:22:04,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:04,489] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:22:04,547] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:22:04,553] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.567 seconds
[2020-11-02 00:22:16,328] {scheduler_job.py:155} INFO - Started process (PID=19200) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:16,332] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:22:16,333] {logging_mixin.py:112} INFO - [2020-11-02 00:22:16,333] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:16,464] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:22:17,317] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:22:17,349] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:17,387] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:22:17,406] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:22:17,411] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.083 seconds
[2020-11-02 00:22:29,836] {scheduler_job.py:155} INFO - Started process (PID=19261) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:29,841] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:22:29,847] {logging_mixin.py:112} INFO - [2020-11-02 00:22:29,847] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:29,985] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:22:30,770] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:22:30,821] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:30,872] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:22:30,892] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:22:30,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-02 00:22:43,314] {scheduler_job.py:155} INFO - Started process (PID=19331) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:43,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:22:43,320] {logging_mixin.py:112} INFO - [2020-11-02 00:22:43,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:43,435] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:22:44,259] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:22:44,290] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:44,338] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:22:44,365] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:22:44,369] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.055 seconds
[2020-11-02 00:22:56,689] {scheduler_job.py:155} INFO - Started process (PID=19390) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:56,693] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:22:56,694] {logging_mixin.py:112} INFO - [2020-11-02 00:22:56,694] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:56,820] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:22:57,619] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:22:57,648] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:22:57,706] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:22:57,725] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:22:57,730] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.041 seconds
[2020-11-02 00:23:10,139] {scheduler_job.py:155} INFO - Started process (PID=19454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:10,150] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:23:10,151] {logging_mixin.py:112} INFO - [2020-11-02 00:23:10,150] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:10,262] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:23:11,054] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:23:11,085] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:11,126] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:23:11,148] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:23:11,159] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.020 seconds
[2020-11-02 00:23:23,493] {scheduler_job.py:155} INFO - Started process (PID=19542) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:23,503] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:23:23,504] {logging_mixin.py:112} INFO - [2020-11-02 00:23:23,504] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:23,683] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:23:24,483] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:23:24,547] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:24,603] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:23:24,623] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:23:24,627] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.134 seconds
[2020-11-02 00:23:36,909] {scheduler_job.py:155} INFO - Started process (PID=19606) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:36,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:23:36,915] {logging_mixin.py:112} INFO - [2020-11-02 00:23:36,915] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:37,029] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:23:37,854] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:23:37,884] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:37,930] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:23:37,951] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:23:37,955] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.046 seconds
[2020-11-02 00:23:50,412] {scheduler_job.py:155} INFO - Started process (PID=19664) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:50,439] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:23:50,440] {logging_mixin.py:112} INFO - [2020-11-02 00:23:50,440] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:50,632] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:23:51,504] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:23:51,541] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:23:51,593] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:23:51,622] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:23:51,628] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.216 seconds
[2020-11-02 00:24:03,744] {scheduler_job.py:155} INFO - Started process (PID=19727) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:03,782] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:24:03,784] {logging_mixin.py:112} INFO - [2020-11-02 00:24:03,783] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:04,040] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:24:05,322] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:24:05,360] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:05,426] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:24:05,450] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:24:05,453] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.710 seconds
[2020-11-02 00:24:17,345] {scheduler_job.py:155} INFO - Started process (PID=19788) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:17,352] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:24:17,353] {logging_mixin.py:112} INFO - [2020-11-02 00:24:17,353] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:17,459] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:24:18,145] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:24:18,175] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:18,217] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:24:18,237] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:24:18,241] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.896 seconds
[2020-11-02 00:24:30,597] {scheduler_job.py:155} INFO - Started process (PID=19850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:30,608] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:24:30,611] {logging_mixin.py:112} INFO - [2020-11-02 00:24:30,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:30,801] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:24:31,558] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:24:31,595] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:31,659] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:24:31,700] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:24:31,714] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-02 00:24:44,051] {scheduler_job.py:155} INFO - Started process (PID=19911) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:44,060] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:24:44,062] {logging_mixin.py:112} INFO - [2020-11-02 00:24:44,061] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:44,240] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:24:45,017] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:24:45,061] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:45,113] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:24:45,134] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:24:45,138] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.087 seconds
[2020-11-02 00:24:57,484] {scheduler_job.py:155} INFO - Started process (PID=19969) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:57,488] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:24:57,489] {logging_mixin.py:112} INFO - [2020-11-02 00:24:57,489] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:57,593] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:24:58,339] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:24:58,374] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:24:58,424] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:24:58,445] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:24:58,449] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.965 seconds
[2020-11-02 00:25:10,991] {scheduler_job.py:155} INFO - Started process (PID=20032) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:11,009] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:25:11,011] {logging_mixin.py:112} INFO - [2020-11-02 00:25:11,011] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:11,158] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:25:11,882] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:25:11,911] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:11,952] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:25:11,971] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:25:11,982] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.992 seconds
[2020-11-02 00:25:24,473] {scheduler_job.py:155} INFO - Started process (PID=20093) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:24,477] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:25:24,478] {logging_mixin.py:112} INFO - [2020-11-02 00:25:24,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:24,607] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:25:25,387] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:25:25,420] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:25,470] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:25:25,502] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:25:25,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.036 seconds
[2020-11-02 00:25:37,845] {scheduler_job.py:155} INFO - Started process (PID=20155) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:37,852] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:25:37,853] {logging_mixin.py:112} INFO - [2020-11-02 00:25:37,852] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:37,995] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:25:38,772] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:25:38,803] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:38,860] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:25:38,883] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:25:38,887] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.042 seconds
[2020-11-02 00:25:51,299] {scheduler_job.py:155} INFO - Started process (PID=20216) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:51,303] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:25:51,304] {logging_mixin.py:112} INFO - [2020-11-02 00:25:51,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:51,423] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:25:52,154] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:25:52,190] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:25:52,234] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:25:52,257] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:25:52,262] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.963 seconds
[2020-11-02 00:26:04,620] {scheduler_job.py:155} INFO - Started process (PID=20270) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:04,625] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:26:04,625] {logging_mixin.py:112} INFO - [2020-11-02 00:26:04,625] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:04,744] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:26:05,482] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:26:05,514] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:05,570] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:26:05,592] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:26:05,596] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.976 seconds
[2020-11-02 00:26:18,384] {scheduler_job.py:155} INFO - Started process (PID=20337) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:18,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:26:18,397] {logging_mixin.py:112} INFO - [2020-11-02 00:26:18,397] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:18,503] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:26:19,412] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:26:19,447] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:19,493] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:26:19,517] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:26:19,521] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.137 seconds
[2020-11-02 00:26:31,801] {scheduler_job.py:155} INFO - Started process (PID=20398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:31,806] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:26:31,806] {logging_mixin.py:112} INFO - [2020-11-02 00:26:31,806] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:31,929] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:26:32,747] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:26:32,778] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:32,825] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:26:32,849] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:26:32,853] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.052 seconds
[2020-11-02 00:26:45,422] {scheduler_job.py:155} INFO - Started process (PID=20453) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:45,432] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:26:45,433] {logging_mixin.py:112} INFO - [2020-11-02 00:26:45,433] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:45,548] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:26:46,333] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:26:46,365] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:46,414] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:26:46,439] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:26:46,443] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.021 seconds
[2020-11-02 00:26:58,684] {scheduler_job.py:155} INFO - Started process (PID=20520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:58,689] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:26:58,690] {logging_mixin.py:112} INFO - [2020-11-02 00:26:58,690] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:58,941] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:26:59,637] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:26:59,670] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:26:59,715] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:26:59,735] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:26:59,741] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.059 seconds
[2020-11-02 00:27:12,143] {scheduler_job.py:155} INFO - Started process (PID=20576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:12,151] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:27:12,152] {logging_mixin.py:112} INFO - [2020-11-02 00:27:12,152] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:12,259] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:27:12,996] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:27:13,026] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:13,067] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:27:13,086] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:27:13,090] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.948 seconds
[2020-11-02 00:27:25,552] {scheduler_job.py:155} INFO - Started process (PID=20637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:25,560] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:27:25,561] {logging_mixin.py:112} INFO - [2020-11-02 00:27:25,560] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:25,717] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:27:26,619] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:27:26,659] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:26,700] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:27:26,722] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:27:26,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.175 seconds
[2020-11-02 00:27:39,050] {scheduler_job.py:155} INFO - Started process (PID=20700) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:39,057] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:27:39,058] {logging_mixin.py:112} INFO - [2020-11-02 00:27:39,058] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:39,236] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:27:40,083] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:27:40,122] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:40,164] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:27:40,187] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:27:40,191] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.141 seconds
[2020-11-02 00:27:52,519] {scheduler_job.py:155} INFO - Started process (PID=20757) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:52,523] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:27:52,524] {logging_mixin.py:112} INFO - [2020-11-02 00:27:52,523] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:52,726] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:27:53,515] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:27:53,556] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:27:53,597] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:27:53,617] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:27:53,621] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.102 seconds
[2020-11-02 00:28:05,736] {scheduler_job.py:155} INFO - Started process (PID=20816) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:05,741] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:28:05,742] {logging_mixin.py:112} INFO - [2020-11-02 00:28:05,742] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:05,883] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:28:06,725] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:28:06,758] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:06,805] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:28:06,826] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:28:06,831] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.095 seconds
[2020-11-02 00:28:19,342] {scheduler_job.py:155} INFO - Started process (PID=20871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:19,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:28:19,369] {logging_mixin.py:112} INFO - [2020-11-02 00:28:19,368] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:19,578] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:28:20,438] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:28:20,472] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:20,528] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:28:20,548] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:28:20,553] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.211 seconds
[2020-11-02 00:28:32,725] {scheduler_job.py:155} INFO - Started process (PID=20930) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:32,734] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:28:32,736] {logging_mixin.py:112} INFO - [2020-11-02 00:28:32,735] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:32,873] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:28:33,661] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:28:33,697] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:33,746] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:28:33,766] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:28:33,774] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.048 seconds
[2020-11-02 00:28:46,226] {scheduler_job.py:155} INFO - Started process (PID=20986) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:46,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:28:46,234] {logging_mixin.py:112} INFO - [2020-11-02 00:28:46,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:46,353] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:28:47,325] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:28:47,357] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:47,404] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:28:47,423] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:28:47,428] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.202 seconds
[2020-11-02 00:28:59,593] {scheduler_job.py:155} INFO - Started process (PID=21054) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:59,597] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:28:59,598] {logging_mixin.py:112} INFO - [2020-11-02 00:28:59,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:28:59,720] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:29:00,530] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:29:00,561] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:00,609] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:29:00,635] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:29:00,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.049 seconds
[2020-11-02 00:29:12,949] {scheduler_job.py:155} INFO - Started process (PID=21125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:12,963] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:29:12,965] {logging_mixin.py:112} INFO - [2020-11-02 00:29:12,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:13,097] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:29:13,837] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:29:13,871] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:13,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:29:14,049] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:29:14,056] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.106 seconds
[2020-11-02 00:29:26,587] {scheduler_job.py:155} INFO - Started process (PID=21180) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:26,592] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:29:26,592] {logging_mixin.py:112} INFO - [2020-11-02 00:29:26,592] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:26,699] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:29:27,637] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:29:27,666] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:27,712] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:29:27,734] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:29:27,741] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.154 seconds
[2020-11-02 00:29:39,929] {scheduler_job.py:155} INFO - Started process (PID=21237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:39,946] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:29:39,948] {logging_mixin.py:112} INFO - [2020-11-02 00:29:39,947] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:40,163] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:29:40,916] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:29:40,959] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:41,027] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:29:41,060] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:29:41,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.137 seconds
[2020-11-02 00:29:53,578] {scheduler_job.py:155} INFO - Started process (PID=21293) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:53,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:29:53,583] {logging_mixin.py:112} INFO - [2020-11-02 00:29:53,582] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:53,729] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:29:54,489] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:29:54,523] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:29:54,583] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:29:54,620] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:29:54,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.047 seconds
[2020-11-02 00:30:06,962] {scheduler_job.py:155} INFO - Started process (PID=21352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:06,970] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:30:06,972] {logging_mixin.py:112} INFO - [2020-11-02 00:30:06,971] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:07,109] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:30:08,043] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:30:08,097] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:08,163] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:30:08,194] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:30:08,201] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.240 seconds
[2020-11-02 00:30:20,589] {scheduler_job.py:155} INFO - Started process (PID=21409) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:20,594] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:30:20,595] {logging_mixin.py:112} INFO - [2020-11-02 00:30:20,595] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:20,716] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:30:21,496] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:30:21,535] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:21,585] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:30:21,610] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:30:21,614] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.025 seconds
[2020-11-02 00:30:34,070] {scheduler_job.py:155} INFO - Started process (PID=21467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:34,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:30:34,078] {logging_mixin.py:112} INFO - [2020-11-02 00:30:34,078] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:34,199] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:30:34,963] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:30:34,995] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:35,055] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:30:35,076] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:30:35,080] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.010 seconds
[2020-11-02 00:30:47,580] {scheduler_job.py:155} INFO - Started process (PID=21536) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:47,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:30:47,590] {logging_mixin.py:112} INFO - [2020-11-02 00:30:47,590] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:47,697] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:30:48,421] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:30:48,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:30:48,517] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:30:48,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:30:48,545] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.964 seconds
[2020-11-02 00:31:00,875] {scheduler_job.py:155} INFO - Started process (PID=21603) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:00,879] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:31:00,880] {logging_mixin.py:112} INFO - [2020-11-02 00:31:00,879] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:01,054] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:31:02,081] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:31:02,115] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:02,168] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:31:02,194] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:31:02,200] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.325 seconds
[2020-11-02 00:31:14,447] {scheduler_job.py:155} INFO - Started process (PID=21660) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:14,461] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:31:14,464] {logging_mixin.py:112} INFO - [2020-11-02 00:31:14,464] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:14,592] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:31:15,313] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:31:15,342] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:15,383] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:31:15,405] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:31:15,409] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.962 seconds
[2020-11-02 00:31:28,021] {scheduler_job.py:155} INFO - Started process (PID=21720) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:28,038] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:31:28,041] {logging_mixin.py:112} INFO - [2020-11-02 00:31:28,040] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:28,213] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:31:28,885] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:31:28,915] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:28,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:31:28,976] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:31:28,980] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.960 seconds
[2020-11-02 00:31:41,600] {scheduler_job.py:155} INFO - Started process (PID=21776) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:41,612] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:31:41,613] {logging_mixin.py:112} INFO - [2020-11-02 00:31:41,613] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:41,737] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:31:42,520] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:31:42,553] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:42,604] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:31:42,632] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:31:42,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.036 seconds
[2020-11-02 00:31:55,046] {scheduler_job.py:155} INFO - Started process (PID=21841) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:55,050] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:31:55,051] {logging_mixin.py:112} INFO - [2020-11-02 00:31:55,051] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:55,275] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:31:56,234] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:31:56,265] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:31:56,322] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:31:56,349] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:31:56,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-02 00:32:08,495] {scheduler_job.py:155} INFO - Started process (PID=21894) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:08,501] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:32:08,502] {logging_mixin.py:112} INFO - [2020-11-02 00:32:08,501] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:08,665] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:32:09,542] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:32:09,579] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:09,631] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:32:09,659] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:32:09,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.172 seconds
[2020-11-02 00:32:21,825] {scheduler_job.py:155} INFO - Started process (PID=21950) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:21,829] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:32:21,830] {logging_mixin.py:112} INFO - [2020-11-02 00:32:21,830] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:21,956] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:32:23,012] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:32:23,048] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:23,097] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:32:23,128] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:32:23,133] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-02 00:32:35,052] {scheduler_job.py:155} INFO - Started process (PID=22007) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:35,057] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:32:35,058] {logging_mixin.py:112} INFO - [2020-11-02 00:32:35,057] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:35,186] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:32:36,107] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:32:36,149] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:36,220] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:32:36,254] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:32:36,262] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.210 seconds
[2020-11-02 00:32:48,284] {scheduler_job.py:155} INFO - Started process (PID=22061) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:48,301] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:32:48,303] {logging_mixin.py:112} INFO - [2020-11-02 00:32:48,302] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:48,507] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:32:49,518] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:32:49,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:32:49,609] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:32:49,640] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:32:49,645] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.361 seconds
[2020-11-02 00:33:01,614] {scheduler_job.py:155} INFO - Started process (PID=22121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:01,652] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:33:01,655] {logging_mixin.py:112} INFO - [2020-11-02 00:33:01,654] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:01,871] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:33:03,073] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:33:03,114] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:03,169] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:33:03,201] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:33:03,206] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.592 seconds
[2020-11-02 00:33:14,930] {scheduler_job.py:155} INFO - Started process (PID=22181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:14,936] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:33:14,937] {logging_mixin.py:112} INFO - [2020-11-02 00:33:14,936] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:15,107] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:33:16,082] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:33:16,129] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:16,219] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:33:16,263] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:33:16,270] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.341 seconds
[2020-11-02 00:33:28,368] {scheduler_job.py:155} INFO - Started process (PID=22238) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:28,373] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:33:28,374] {logging_mixin.py:112} INFO - [2020-11-02 00:33:28,374] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:28,532] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:33:29,563] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:33:29,606] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:29,661] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:33:29,690] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:33:29,695] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.328 seconds
[2020-11-02 00:33:41,693] {scheduler_job.py:155} INFO - Started process (PID=22293) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:41,707] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:33:41,708] {logging_mixin.py:112} INFO - [2020-11-02 00:33:41,708] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:41,872] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:33:42,810] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:33:42,845] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:42,890] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:33:42,927] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:33:42,932] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.239 seconds
[2020-11-02 00:33:54,955] {scheduler_job.py:155} INFO - Started process (PID=22349) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:54,962] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:33:54,964] {logging_mixin.py:112} INFO - [2020-11-02 00:33:54,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:55,157] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:33:56,529] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:33:56,577] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:33:56,637] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:33:56,665] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:33:56,671] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.715 seconds
[2020-11-02 00:34:08,335] {scheduler_job.py:155} INFO - Started process (PID=22402) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:08,345] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:34:08,346] {logging_mixin.py:112} INFO - [2020-11-02 00:34:08,345] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:08,528] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:34:09,634] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:34:09,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:09,734] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:34:09,772] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:34:09,778] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.443 seconds
[2020-11-02 00:34:21,756] {scheduler_job.py:155} INFO - Started process (PID=22463) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:21,760] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:34:21,761] {logging_mixin.py:112} INFO - [2020-11-02 00:34:21,761] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:21,889] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:34:22,612] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:34:22,651] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:22,695] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:34:22,722] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:34:22,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.971 seconds
[2020-11-02 00:34:34,070] {scheduler_job.py:155} INFO - Started process (PID=22519) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:34,076] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:34:34,077] {logging_mixin.py:112} INFO - [2020-11-02 00:34:34,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:34,278] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:34:35,390] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:34:35,429] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:35,482] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:34:35,509] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:34:35,514] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.445 seconds
[2020-11-02 00:34:47,385] {scheduler_job.py:155} INFO - Started process (PID=22574) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:47,389] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:34:47,391] {logging_mixin.py:112} INFO - [2020-11-02 00:34:47,390] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:47,571] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:34:48,426] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:34:48,462] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:34:48,513] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:34:48,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:34:48,544] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.160 seconds
[2020-11-02 00:35:00,733] {scheduler_job.py:155} INFO - Started process (PID=22632) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:00,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:35:00,740] {logging_mixin.py:112} INFO - [2020-11-02 00:35:00,739] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:00,913] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:35:01,819] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:35:01,856] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:01,907] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:35:01,946] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:35:01,951] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.218 seconds
[2020-11-02 00:35:13,942] {scheduler_job.py:155} INFO - Started process (PID=22696) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:13,953] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:35:13,954] {logging_mixin.py:112} INFO - [2020-11-02 00:35:13,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:14,135] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:35:15,207] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:35:15,247] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:15,306] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:35:15,337] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:35:15,343] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.401 seconds
[2020-11-02 00:35:27,268] {scheduler_job.py:155} INFO - Started process (PID=22757) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:27,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:35:27,276] {logging_mixin.py:112} INFO - [2020-11-02 00:35:27,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:27,407] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:35:28,368] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:35:28,405] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:28,470] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:35:28,517] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:35:28,522] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.254 seconds
[2020-11-02 00:35:40,523] {scheduler_job.py:155} INFO - Started process (PID=22814) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:40,528] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:35:40,528] {logging_mixin.py:112} INFO - [2020-11-02 00:35:40,528] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:40,662] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:35:41,524] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:35:41,570] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:41,620] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:35:41,643] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:35:41,649] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.125 seconds
[2020-11-02 00:35:53,846] {scheduler_job.py:155} INFO - Started process (PID=22868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:53,850] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:35:53,851] {logging_mixin.py:112} INFO - [2020-11-02 00:35:53,851] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:54,045] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:35:55,134] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:35:55,168] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:35:55,212] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:35:55,239] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:35:55,245] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.399 seconds
[2020-11-02 00:36:07,141] {scheduler_job.py:155} INFO - Started process (PID=22928) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:07,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:36:07,147] {logging_mixin.py:112} INFO - [2020-11-02 00:36:07,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:07,271] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:36:08,163] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:36:08,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:08,258] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:36:08,280] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:36:08,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.144 seconds
[2020-11-02 00:36:20,489] {scheduler_job.py:155} INFO - Started process (PID=22985) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:20,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:36:20,495] {logging_mixin.py:112} INFO - [2020-11-02 00:36:20,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:20,679] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:36:21,917] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/sensors/base_sensor_operator.py:71: PendingDeprecationWarning: Invalid arguments were passed to S3KeySensor (task_id: get_new_json). Support for passing such arguments will be dropped in Airflow 2.0. Invalid arguments were:
*args: ()
**kwargs: {'s3_conn_id': 's3_task'}
  super(BaseSensorOperator, self).__init__(*args, **kwargs)
[2020-11-02 00:36:21,971] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:22,036] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-02 00:36:22,068] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-02 00:36:22,077] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.588 seconds
[2020-11-02 00:36:33,778] {scheduler_job.py:155} INFO - Started process (PID=23039) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:33,784] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:36:33,785] {logging_mixin.py:112} INFO - [2020-11-02 00:36:33,784] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:33,956] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:36:34,870] {logging_mixin.py:112} INFO - [2020-11-02 00:36:34,868] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:36:34,870] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:34,907] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.129 seconds
[2020-11-02 00:36:47,010] {scheduler_job.py:155} INFO - Started process (PID=23097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:47,015] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:36:47,016] {logging_mixin.py:112} INFO - [2020-11-02 00:36:47,016] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:47,226] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:36:48,355] {logging_mixin.py:112} INFO - [2020-11-02 00:36:48,354] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:36:48,358] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:36:48,391] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.381 seconds
[2020-11-02 00:37:00,365] {scheduler_job.py:155} INFO - Started process (PID=23199) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:00,372] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:37:00,374] {logging_mixin.py:112} INFO - [2020-11-02 00:37:00,373] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:00,511] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:37:01,519] {logging_mixin.py:112} INFO - [2020-11-02 00:37:01,517] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:37:01,519] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:01,548] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-02 00:37:13,628] {scheduler_job.py:155} INFO - Started process (PID=23263) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:13,634] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:37:13,635] {logging_mixin.py:112} INFO - [2020-11-02 00:37:13,635] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:13,774] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:37:14,579] {logging_mixin.py:112} INFO - [2020-11-02 00:37:14,578] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:37:14,579] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:14,607] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-02 00:37:26,874] {scheduler_job.py:155} INFO - Started process (PID=23318) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:26,884] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:37:26,889] {logging_mixin.py:112} INFO - [2020-11-02 00:37:26,889] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:27,013] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:37:27,852] {logging_mixin.py:112} INFO - [2020-11-02 00:37:27,851] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:37:27,853] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:27,880] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.007 seconds
[2020-11-02 00:37:40,157] {scheduler_job.py:155} INFO - Started process (PID=23382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:40,184] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:37:40,185] {logging_mixin.py:112} INFO - [2020-11-02 00:37:40,185] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:40,401] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:37:41,216] {logging_mixin.py:112} INFO - [2020-11-02 00:37:41,215] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:37:41,217] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:41,246] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.089 seconds
[2020-11-02 00:37:53,433] {scheduler_job.py:155} INFO - Started process (PID=23442) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:53,437] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:37:53,438] {logging_mixin.py:112} INFO - [2020-11-02 00:37:53,438] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:53,564] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:37:54,384] {logging_mixin.py:112} INFO - [2020-11-02 00:37:54,383] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:37:54,385] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:37:54,413] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.980 seconds
[2020-11-02 00:38:06,763] {scheduler_job.py:155} INFO - Started process (PID=23502) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:38:06,772] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-02 00:38:06,773] {logging_mixin.py:112} INFO - [2020-11-02 00:38:06,773] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:38:06,905] {logging_mixin.py:112} WARNING - /home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'S3KeySensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
[2020-11-02 00:38:07,713] {logging_mixin.py:112} INFO - [2020-11-02 00:38:07,712] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 24, in <module>
    import codec
ModuleNotFoundError: No module named 'codec'
[2020-11-02 00:38:07,714] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-02 00:38:07,748] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.985 seconds

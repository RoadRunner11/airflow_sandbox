[2020-11-05 11:37:35,667] {scheduler_job.py:155} INFO - Started process (PID=6076) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:37:35,677] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:37:35,678] {logging_mixin.py:112} INFO - [2020-11-05 11:37:35,678] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:37:37,582] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:37:37,643] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:37:37,661] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:37:37,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.999 seconds
[2020-11-05 11:37:50,006] {scheduler_job.py:155} INFO - Started process (PID=6213) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:37:50,013] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:37:50,021] {logging_mixin.py:112} INFO - [2020-11-05 11:37:50,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:37:52,646] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:37:52,725] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:37:52,763] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:37:52,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.767 seconds
[2020-11-05 11:38:16,570] {scheduler_job.py:155} INFO - Started process (PID=6532) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:16,575] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:38:16,576] {logging_mixin.py:112} INFO - [2020-11-05 11:38:16,576] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:17,971] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:18,027] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:38:18,057] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:38:18,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.496 seconds
[2020-11-05 11:38:42,014] {scheduler_job.py:155} INFO - Started process (PID=6713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:42,021] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:38:42,022] {logging_mixin.py:112} INFO - [2020-11-05 11:38:42,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:43,409] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:43,490] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:38:43,526] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:38:43,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.528 seconds
[2020-11-05 11:38:55,341] {scheduler_job.py:155} INFO - Started process (PID=6780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:55,360] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:38:55,361] {logging_mixin.py:112} INFO - [2020-11-05 11:38:55,360] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:58,049] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:38:58,127] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:38:58,153] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:38:58,161] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.821 seconds
[2020-11-05 11:39:09,691] {scheduler_job.py:155} INFO - Started process (PID=6845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:09,701] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:39:09,704] {logging_mixin.py:112} INFO - [2020-11-05 11:39:09,703] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:11,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:11,304] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:39:11,322] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:39:11,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.636 seconds
[2020-11-05 11:39:22,938] {scheduler_job.py:155} INFO - Started process (PID=6914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:22,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:39:22,942] {logging_mixin.py:112} INFO - [2020-11-05 11:39:22,941] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:25,159] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:25,218] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:39:25,246] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:39:25,251] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.313 seconds
[2020-11-05 11:39:37,214] {scheduler_job.py:155} INFO - Started process (PID=6979) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:37,221] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:39:37,226] {logging_mixin.py:112} INFO - [2020-11-05 11:39:37,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:38,787] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:38,823] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:39:38,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:39:38,842] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.628 seconds
[2020-11-05 11:39:50,416] {scheduler_job.py:155} INFO - Started process (PID=7050) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:50,430] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:39:50,431] {logging_mixin.py:112} INFO - [2020-11-05 11:39:50,431] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:51,724] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:39:51,795] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:39:51,813] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:39:51,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.403 seconds
[2020-11-05 11:40:03,726] {scheduler_job.py:155} INFO - Started process (PID=7126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:03,729] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:40:03,731] {logging_mixin.py:112} INFO - [2020-11-05 11:40:03,729] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:04,836] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:04,892] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:40:04,914] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:40:04,919] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.194 seconds
[2020-11-05 11:40:17,014] {scheduler_job.py:155} INFO - Started process (PID=7196) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:17,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:40:17,018] {logging_mixin.py:112} INFO - [2020-11-05 11:40:17,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:18,131] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:18,173] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:40:18,190] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:40:18,196] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.182 seconds
[2020-11-05 11:40:30,208] {scheduler_job.py:155} INFO - Started process (PID=7270) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:30,211] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:40:30,212] {logging_mixin.py:112} INFO - [2020-11-05 11:40:30,212] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:31,501] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:31,558] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:40:31,576] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:40:31,581] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.373 seconds
[2020-11-05 11:40:43,442] {scheduler_job.py:155} INFO - Started process (PID=7352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:43,448] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:40:43,449] {logging_mixin.py:112} INFO - [2020-11-05 11:40:43,448] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:45,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:45,193] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:40:45,210] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:40:45,215] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.773 seconds
[2020-11-05 11:40:56,710] {scheduler_job.py:155} INFO - Started process (PID=7425) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:56,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:40:56,715] {logging_mixin.py:112} INFO - [2020-11-05 11:40:56,715] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:57,836] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:40:57,883] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:40:57,900] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:40:57,904] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.195 seconds
[2020-11-05 11:41:09,919] {scheduler_job.py:155} INFO - Started process (PID=7488) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:09,923] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:41:09,924] {logging_mixin.py:112} INFO - [2020-11-05 11:41:09,924] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:11,157] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:11,202] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:41:11,228] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:41:11,234] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.314 seconds
[2020-11-05 11:41:23,196] {scheduler_job.py:155} INFO - Started process (PID=7565) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:23,218] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:41:23,225] {logging_mixin.py:112} INFO - [2020-11-05 11:41:23,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:25,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:25,238] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:41:25,272] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:41:25,276] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.081 seconds
[2020-11-05 11:41:37,388] {scheduler_job.py:155} INFO - Started process (PID=7634) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:37,392] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:41:37,393] {logging_mixin.py:112} INFO - [2020-11-05 11:41:37,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:38,504] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:38,554] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:41:38,573] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:41:38,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.190 seconds
[2020-11-05 11:41:50,606] {scheduler_job.py:155} INFO - Started process (PID=7695) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:50,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:41:50,610] {logging_mixin.py:112} INFO - [2020-11-05 11:41:50,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:52,983] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:41:53,094] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:41:53,120] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:41:53,127] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.521 seconds
[2020-11-05 11:42:04,985] {scheduler_job.py:155} INFO - Started process (PID=7767) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:04,992] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:42:04,993] {logging_mixin.py:112} INFO - [2020-11-05 11:42:04,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:06,548] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:06,615] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:42:06,633] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:42:06,638] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.653 seconds
[2020-11-05 11:42:18,197] {scheduler_job.py:155} INFO - Started process (PID=7832) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:18,201] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:42:18,202] {logging_mixin.py:112} INFO - [2020-11-05 11:42:18,201] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:19,485] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:19,527] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:42:19,545] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:42:19,549] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.352 seconds
[2020-11-05 11:42:31,509] {scheduler_job.py:155} INFO - Started process (PID=7901) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:31,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:42:31,514] {logging_mixin.py:112} INFO - [2020-11-05 11:42:31,514] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:32,737] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:32,780] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:42:32,797] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:42:32,800] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.291 seconds
[2020-11-05 11:42:44,801] {scheduler_job.py:155} INFO - Started process (PID=7970) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:44,808] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:42:44,809] {logging_mixin.py:112} INFO - [2020-11-05 11:42:44,808] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:46,135] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:46,183] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:42:46,205] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:42:46,210] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.410 seconds
[2020-11-05 11:42:58,129] {scheduler_job.py:155} INFO - Started process (PID=8040) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:42:58,134] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:42:58,135] {logging_mixin.py:112} INFO - [2020-11-05 11:42:58,135] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:00,474] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:00,519] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:43:00,536] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:43:00,540] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.412 seconds
[2020-11-05 11:43:12,387] {scheduler_job.py:155} INFO - Started process (PID=8108) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:12,390] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:43:12,391] {logging_mixin.py:112} INFO - [2020-11-05 11:43:12,391] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:13,602] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:13,653] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:43:13,669] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:43:13,673] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.287 seconds
[2020-11-05 11:43:25,601] {scheduler_job.py:155} INFO - Started process (PID=8206) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:25,613] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:43:25,613] {logging_mixin.py:112} INFO - [2020-11-05 11:43:25,613] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:27,507] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:27,554] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:43:27,571] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:43:27,575] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.975 seconds
[2020-11-05 11:43:38,986] {scheduler_job.py:155} INFO - Started process (PID=8284) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:38,994] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:43:39,001] {logging_mixin.py:112} INFO - [2020-11-05 11:43:39,001] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:40,536] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:40,577] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:43:40,605] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:43:40,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.626 seconds
[2020-11-05 11:43:52,183] {scheduler_job.py:155} INFO - Started process (PID=8352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:52,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:43:52,187] {logging_mixin.py:112} INFO - [2020-11-05 11:43:52,187] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:53,973] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:43:54,035] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:43:54,065] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:43:54,073] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.890 seconds
[2020-11-05 11:44:05,511] {scheduler_job.py:155} INFO - Started process (PID=8419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:05,523] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:44:05,524] {logging_mixin.py:112} INFO - [2020-11-05 11:44:05,524] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:06,896] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:06,932] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:44:06,947] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:44:06,950] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.439 seconds
[2020-11-05 11:44:18,719] {scheduler_job.py:155} INFO - Started process (PID=8480) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:18,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:44:18,724] {logging_mixin.py:112} INFO - [2020-11-05 11:44:18,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:19,782] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:19,819] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:44:19,836] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:44:19,849] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.130 seconds
[2020-11-05 11:44:31,882] {scheduler_job.py:155} INFO - Started process (PID=8556) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:31,891] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:44:31,891] {logging_mixin.py:112} INFO - [2020-11-05 11:44:31,891] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:33,146] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:33,210] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:44:33,234] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:44:33,241] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.360 seconds
[2020-11-05 11:44:45,164] {scheduler_job.py:155} INFO - Started process (PID=8624) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:45,173] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:44:45,173] {logging_mixin.py:112} INFO - [2020-11-05 11:44:45,173] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:46,532] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:46,576] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:44:46,596] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:44:46,600] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.436 seconds
[2020-11-05 11:44:58,350] {scheduler_job.py:155} INFO - Started process (PID=8695) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:44:58,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:44:58,377] {logging_mixin.py:112} INFO - [2020-11-05 11:44:58,377] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:00,144] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:00,194] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:45:00,217] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:45:00,221] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.872 seconds
[2020-11-05 11:45:11,531] {scheduler_job.py:155} INFO - Started process (PID=8759) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:11,538] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:45:11,546] {logging_mixin.py:112} INFO - [2020-11-05 11:45:11,545] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:12,926] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:12,969] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:45:12,983] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:45:12,987] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.456 seconds
[2020-11-05 11:45:24,799] {scheduler_job.py:155} INFO - Started process (PID=8822) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:24,804] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:45:24,805] {logging_mixin.py:112} INFO - [2020-11-05 11:45:24,804] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:25,921] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:25,961] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:45:25,976] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:45:25,980] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.181 seconds
[2020-11-05 11:45:37,986] {scheduler_job.py:155} INFO - Started process (PID=8887) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:37,994] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:45:37,995] {logging_mixin.py:112} INFO - [2020-11-05 11:45:37,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:39,323] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:39,379] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:45:39,404] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:45:39,410] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.424 seconds
[2020-11-05 11:45:51,199] {scheduler_job.py:155} INFO - Started process (PID=8957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:51,202] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:45:51,202] {logging_mixin.py:112} INFO - [2020-11-05 11:45:51,202] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:52,376] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:45:52,412] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:45:52,427] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:45:52,436] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.237 seconds
[2020-11-05 11:46:04,478] {scheduler_job.py:155} INFO - Started process (PID=9029) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:04,482] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:46:04,483] {logging_mixin.py:112} INFO - [2020-11-05 11:46:04,483] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:05,641] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:05,692] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:46:05,707] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:46:05,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-05 11:46:17,684] {scheduler_job.py:155} INFO - Started process (PID=9091) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:17,738] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:46:17,741] {logging_mixin.py:112} INFO - [2020-11-05 11:46:17,741] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:18,888] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:18,931] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:46:18,948] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:46:18,952] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.267 seconds
[2020-11-05 11:46:30,895] {scheduler_job.py:155} INFO - Started process (PID=9162) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:30,909] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:46:30,909] {logging_mixin.py:112} INFO - [2020-11-05 11:46:30,909] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:32,593] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:32,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:46:32,659] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:46:32,664] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.769 seconds
[2020-11-05 11:46:44,081] {scheduler_job.py:155} INFO - Started process (PID=9234) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:44,085] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:46:44,086] {logging_mixin.py:112} INFO - [2020-11-05 11:46:44,086] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:45,230] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:45,292] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:46:45,308] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:46:45,312] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.231 seconds
[2020-11-05 11:46:57,276] {scheduler_job.py:155} INFO - Started process (PID=9303) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:57,287] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:46:57,294] {logging_mixin.py:112} INFO - [2020-11-05 11:46:57,294] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:59,048] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:46:59,112] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:46:59,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:46:59,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.868 seconds
[2020-11-05 11:47:10,428] {scheduler_job.py:155} INFO - Started process (PID=9372) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:10,432] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:47:10,432] {logging_mixin.py:112} INFO - [2020-11-05 11:47:10,432] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:11,472] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:11,509] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:47:11,523] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:47:11,526] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.098 seconds
[2020-11-05 11:47:23,610] {scheduler_job.py:155} INFO - Started process (PID=9447) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:23,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:47:23,615] {logging_mixin.py:112} INFO - [2020-11-05 11:47:23,614] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:24,897] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:24,937] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:47:24,952] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:47:24,956] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.346 seconds
[2020-11-05 11:47:36,973] {scheduler_job.py:155} INFO - Started process (PID=9517) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:36,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:47:36,977] {logging_mixin.py:112} INFO - [2020-11-05 11:47:36,977] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:38,188] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:38,233] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:47:38,254] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:47:38,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.286 seconds
[2020-11-05 11:47:50,172] {scheduler_job.py:155} INFO - Started process (PID=9588) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:50,175] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:47:50,175] {logging_mixin.py:112} INFO - [2020-11-05 11:47:50,175] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:51,509] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:47:51,565] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:47:51,586] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:47:51,590] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.419 seconds
[2020-11-05 11:48:03,376] {scheduler_job.py:155} INFO - Started process (PID=9662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:03,382] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:48:03,382] {logging_mixin.py:112} INFO - [2020-11-05 11:48:03,382] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:04,916] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:04,953] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:48:04,971] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:48:04,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.599 seconds
[2020-11-05 11:48:16,601] {scheduler_job.py:155} INFO - Started process (PID=9737) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:16,604] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:48:16,604] {logging_mixin.py:112} INFO - [2020-11-05 11:48:16,604] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:17,721] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:17,768] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:48:17,786] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:48:17,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.190 seconds
[2020-11-05 11:48:29,837] {scheduler_job.py:155} INFO - Started process (PID=9822) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:29,843] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:48:29,846] {logging_mixin.py:112} INFO - [2020-11-05 11:48:29,845] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:31,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:31,188] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:48:31,203] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:48:31,207] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.370 seconds
[2020-11-05 11:48:43,078] {scheduler_job.py:155} INFO - Started process (PID=9890) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:43,085] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:48:43,085] {logging_mixin.py:112} INFO - [2020-11-05 11:48:43,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:44,092] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:44,125] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:48:44,139] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:48:44,143] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.064 seconds
[2020-11-05 11:48:56,318] {scheduler_job.py:155} INFO - Started process (PID=9967) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:56,322] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:48:56,323] {logging_mixin.py:112} INFO - [2020-11-05 11:48:56,323] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:57,360] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:48:57,395] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:48:57,411] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:48:57,415] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.097 seconds
[2020-11-05 11:49:09,654] {scheduler_job.py:155} INFO - Started process (PID=10041) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:09,657] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:49:09,658] {logging_mixin.py:112} INFO - [2020-11-05 11:49:09,657] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:10,653] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:10,686] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:49:10,703] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:49:10,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.053 seconds
[2020-11-05 11:49:22,945] {scheduler_job.py:155} INFO - Started process (PID=10118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:22,949] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:49:22,950] {logging_mixin.py:112} INFO - [2020-11-05 11:49:22,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:23,986] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:24,023] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:49:24,050] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:49:24,054] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.109 seconds
[2020-11-05 11:49:36,131] {scheduler_job.py:155} INFO - Started process (PID=10214) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:36,145] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:49:36,146] {logging_mixin.py:112} INFO - [2020-11-05 11:49:36,146] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:37,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:37,530] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:49:37,543] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:49:37,546] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.416 seconds
[2020-11-05 11:49:49,355] {scheduler_job.py:155} INFO - Started process (PID=10277) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:49,358] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:49:49,358] {logging_mixin.py:112} INFO - [2020-11-05 11:49:49,358] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:50,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:49:50,592] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:49:50,607] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:49:50,611] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.256 seconds
[2020-11-05 11:50:02,588] {scheduler_job.py:155} INFO - Started process (PID=10353) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:02,590] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:50:02,591] {logging_mixin.py:112} INFO - [2020-11-05 11:50:02,591] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:03,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:03,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:50:03,999] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:50:04,003] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.416 seconds
[2020-11-05 11:50:15,784] {scheduler_job.py:155} INFO - Started process (PID=10417) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:15,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:50:15,788] {logging_mixin.py:112} INFO - [2020-11-05 11:50:15,788] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:17,604] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:17,664] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:50:17,685] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:50:17,689] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.905 seconds
[2020-11-05 11:50:28,971] {scheduler_job.py:155} INFO - Started process (PID=10487) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:28,983] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:50:28,984] {logging_mixin.py:112} INFO - [2020-11-05 11:50:28,984] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:30,184] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:30,251] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:50:30,282] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:50:30,287] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.316 seconds
[2020-11-05 11:50:42,176] {scheduler_job.py:155} INFO - Started process (PID=10556) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:42,179] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:50:42,180] {logging_mixin.py:112} INFO - [2020-11-05 11:50:42,179] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:43,195] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:43,232] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:50:43,247] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:50:43,250] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.074 seconds
[2020-11-05 11:50:55,556] {scheduler_job.py:155} INFO - Started process (PID=10632) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:55,561] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:50:55,563] {logging_mixin.py:112} INFO - [2020-11-05 11:50:55,562] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:56,566] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:50:56,608] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:50:56,621] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:50:56,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.069 seconds
[2020-11-05 11:51:08,760] {scheduler_job.py:155} INFO - Started process (PID=10706) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:08,763] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:51:08,764] {logging_mixin.py:112} INFO - [2020-11-05 11:51:08,763] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:09,740] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:09,783] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:51:09,806] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:51:09,812] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.053 seconds
[2020-11-05 11:51:21,985] {scheduler_job.py:155} INFO - Started process (PID=10777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:21,989] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:51:21,990] {logging_mixin.py:112} INFO - [2020-11-05 11:51:21,990] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:23,239] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:23,298] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:51:23,341] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:51:23,353] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.368 seconds
[2020-11-05 11:51:35,182] {scheduler_job.py:155} INFO - Started process (PID=10849) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:35,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:51:35,192] {logging_mixin.py:112} INFO - [2020-11-05 11:51:35,192] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:36,877] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:36,921] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:51:36,941] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:51:36,950] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.769 seconds
[2020-11-05 11:51:48,382] {scheduler_job.py:155} INFO - Started process (PID=10914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:48,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:51:48,397] {logging_mixin.py:112} INFO - [2020-11-05 11:51:48,396] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:49,558] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:51:49,589] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:51:49,604] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:51:49,608] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.227 seconds
[2020-11-05 11:52:01,570] {scheduler_job.py:155} INFO - Started process (PID=10981) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:01,581] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:52:01,589] {logging_mixin.py:112} INFO - [2020-11-05 11:52:01,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:02,809] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:02,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:52:02,909] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:52:02,915] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.345 seconds
[2020-11-05 11:52:14,847] {scheduler_job.py:155} INFO - Started process (PID=11044) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:14,851] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:52:14,851] {logging_mixin.py:112} INFO - [2020-11-05 11:52:14,851] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:16,036] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:16,074] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:52:16,090] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:52:16,094] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-05 11:52:28,020] {scheduler_job.py:155} INFO - Started process (PID=11117) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:28,023] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:52:28,024] {logging_mixin.py:112} INFO - [2020-11-05 11:52:28,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:29,093] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:29,134] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:52:29,154] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:52:29,158] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.139 seconds
[2020-11-05 11:52:41,266] {scheduler_job.py:155} INFO - Started process (PID=11188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:41,269] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:52:41,271] {logging_mixin.py:112} INFO - [2020-11-05 11:52:41,271] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:42,646] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:42,687] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:52:42,707] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:52:42,712] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.446 seconds
[2020-11-05 11:52:54,663] {scheduler_job.py:155} INFO - Started process (PID=11265) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:54,667] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:52:54,667] {logging_mixin.py:112} INFO - [2020-11-05 11:52:54,667] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:55,693] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:52:55,731] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:52:55,746] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:52:55,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.088 seconds
[2020-11-05 11:53:07,973] {scheduler_job.py:155} INFO - Started process (PID=11342) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:07,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:53:07,979] {logging_mixin.py:112} INFO - [2020-11-05 11:53:07,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:09,169] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:09,211] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:53:09,225] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:53:09,228] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.256 seconds
[2020-11-05 11:53:21,203] {scheduler_job.py:155} INFO - Started process (PID=11410) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:21,207] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:53:21,208] {logging_mixin.py:112} INFO - [2020-11-05 11:53:21,208] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:22,310] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:22,358] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:53:22,373] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:53:22,377] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.174 seconds
[2020-11-05 11:53:34,427] {scheduler_job.py:155} INFO - Started process (PID=11481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:34,440] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:53:34,441] {logging_mixin.py:112} INFO - [2020-11-05 11:53:34,440] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:35,871] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:35,934] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:53:35,952] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:53:35,956] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.529 seconds
[2020-11-05 11:53:47,715] {scheduler_job.py:155} INFO - Started process (PID=11551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:47,720] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:53:47,720] {logging_mixin.py:112} INFO - [2020-11-05 11:53:47,720] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:48,726] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:53:48,768] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:53:48,782] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:53:48,785] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.070 seconds
[2020-11-05 11:54:00,958] {scheduler_job.py:155} INFO - Started process (PID=11623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:00,963] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:54:00,971] {logging_mixin.py:112} INFO - [2020-11-05 11:54:00,971] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:02,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:02,294] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:54:02,321] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:54:02,325] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.366 seconds
[2020-11-05 11:54:14,218] {scheduler_job.py:155} INFO - Started process (PID=11695) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:14,223] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:54:14,224] {logging_mixin.py:112} INFO - [2020-11-05 11:54:14,224] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:15,263] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:15,300] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:54:15,316] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:54:15,320] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-05 11:54:27,450] {scheduler_job.py:155} INFO - Started process (PID=11760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:27,454] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:54:27,454] {logging_mixin.py:112} INFO - [2020-11-05 11:54:27,454] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:28,535] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:28,578] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:54:28,592] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:54:28,597] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.147 seconds
[2020-11-05 11:54:40,669] {scheduler_job.py:155} INFO - Started process (PID=11832) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:40,673] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:54:40,674] {logging_mixin.py:112} INFO - [2020-11-05 11:54:40,673] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:41,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:41,789] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:54:41,803] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:54:41,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.138 seconds
[2020-11-05 11:54:53,903] {scheduler_job.py:155} INFO - Started process (PID=11901) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:53,906] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:54:53,907] {logging_mixin.py:112} INFO - [2020-11-05 11:54:53,907] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:55,116] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:54:55,163] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:54:55,194] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:54:55,202] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.299 seconds
[2020-11-05 11:55:07,143] {scheduler_job.py:155} INFO - Started process (PID=11975) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:07,160] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:55:07,166] {logging_mixin.py:112} INFO - [2020-11-05 11:55:07,166] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:08,531] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:08,565] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:55:08,580] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:55:08,585] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.442 seconds
[2020-11-05 11:55:20,330] {scheduler_job.py:155} INFO - Started process (PID=12048) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:20,333] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:55:20,334] {logging_mixin.py:112} INFO - [2020-11-05 11:55:20,334] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:21,445] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:21,492] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:55:21,509] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:55:21,513] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-05 11:55:33,553] {scheduler_job.py:155} INFO - Started process (PID=12117) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:33,557] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:55:33,559] {logging_mixin.py:112} INFO - [2020-11-05 11:55:33,558] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:35,001] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:35,046] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:55:35,064] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:55:35,068] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.515 seconds
[2020-11-05 11:55:46,817] {scheduler_job.py:155} INFO - Started process (PID=12188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:46,821] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:55:46,822] {logging_mixin.py:112} INFO - [2020-11-05 11:55:46,821] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:47,931] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:55:47,971] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:55:47,991] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:55:47,996] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.179 seconds
[2020-11-05 11:56:00,039] {scheduler_job.py:155} INFO - Started process (PID=12267) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:00,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:56:00,047] {logging_mixin.py:112} INFO - [2020-11-05 11:56:00,046] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:01,613] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:01,656] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:56:01,680] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:56:01,687] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.648 seconds
[2020-11-05 11:56:13,276] {scheduler_job.py:155} INFO - Started process (PID=12332) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:13,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:56:13,280] {logging_mixin.py:112} INFO - [2020-11-05 11:56:13,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:14,952] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:15,015] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:56:15,035] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:56:15,041] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.766 seconds
[2020-11-05 11:56:26,548] {scheduler_job.py:155} INFO - Started process (PID=12398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:26,555] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:56:26,557] {logging_mixin.py:112} INFO - [2020-11-05 11:56:26,557] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:27,754] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:27,796] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:56:27,815] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:56:27,820] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.272 seconds
[2020-11-05 11:56:39,787] {scheduler_job.py:155} INFO - Started process (PID=12471) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:39,793] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:56:39,793] {logging_mixin.py:112} INFO - [2020-11-05 11:56:39,793] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:41,639] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:41,700] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:56:41,722] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:56:41,729] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.941 seconds
[2020-11-05 11:56:53,041] {scheduler_job.py:155} INFO - Started process (PID=12532) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:53,044] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:56:53,045] {logging_mixin.py:112} INFO - [2020-11-05 11:56:53,045] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:54,244] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:56:54,279] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:56:54,295] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:56:54,298] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.257 seconds
[2020-11-05 11:57:06,208] {scheduler_job.py:155} INFO - Started process (PID=12602) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:06,213] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:57:06,213] {logging_mixin.py:112} INFO - [2020-11-05 11:57:06,213] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:07,922] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:07,990] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:57:08,032] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:57:08,038] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.830 seconds
[2020-11-05 11:57:19,474] {scheduler_job.py:155} INFO - Started process (PID=12666) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:19,488] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:57:19,492] {logging_mixin.py:112} INFO - [2020-11-05 11:57:19,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:20,785] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:20,823] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:57:20,842] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:57:20,846] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.372 seconds
[2020-11-05 11:57:32,721] {scheduler_job.py:155} INFO - Started process (PID=12734) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:32,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:57:32,725] {logging_mixin.py:112} INFO - [2020-11-05 11:57:32,725] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:33,743] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:33,797] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:57:33,813] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:57:33,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.097 seconds
[2020-11-05 11:57:45,909] {scheduler_job.py:155} INFO - Started process (PID=12809) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:45,913] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:57:45,914] {logging_mixin.py:112} INFO - [2020-11-05 11:57:45,913] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:46,923] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:46,960] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:57:46,978] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:57:46,981] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.072 seconds
[2020-11-05 11:57:59,185] {scheduler_job.py:155} INFO - Started process (PID=12880) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:57:59,190] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:57:59,190] {logging_mixin.py:112} INFO - [2020-11-05 11:57:59,190] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:00,251] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:00,288] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:58:00,307] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:58:00,311] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.127 seconds
[2020-11-05 11:58:12,352] {scheduler_job.py:155} INFO - Started process (PID=12957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:12,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:58:12,358] {logging_mixin.py:112} INFO - [2020-11-05 11:58:12,358] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:13,508] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:13,552] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:58:13,568] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:58:13,572] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.220 seconds
[2020-11-05 11:58:25,566] {scheduler_job.py:155} INFO - Started process (PID=13032) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:25,570] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:58:25,571] {logging_mixin.py:112} INFO - [2020-11-05 11:58:25,571] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:26,610] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:26,650] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:58:26,666] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:58:26,671] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.105 seconds
[2020-11-05 11:58:38,859] {scheduler_job.py:155} INFO - Started process (PID=13104) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:38,870] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:58:38,871] {logging_mixin.py:112} INFO - [2020-11-05 11:58:38,871] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:40,051] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:40,090] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:58:40,109] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:58:40,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.253 seconds
[2020-11-05 11:58:52,061] {scheduler_job.py:155} INFO - Started process (PID=13177) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:52,065] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:58:52,066] {logging_mixin.py:112} INFO - [2020-11-05 11:58:52,066] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:53,137] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:58:53,178] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:58:53,194] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:58:53,198] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.136 seconds
[2020-11-05 11:59:05,204] {scheduler_job.py:155} INFO - Started process (PID=13281) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:05,208] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:59:05,209] {logging_mixin.py:112} INFO - [2020-11-05 11:59:05,209] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:06,233] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:06,271] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:59:06,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:59:06,292] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.088 seconds
[2020-11-05 11:59:18,429] {scheduler_job.py:155} INFO - Started process (PID=13358) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:18,434] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:59:18,435] {logging_mixin.py:112} INFO - [2020-11-05 11:59:18,434] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:19,448] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:19,488] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:59:19,504] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:59:19,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-05 11:59:31,727] {scheduler_job.py:155} INFO - Started process (PID=13435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:31,740] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:59:31,741] {logging_mixin.py:112} INFO - [2020-11-05 11:59:31,741] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:33,004] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:33,050] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:59:33,069] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:59:33,074] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-05 11:59:44,900] {scheduler_job.py:155} INFO - Started process (PID=13508) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:44,906] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:59:44,907] {logging_mixin.py:112} INFO - [2020-11-05 11:59:44,907] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:45,920] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:45,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:59:45,974] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:59:45,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-05 11:59:58,130] {scheduler_job.py:155} INFO - Started process (PID=13587) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:58,133] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 11:59:58,134] {logging_mixin.py:112} INFO - [2020-11-05 11:59:58,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:59,308] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 11:59:59,343] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 11:59:59,362] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 11:59:59,366] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.236 seconds
[2020-11-05 12:00:11,340] {scheduler_job.py:155} INFO - Started process (PID=13662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:11,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:00:11,364] {logging_mixin.py:112} INFO - [2020-11-05 12:00:11,364] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:12,593] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:12,640] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:00:12,661] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:00:12,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.325 seconds
[2020-11-05 12:00:24,661] {scheduler_job.py:155} INFO - Started process (PID=13733) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:24,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:00:24,677] {logging_mixin.py:112} INFO - [2020-11-05 12:00:24,677] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:25,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:25,977] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:00:25,994] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:00:25,998] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.337 seconds
[2020-11-05 12:00:37,872] {scheduler_job.py:155} INFO - Started process (PID=13800) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:37,877] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:00:37,878] {logging_mixin.py:112} INFO - [2020-11-05 12:00:37,878] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:38,995] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:39,032] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:00:39,051] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:00:39,055] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.183 seconds
[2020-11-05 12:00:51,107] {scheduler_job.py:155} INFO - Started process (PID=13900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:51,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:00:51,112] {logging_mixin.py:112} INFO - [2020-11-05 12:00:51,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:52,552] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:00:52,592] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:00:52,615] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:00:52,621] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.515 seconds
[2020-11-05 12:01:04,352] {scheduler_job.py:155} INFO - Started process (PID=13966) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:04,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:01:04,358] {logging_mixin.py:112} INFO - [2020-11-05 12:01:04,357] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:05,379] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:05,418] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:01:05,434] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:01:05,438] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.086 seconds
[2020-11-05 12:01:17,552] {scheduler_job.py:155} INFO - Started process (PID=14043) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:17,555] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:01:17,555] {logging_mixin.py:112} INFO - [2020-11-05 12:01:17,555] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:18,547] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:18,584] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:01:18,599] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:01:18,602] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.050 seconds
[2020-11-05 12:01:30,795] {scheduler_job.py:155} INFO - Started process (PID=14116) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:30,798] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:01:30,799] {logging_mixin.py:112} INFO - [2020-11-05 12:01:30,799] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:31,806] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:31,848] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:01:31,871] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:01:31,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.083 seconds
[2020-11-05 12:01:44,020] {scheduler_job.py:155} INFO - Started process (PID=14189) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:44,024] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:01:44,024] {logging_mixin.py:112} INFO - [2020-11-05 12:01:44,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:45,271] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:45,308] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:01:45,321] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:01:45,325] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.304 seconds
[2020-11-05 12:01:57,264] {scheduler_job.py:155} INFO - Started process (PID=14258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:57,268] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:01:57,268] {logging_mixin.py:112} INFO - [2020-11-05 12:01:57,268] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:58,268] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:01:58,304] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:01:58,318] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:01:58,321] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.057 seconds
[2020-11-05 12:02:10,466] {scheduler_job.py:155} INFO - Started process (PID=14329) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:10,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:02:10,470] {logging_mixin.py:112} INFO - [2020-11-05 12:02:10,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:11,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:11,549] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:02:11,566] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:02:11,571] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.105 seconds
[2020-11-05 12:02:23,683] {scheduler_job.py:155} INFO - Started process (PID=14398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:23,687] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:02:23,688] {logging_mixin.py:112} INFO - [2020-11-05 12:02:23,688] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:24,730] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:24,770] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:02:24,787] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:02:24,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.108 seconds
[2020-11-05 12:02:36,891] {scheduler_job.py:155} INFO - Started process (PID=14467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:36,903] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:02:36,913] {logging_mixin.py:112} INFO - [2020-11-05 12:02:36,913] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:38,230] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:38,271] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:02:38,290] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:02:38,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.403 seconds
[2020-11-05 12:02:50,063] {scheduler_job.py:155} INFO - Started process (PID=14534) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:50,069] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:02:50,077] {logging_mixin.py:112} INFO - [2020-11-05 12:02:50,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:51,581] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:02:51,636] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:02:51,661] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:02:51,665] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.601 seconds
[2020-11-05 12:03:03,318] {scheduler_job.py:155} INFO - Started process (PID=14608) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:03,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:03:03,321] {logging_mixin.py:112} INFO - [2020-11-05 12:03:03,321] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:04,311] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:04,351] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:03:04,368] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:03:04,372] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.054 seconds
[2020-11-05 12:03:16,524] {scheduler_job.py:155} INFO - Started process (PID=14683) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:16,527] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:03:16,528] {logging_mixin.py:112} INFO - [2020-11-05 12:03:16,527] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:17,990] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:18,033] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:03:18,050] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:03:18,053] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.529 seconds
[2020-11-05 12:03:29,898] {scheduler_job.py:155} INFO - Started process (PID=14753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:29,905] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:03:29,906] {logging_mixin.py:112} INFO - [2020-11-05 12:03:29,906] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:31,641] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:31,698] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:03:31,726] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:03:31,732] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.834 seconds
[2020-11-05 12:03:43,147] {scheduler_job.py:155} INFO - Started process (PID=14812) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:43,171] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:03:43,172] {logging_mixin.py:112} INFO - [2020-11-05 12:03:43,172] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:44,817] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:44,869] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:03:44,887] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:03:44,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.744 seconds
[2020-11-05 12:03:56,337] {scheduler_job.py:155} INFO - Started process (PID=14876) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:56,349] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:03:56,350] {logging_mixin.py:112} INFO - [2020-11-05 12:03:56,350] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:57,950] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:03:58,009] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:03:58,025] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:03:58,029] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.692 seconds
[2020-11-05 12:04:09,587] {scheduler_job.py:155} INFO - Started process (PID=14947) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:09,598] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:04:09,601] {logging_mixin.py:112} INFO - [2020-11-05 12:04:09,601] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:10,592] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:10,625] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:04:10,640] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:04:10,644] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.057 seconds
[2020-11-05 12:04:22,779] {scheduler_job.py:155} INFO - Started process (PID=15019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:22,782] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:04:22,782] {logging_mixin.py:112} INFO - [2020-11-05 12:04:22,782] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:24,139] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:24,183] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:04:24,199] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:04:24,203] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.424 seconds
[2020-11-05 12:04:36,077] {scheduler_job.py:155} INFO - Started process (PID=15084) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:36,082] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:04:36,083] {logging_mixin.py:112} INFO - [2020-11-05 12:04:36,083] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:37,085] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:37,119] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:04:37,135] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:04:37,139] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-05 12:04:49,280] {scheduler_job.py:155} INFO - Started process (PID=15163) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:49,283] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:04:49,284] {logging_mixin.py:112} INFO - [2020-11-05 12:04:49,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:50,317] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:04:50,406] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:04:50,430] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:04:50,436] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.156 seconds
[2020-11-05 12:05:02,536] {scheduler_job.py:155} INFO - Started process (PID=15240) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:02,541] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:05:02,541] {logging_mixin.py:112} INFO - [2020-11-05 12:05:02,541] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:03,603] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:03,648] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:05:03,664] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:05:03,668] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.132 seconds
[2020-11-05 12:05:15,774] {scheduler_job.py:155} INFO - Started process (PID=15315) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:15,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:05:15,786] {logging_mixin.py:112} INFO - [2020-11-05 12:05:15,785] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:17,194] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:17,241] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:05:17,264] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:05:17,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.498 seconds
[2020-11-05 12:05:29,095] {scheduler_job.py:155} INFO - Started process (PID=15392) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:29,099] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:05:29,099] {logging_mixin.py:112} INFO - [2020-11-05 12:05:29,099] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:30,129] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:30,164] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:05:30,180] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:05:30,185] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.090 seconds
[2020-11-05 12:05:42,248] {scheduler_job.py:155} INFO - Started process (PID=15464) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:42,253] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:05:42,254] {logging_mixin.py:112} INFO - [2020-11-05 12:05:42,254] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:43,272] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:43,311] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:05:43,327] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:05:43,331] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.083 seconds
[2020-11-05 12:05:55,457] {scheduler_job.py:155} INFO - Started process (PID=15543) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:55,462] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:05:55,463] {logging_mixin.py:112} INFO - [2020-11-05 12:05:55,463] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:56,434] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:05:56,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:05:56,501] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:05:56,504] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.047 seconds
[2020-11-05 12:06:08,746] {scheduler_job.py:155} INFO - Started process (PID=15619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:08,749] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:06:08,749] {logging_mixin.py:112} INFO - [2020-11-05 12:06:08,749] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:09,734] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:09,773] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:06:09,787] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:06:09,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.045 seconds
[2020-11-05 12:06:21,947] {scheduler_job.py:155} INFO - Started process (PID=15698) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:21,954] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:06:21,954] {logging_mixin.py:112} INFO - [2020-11-05 12:06:21,954] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:22,966] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:23,001] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:06:23,026] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:06:23,030] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.083 seconds
[2020-11-05 12:06:35,235] {scheduler_job.py:155} INFO - Started process (PID=15774) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:35,238] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:06:35,239] {logging_mixin.py:112} INFO - [2020-11-05 12:06:35,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:36,292] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:36,335] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:06:36,350] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:06:36,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.119 seconds
[2020-11-05 12:06:48,453] {scheduler_job.py:155} INFO - Started process (PID=15851) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:48,457] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:06:48,457] {logging_mixin.py:112} INFO - [2020-11-05 12:06:48,457] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:49,672] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:06:49,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:06:49,722] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:06:49,726] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.274 seconds
[2020-11-05 12:07:01,671] {scheduler_job.py:155} INFO - Started process (PID=15915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:01,678] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:07:01,678] {logging_mixin.py:112} INFO - [2020-11-05 12:07:01,678] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:02,899] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:02,945] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:07:02,963] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:07:02,966] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.296 seconds
[2020-11-05 12:07:14,864] {scheduler_job.py:155} INFO - Started process (PID=15986) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:14,867] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:07:14,868] {logging_mixin.py:112} INFO - [2020-11-05 12:07:14,868] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:16,211] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:16,260] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:07:16,280] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:07:16,286] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.422 seconds
[2020-11-05 12:07:28,181] {scheduler_job.py:155} INFO - Started process (PID=16059) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:28,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:07:28,187] {logging_mixin.py:112} INFO - [2020-11-05 12:07:28,187] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:29,269] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:29,312] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:07:29,331] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:07:29,335] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.154 seconds
[2020-11-05 12:07:41,410] {scheduler_job.py:155} INFO - Started process (PID=16136) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:41,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:07:41,416] {logging_mixin.py:112} INFO - [2020-11-05 12:07:41,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:42,415] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:42,452] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:07:42,468] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:07:42,472] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.062 seconds
[2020-11-05 12:07:54,616] {scheduler_job.py:155} INFO - Started process (PID=16212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:54,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:07:54,622] {logging_mixin.py:112} INFO - [2020-11-05 12:07:54,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:55,838] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:07:55,871] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:07:55,888] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:07:55,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.275 seconds
[2020-11-05 12:08:07,974] {scheduler_job.py:155} INFO - Started process (PID=16282) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:07,978] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:08:07,979] {logging_mixin.py:112} INFO - [2020-11-05 12:08:07,979] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:09,278] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:09,337] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:08:09,371] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:08:09,375] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.401 seconds
[2020-11-05 12:08:21,189] {scheduler_job.py:155} INFO - Started process (PID=16358) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:21,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:08:21,197] {logging_mixin.py:112} INFO - [2020-11-05 12:08:21,197] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:22,586] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:22,643] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:08:22,664] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:08:22,668] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.479 seconds
[2020-11-05 12:08:34,446] {scheduler_job.py:155} INFO - Started process (PID=16434) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:34,450] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:08:34,451] {logging_mixin.py:112} INFO - [2020-11-05 12:08:34,451] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:35,776] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:35,822] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:08:35,843] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:08:35,850] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.404 seconds
[2020-11-05 12:08:47,676] {scheduler_job.py:155} INFO - Started process (PID=16510) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:47,685] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:08:47,691] {logging_mixin.py:112} INFO - [2020-11-05 12:08:47,691] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:48,907] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:08:48,945] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:08:48,963] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:08:48,967] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.291 seconds
[2020-11-05 12:09:00,885] {scheduler_job.py:155} INFO - Started process (PID=16587) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:00,890] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:09:00,891] {logging_mixin.py:112} INFO - [2020-11-05 12:09:00,890] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:02,322] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:02,377] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:09:02,409] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:09:02,415] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.530 seconds
[2020-11-05 12:09:14,123] {scheduler_job.py:155} INFO - Started process (PID=16662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:14,127] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:09:14,127] {logging_mixin.py:112} INFO - [2020-11-05 12:09:14,127] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:15,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:15,198] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:09:15,215] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:09:15,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.096 seconds
[2020-11-05 12:09:27,345] {scheduler_job.py:155} INFO - Started process (PID=16739) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:27,351] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:09:27,353] {logging_mixin.py:112} INFO - [2020-11-05 12:09:27,353] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:28,470] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:28,520] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:09:28,549] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:09:28,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.213 seconds
[2020-11-05 12:09:40,604] {scheduler_job.py:155} INFO - Started process (PID=16809) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:40,607] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:09:40,608] {logging_mixin.py:112} INFO - [2020-11-05 12:09:40,608] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:41,628] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:41,673] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:09:41,689] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:09:41,693] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.090 seconds
[2020-11-05 12:09:53,880] {scheduler_job.py:155} INFO - Started process (PID=16887) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:53,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:09:53,884] {logging_mixin.py:112} INFO - [2020-11-05 12:09:53,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:54,872] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:09:54,923] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:09:54,947] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:09:54,952] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.072 seconds
[2020-11-05 12:10:07,175] {scheduler_job.py:155} INFO - Started process (PID=16956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:07,179] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:10:07,179] {logging_mixin.py:112} INFO - [2020-11-05 12:10:07,179] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:08,252] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:08,290] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:10:08,308] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:10:08,312] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.138 seconds
[2020-11-05 12:10:20,391] {scheduler_job.py:155} INFO - Started process (PID=17029) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:20,405] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:10:20,405] {logging_mixin.py:112} INFO - [2020-11-05 12:10:20,405] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:21,612] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:21,652] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:10:21,669] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:10:21,672] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.281 seconds
[2020-11-05 12:10:33,671] {scheduler_job.py:155} INFO - Started process (PID=17097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:33,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:10:33,677] {logging_mixin.py:112} INFO - [2020-11-05 12:10:33,677] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:34,725] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:34,765] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:10:34,784] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:10:34,788] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-05 12:10:46,891] {scheduler_job.py:155} INFO - Started process (PID=17169) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:46,897] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:10:46,897] {logging_mixin.py:112} INFO - [2020-11-05 12:10:46,897] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:47,938] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:10:47,973] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:10:47,989] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:10:47,994] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-05 12:11:00,070] {scheduler_job.py:155} INFO - Started process (PID=17245) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:00,074] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:11:00,075] {logging_mixin.py:112} INFO - [2020-11-05 12:11:00,075] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:01,147] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:01,187] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:11:01,203] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:11:01,207] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.137 seconds
[2020-11-05 12:11:13,385] {scheduler_job.py:155} INFO - Started process (PID=17320) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:13,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:11:13,392] {logging_mixin.py:112} INFO - [2020-11-05 12:11:13,392] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:14,414] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:14,460] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:11:14,477] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:11:14,480] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.095 seconds
[2020-11-05 12:11:26,584] {scheduler_job.py:155} INFO - Started process (PID=17398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:26,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:11:26,588] {logging_mixin.py:112} INFO - [2020-11-05 12:11:26,588] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:27,649] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:27,682] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:11:27,701] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:11:27,705] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.121 seconds
[2020-11-05 12:11:39,822] {scheduler_job.py:155} INFO - Started process (PID=17472) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:39,827] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:11:39,827] {logging_mixin.py:112} INFO - [2020-11-05 12:11:39,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:40,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:40,869] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:11:40,886] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:11:40,890] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.068 seconds
[2020-11-05 12:11:53,118] {scheduler_job.py:155} INFO - Started process (PID=17548) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:53,131] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:11:53,132] {logging_mixin.py:112} INFO - [2020-11-05 12:11:53,132] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:54,557] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:11:54,615] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:11:54,632] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:11:54,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.520 seconds
[2020-11-05 12:12:06,375] {scheduler_job.py:155} INFO - Started process (PID=17618) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:06,378] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:12:06,379] {logging_mixin.py:112} INFO - [2020-11-05 12:12:06,379] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:07,485] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:07,524] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:12:07,541] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:12:07,546] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.170 seconds
[2020-11-05 12:12:19,657] {scheduler_job.py:155} INFO - Started process (PID=17686) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:19,660] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:12:19,661] {logging_mixin.py:112} INFO - [2020-11-05 12:12:19,661] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:20,780] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:20,820] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:12:20,836] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:12:20,840] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.182 seconds
[2020-11-05 12:12:32,975] {scheduler_job.py:155} INFO - Started process (PID=17760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:32,983] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:12:32,984] {logging_mixin.py:112} INFO - [2020-11-05 12:12:32,984] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:34,051] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:34,090] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:12:34,106] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:12:34,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.135 seconds
[2020-11-05 12:12:46,236] {scheduler_job.py:155} INFO - Started process (PID=17835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:46,257] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:12:46,258] {logging_mixin.py:112} INFO - [2020-11-05 12:12:46,257] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:47,918] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:47,967] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:12:47,988] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:12:47,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.756 seconds
[2020-11-05 12:12:59,429] {scheduler_job.py:155} INFO - Started process (PID=17902) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:12:59,433] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:12:59,434] {logging_mixin.py:112} INFO - [2020-11-05 12:12:59,434] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:00,689] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:00,722] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:13:00,736] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:13:00,740] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.312 seconds
[2020-11-05 12:13:12,653] {scheduler_job.py:155} INFO - Started process (PID=17967) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:12,657] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:13:12,657] {logging_mixin.py:112} INFO - [2020-11-05 12:13:12,657] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:13,756] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:13,805] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:13:13,833] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:13:13,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.186 seconds
[2020-11-05 12:13:25,912] {scheduler_job.py:155} INFO - Started process (PID=18046) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:25,916] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:13:25,917] {logging_mixin.py:112} INFO - [2020-11-05 12:13:25,917] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:27,129] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:27,178] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:13:27,195] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:13:27,199] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.287 seconds
[2020-11-05 12:13:39,152] {scheduler_job.py:155} INFO - Started process (PID=18114) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:39,157] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:13:39,158] {logging_mixin.py:112} INFO - [2020-11-05 12:13:39,158] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:40,219] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:40,258] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:13:40,272] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:13:40,276] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.124 seconds
[2020-11-05 12:13:52,418] {scheduler_job.py:155} INFO - Started process (PID=18190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:52,422] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:13:52,423] {logging_mixin.py:112} INFO - [2020-11-05 12:13:52,423] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:53,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:13:53,734] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:13:53,750] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:13:53,754] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.336 seconds
[2020-11-05 12:14:05,623] {scheduler_job.py:155} INFO - Started process (PID=18268) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:05,626] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:14:05,626] {logging_mixin.py:112} INFO - [2020-11-05 12:14:05,626] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:06,638] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:06,676] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:14:06,698] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:14:06,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.079 seconds
[2020-11-05 12:14:18,786] {scheduler_job.py:155} INFO - Started process (PID=18340) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:18,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:14:18,792] {logging_mixin.py:112} INFO - [2020-11-05 12:14:18,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:19,776] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:19,816] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:14:19,836] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:14:19,839] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.053 seconds
[2020-11-05 12:14:31,964] {scheduler_job.py:155} INFO - Started process (PID=18415) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:31,968] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:14:31,968] {logging_mixin.py:112} INFO - [2020-11-05 12:14:31,968] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:33,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:33,200] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:14:33,218] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:14:33,222] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.258 seconds
[2020-11-05 12:14:45,198] {scheduler_job.py:155} INFO - Started process (PID=18491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:45,203] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:14:45,203] {logging_mixin.py:112} INFO - [2020-11-05 12:14:45,203] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:46,311] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:46,351] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:14:46,370] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:14:46,374] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.176 seconds
[2020-11-05 12:14:58,430] {scheduler_job.py:155} INFO - Started process (PID=18567) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:58,434] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:14:58,435] {logging_mixin.py:112} INFO - [2020-11-05 12:14:58,435] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:59,709] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:14:59,754] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:14:59,773] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:14:59,777] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-05 12:15:11,676] {scheduler_job.py:155} INFO - Started process (PID=18637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:11,680] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:15:11,681] {logging_mixin.py:112} INFO - [2020-11-05 12:15:11,681] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:12,739] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:12,776] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:15:12,794] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:15:12,799] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.124 seconds
[2020-11-05 12:15:24,931] {scheduler_job.py:155} INFO - Started process (PID=18714) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:24,942] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:15:24,946] {logging_mixin.py:112} INFO - [2020-11-05 12:15:24,945] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:26,385] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:26,422] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:15:26,440] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:15:26,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.513 seconds
[2020-11-05 12:15:38,244] {scheduler_job.py:155} INFO - Started process (PID=18782) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:38,249] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:15:38,250] {logging_mixin.py:112} INFO - [2020-11-05 12:15:38,250] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:39,315] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:39,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:15:39,376] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:15:39,381] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.137 seconds
[2020-11-05 12:15:51,458] {scheduler_job.py:155} INFO - Started process (PID=18853) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:51,461] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:15:51,462] {logging_mixin.py:112} INFO - [2020-11-05 12:15:51,462] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:52,726] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:15:52,770] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:15:52,787] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:15:52,791] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.333 seconds
[2020-11-05 12:16:04,648] {scheduler_job.py:155} INFO - Started process (PID=18923) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:04,652] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:16:04,652] {logging_mixin.py:112} INFO - [2020-11-05 12:16:04,652] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:05,868] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:05,920] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:16:05,935] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:16:05,939] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.291 seconds
[2020-11-05 12:16:17,889] {scheduler_job.py:155} INFO - Started process (PID=18999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:17,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:16:17,893] {logging_mixin.py:112} INFO - [2020-11-05 12:16:17,893] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:19,170] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:19,216] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:16:19,236] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:16:19,241] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.352 seconds
[2020-11-05 12:16:31,146] {scheduler_job.py:155} INFO - Started process (PID=19075) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:31,149] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:16:31,150] {logging_mixin.py:112} INFO - [2020-11-05 12:16:31,150] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:32,343] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:32,383] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:16:32,400] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:16:32,404] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.258 seconds
[2020-11-05 12:16:44,416] {scheduler_job.py:155} INFO - Started process (PID=19144) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:44,420] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:16:44,420] {logging_mixin.py:112} INFO - [2020-11-05 12:16:44,420] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:45,421] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:45,462] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:16:45,477] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:16:45,480] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.065 seconds
[2020-11-05 12:16:57,613] {scheduler_job.py:155} INFO - Started process (PID=19220) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:57,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:16:57,622] {logging_mixin.py:112} INFO - [2020-11-05 12:16:57,622] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:59,056] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:16:59,091] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:16:59,106] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:16:59,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.497 seconds
[2020-11-05 12:17:10,862] {scheduler_job.py:155} INFO - Started process (PID=19287) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:10,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:17:10,866] {logging_mixin.py:112} INFO - [2020-11-05 12:17:10,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:11,901] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:11,938] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:17:11,954] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:17:11,958] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.097 seconds
[2020-11-05 12:17:24,064] {scheduler_job.py:155} INFO - Started process (PID=19353) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:24,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:17:24,073] {logging_mixin.py:112} INFO - [2020-11-05 12:17:24,072] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:25,199] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:25,243] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:17:25,260] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:17:25,265] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.200 seconds
[2020-11-05 12:17:37,296] {scheduler_job.py:155} INFO - Started process (PID=19421) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:37,299] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:17:37,300] {logging_mixin.py:112} INFO - [2020-11-05 12:17:37,300] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:38,409] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:38,462] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:17:38,480] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:17:38,484] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.189 seconds
[2020-11-05 12:17:50,563] {scheduler_job.py:155} INFO - Started process (PID=19483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:50,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:17:50,567] {logging_mixin.py:112} INFO - [2020-11-05 12:17:50,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:51,556] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:17:51,596] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:17:51,611] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:17:51,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.052 seconds
[2020-11-05 12:18:03,747] {scheduler_job.py:155} INFO - Started process (PID=19551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:03,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:18:03,752] {logging_mixin.py:112} INFO - [2020-11-05 12:18:03,751] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:04,873] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:04,913] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:18:04,930] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:18:04,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.187 seconds
[2020-11-05 12:18:16,976] {scheduler_job.py:155} INFO - Started process (PID=19609) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:16,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:18:16,980] {logging_mixin.py:112} INFO - [2020-11-05 12:18:16,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:17,954] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:17,997] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:18:18,018] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:18:18,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.046 seconds
[2020-11-05 12:18:30,185] {scheduler_job.py:155} INFO - Started process (PID=19680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:30,205] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:18:30,205] {logging_mixin.py:112} INFO - [2020-11-05 12:18:30,205] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:31,646] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:31,687] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:18:31,705] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:18:31,709] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.524 seconds
[2020-11-05 12:18:43,473] {scheduler_job.py:155} INFO - Started process (PID=19736) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:43,478] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:18:43,479] {logging_mixin.py:112} INFO - [2020-11-05 12:18:43,479] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:44,720] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:44,759] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:18:44,775] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:18:44,779] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.307 seconds
[2020-11-05 12:18:56,693] {scheduler_job.py:155} INFO - Started process (PID=19793) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:56,697] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:18:56,698] {logging_mixin.py:112} INFO - [2020-11-05 12:18:56,698] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:57,935] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:18:57,974] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:18:57,991] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:18:57,995] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.302 seconds
[2020-11-05 12:19:09,856] {scheduler_job.py:155} INFO - Started process (PID=19855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:09,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:19:09,861] {logging_mixin.py:112} INFO - [2020-11-05 12:19:09,861] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:10,943] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:10,982] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:19:10,999] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:19:11,004] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.147 seconds
[2020-11-05 12:19:23,067] {scheduler_job.py:155} INFO - Started process (PID=19912) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:23,070] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:19:23,070] {logging_mixin.py:112} INFO - [2020-11-05 12:19:23,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:24,547] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:24,583] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:19:24,599] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:19:24,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.537 seconds
[2020-11-05 12:19:36,322] {scheduler_job.py:155} INFO - Started process (PID=19971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:36,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:19:36,327] {logging_mixin.py:112} INFO - [2020-11-05 12:19:36,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:37,626] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:37,679] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:19:37,703] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:19:37,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.389 seconds
[2020-11-05 12:19:49,534] {scheduler_job.py:155} INFO - Started process (PID=20025) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:49,538] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:19:49,539] {logging_mixin.py:112} INFO - [2020-11-05 12:19:49,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:50,834] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:19:50,868] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:19:50,883] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:19:50,889] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.355 seconds
[2020-11-05 12:20:02,765] {scheduler_job.py:155} INFO - Started process (PID=20085) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:02,769] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:20:02,770] {logging_mixin.py:112} INFO - [2020-11-05 12:20:02,769] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:04,118] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:04,159] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:20:04,176] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:20:04,180] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.416 seconds
[2020-11-05 12:20:15,977] {scheduler_job.py:155} INFO - Started process (PID=20139) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:15,982] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:20:15,982] {logging_mixin.py:112} INFO - [2020-11-05 12:20:15,982] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:17,058] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:17,103] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:20:17,119] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:20:17,123] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.146 seconds
[2020-11-05 12:20:29,251] {scheduler_job.py:155} INFO - Started process (PID=20193) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:29,264] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:20:29,264] {logging_mixin.py:112} INFO - [2020-11-05 12:20:29,264] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:30,805] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:30,841] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:20:30,858] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:20:30,862] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.611 seconds
[2020-11-05 12:20:42,537] {scheduler_job.py:155} INFO - Started process (PID=20253) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:42,543] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:20:42,544] {logging_mixin.py:112} INFO - [2020-11-05 12:20:42,544] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:43,589] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:43,624] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:20:43,642] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:20:43,647] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.110 seconds
[2020-11-05 12:20:55,776] {scheduler_job.py:155} INFO - Started process (PID=20311) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:55,780] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:20:55,781] {logging_mixin.py:112} INFO - [2020-11-05 12:20:55,781] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:56,823] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:20:56,861] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:20:56,878] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:20:56,882] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.106 seconds
[2020-11-05 12:21:08,961] {scheduler_job.py:155} INFO - Started process (PID=20372) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:08,964] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:21:08,964] {logging_mixin.py:112} INFO - [2020-11-05 12:21:08,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:10,010] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:10,043] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:21:10,059] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:21:10,063] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-05 12:21:22,252] {scheduler_job.py:155} INFO - Started process (PID=20424) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:22,257] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:21:22,258] {logging_mixin.py:112} INFO - [2020-11-05 12:21:22,258] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:23,310] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:23,348] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:21:23,364] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:21:23,369] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-05 12:21:35,405] {scheduler_job.py:155} INFO - Started process (PID=20486) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:35,408] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:21:35,409] {logging_mixin.py:112} INFO - [2020-11-05 12:21:35,408] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:36,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:36,589] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:21:36,605] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:21:36,610] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.205 seconds
[2020-11-05 12:21:48,633] {scheduler_job.py:155} INFO - Started process (PID=20538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:48,637] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:21:48,637] {logging_mixin.py:112} INFO - [2020-11-05 12:21:48,637] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:49,735] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:21:49,780] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:21:49,800] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:21:49,804] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.171 seconds
[2020-11-05 12:22:01,844] {scheduler_job.py:155} INFO - Started process (PID=20597) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:01,848] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:22:01,849] {logging_mixin.py:112} INFO - [2020-11-05 12:22:01,849] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:03,609] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:03,701] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:22:03,721] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:22:03,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.882 seconds
[2020-11-05 12:22:15,076] {scheduler_job.py:155} INFO - Started process (PID=20654) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:15,086] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:22:15,089] {logging_mixin.py:112} INFO - [2020-11-05 12:22:15,089] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:16,370] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:16,406] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:22:16,420] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:22:16,424] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.349 seconds
[2020-11-05 12:22:28,306] {scheduler_job.py:155} INFO - Started process (PID=20709) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:28,311] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:22:28,312] {logging_mixin.py:112} INFO - [2020-11-05 12:22:28,311] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:29,495] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:29,537] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:22:29,553] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:22:29,557] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.252 seconds
[2020-11-05 12:22:41,503] {scheduler_job.py:155} INFO - Started process (PID=20768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:41,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:22:41,507] {logging_mixin.py:112} INFO - [2020-11-05 12:22:41,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:42,580] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:42,618] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:22:42,632] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:22:42,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.134 seconds
[2020-11-05 12:22:54,706] {scheduler_job.py:155} INFO - Started process (PID=20823) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:54,711] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:22:54,711] {logging_mixin.py:112} INFO - [2020-11-05 12:22:54,711] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:55,831] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:22:55,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:22:55,891] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:22:55,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.188 seconds
[2020-11-05 12:23:07,905] {scheduler_job.py:155} INFO - Started process (PID=20882) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:07,909] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:23:07,910] {logging_mixin.py:112} INFO - [2020-11-05 12:23:07,910] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:09,021] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:09,082] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:23:09,099] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:23:09,103] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.198 seconds
[2020-11-05 12:23:21,128] {scheduler_job.py:155} INFO - Started process (PID=20936) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:21,132] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:23:21,133] {logging_mixin.py:112} INFO - [2020-11-05 12:23:21,133] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:22,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:22,198] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:23:22,216] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:23:22,220] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.092 seconds
[2020-11-05 12:23:34,334] {scheduler_job.py:155} INFO - Started process (PID=20996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:34,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:23:34,348] {logging_mixin.py:112} INFO - [2020-11-05 12:23:34,348] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:35,711] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:35,749] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:23:35,766] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:23:35,770] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.436 seconds
[2020-11-05 12:23:47,557] {scheduler_job.py:155} INFO - Started process (PID=21052) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:47,562] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:23:47,563] {logging_mixin.py:112} INFO - [2020-11-05 12:23:47,563] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:48,628] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:23:48,667] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:23:48,684] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:23:48,687] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.130 seconds
[2020-11-05 12:24:00,908] {scheduler_job.py:155} INFO - Started process (PID=21112) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:00,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:24:00,916] {logging_mixin.py:112} INFO - [2020-11-05 12:24:00,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:02,404] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:02,448] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:24:02,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:24:02,478] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.570 seconds
[2020-11-05 12:24:14,083] {scheduler_job.py:155} INFO - Started process (PID=21174) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:14,087] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:24:14,088] {logging_mixin.py:112} INFO - [2020-11-05 12:24:14,088] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:15,184] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:15,218] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:24:15,237] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:24:15,240] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.157 seconds
[2020-11-05 12:24:27,295] {scheduler_job.py:155} INFO - Started process (PID=21227) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:27,299] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:24:27,299] {logging_mixin.py:112} INFO - [2020-11-05 12:24:27,299] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:28,367] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:28,405] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:24:28,421] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:24:28,427] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.132 seconds
[2020-11-05 12:24:40,490] {scheduler_job.py:155} INFO - Started process (PID=21288) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:40,496] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:24:40,509] {logging_mixin.py:112} INFO - [2020-11-05 12:24:40,509] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:41,914] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:41,951] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:24:41,968] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:24:41,972] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.482 seconds
[2020-11-05 12:24:53,718] {scheduler_job.py:155} INFO - Started process (PID=21341) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:53,723] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:24:53,724] {logging_mixin.py:112} INFO - [2020-11-05 12:24:53,723] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:54,959] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:24:55,003] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:24:55,024] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:24:55,030] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.311 seconds
[2020-11-05 12:25:06,930] {scheduler_job.py:155} INFO - Started process (PID=21419) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:06,943] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:25:06,945] {logging_mixin.py:112} INFO - [2020-11-05 12:25:06,945] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:08,810] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:08,871] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:25:08,888] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:25:08,892] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.962 seconds
[2020-11-05 12:25:20,136] {scheduler_job.py:155} INFO - Started process (PID=21473) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:20,142] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:25:20,145] {logging_mixin.py:112} INFO - [2020-11-05 12:25:20,144] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:21,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:21,607] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:25:21,635] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:25:21,640] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.504 seconds
[2020-11-05 12:25:33,326] {scheduler_job.py:155} INFO - Started process (PID=21528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:33,329] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:25:33,330] {logging_mixin.py:112} INFO - [2020-11-05 12:25:33,329] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:34,468] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:34,505] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:25:34,523] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:25:34,528] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.202 seconds
[2020-11-05 12:25:46,535] {scheduler_job.py:155} INFO - Started process (PID=21587) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:46,538] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:25:46,539] {logging_mixin.py:112} INFO - [2020-11-05 12:25:46,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:47,586] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:47,628] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:25:47,646] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:25:47,651] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-05 12:25:59,721] {scheduler_job.py:155} INFO - Started process (PID=21640) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:25:59,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:25:59,725] {logging_mixin.py:112} INFO - [2020-11-05 12:25:59,725] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:00,776] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:00,815] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:26:00,833] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:26:00,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-05 12:26:12,984] {scheduler_job.py:155} INFO - Started process (PID=21700) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:12,988] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:26:12,989] {logging_mixin.py:112} INFO - [2020-11-05 12:26:12,989] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:14,390] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:14,424] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:26:14,440] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:26:14,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.460 seconds
[2020-11-05 12:26:26,179] {scheduler_job.py:155} INFO - Started process (PID=21753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:26,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:26:26,188] {logging_mixin.py:112} INFO - [2020-11-05 12:26:26,187] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:27,489] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:27,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:26:27,556] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:26:27,562] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.383 seconds
[2020-11-05 12:26:39,386] {scheduler_job.py:155} INFO - Started process (PID=21813) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:39,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:26:39,400] {logging_mixin.py:112} INFO - [2020-11-05 12:26:39,398] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:41,065] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:41,100] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:26:41,117] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:26:41,122] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.736 seconds
[2020-11-05 12:26:52,583] {scheduler_job.py:155} INFO - Started process (PID=21865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:52,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:26:52,588] {logging_mixin.py:112} INFO - [2020-11-05 12:26:52,587] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:53,719] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:26:53,760] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:26:53,778] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:26:53,784] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.201 seconds
[2020-11-05 12:27:05,783] {scheduler_job.py:155} INFO - Started process (PID=21920) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:05,788] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:27:05,797] {logging_mixin.py:112} INFO - [2020-11-05 12:27:05,797] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:07,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:07,624] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:27:07,660] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:27:07,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.883 seconds
[2020-11-05 12:27:18,958] {scheduler_job.py:155} INFO - Started process (PID=21982) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:19,098] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:27:19,098] {logging_mixin.py:112} INFO - [2020-11-05 12:27:19,098] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:20,157] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:20,198] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:27:20,214] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:27:20,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.262 seconds
[2020-11-05 12:27:32,252] {scheduler_job.py:155} INFO - Started process (PID=22035) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:32,257] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:27:32,258] {logging_mixin.py:112} INFO - [2020-11-05 12:27:32,258] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:33,768] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:33,827] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:27:33,863] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:27:33,884] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.632 seconds
[2020-11-05 12:27:45,408] {scheduler_job.py:155} INFO - Started process (PID=22108) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:45,412] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:27:45,413] {logging_mixin.py:112} INFO - [2020-11-05 12:27:45,413] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:46,703] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:46,765] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:27:46,789] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:27:46,796] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.388 seconds
[2020-11-05 12:27:58,635] {scheduler_job.py:155} INFO - Started process (PID=22181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:58,639] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:27:58,640] {logging_mixin.py:112} INFO - [2020-11-05 12:27:58,640] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:59,640] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:27:59,684] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:27:59,702] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:27:59,706] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.071 seconds
[2020-11-05 12:28:11,816] {scheduler_job.py:155} INFO - Started process (PID=22260) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:11,821] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:28:11,822] {logging_mixin.py:112} INFO - [2020-11-05 12:28:11,822] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:13,498] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:13,543] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:28:13,562] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:28:13,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.751 seconds
[2020-11-05 12:28:25,017] {scheduler_job.py:155} INFO - Started process (PID=22331) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:25,020] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:28:25,021] {logging_mixin.py:112} INFO - [2020-11-05 12:28:25,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:26,047] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:26,090] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:28:26,107] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:28:26,112] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.096 seconds
[2020-11-05 12:28:38,187] {scheduler_job.py:155} INFO - Started process (PID=22407) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:38,192] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:28:38,194] {logging_mixin.py:112} INFO - [2020-11-05 12:28:38,193] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:39,530] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:39,569] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:28:39,585] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:28:39,589] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.403 seconds
[2020-11-05 12:28:51,431] {scheduler_job.py:155} INFO - Started process (PID=22475) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:51,441] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:28:51,442] {logging_mixin.py:112} INFO - [2020-11-05 12:28:51,442] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:52,479] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:28:52,517] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:28:52,535] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:28:52,539] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.108 seconds
[2020-11-05 12:29:04,616] {scheduler_job.py:155} INFO - Started process (PID=22552) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:04,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:29:04,623] {logging_mixin.py:112} INFO - [2020-11-05 12:29:04,623] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:05,714] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:05,758] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:29:05,785] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:29:05,790] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.174 seconds
[2020-11-05 12:29:17,787] {scheduler_job.py:155} INFO - Started process (PID=22624) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:17,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:29:17,800] {logging_mixin.py:112} INFO - [2020-11-05 12:29:17,792] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:18,927] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:18,971] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:29:18,989] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:29:18,994] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.207 seconds
[2020-11-05 12:29:31,063] {scheduler_job.py:155} INFO - Started process (PID=22695) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:31,070] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:29:31,070] {logging_mixin.py:112} INFO - [2020-11-05 12:29:31,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:32,117] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:32,159] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:29:32,177] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:29:32,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.118 seconds
[2020-11-05 12:29:44,303] {scheduler_job.py:155} INFO - Started process (PID=22768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:44,309] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:29:44,310] {logging_mixin.py:112} INFO - [2020-11-05 12:29:44,310] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:45,784] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:45,834] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:29:45,852] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:29:45,856] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.554 seconds
[2020-11-05 12:29:57,552] {scheduler_job.py:155} INFO - Started process (PID=22836) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:57,556] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:29:57,557] {logging_mixin.py:112} INFO - [2020-11-05 12:29:57,557] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:58,622] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:29:58,663] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:29:58,681] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:29:58,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.132 seconds
[2020-11-05 12:30:10,774] {scheduler_job.py:155} INFO - Started process (PID=22914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:10,776] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:30:10,777] {logging_mixin.py:112} INFO - [2020-11-05 12:30:10,777] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:12,260] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:12,307] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:30:12,324] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:30:12,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.555 seconds
[2020-11-05 12:30:24,013] {scheduler_job.py:155} INFO - Started process (PID=22987) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:24,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:30:24,017] {logging_mixin.py:112} INFO - [2020-11-05 12:30:24,017] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:25,194] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:25,241] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:30:25,260] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:30:25,266] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.252 seconds
[2020-11-05 12:30:37,231] {scheduler_job.py:155} INFO - Started process (PID=23060) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:37,235] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:30:37,235] {logging_mixin.py:112} INFO - [2020-11-05 12:30:37,235] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:38,656] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:38,700] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:30:38,719] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:30:38,724] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.493 seconds
[2020-11-05 12:30:50,467] {scheduler_job.py:155} INFO - Started process (PID=23133) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:50,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:30:50,471] {logging_mixin.py:112} INFO - [2020-11-05 12:30:50,471] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:51,643] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:30:51,679] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:30:51,695] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:30:51,699] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-05 12:31:03,689] {scheduler_job.py:155} INFO - Started process (PID=23204) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:03,694] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:31:03,694] {logging_mixin.py:112} INFO - [2020-11-05 12:31:03,694] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:04,672] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:04,710] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:31:04,732] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:31:04,736] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.047 seconds
[2020-11-05 12:31:16,857] {scheduler_job.py:155} INFO - Started process (PID=23279) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:16,860] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:31:16,860] {logging_mixin.py:112} INFO - [2020-11-05 12:31:16,860] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:17,954] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:17,989] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:31:18,004] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:31:18,008] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.151 seconds
[2020-11-05 12:31:30,144] {scheduler_job.py:155} INFO - Started process (PID=23348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:30,147] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:31:30,147] {logging_mixin.py:112} INFO - [2020-11-05 12:31:30,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:31,100] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:31,141] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:31:31,159] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:31:31,163] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.020 seconds
[2020-11-05 12:31:43,361] {scheduler_job.py:155} INFO - Started process (PID=23427) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:43,381] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:31:43,381] {logging_mixin.py:112} INFO - [2020-11-05 12:31:43,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:44,838] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:44,878] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:31:44,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:31:44,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.536 seconds
[2020-11-05 12:31:56,548] {scheduler_job.py:155} INFO - Started process (PID=23496) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:56,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:31:56,553] {logging_mixin.py:112} INFO - [2020-11-05 12:31:56,553] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:57,576] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:31:57,626] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:31:57,645] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:31:57,649] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.101 seconds
[2020-11-05 12:32:09,714] {scheduler_job.py:155} INFO - Started process (PID=23566) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:09,720] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:32:09,722] {logging_mixin.py:112} INFO - [2020-11-05 12:32:09,722] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:10,879] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:10,925] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:32:10,942] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:32:10,947] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-05 12:32:22,957] {scheduler_job.py:155} INFO - Started process (PID=23639) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:22,962] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:32:22,962] {logging_mixin.py:112} INFO - [2020-11-05 12:32:22,962] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:23,920] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:23,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:32:23,973] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:32:23,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.022 seconds
[2020-11-05 12:32:36,206] {scheduler_job.py:155} INFO - Started process (PID=23714) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:36,211] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:32:36,212] {logging_mixin.py:112} INFO - [2020-11-05 12:32:36,212] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:37,232] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:37,273] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:32:37,289] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:32:37,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.087 seconds
[2020-11-05 12:32:49,391] {scheduler_job.py:155} INFO - Started process (PID=23791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:49,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:32:49,397] {logging_mixin.py:112} INFO - [2020-11-05 12:32:49,397] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:50,712] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:32:50,751] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:32:50,768] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:32:50,772] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.380 seconds
[2020-11-05 12:33:02,658] {scheduler_job.py:155} INFO - Started process (PID=23868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:02,662] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:33:02,663] {logging_mixin.py:112} INFO - [2020-11-05 12:33:02,663] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:03,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:03,728] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:33:03,747] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:33:03,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.093 seconds
[2020-11-05 12:33:15,949] {scheduler_job.py:155} INFO - Started process (PID=23946) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:15,955] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:33:15,955] {logging_mixin.py:112} INFO - [2020-11-05 12:33:15,955] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:17,259] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:17,297] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:33:17,316] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:33:17,320] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.371 seconds
[2020-11-05 12:33:29,220] {scheduler_job.py:155} INFO - Started process (PID=24020) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:29,224] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:33:29,224] {logging_mixin.py:112} INFO - [2020-11-05 12:33:29,224] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:30,423] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:30,476] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:33:30,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:33:30,498] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.278 seconds
[2020-11-05 12:33:42,442] {scheduler_job.py:155} INFO - Started process (PID=24108) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:42,446] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:33:42,447] {logging_mixin.py:112} INFO - [2020-11-05 12:33:42,446] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:43,588] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:43,637] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:33:43,656] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:33:43,661] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.219 seconds
[2020-11-05 12:33:55,661] {scheduler_job.py:155} INFO - Started process (PID=24180) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:55,666] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:33:55,666] {logging_mixin.py:112} INFO - [2020-11-05 12:33:55,666] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:56,852] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:33:56,890] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:33:56,905] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:33:56,908] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-05 12:34:08,887] {scheduler_job.py:155} INFO - Started process (PID=24251) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:08,895] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:34:08,896] {logging_mixin.py:112} INFO - [2020-11-05 12:34:08,895] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:09,842] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:09,895] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:34:09,919] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:34:09,929] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.042 seconds
[2020-11-05 12:34:22,108] {scheduler_job.py:155} INFO - Started process (PID=24328) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:22,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:34:22,112] {logging_mixin.py:112} INFO - [2020-11-05 12:34:22,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:23,187] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:23,248] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:34:23,272] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:34:23,278] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.170 seconds
[2020-11-05 12:34:35,326] {scheduler_job.py:155} INFO - Started process (PID=24403) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:35,330] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:34:35,331] {logging_mixin.py:112} INFO - [2020-11-05 12:34:35,330] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:36,426] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:36,475] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:34:36,497] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:34:36,501] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.175 seconds
[2020-11-05 12:34:48,593] {scheduler_job.py:155} INFO - Started process (PID=24481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:48,597] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:34:48,598] {logging_mixin.py:112} INFO - [2020-11-05 12:34:48,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:49,609] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:34:49,647] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:34:49,665] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:34:49,669] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.077 seconds
[2020-11-05 12:35:01,873] {scheduler_job.py:155} INFO - Started process (PID=24562) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:01,877] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:35:01,877] {logging_mixin.py:112} INFO - [2020-11-05 12:35:01,877] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:02,885] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:02,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:35:02,946] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:35:02,951] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-05 12:35:15,065] {scheduler_job.py:155} INFO - Started process (PID=24643) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:15,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:35:15,078] {logging_mixin.py:112} INFO - [2020-11-05 12:35:15,078] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:16,367] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:16,418] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:35:16,437] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:35:16,441] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.376 seconds
[2020-11-05 12:35:28,287] {scheduler_job.py:155} INFO - Started process (PID=24713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:28,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:35:28,293] {logging_mixin.py:112} INFO - [2020-11-05 12:35:28,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:29,274] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:29,317] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:35:29,334] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:35:29,337] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.050 seconds
[2020-11-05 12:35:41,492] {scheduler_job.py:155} INFO - Started process (PID=24791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:41,496] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:35:41,497] {logging_mixin.py:112} INFO - [2020-11-05 12:35:41,496] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:42,493] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:42,538] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:35:42,556] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:35:42,560] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.068 seconds
[2020-11-05 12:35:54,672] {scheduler_job.py:155} INFO - Started process (PID=24865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:54,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:35:54,677] {logging_mixin.py:112} INFO - [2020-11-05 12:35:54,677] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:55,740] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:35:55,782] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:35:55,801] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:35:55,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.133 seconds
[2020-11-05 12:36:07,894] {scheduler_job.py:155} INFO - Started process (PID=24941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:07,897] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:36:07,898] {logging_mixin.py:112} INFO - [2020-11-05 12:36:07,898] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:08,927] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:08,970] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:36:08,986] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:36:08,990] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.095 seconds
[2020-11-05 12:36:21,087] {scheduler_job.py:155} INFO - Started process (PID=25014) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:21,091] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:36:21,092] {logging_mixin.py:112} INFO - [2020-11-05 12:36:21,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:22,205] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:22,245] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:36:22,260] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:36:22,263] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.177 seconds
[2020-11-05 12:36:34,329] {scheduler_job.py:155} INFO - Started process (PID=25093) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:34,332] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:36:34,333] {logging_mixin.py:112} INFO - [2020-11-05 12:36:34,333] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:35,325] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:35,365] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:36:35,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:36:35,387] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.058 seconds
[2020-11-05 12:36:47,544] {scheduler_job.py:155} INFO - Started process (PID=25172) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:47,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:36:47,555] {logging_mixin.py:112} INFO - [2020-11-05 12:36:47,555] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:48,927] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:36:48,963] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:36:48,979] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:36:48,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.439 seconds
[2020-11-05 12:37:00,782] {scheduler_job.py:155} INFO - Started process (PID=25240) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:00,786] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:37:00,787] {logging_mixin.py:112} INFO - [2020-11-05 12:37:00,787] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:01,846] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:01,888] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:37:01,911] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:37:01,916] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.134 seconds
[2020-11-05 12:37:13,971] {scheduler_job.py:155} INFO - Started process (PID=25307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:13,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:37:13,977] {logging_mixin.py:112} INFO - [2020-11-05 12:37:13,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:15,645] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:15,703] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:37:15,735] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:37:15,741] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.771 seconds
[2020-11-05 12:37:27,145] {scheduler_job.py:155} INFO - Started process (PID=25381) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:27,150] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:37:27,154] {logging_mixin.py:112} INFO - [2020-11-05 12:37:27,151] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:28,288] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:28,331] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:37:28,346] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:37:28,351] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.205 seconds
[2020-11-05 12:37:40,393] {scheduler_job.py:155} INFO - Started process (PID=25454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:40,397] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:37:40,398] {logging_mixin.py:112} INFO - [2020-11-05 12:37:40,398] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:41,378] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:41,419] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:37:41,436] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:37:41,439] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.047 seconds
[2020-11-05 12:37:53,605] {scheduler_job.py:155} INFO - Started process (PID=25530) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:53,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:37:53,612] {logging_mixin.py:112} INFO - [2020-11-05 12:37:53,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:54,585] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:37:54,639] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:37:54,655] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:37:54,659] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.054 seconds
[2020-11-05 12:38:06,830] {scheduler_job.py:155} INFO - Started process (PID=25602) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:06,835] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:38:06,836] {logging_mixin.py:112} INFO - [2020-11-05 12:38:06,836] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:08,020] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:08,068] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:38:08,085] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:38:08,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.259 seconds
[2020-11-05 12:38:20,036] {scheduler_job.py:155} INFO - Started process (PID=25678) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:20,047] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:38:20,048] {logging_mixin.py:112} INFO - [2020-11-05 12:38:20,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:21,277] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:21,310] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:38:21,324] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:38:21,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.292 seconds
[2020-11-05 12:38:33,272] {scheduler_job.py:155} INFO - Started process (PID=25750) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:33,277] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:38:33,277] {logging_mixin.py:112} INFO - [2020-11-05 12:38:33,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:34,289] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:34,331] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:38:34,346] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:38:34,349] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.078 seconds
[2020-11-05 12:38:46,492] {scheduler_job.py:155} INFO - Started process (PID=25826) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:46,507] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:38:46,508] {logging_mixin.py:112} INFO - [2020-11-05 12:38:46,508] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:49,303] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:38:49,373] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:38:49,394] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:38:49,398] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.906 seconds
[2020-11-05 12:39:00,824] {scheduler_job.py:155} INFO - Started process (PID=25901) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:00,827] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:39:00,828] {logging_mixin.py:112} INFO - [2020-11-05 12:39:00,828] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:01,841] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:01,881] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:39:01,897] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:39:01,902] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.077 seconds
[2020-11-05 12:39:14,041] {scheduler_job.py:155} INFO - Started process (PID=25977) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:14,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:39:14,047] {logging_mixin.py:112} INFO - [2020-11-05 12:39:14,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:15,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:15,481] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:39:15,502] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:39:15,507] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.466 seconds
[2020-11-05 12:39:27,255] {scheduler_job.py:155} INFO - Started process (PID=26047) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:27,258] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:39:27,258] {logging_mixin.py:112} INFO - [2020-11-05 12:39:27,258] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:28,495] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:28,546] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:39:28,571] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:39:28,576] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.322 seconds
[2020-11-05 12:39:40,452] {scheduler_job.py:155} INFO - Started process (PID=26116) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:40,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:39:40,456] {logging_mixin.py:112} INFO - [2020-11-05 12:39:40,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:41,418] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:41,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:39:41,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:39:41,477] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.025 seconds
[2020-11-05 12:39:53,617] {scheduler_job.py:155} INFO - Started process (PID=26194) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:53,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:39:53,622] {logging_mixin.py:112} INFO - [2020-11-05 12:39:53,622] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:54,841] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:39:54,895] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:39:54,914] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:39:54,918] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.301 seconds
[2020-11-05 12:40:06,929] {scheduler_job.py:155} INFO - Started process (PID=26263) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:06,933] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:40:06,933] {logging_mixin.py:112} INFO - [2020-11-05 12:40:06,933] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:07,876] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:07,926] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:40:07,945] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:40:07,949] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.020 seconds
[2020-11-05 12:40:20,306] {scheduler_job.py:155} INFO - Started process (PID=26344) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:20,354] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:40:20,355] {logging_mixin.py:112} INFO - [2020-11-05 12:40:20,354] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:22,297] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:22,346] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:40:22,368] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:40:22,372] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.066 seconds
[2020-11-05 12:40:34,391] {scheduler_job.py:155} INFO - Started process (PID=26409) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:34,395] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:40:34,395] {logging_mixin.py:112} INFO - [2020-11-05 12:40:34,395] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:35,368] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:35,411] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:40:35,431] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:40:35,438] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.047 seconds
[2020-11-05 12:40:47,558] {scheduler_job.py:155} INFO - Started process (PID=26484) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:47,574] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:40:47,575] {logging_mixin.py:112} INFO - [2020-11-05 12:40:47,575] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:48,703] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:40:48,744] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:40:48,766] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:40:48,771] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.213 seconds
[2020-11-05 12:41:00,765] {scheduler_job.py:155} INFO - Started process (PID=26559) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:00,770] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:41:00,770] {logging_mixin.py:112} INFO - [2020-11-05 12:41:00,770] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:01,950] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:01,989] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:41:02,006] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:41:02,010] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.245 seconds
[2020-11-05 12:41:14,057] {scheduler_job.py:155} INFO - Started process (PID=26617) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:14,062] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:41:14,064] {logging_mixin.py:112} INFO - [2020-11-05 12:41:14,064] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:15,751] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:15,798] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:41:15,814] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:41:15,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.760 seconds
[2020-11-05 12:41:27,261] {scheduler_job.py:155} INFO - Started process (PID=26681) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:27,265] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:41:27,266] {logging_mixin.py:112} INFO - [2020-11-05 12:41:27,265] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:28,436] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:28,492] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:41:28,512] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:41:28,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.256 seconds
[2020-11-05 12:41:40,448] {scheduler_job.py:155} INFO - Started process (PID=26750) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:40,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:41:40,452] {logging_mixin.py:112} INFO - [2020-11-05 12:41:40,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:41,806] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:41,854] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:41:41,873] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:41:41,879] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.431 seconds
[2020-11-05 12:41:53,735] {scheduler_job.py:155} INFO - Started process (PID=26822) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:53,741] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:41:53,743] {logging_mixin.py:112} INFO - [2020-11-05 12:41:53,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:55,282] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:41:55,341] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:41:55,374] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:41:55,384] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.649 seconds
[2020-11-05 12:42:06,968] {scheduler_job.py:155} INFO - Started process (PID=26881) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:06,975] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:42:06,975] {logging_mixin.py:112} INFO - [2020-11-05 12:42:06,975] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:08,613] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:08,680] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:42:08,717] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:42:08,723] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.756 seconds
[2020-11-05 12:42:20,172] {scheduler_job.py:155} INFO - Started process (PID=26947) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:20,175] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:42:20,176] {logging_mixin.py:112} INFO - [2020-11-05 12:42:20,175] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:22,294] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:22,332] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:42:22,348] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:42:22,352] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.180 seconds
[2020-11-05 12:42:34,479] {scheduler_job.py:155} INFO - Started process (PID=27008) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:34,503] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:42:34,506] {logging_mixin.py:112} INFO - [2020-11-05 12:42:34,503] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:35,763] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:35,803] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:42:35,824] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:42:35,831] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.352 seconds
[2020-11-05 12:42:47,648] {scheduler_job.py:155} INFO - Started process (PID=27067) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:47,653] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:42:47,654] {logging_mixin.py:112} INFO - [2020-11-05 12:42:47,654] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:48,903] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:42:48,954] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:42:49,003] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:42:49,009] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.361 seconds
[2020-11-05 12:43:00,874] {scheduler_job.py:155} INFO - Started process (PID=27131) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:00,878] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:43:00,879] {logging_mixin.py:112} INFO - [2020-11-05 12:43:00,879] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:02,073] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:02,117] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:43:02,132] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:43:02,137] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.263 seconds
[2020-11-05 12:43:14,132] {scheduler_job.py:155} INFO - Started process (PID=27188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:14,165] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:43:14,165] {logging_mixin.py:112} INFO - [2020-11-05 12:43:14,165] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:15,418] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:15,470] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:43:15,490] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:43:15,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.362 seconds
[2020-11-05 12:43:27,453] {scheduler_job.py:155} INFO - Started process (PID=27252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:27,458] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:43:27,459] {logging_mixin.py:112} INFO - [2020-11-05 12:43:27,459] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:28,706] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:28,752] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:43:28,780] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:43:28,785] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.332 seconds
[2020-11-05 12:43:40,670] {scheduler_job.py:155} INFO - Started process (PID=27310) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:40,685] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:43:40,686] {logging_mixin.py:112} INFO - [2020-11-05 12:43:40,685] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:42,138] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:42,194] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:43:42,221] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:43:42,226] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.556 seconds
[2020-11-05 12:43:53,897] {scheduler_job.py:155} INFO - Started process (PID=27379) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:53,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:43:53,902] {logging_mixin.py:112} INFO - [2020-11-05 12:43:53,901] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:55,682] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:43:55,733] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:43:55,758] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:43:55,762] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.865 seconds
[2020-11-05 12:44:07,151] {scheduler_job.py:155} INFO - Started process (PID=27437) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:07,155] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:44:07,155] {logging_mixin.py:112} INFO - [2020-11-05 12:44:07,155] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:08,231] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:08,298] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:44:08,315] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:44:08,319] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.168 seconds
[2020-11-05 12:44:20,331] {scheduler_job.py:155} INFO - Started process (PID=27497) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:20,335] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:44:20,336] {logging_mixin.py:112} INFO - [2020-11-05 12:44:20,335] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:21,748] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:21,798] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:44:21,817] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:44:21,820] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.489 seconds
[2020-11-05 12:44:33,614] {scheduler_job.py:155} INFO - Started process (PID=27560) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:33,618] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:44:33,619] {logging_mixin.py:112} INFO - [2020-11-05 12:44:33,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:35,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:35,284] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:44:35,308] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:44:35,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.702 seconds
[2020-11-05 12:44:46,836] {scheduler_job.py:155} INFO - Started process (PID=27619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:46,840] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:44:46,841] {logging_mixin.py:112} INFO - [2020-11-05 12:44:46,841] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:48,040] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:44:48,082] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:44:48,112] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:44:48,117] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.281 seconds
[2020-11-05 12:45:00,031] {scheduler_job.py:155} INFO - Started process (PID=27684) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:00,036] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:45:00,037] {logging_mixin.py:112} INFO - [2020-11-05 12:45:00,036] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:01,429] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:01,465] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:45:01,481] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:45:01,485] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.454 seconds
[2020-11-05 12:45:13,257] {scheduler_job.py:155} INFO - Started process (PID=27747) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:13,261] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:45:13,262] {logging_mixin.py:112} INFO - [2020-11-05 12:45:13,261] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:14,757] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:14,790] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:45:14,804] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:45:14,807] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.550 seconds
[2020-11-05 12:45:26,462] {scheduler_job.py:155} INFO - Started process (PID=27814) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:26,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:45:26,472] {logging_mixin.py:112} INFO - [2020-11-05 12:45:26,472] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:27,763] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:27,805] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:45:27,824] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:45:27,830] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.369 seconds
[2020-11-05 12:45:39,723] {scheduler_job.py:155} INFO - Started process (PID=27871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:39,726] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:45:39,727] {logging_mixin.py:112} INFO - [2020-11-05 12:45:39,727] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:40,797] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:40,864] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:45:40,897] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:45:40,902] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.180 seconds
[2020-11-05 12:45:52,997] {scheduler_job.py:155} INFO - Started process (PID=27940) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:53,008] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:45:53,008] {logging_mixin.py:112} INFO - [2020-11-05 12:45:53,008] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:54,458] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:45:54,500] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:45:54,529] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:45:54,535] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.538 seconds
[2020-11-05 12:46:06,228] {scheduler_job.py:155} INFO - Started process (PID=28001) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:06,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:46:06,234] {logging_mixin.py:112} INFO - [2020-11-05 12:46:06,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:07,370] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:07,418] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:46:07,443] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:46:07,446] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.218 seconds
[2020-11-05 12:46:19,439] {scheduler_job.py:155} INFO - Started process (PID=28060) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:19,444] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:46:19,459] {logging_mixin.py:112} INFO - [2020-11-05 12:46:19,458] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:20,722] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:20,758] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:46:20,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:46:20,779] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.339 seconds
[2020-11-05 12:46:32,640] {scheduler_job.py:155} INFO - Started process (PID=28125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:32,643] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:46:32,644] {logging_mixin.py:112} INFO - [2020-11-05 12:46:32,644] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:33,841] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:33,881] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:46:33,901] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:46:33,906] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.266 seconds
[2020-11-05 12:46:45,854] {scheduler_job.py:155} INFO - Started process (PID=28185) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:45,859] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:46:45,859] {logging_mixin.py:112} INFO - [2020-11-05 12:46:45,859] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:47,156] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:47,206] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:46:47,225] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:46:47,231] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.377 seconds
[2020-11-05 12:46:59,129] {scheduler_job.py:155} INFO - Started process (PID=28262) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:46:59,165] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:46:59,169] {logging_mixin.py:112} INFO - [2020-11-05 12:46:59,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:00,644] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:00,699] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:47:00,715] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:47:00,720] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.591 seconds
[2020-11-05 12:47:12,406] {scheduler_job.py:155} INFO - Started process (PID=28321) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:12,416] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:47:12,416] {logging_mixin.py:112} INFO - [2020-11-05 12:47:12,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:13,578] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:13,622] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:47:13,636] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:47:13,639] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.234 seconds
[2020-11-05 12:47:25,640] {scheduler_job.py:155} INFO - Started process (PID=28386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:25,644] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:47:25,645] {logging_mixin.py:112} INFO - [2020-11-05 12:47:25,644] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:27,042] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:27,089] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:47:27,123] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:47:27,132] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.491 seconds
[2020-11-05 12:47:38,877] {scheduler_job.py:155} INFO - Started process (PID=28447) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:38,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:47:38,884] {logging_mixin.py:112} INFO - [2020-11-05 12:47:38,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:40,346] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:40,402] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:47:40,427] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:47:40,434] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.558 seconds
[2020-11-05 12:47:52,124] {scheduler_job.py:155} INFO - Started process (PID=28511) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:52,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:47:52,130] {logging_mixin.py:112} INFO - [2020-11-05 12:47:52,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:53,622] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:47:53,682] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:47:53,699] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:47:53,703] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.579 seconds
[2020-11-05 12:48:05,337] {scheduler_job.py:155} INFO - Started process (PID=28576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:05,345] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:48:05,346] {logging_mixin.py:112} INFO - [2020-11-05 12:48:05,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:06,585] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:06,631] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:48:06,648] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:48:06,654] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.318 seconds
[2020-11-05 12:48:18,579] {scheduler_job.py:155} INFO - Started process (PID=28636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:18,583] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:48:18,584] {logging_mixin.py:112} INFO - [2020-11-05 12:48:18,584] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:19,882] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:19,927] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:48:19,944] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:48:19,948] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.368 seconds
[2020-11-05 12:48:31,832] {scheduler_job.py:155} INFO - Started process (PID=28703) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:31,836] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:48:31,836] {logging_mixin.py:112} INFO - [2020-11-05 12:48:31,836] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:33,233] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:33,268] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:48:33,286] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:48:33,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.459 seconds
[2020-11-05 12:48:45,138] {scheduler_job.py:155} INFO - Started process (PID=28761) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:45,143] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:48:45,144] {logging_mixin.py:112} INFO - [2020-11-05 12:48:45,144] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:46,400] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:46,436] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:48:46,453] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:48:46,456] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.318 seconds
[2020-11-05 12:48:58,371] {scheduler_job.py:155} INFO - Started process (PID=28831) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:48:58,377] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:48:58,387] {logging_mixin.py:112} INFO - [2020-11-05 12:48:58,387] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:00,280] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:00,330] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:49:00,354] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:49:00,366] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.994 seconds
[2020-11-05 12:49:11,644] {scheduler_job.py:155} INFO - Started process (PID=28893) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:11,649] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:49:11,650] {logging_mixin.py:112} INFO - [2020-11-05 12:49:11,650] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:12,859] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:12,906] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:49:12,930] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:49:12,939] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.295 seconds
[2020-11-05 12:49:24,853] {scheduler_job.py:155} INFO - Started process (PID=28958) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:24,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:49:24,862] {logging_mixin.py:112} INFO - [2020-11-05 12:49:24,862] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:26,376] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:26,441] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:49:26,458] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:49:26,462] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.610 seconds
[2020-11-05 12:49:38,075] {scheduler_job.py:155} INFO - Started process (PID=29019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:38,081] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:49:38,081] {logging_mixin.py:112} INFO - [2020-11-05 12:49:38,081] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:39,325] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:39,362] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:49:39,378] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:49:39,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-05 12:49:51,327] {scheduler_job.py:155} INFO - Started process (PID=29081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:51,331] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:49:51,332] {logging_mixin.py:112} INFO - [2020-11-05 12:49:51,332] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:52,745] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:49:52,789] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:49:52,804] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:49:52,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.483 seconds
[2020-11-05 12:50:04,553] {scheduler_job.py:155} INFO - Started process (PID=29146) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:04,555] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:50:04,556] {logging_mixin.py:112} INFO - [2020-11-05 12:50:04,556] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:05,633] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:05,676] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:50:05,691] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:50:05,695] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.142 seconds
[2020-11-05 12:50:17,783] {scheduler_job.py:155} INFO - Started process (PID=29209) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:17,790] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:50:17,791] {logging_mixin.py:112} INFO - [2020-11-05 12:50:17,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:19,005] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:19,052] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:50:19,071] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:50:19,076] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.292 seconds
[2020-11-05 12:50:31,032] {scheduler_job.py:155} INFO - Started process (PID=29269) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:31,038] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:50:31,039] {logging_mixin.py:112} INFO - [2020-11-05 12:50:31,038] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:32,124] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:32,185] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:50:32,200] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:50:32,204] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.172 seconds
[2020-11-05 12:50:44,266] {scheduler_job.py:155} INFO - Started process (PID=29336) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:44,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:50:44,276] {logging_mixin.py:112} INFO - [2020-11-05 12:50:44,275] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:45,316] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:45,352] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:50:45,371] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:50:45,375] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.108 seconds
[2020-11-05 12:50:57,554] {scheduler_job.py:155} INFO - Started process (PID=29399) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:57,558] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:50:57,558] {logging_mixin.py:112} INFO - [2020-11-05 12:50:57,558] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:58,644] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:50:58,691] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:50:58,707] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:50:58,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.157 seconds
[2020-11-05 12:51:10,857] {scheduler_job.py:155} INFO - Started process (PID=29459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:10,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:51:10,869] {logging_mixin.py:112} INFO - [2020-11-05 12:51:10,869] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:11,961] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:12,004] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:51:12,019] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:51:12,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.165 seconds
[2020-11-05 12:51:24,180] {scheduler_job.py:155} INFO - Started process (PID=29529) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:24,187] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:51:24,189] {logging_mixin.py:112} INFO - [2020-11-05 12:51:24,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:25,267] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:25,307] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:51:25,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:51:25,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.146 seconds
[2020-11-05 12:51:37,429] {scheduler_job.py:155} INFO - Started process (PID=29586) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:37,433] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:51:37,433] {logging_mixin.py:112} INFO - [2020-11-05 12:51:37,433] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:38,506] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:38,540] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:51:38,558] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:51:38,562] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.133 seconds
[2020-11-05 12:51:50,688] {scheduler_job.py:155} INFO - Started process (PID=29649) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:50,692] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:51:50,693] {logging_mixin.py:112} INFO - [2020-11-05 12:51:50,693] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:51,779] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:51:51,819] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:51:51,832] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:51:51,837] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.149 seconds
[2020-11-05 12:52:03,943] {scheduler_job.py:155} INFO - Started process (PID=29713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:03,946] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:52:03,946] {logging_mixin.py:112} INFO - [2020-11-05 12:52:03,946] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:05,003] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:05,046] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:52:05,075] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:52:05,079] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.137 seconds
[2020-11-05 12:52:17,213] {scheduler_job.py:155} INFO - Started process (PID=29777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:17,221] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:52:17,223] {logging_mixin.py:112} INFO - [2020-11-05 12:52:17,222] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:18,641] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:18,718] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:52:18,744] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:52:18,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.538 seconds
[2020-11-05 12:52:30,404] {scheduler_job.py:155} INFO - Started process (PID=29844) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:30,409] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:52:30,410] {logging_mixin.py:112} INFO - [2020-11-05 12:52:30,410] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:31,591] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:31,643] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:52:31,661] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:52:31,667] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.263 seconds
[2020-11-05 12:52:43,608] {scheduler_job.py:155} INFO - Started process (PID=29910) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:43,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:52:43,611] {logging_mixin.py:112} INFO - [2020-11-05 12:52:43,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:44,833] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:44,908] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:52:44,938] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:52:44,944] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.336 seconds
[2020-11-05 12:52:56,865] {scheduler_job.py:155} INFO - Started process (PID=29972) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:56,869] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:52:56,870] {logging_mixin.py:112} INFO - [2020-11-05 12:52:56,870] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:58,054] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:52:58,094] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:52:58,112] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:52:58,116] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.251 seconds
[2020-11-05 12:53:10,084] {scheduler_job.py:155} INFO - Started process (PID=30032) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:10,088] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:53:10,091] {logging_mixin.py:112} INFO - [2020-11-05 12:53:10,090] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:11,251] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:11,318] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:53:11,334] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:53:11,338] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.254 seconds
[2020-11-05 12:53:23,357] {scheduler_job.py:155} INFO - Started process (PID=30094) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:23,361] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:53:23,362] {logging_mixin.py:112} INFO - [2020-11-05 12:53:23,362] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:24,481] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:24,524] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:53:24,547] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:53:24,551] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.193 seconds
[2020-11-05 12:53:36,562] {scheduler_job.py:155} INFO - Started process (PID=30155) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:36,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:53:36,567] {logging_mixin.py:112} INFO - [2020-11-05 12:53:36,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:37,705] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:37,752] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:53:37,767] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:53:37,771] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.209 seconds
[2020-11-05 12:53:49,860] {scheduler_job.py:155} INFO - Started process (PID=30220) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:49,864] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:53:49,865] {logging_mixin.py:112} INFO - [2020-11-05 12:53:49,864] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:50,986] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:53:51,025] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:53:51,040] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:53:51,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.185 seconds
[2020-11-05 12:54:03,065] {scheduler_job.py:155} INFO - Started process (PID=30282) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:03,068] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:54:03,069] {logging_mixin.py:112} INFO - [2020-11-05 12:54:03,069] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:04,320] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:04,371] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:54:04,388] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:54:04,391] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.327 seconds
[2020-11-05 12:54:16,270] {scheduler_job.py:155} INFO - Started process (PID=30343) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:16,274] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:54:16,274] {logging_mixin.py:112} INFO - [2020-11-05 12:54:16,274] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:17,526] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:17,562] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:54:17,578] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:54:17,582] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.312 seconds
[2020-11-05 12:54:29,509] {scheduler_job.py:155} INFO - Started process (PID=30406) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:29,513] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:54:29,513] {logging_mixin.py:112} INFO - [2020-11-05 12:54:29,513] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:30,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:30,598] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:54:30,615] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:54:30,619] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.110 seconds
[2020-11-05 12:54:42,750] {scheduler_job.py:155} INFO - Started process (PID=30468) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:42,755] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:54:42,756] {logging_mixin.py:112} INFO - [2020-11-05 12:54:42,756] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:43,960] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:44,004] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:54:44,022] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:54:44,028] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.278 seconds
[2020-11-05 12:54:55,958] {scheduler_job.py:155} INFO - Started process (PID=30533) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:55,962] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:54:55,963] {logging_mixin.py:112} INFO - [2020-11-05 12:54:55,962] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:57,101] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:54:57,147] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:54:57,163] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:54:57,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.209 seconds
[2020-11-05 12:55:09,172] {scheduler_job.py:155} INFO - Started process (PID=30593) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:09,175] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:55:09,176] {logging_mixin.py:112} INFO - [2020-11-05 12:55:09,175] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:10,614] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:10,682] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:55:10,713] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:55:10,719] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.547 seconds
[2020-11-05 12:55:22,410] {scheduler_job.py:155} INFO - Started process (PID=30657) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:22,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:55:22,415] {logging_mixin.py:112} INFO - [2020-11-05 12:55:22,415] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:23,565] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:23,607] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:55:23,628] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:55:23,638] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.228 seconds
[2020-11-05 12:55:35,677] {scheduler_job.py:155} INFO - Started process (PID=30718) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:35,679] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:55:35,680] {logging_mixin.py:112} INFO - [2020-11-05 12:55:35,680] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:36,832] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:36,866] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:55:36,882] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:55:36,886] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.209 seconds
[2020-11-05 12:55:48,940] {scheduler_job.py:155} INFO - Started process (PID=30780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:48,955] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:55:48,956] {logging_mixin.py:112} INFO - [2020-11-05 12:55:48,956] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:50,133] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:55:50,181] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:55:50,201] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:55:50,206] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.265 seconds
[2020-11-05 12:56:02,129] {scheduler_job.py:155} INFO - Started process (PID=30840) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:02,137] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:56:02,138] {logging_mixin.py:112} INFO - [2020-11-05 12:56:02,138] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:03,347] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:03,390] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:56:03,414] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:56:03,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.290 seconds
[2020-11-05 12:56:15,349] {scheduler_job.py:155} INFO - Started process (PID=30900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:15,353] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:56:15,355] {logging_mixin.py:112} INFO - [2020-11-05 12:56:15,354] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:16,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:16,711] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:56:16,726] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:56:16,731] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.382 seconds
[2020-11-05 12:56:28,604] {scheduler_job.py:155} INFO - Started process (PID=30960) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:28,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:56:28,609] {logging_mixin.py:112} INFO - [2020-11-05 12:56:28,609] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:29,899] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:29,948] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:56:29,973] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:56:29,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.374 seconds
[2020-11-05 12:56:41,786] {scheduler_job.py:155} INFO - Started process (PID=31021) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:41,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:56:41,792] {logging_mixin.py:112} INFO - [2020-11-05 12:56:41,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:43,233] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:43,287] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:56:43,308] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:56:43,313] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.527 seconds
[2020-11-05 12:56:55,021] {scheduler_job.py:155} INFO - Started process (PID=31085) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:55,027] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:56:55,028] {logging_mixin.py:112} INFO - [2020-11-05 12:56:55,028] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:56,224] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:56:56,280] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:56:56,299] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:56:56,303] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.283 seconds
[2020-11-05 12:57:08,242] {scheduler_job.py:155} INFO - Started process (PID=31146) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:08,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:57:08,246] {logging_mixin.py:112} INFO - [2020-11-05 12:57:08,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:09,525] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:09,578] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:57:09,601] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:57:09,610] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.369 seconds
[2020-11-05 12:57:21,447] {scheduler_job.py:155} INFO - Started process (PID=31207) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:21,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:57:21,451] {logging_mixin.py:112} INFO - [2020-11-05 12:57:21,451] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:22,604] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:22,666] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:57:22,697] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:57:22,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.255 seconds
[2020-11-05 12:57:34,701] {scheduler_job.py:155} INFO - Started process (PID=31268) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:34,704] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:57:34,705] {logging_mixin.py:112} INFO - [2020-11-05 12:57:34,705] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:35,875] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:35,922] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:57:35,943] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:57:35,948] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-05 12:57:47,950] {scheduler_job.py:155} INFO - Started process (PID=31333) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:47,955] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:57:47,956] {logging_mixin.py:112} INFO - [2020-11-05 12:57:47,956] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:49,117] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:57:49,155] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:57:49,170] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:57:49,174] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.224 seconds
[2020-11-05 12:58:01,279] {scheduler_job.py:155} INFO - Started process (PID=31394) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:01,283] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:58:01,284] {logging_mixin.py:112} INFO - [2020-11-05 12:58:01,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:02,466] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:02,514] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:58:02,533] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:58:02,539] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.261 seconds
[2020-11-05 12:58:14,503] {scheduler_job.py:155} INFO - Started process (PID=31455) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:14,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:58:14,507] {logging_mixin.py:112} INFO - [2020-11-05 12:58:14,506] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:15,842] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:15,879] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:58:15,894] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:58:15,898] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.395 seconds
[2020-11-05 12:58:27,755] {scheduler_job.py:155} INFO - Started process (PID=31527) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:27,763] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:58:27,764] {logging_mixin.py:112} INFO - [2020-11-05 12:58:27,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:29,031] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:29,068] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:58:29,082] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:58:29,086] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.331 seconds
[2020-11-05 12:58:40,942] {scheduler_job.py:155} INFO - Started process (PID=31585) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:40,945] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:58:40,945] {logging_mixin.py:112} INFO - [2020-11-05 12:58:40,945] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:42,166] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:42,201] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:58:42,216] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:58:42,221] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-05 12:58:54,178] {scheduler_job.py:155} INFO - Started process (PID=31655) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:54,182] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:58:54,182] {logging_mixin.py:112} INFO - [2020-11-05 12:58:54,182] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:55,343] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:58:55,389] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:58:55,412] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:58:55,418] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.241 seconds
[2020-11-05 12:59:07,442] {scheduler_job.py:155} INFO - Started process (PID=31719) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:07,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:59:07,446] {logging_mixin.py:112} INFO - [2020-11-05 12:59:07,446] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:08,577] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:08,638] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:59:08,665] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:59:08,672] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.230 seconds
[2020-11-05 12:59:20,655] {scheduler_job.py:155} INFO - Started process (PID=31782) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:20,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:59:20,659] {logging_mixin.py:112} INFO - [2020-11-05 12:59:20,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:21,824] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:21,868] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:59:21,890] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:59:21,896] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.241 seconds
[2020-11-05 12:59:33,903] {scheduler_job.py:155} INFO - Started process (PID=31844) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:33,907] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:59:33,908] {logging_mixin.py:112} INFO - [2020-11-05 12:59:33,908] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:35,322] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:35,365] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:59:35,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:59:35,389] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.486 seconds
[2020-11-05 12:59:47,113] {scheduler_job.py:155} INFO - Started process (PID=31906) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:47,118] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 12:59:47,118] {logging_mixin.py:112} INFO - [2020-11-05 12:59:47,118] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:48,234] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 12:59:48,280] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 12:59:48,295] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 12:59:48,300] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.187 seconds
[2020-11-05 13:00:00,300] {scheduler_job.py:155} INFO - Started process (PID=31968) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:00,304] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:00:00,304] {logging_mixin.py:112} INFO - [2020-11-05 13:00:00,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:01,396] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:01,464] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:00:01,479] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:00:01,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.182 seconds
[2020-11-05 13:00:13,576] {scheduler_job.py:155} INFO - Started process (PID=32029) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:13,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:00:13,583] {logging_mixin.py:112} INFO - [2020-11-05 13:00:13,582] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:14,691] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:14,727] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:00:14,743] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:00:14,747] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.171 seconds
[2020-11-05 13:00:26,758] {scheduler_job.py:155} INFO - Started process (PID=32090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:26,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:00:26,763] {logging_mixin.py:112} INFO - [2020-11-05 13:00:26,763] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:28,433] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:28,502] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:00:28,522] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:00:28,527] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.770 seconds
[2020-11-05 13:00:39,960] {scheduler_job.py:155} INFO - Started process (PID=32151) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:39,964] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:00:39,965] {logging_mixin.py:112} INFO - [2020-11-05 13:00:39,965] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:41,292] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:41,332] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:00:41,356] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:00:41,362] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.402 seconds
[2020-11-05 13:00:53,165] {scheduler_job.py:155} INFO - Started process (PID=32210) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:53,168] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:00:53,168] {logging_mixin.py:112} INFO - [2020-11-05 13:00:53,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:54,403] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:00:54,454] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:00:54,470] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:00:54,474] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-05 13:01:06,450] {scheduler_job.py:155} INFO - Started process (PID=32271) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:06,454] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:01:06,455] {logging_mixin.py:112} INFO - [2020-11-05 13:01:06,454] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:07,560] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:07,625] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:01:07,659] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:01:07,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.215 seconds
[2020-11-05 13:01:19,701] {scheduler_job.py:155} INFO - Started process (PID=32335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:19,708] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:01:19,709] {logging_mixin.py:112} INFO - [2020-11-05 13:01:19,708] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:20,788] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:20,834] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:01:20,851] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:01:20,860] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.160 seconds
[2020-11-05 13:01:32,986] {scheduler_job.py:155} INFO - Started process (PID=32401) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:32,993] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:01:32,994] {logging_mixin.py:112} INFO - [2020-11-05 13:01:32,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:34,382] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:34,469] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:01:34,502] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:01:34,512] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.526 seconds
[2020-11-05 13:01:46,158] {scheduler_job.py:155} INFO - Started process (PID=32462) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:46,162] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:01:46,163] {logging_mixin.py:112} INFO - [2020-11-05 13:01:46,162] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:47,211] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:47,252] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:01:47,271] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:01:47,274] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-05 13:01:59,514] {scheduler_job.py:155} INFO - Started process (PID=32532) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:01:59,520] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:01:59,522] {logging_mixin.py:112} INFO - [2020-11-05 13:01:59,522] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:00,783] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:00,833] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:02:00,853] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:02:00,858] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.344 seconds
[2020-11-05 13:02:12,691] {scheduler_job.py:155} INFO - Started process (PID=32591) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:12,695] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:02:12,696] {logging_mixin.py:112} INFO - [2020-11-05 13:02:12,696] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:13,814] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:13,855] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:02:13,871] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:02:13,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.186 seconds
[2020-11-05 13:02:25,934] {scheduler_job.py:155} INFO - Started process (PID=32655) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:25,949] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:02:25,950] {logging_mixin.py:112} INFO - [2020-11-05 13:02:25,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:27,133] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:27,178] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:02:27,196] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:02:27,202] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.269 seconds
[2020-11-05 13:02:39,138] {scheduler_job.py:155} INFO - Started process (PID=32714) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:39,141] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:02:39,142] {logging_mixin.py:112} INFO - [2020-11-05 13:02:39,142] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:40,210] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:40,247] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:02:40,265] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:02:40,269] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.132 seconds
[2020-11-05 13:02:52,369] {scheduler_job.py:155} INFO - Started process (PID=310) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:52,372] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:02:52,372] {logging_mixin.py:112} INFO - [2020-11-05 13:02:52,372] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:53,420] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:02:53,459] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:02:53,474] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:02:53,477] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.109 seconds
[2020-11-05 13:03:05,599] {scheduler_job.py:155} INFO - Started process (PID=380) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:05,606] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:03:05,607] {logging_mixin.py:112} INFO - [2020-11-05 13:03:05,606] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:06,827] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:06,872] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:03:06,888] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:03:06,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.292 seconds
[2020-11-05 13:03:18,808] {scheduler_job.py:155} INFO - Started process (PID=441) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:18,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:03:18,814] {logging_mixin.py:112} INFO - [2020-11-05 13:03:18,813] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:19,852] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:19,898] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:03:19,918] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:03:19,924] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-05 13:03:32,130] {scheduler_job.py:155} INFO - Started process (PID=513) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:32,141] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:03:32,143] {logging_mixin.py:112} INFO - [2020-11-05 13:03:32,142] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:33,364] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:33,417] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:03:33,434] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:03:33,438] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.309 seconds
[2020-11-05 13:03:45,320] {scheduler_job.py:155} INFO - Started process (PID=601) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:45,324] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:03:45,325] {logging_mixin.py:112} INFO - [2020-11-05 13:03:45,324] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:46,414] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:46,459] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:03:46,482] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:03:46,486] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.166 seconds
[2020-11-05 13:03:58,549] {scheduler_job.py:155} INFO - Started process (PID=673) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:58,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:03:58,553] {logging_mixin.py:112} INFO - [2020-11-05 13:03:58,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:59,777] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:03:59,825] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:03:59,847] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:03:59,851] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.302 seconds
[2020-11-05 13:04:11,806] {scheduler_job.py:155} INFO - Started process (PID=743) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:11,809] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:04:11,810] {logging_mixin.py:112} INFO - [2020-11-05 13:04:11,810] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:12,907] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:12,952] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:04:12,970] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:04:12,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.169 seconds
[2020-11-05 13:04:25,070] {scheduler_job.py:155} INFO - Started process (PID=805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:25,075] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:04:25,076] {logging_mixin.py:112} INFO - [2020-11-05 13:04:25,076] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:26,535] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:26,602] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:04:26,638] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:04:26,646] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.576 seconds
[2020-11-05 13:04:38,286] {scheduler_job.py:155} INFO - Started process (PID=863) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:38,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:04:38,292] {logging_mixin.py:112} INFO - [2020-11-05 13:04:38,291] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:39,467] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:39,503] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:04:39,520] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:04:39,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.237 seconds
[2020-11-05 13:04:51,477] {scheduler_job.py:155} INFO - Started process (PID=924) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:51,480] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:04:51,481] {logging_mixin.py:112} INFO - [2020-11-05 13:04:51,481] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:52,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:04:52,533] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:04:52,552] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:04:52,556] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.079 seconds
[2020-11-05 13:05:04,672] {scheduler_job.py:155} INFO - Started process (PID=995) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:04,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:05:04,677] {logging_mixin.py:112} INFO - [2020-11-05 13:05:04,677] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:06,012] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:06,059] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:05:06,079] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:05:06,083] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.411 seconds
[2020-11-05 13:05:17,860] {scheduler_job.py:155} INFO - Started process (PID=1056) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:17,863] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:05:17,863] {logging_mixin.py:112} INFO - [2020-11-05 13:05:17,863] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:19,037] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:19,085] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:05:19,101] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:05:19,104] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.245 seconds
[2020-11-05 13:05:31,087] {scheduler_job.py:155} INFO - Started process (PID=1123) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:31,090] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:05:31,090] {logging_mixin.py:112} INFO - [2020-11-05 13:05:31,090] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:32,104] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:32,140] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:05:32,159] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:05:32,164] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.077 seconds
[2020-11-05 13:05:44,275] {scheduler_job.py:155} INFO - Started process (PID=1185) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:44,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:05:44,279] {logging_mixin.py:112} INFO - [2020-11-05 13:05:44,279] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:45,669] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:45,714] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:05:45,735] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:05:45,742] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.467 seconds
[2020-11-05 13:05:57,475] {scheduler_job.py:155} INFO - Started process (PID=1248) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:57,488] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:05:57,498] {logging_mixin.py:112} INFO - [2020-11-05 13:05:57,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:58,648] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:05:58,689] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:05:58,704] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:05:58,708] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-05 13:06:10,722] {scheduler_job.py:155} INFO - Started process (PID=1307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:10,726] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:06:10,727] {logging_mixin.py:112} INFO - [2020-11-05 13:06:10,727] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:11,870] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:11,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:06:11,951] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:06:11,954] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-05 13:06:23,933] {scheduler_job.py:155} INFO - Started process (PID=1369) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:23,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:06:23,942] {logging_mixin.py:112} INFO - [2020-11-05 13:06:23,942] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:25,173] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:25,211] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:06:25,230] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:06:25,233] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.300 seconds
[2020-11-05 13:06:37,174] {scheduler_job.py:155} INFO - Started process (PID=1435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:37,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:06:37,189] {logging_mixin.py:112} INFO - [2020-11-05 13:06:37,189] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:38,531] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:38,575] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:06:38,598] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:06:38,602] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.428 seconds
[2020-11-05 13:06:50,414] {scheduler_job.py:155} INFO - Started process (PID=1496) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:50,418] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:06:50,418] {logging_mixin.py:112} INFO - [2020-11-05 13:06:50,418] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:51,458] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:06:51,501] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:06:51,515] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:06:51,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.104 seconds
[2020-11-05 13:07:03,696] {scheduler_job.py:155} INFO - Started process (PID=1594) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:03,701] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:07:03,702] {logging_mixin.py:112} INFO - [2020-11-05 13:07:03,701] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:05,031] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:05,071] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:07:05,093] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:07:05,098] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.401 seconds
[2020-11-05 13:07:16,985] {scheduler_job.py:155} INFO - Started process (PID=1657) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:16,988] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:07:16,989] {logging_mixin.py:112} INFO - [2020-11-05 13:07:16,988] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:18,050] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:18,099] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:07:18,120] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:07:18,129] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.144 seconds
[2020-11-05 13:07:30,237] {scheduler_job.py:155} INFO - Started process (PID=1748) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:30,244] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:07:30,245] {logging_mixin.py:112} INFO - [2020-11-05 13:07:30,244] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:31,565] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:31,624] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:07:31,651] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:07:31,658] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.421 seconds
[2020-11-05 13:07:43,453] {scheduler_job.py:155} INFO - Started process (PID=1894) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:43,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:07:43,461] {logging_mixin.py:112} INFO - [2020-11-05 13:07:43,460] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:44,555] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:44,599] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:07:44,620] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:07:44,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.172 seconds
[2020-11-05 13:07:56,616] {scheduler_job.py:155} INFO - Started process (PID=2037) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:56,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:07:56,620] {logging_mixin.py:112} INFO - [2020-11-05 13:07:56,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:57,673] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:07:57,710] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:07:57,727] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:07:57,733] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-05 13:08:09,883] {scheduler_job.py:155} INFO - Started process (PID=2121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:09,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:08:09,887] {logging_mixin.py:112} INFO - [2020-11-05 13:08:09,887] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:11,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:11,167] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:08:11,182] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:08:11,186] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.303 seconds
[2020-11-05 13:08:23,144] {scheduler_job.py:155} INFO - Started process (PID=2199) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:23,148] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:08:23,148] {logging_mixin.py:112} INFO - [2020-11-05 13:08:23,148] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:24,271] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:24,328] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:08:24,359] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:08:24,367] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.223 seconds
[2020-11-05 13:08:36,386] {scheduler_job.py:155} INFO - Started process (PID=2276) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:36,390] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:08:36,391] {logging_mixin.py:112} INFO - [2020-11-05 13:08:36,391] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:37,718] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:37,763] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:08:37,779] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:08:37,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.397 seconds
[2020-11-05 13:08:49,610] {scheduler_job.py:155} INFO - Started process (PID=2372) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:49,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:08:49,615] {logging_mixin.py:112} INFO - [2020-11-05 13:08:49,615] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:50,883] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:08:50,924] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:08:50,941] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:08:50,945] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.335 seconds
[2020-11-05 13:09:02,841] {scheduler_job.py:155} INFO - Started process (PID=2481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:02,845] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:09:02,846] {logging_mixin.py:112} INFO - [2020-11-05 13:09:02,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:04,181] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:04,234] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:09:04,254] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:09:04,261] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.420 seconds
[2020-11-05 13:09:16,041] {scheduler_job.py:155} INFO - Started process (PID=2575) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:16,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:09:16,047] {logging_mixin.py:112} INFO - [2020-11-05 13:09:16,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:17,251] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:17,286] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:09:17,302] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:09:17,307] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.265 seconds
[2020-11-05 13:09:29,304] {scheduler_job.py:155} INFO - Started process (PID=2637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:29,314] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:09:29,317] {logging_mixin.py:112} INFO - [2020-11-05 13:09:29,316] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:30,758] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:30,793] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:09:30,808] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:09:30,812] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.508 seconds
[2020-11-05 13:09:42,490] {scheduler_job.py:155} INFO - Started process (PID=2713) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:42,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:09:42,494] {logging_mixin.py:112} INFO - [2020-11-05 13:09:42,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:43,710] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:43,748] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:09:43,770] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:09:43,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.283 seconds
[2020-11-05 13:09:55,751] {scheduler_job.py:155} INFO - Started process (PID=2786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:55,756] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:09:55,757] {logging_mixin.py:112} INFO - [2020-11-05 13:09:55,756] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:56,987] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:09:57,019] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:09:57,040] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:09:57,046] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.295 seconds
[2020-11-05 13:10:08,935] {scheduler_job.py:155} INFO - Started process (PID=2845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:08,940] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:10:08,940] {logging_mixin.py:112} INFO - [2020-11-05 13:10:08,940] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:10,301] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:10,355] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:10:10,383] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:10:10,396] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.461 seconds
[2020-11-05 13:10:22,124] {scheduler_job.py:155} INFO - Started process (PID=2911) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:22,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:10:22,129] {logging_mixin.py:112} INFO - [2020-11-05 13:10:22,129] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:23,287] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:23,335] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:10:23,356] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:10:23,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.239 seconds
[2020-11-05 13:10:35,357] {scheduler_job.py:155} INFO - Started process (PID=2970) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:35,361] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:10:35,364] {logging_mixin.py:112} INFO - [2020-11-05 13:10:35,361] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:36,886] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:36,965] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:10:36,986] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:10:36,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.634 seconds
[2020-11-05 13:10:48,629] {scheduler_job.py:155} INFO - Started process (PID=3035) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:48,633] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:10:48,634] {logging_mixin.py:112} INFO - [2020-11-05 13:10:48,634] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:49,855] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:10:49,893] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:10:49,913] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:10:49,916] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.287 seconds
[2020-11-05 13:11:01,850] {scheduler_job.py:155} INFO - Started process (PID=3099) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:01,854] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:11:01,854] {logging_mixin.py:112} INFO - [2020-11-05 13:11:01,854] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:03,280] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:03,328] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:11:03,345] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:11:03,349] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.499 seconds
[2020-11-05 13:11:15,060] {scheduler_job.py:155} INFO - Started process (PID=3157) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:15,065] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:11:15,067] {logging_mixin.py:112} INFO - [2020-11-05 13:11:15,066] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:16,189] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:16,240] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:11:16,257] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:11:16,262] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.202 seconds
[2020-11-05 13:11:28,231] {scheduler_job.py:155} INFO - Started process (PID=3225) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:28,235] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:11:28,236] {logging_mixin.py:112} INFO - [2020-11-05 13:11:28,236] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:29,345] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:29,397] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:11:29,428] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:11:29,434] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.203 seconds
[2020-11-05 13:11:41,580] {scheduler_job.py:155} INFO - Started process (PID=3300) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:41,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:11:41,590] {logging_mixin.py:112} INFO - [2020-11-05 13:11:41,590] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:42,839] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:42,903] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:11:42,922] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:11:42,926] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-05 13:11:54,768] {scheduler_job.py:155} INFO - Started process (PID=3370) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:54,774] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:11:54,774] {logging_mixin.py:112} INFO - [2020-11-05 13:11:54,774] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:56,101] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:11:56,146] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:11:56,168] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:11:56,174] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.406 seconds
[2020-11-05 13:12:07,983] {scheduler_job.py:155} INFO - Started process (PID=3429) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:07,990] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:12:07,991] {logging_mixin.py:112} INFO - [2020-11-05 13:12:07,991] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:09,749] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:09,803] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:12:09,831] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:12:09,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.859 seconds
[2020-11-05 13:12:21,381] {scheduler_job.py:155} INFO - Started process (PID=3490) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:21,386] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:12:21,389] {logging_mixin.py:112} INFO - [2020-11-05 13:12:21,387] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:22,549] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:22,588] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:12:22,609] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:12:22,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.237 seconds
[2020-11-05 13:12:34,717] {scheduler_job.py:155} INFO - Started process (PID=3549) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:34,721] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:12:34,721] {logging_mixin.py:112} INFO - [2020-11-05 13:12:34,721] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:36,235] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:36,332] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:12:36,362] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:12:36,369] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.652 seconds
[2020-11-05 13:12:48,060] {scheduler_job.py:155} INFO - Started process (PID=3605) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:48,065] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:12:48,066] {logging_mixin.py:112} INFO - [2020-11-05 13:12:48,066] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:49,888] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:12:49,992] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:12:50,022] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:12:50,029] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.970 seconds
[2020-11-05 13:13:01,231] {scheduler_job.py:155} INFO - Started process (PID=3662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:01,235] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:13:01,236] {logging_mixin.py:112} INFO - [2020-11-05 13:13:01,236] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:02,562] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:02,596] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:13:02,612] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:13:02,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.384 seconds
[2020-11-05 13:13:14,468] {scheduler_job.py:155} INFO - Started process (PID=3721) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:14,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:13:14,473] {logging_mixin.py:112} INFO - [2020-11-05 13:13:14,473] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:15,616] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:15,653] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:13:15,669] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:13:15,675] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.207 seconds
[2020-11-05 13:13:27,626] {scheduler_job.py:155} INFO - Started process (PID=3786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:27,631] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:13:27,632] {logging_mixin.py:112} INFO - [2020-11-05 13:13:27,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:28,816] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:28,873] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:13:28,914] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:13:28,920] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.294 seconds
[2020-11-05 13:13:40,849] {scheduler_job.py:155} INFO - Started process (PID=3848) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:40,858] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:13:40,859] {logging_mixin.py:112} INFO - [2020-11-05 13:13:40,859] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:41,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:41,985] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:13:42,010] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:13:42,014] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.165 seconds
[2020-11-05 13:13:54,122] {scheduler_job.py:155} INFO - Started process (PID=3904) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:54,127] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:13:54,128] {logging_mixin.py:112} INFO - [2020-11-05 13:13:54,128] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:55,265] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:13:55,330] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:13:55,348] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:13:55,352] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.230 seconds
[2020-11-05 13:14:07,395] {scheduler_job.py:155} INFO - Started process (PID=3966) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:07,418] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:14:07,419] {logging_mixin.py:112} INFO - [2020-11-05 13:14:07,419] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:08,623] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:08,662] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:14:08,680] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:14:08,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.288 seconds
[2020-11-05 13:14:20,605] {scheduler_job.py:155} INFO - Started process (PID=4030) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:20,608] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:14:20,609] {logging_mixin.py:112} INFO - [2020-11-05 13:14:20,609] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:21,829] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:21,875] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:14:21,895] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:14:21,899] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.294 seconds
[2020-11-05 13:14:33,790] {scheduler_job.py:155} INFO - Started process (PID=4094) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:33,797] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:14:33,799] {logging_mixin.py:112} INFO - [2020-11-05 13:14:33,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:35,382] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:35,438] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:14:35,466] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:14:35,471] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.681 seconds
[2020-11-05 13:14:46,989] {scheduler_job.py:155} INFO - Started process (PID=4163) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:46,991] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:14:46,992] {logging_mixin.py:112} INFO - [2020-11-05 13:14:46,992] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:48,242] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:14:48,288] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:14:48,305] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:14:48,309] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.320 seconds
[2020-11-05 13:15:00,162] {scheduler_job.py:155} INFO - Started process (PID=4221) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:00,166] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:15:00,166] {logging_mixin.py:112} INFO - [2020-11-05 13:15:00,166] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:01,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:01,502] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:15:01,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:15:01,544] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.383 seconds
[2020-11-05 13:15:13,393] {scheduler_job.py:155} INFO - Started process (PID=4284) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:13,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:15:13,397] {logging_mixin.py:112} INFO - [2020-11-05 13:15:13,396] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:14,636] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:14,676] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:15:14,695] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:15:14,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.307 seconds
[2020-11-05 13:15:26,609] {scheduler_job.py:155} INFO - Started process (PID=4347) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:26,612] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:15:26,613] {logging_mixin.py:112} INFO - [2020-11-05 13:15:26,613] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:27,797] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:27,845] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:15:27,860] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:15:27,864] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.255 seconds
[2020-11-05 13:15:39,854] {scheduler_job.py:155} INFO - Started process (PID=4416) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:39,857] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:15:39,857] {logging_mixin.py:112} INFO - [2020-11-05 13:15:39,857] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:41,059] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:41,104] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:15:41,126] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:15:41,130] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.276 seconds
[2020-11-05 13:15:53,059] {scheduler_job.py:155} INFO - Started process (PID=4475) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:53,062] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:15:53,065] {logging_mixin.py:112} INFO - [2020-11-05 13:15:53,064] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:54,261] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:15:54,309] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:15:54,330] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:15:54,334] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.275 seconds
[2020-11-05 13:16:06,275] {scheduler_job.py:155} INFO - Started process (PID=4545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:06,286] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:16:06,287] {logging_mixin.py:112} INFO - [2020-11-05 13:16:06,287] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:07,946] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:07,985] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:16:07,999] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:16:08,002] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.728 seconds
[2020-11-05 13:16:19,453] {scheduler_job.py:155} INFO - Started process (PID=4607) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:19,457] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:16:19,458] {logging_mixin.py:112} INFO - [2020-11-05 13:16:19,458] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:20,464] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:20,501] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:16:20,515] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:16:20,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.065 seconds
[2020-11-05 13:16:32,627] {scheduler_job.py:155} INFO - Started process (PID=4670) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:32,631] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:16:32,632] {logging_mixin.py:112} INFO - [2020-11-05 13:16:32,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:33,686] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:33,731] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:16:33,752] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:16:33,762] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.136 seconds
[2020-11-05 13:16:45,823] {scheduler_job.py:155} INFO - Started process (PID=4732) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:45,826] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:16:45,827] {logging_mixin.py:112} INFO - [2020-11-05 13:16:45,826] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:46,907] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:46,948] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:16:46,970] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:16:46,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.151 seconds
[2020-11-05 13:16:59,049] {scheduler_job.py:155} INFO - Started process (PID=4794) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:16:59,056] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:16:59,057] {logging_mixin.py:112} INFO - [2020-11-05 13:16:59,056] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:00,190] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:00,249] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:17:00,271] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:17:00,277] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.228 seconds
[2020-11-05 13:17:12,314] {scheduler_job.py:155} INFO - Started process (PID=4857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:12,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:17:12,321] {logging_mixin.py:112} INFO - [2020-11-05 13:17:12,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:13,512] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:13,545] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:17:13,558] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:17:13,561] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.247 seconds
[2020-11-05 13:17:25,498] {scheduler_job.py:155} INFO - Started process (PID=4931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:25,503] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:17:25,503] {logging_mixin.py:112} INFO - [2020-11-05 13:17:25,503] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:26,729] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:26,785] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:17:26,803] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:17:26,807] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-05 13:17:38,684] {scheduler_job.py:155} INFO - Started process (PID=4992) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:38,687] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:17:38,688] {logging_mixin.py:112} INFO - [2020-11-05 13:17:38,688] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:39,925] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:39,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:17:39,970] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:17:39,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.290 seconds
[2020-11-05 13:17:51,956] {scheduler_job.py:155} INFO - Started process (PID=5053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:51,960] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:17:51,960] {logging_mixin.py:112} INFO - [2020-11-05 13:17:51,960] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:53,019] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:17:53,076] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:17:53,094] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:17:53,101] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.145 seconds
[2020-11-05 13:18:05,183] {scheduler_job.py:155} INFO - Started process (PID=5123) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:05,187] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:18:05,188] {logging_mixin.py:112} INFO - [2020-11-05 13:18:05,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:06,631] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:06,671] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:18:06,686] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:18:06,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.507 seconds
[2020-11-05 13:18:18,505] {scheduler_job.py:155} INFO - Started process (PID=5189) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:18,508] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:18:18,509] {logging_mixin.py:112} INFO - [2020-11-05 13:18:18,509] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:19,801] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:19,844] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:18:19,860] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:18:19,864] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.359 seconds
[2020-11-05 13:18:31,716] {scheduler_job.py:155} INFO - Started process (PID=5253) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:31,721] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:18:31,722] {logging_mixin.py:112} INFO - [2020-11-05 13:18:31,722] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:32,830] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:32,884] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:18:32,910] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:18:32,920] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.204 seconds
[2020-11-05 13:18:44,946] {scheduler_job.py:155} INFO - Started process (PID=5313) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:44,951] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:18:44,952] {logging_mixin.py:112} INFO - [2020-11-05 13:18:44,952] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:46,050] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:46,094] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:18:46,112] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:18:46,115] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.169 seconds
[2020-11-05 13:18:58,123] {scheduler_job.py:155} INFO - Started process (PID=5375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:58,128] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:18:58,129] {logging_mixin.py:112} INFO - [2020-11-05 13:18:58,129] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:59,216] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:18:59,254] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:18:59,271] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:18:59,276] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-05 13:19:11,340] {scheduler_job.py:155} INFO - Started process (PID=5438) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:11,344] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:19:11,345] {logging_mixin.py:112} INFO - [2020-11-05 13:19:11,344] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:12,452] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:12,485] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:19:12,504] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:19:12,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.168 seconds
[2020-11-05 13:19:24,562] {scheduler_job.py:155} INFO - Started process (PID=5499) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:24,567] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:19:24,568] {logging_mixin.py:112} INFO - [2020-11-05 13:19:24,568] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:25,648] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:25,704] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:19:25,736] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:19:25,741] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.179 seconds
[2020-11-05 13:19:37,808] {scheduler_job.py:155} INFO - Started process (PID=5563) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:37,814] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:19:37,814] {logging_mixin.py:112} INFO - [2020-11-05 13:19:37,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:38,885] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:38,928] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:19:38,951] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:19:38,955] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.147 seconds
[2020-11-05 13:19:51,034] {scheduler_job.py:155} INFO - Started process (PID=5626) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:51,037] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:19:51,037] {logging_mixin.py:112} INFO - [2020-11-05 13:19:51,037] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:52,164] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:19:52,209] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:19:52,225] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:19:52,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.197 seconds
[2020-11-05 13:20:04,284] {scheduler_job.py:155} INFO - Started process (PID=5690) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:04,288] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:20:04,288] {logging_mixin.py:112} INFO - [2020-11-05 13:20:04,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:05,394] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:05,447] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:20:05,464] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:20:05,468] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.184 seconds
[2020-11-05 13:20:17,500] {scheduler_job.py:155} INFO - Started process (PID=5760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:17,515] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:20:17,516] {logging_mixin.py:112} INFO - [2020-11-05 13:20:17,515] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:18,693] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:18,737] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:20:18,754] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:20:18,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.262 seconds
[2020-11-05 13:20:30,696] {scheduler_job.py:155} INFO - Started process (PID=5834) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:30,704] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:20:30,706] {logging_mixin.py:112} INFO - [2020-11-05 13:20:30,706] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:31,852] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:31,897] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:20:31,914] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:20:31,918] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.222 seconds
[2020-11-05 13:20:43,931] {scheduler_job.py:155} INFO - Started process (PID=5915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:43,935] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:20:43,935] {logging_mixin.py:112} INFO - [2020-11-05 13:20:43,935] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:45,005] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:45,044] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:20:45,060] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:20:45,064] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.133 seconds
[2020-11-05 13:20:57,163] {scheduler_job.py:155} INFO - Started process (PID=5982) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:57,173] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:20:57,176] {logging_mixin.py:112} INFO - [2020-11-05 13:20:57,175] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:58,444] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:20:58,491] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:20:58,509] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:20:58,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.353 seconds
[2020-11-05 13:21:10,336] {scheduler_job.py:155} INFO - Started process (PID=6043) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:10,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:21:10,344] {logging_mixin.py:112} INFO - [2020-11-05 13:21:10,343] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:11,482] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:11,529] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:21:11,560] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:21:11,566] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.230 seconds
[2020-11-05 13:21:23,563] {scheduler_job.py:155} INFO - Started process (PID=6103) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:23,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:21:23,566] {logging_mixin.py:112} INFO - [2020-11-05 13:21:23,566] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:24,950] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:24,987] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:21:25,008] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:21:25,016] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.452 seconds
[2020-11-05 13:21:36,795] {scheduler_job.py:155} INFO - Started process (PID=6167) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:36,798] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:21:36,799] {logging_mixin.py:112} INFO - [2020-11-05 13:21:36,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:37,888] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:37,936] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:21:37,953] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:21:37,957] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.162 seconds
[2020-11-05 13:21:50,031] {scheduler_job.py:155} INFO - Started process (PID=6228) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:50,035] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:21:50,035] {logging_mixin.py:112} INFO - [2020-11-05 13:21:50,035] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:51,146] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:21:51,189] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:21:51,208] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:21:51,212] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.181 seconds
[2020-11-05 13:22:03,224] {scheduler_job.py:155} INFO - Started process (PID=6308) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:03,229] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:22:03,229] {logging_mixin.py:112} INFO - [2020-11-05 13:22:03,229] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:04,340] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:04,380] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:22:04,414] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:22:04,418] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.195 seconds
[2020-11-05 13:22:16,405] {scheduler_job.py:155} INFO - Started process (PID=6368) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:16,410] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:22:16,412] {logging_mixin.py:112} INFO - [2020-11-05 13:22:16,410] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:17,553] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:17,593] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:22:17,611] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:22:17,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.213 seconds
[2020-11-05 13:22:29,594] {scheduler_job.py:155} INFO - Started process (PID=6431) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:29,599] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:22:29,600] {logging_mixin.py:112} INFO - [2020-11-05 13:22:29,600] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:30,747] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:30,793] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:22:30,812] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:22:30,818] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.224 seconds
[2020-11-05 13:22:42,878] {scheduler_job.py:155} INFO - Started process (PID=6495) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:42,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:22:42,884] {logging_mixin.py:112} INFO - [2020-11-05 13:22:42,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:43,960] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:43,997] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:22:44,016] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:22:44,020] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.142 seconds
[2020-11-05 13:22:56,146] {scheduler_job.py:155} INFO - Started process (PID=6562) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:56,150] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:22:56,154] {logging_mixin.py:112} INFO - [2020-11-05 13:22:56,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:57,324] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:22:57,371] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:22:57,388] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:22:57,395] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-05 13:23:09,377] {scheduler_job.py:155} INFO - Started process (PID=6623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:09,383] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:23:09,383] {logging_mixin.py:112} INFO - [2020-11-05 13:23:09,383] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:10,883] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:10,917] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:23:10,934] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:23:10,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.561 seconds
[2020-11-05 13:23:22,653] {scheduler_job.py:155} INFO - Started process (PID=6684) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:22,664] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:23:22,666] {logging_mixin.py:112} INFO - [2020-11-05 13:23:22,665] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:24,312] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:24,361] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:23:24,385] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:23:24,391] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.738 seconds
[2020-11-05 13:23:35,904] {scheduler_job.py:155} INFO - Started process (PID=6749) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:35,910] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:23:35,911] {logging_mixin.py:112} INFO - [2020-11-05 13:23:35,910] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:37,005] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:37,042] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:23:37,061] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:23:37,065] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.160 seconds
[2020-11-05 13:23:49,202] {scheduler_job.py:155} INFO - Started process (PID=6807) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:49,209] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:23:49,210] {logging_mixin.py:112} INFO - [2020-11-05 13:23:49,210] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:50,263] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:23:50,327] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:23:50,381] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:23:50,394] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.192 seconds
[2020-11-05 13:24:02,397] {scheduler_job.py:155} INFO - Started process (PID=6871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:02,400] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:24:02,401] {logging_mixin.py:112} INFO - [2020-11-05 13:24:02,400] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:03,579] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:03,615] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:24:03,631] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:24:03,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.239 seconds
[2020-11-05 13:24:15,660] {scheduler_job.py:155} INFO - Started process (PID=6938) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:15,671] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:24:15,672] {logging_mixin.py:112} INFO - [2020-11-05 13:24:15,671] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:17,055] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:17,163] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:24:17,234] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:24:17,252] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.592 seconds
[2020-11-05 13:24:28,989] {scheduler_job.py:155} INFO - Started process (PID=7002) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:28,992] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:24:28,994] {logging_mixin.py:112} INFO - [2020-11-05 13:24:28,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:30,263] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:30,309] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:24:30,329] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:24:30,333] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.344 seconds
[2020-11-05 13:24:42,196] {scheduler_job.py:155} INFO - Started process (PID=7061) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:42,200] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:24:42,201] {logging_mixin.py:112} INFO - [2020-11-05 13:24:42,201] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:43,426] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:43,491] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:24:43,518] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:24:43,526] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.330 seconds
[2020-11-05 13:24:55,428] {scheduler_job.py:155} INFO - Started process (PID=7122) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:55,433] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:24:55,434] {logging_mixin.py:112} INFO - [2020-11-05 13:24:55,434] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:56,744] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:24:56,785] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:24:56,802] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:24:56,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.379 seconds
[2020-11-05 13:25:08,652] {scheduler_job.py:155} INFO - Started process (PID=7188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:08,656] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:25:08,657] {logging_mixin.py:112} INFO - [2020-11-05 13:25:08,656] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:09,835] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:09,877] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:25:09,895] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:25:09,901] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-05 13:25:21,864] {scheduler_job.py:155} INFO - Started process (PID=7250) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:21,868] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:25:21,868] {logging_mixin.py:112} INFO - [2020-11-05 13:25:21,868] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:23,096] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:23,145] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:25:23,167] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:25:23,172] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-05 13:25:35,094] {scheduler_job.py:155} INFO - Started process (PID=7313) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:35,102] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:25:35,102] {logging_mixin.py:112} INFO - [2020-11-05 13:25:35,102] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:36,465] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:36,515] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:25:36,535] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:25:36,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.449 seconds
[2020-11-05 13:25:48,290] {scheduler_job.py:155} INFO - Started process (PID=7375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:48,295] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:25:48,295] {logging_mixin.py:112} INFO - [2020-11-05 13:25:48,295] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:49,515] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:25:49,557] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:25:49,578] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:25:49,582] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.292 seconds
[2020-11-05 13:26:01,543] {scheduler_job.py:155} INFO - Started process (PID=7431) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:01,547] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:26:01,547] {logging_mixin.py:112} INFO - [2020-11-05 13:26:01,547] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:02,814] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:02,862] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:26:02,881] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:26:02,885] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.342 seconds
[2020-11-05 13:26:14,749] {scheduler_job.py:155} INFO - Started process (PID=7491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:14,752] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:26:14,753] {logging_mixin.py:112} INFO - [2020-11-05 13:26:14,753] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:15,932] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:15,993] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:26:16,014] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:26:16,020] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.271 seconds
[2020-11-05 13:26:27,998] {scheduler_job.py:155} INFO - Started process (PID=7558) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:28,013] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:26:28,014] {logging_mixin.py:112} INFO - [2020-11-05 13:26:28,014] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:29,380] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:29,436] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:26:29,455] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:26:29,458] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.460 seconds
[2020-11-05 13:26:41,228] {scheduler_job.py:155} INFO - Started process (PID=7624) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:41,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:26:41,243] {logging_mixin.py:112} INFO - [2020-11-05 13:26:41,242] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:42,301] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:42,350] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:26:42,367] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:26:42,373] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.145 seconds
[2020-11-05 13:26:54,464] {scheduler_job.py:155} INFO - Started process (PID=7686) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:54,469] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:26:54,470] {logging_mixin.py:112} INFO - [2020-11-05 13:26:54,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:55,833] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:26:55,873] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:26:55,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:26:55,898] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.434 seconds
[2020-11-05 13:27:07,667] {scheduler_job.py:155} INFO - Started process (PID=7746) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:07,673] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:27:07,674] {logging_mixin.py:112} INFO - [2020-11-05 13:27:07,674] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:08,790] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:08,840] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:27:08,860] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:27:08,866] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.199 seconds
[2020-11-05 13:27:20,884] {scheduler_job.py:155} INFO - Started process (PID=7808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:20,894] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:27:20,894] {logging_mixin.py:112} INFO - [2020-11-05 13:27:20,894] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:22,140] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:22,197] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:27:22,218] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:27:22,222] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.338 seconds
[2020-11-05 13:27:34,095] {scheduler_job.py:155} INFO - Started process (PID=7874) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:34,100] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:27:34,100] {logging_mixin.py:112} INFO - [2020-11-05 13:27:34,100] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:35,322] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:35,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:27:35,423] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:27:35,433] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.339 seconds
[2020-11-05 13:27:47,299] {scheduler_job.py:155} INFO - Started process (PID=7932) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:47,302] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:27:47,303] {logging_mixin.py:112} INFO - [2020-11-05 13:27:47,303] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:48,593] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:27:48,628] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:27:48,644] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:27:48,649] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.350 seconds
[2020-11-05 13:28:00,545] {scheduler_job.py:155} INFO - Started process (PID=7994) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:00,549] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:28:00,550] {logging_mixin.py:112} INFO - [2020-11-05 13:28:00,550] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:01,639] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:01,699] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:28:01,717] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:28:01,721] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.176 seconds
[2020-11-05 13:28:13,795] {scheduler_job.py:155} INFO - Started process (PID=8053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:13,798] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:28:13,799] {logging_mixin.py:112} INFO - [2020-11-05 13:28:13,799] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:14,945] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:14,992] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:28:15,013] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:28:15,017] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.222 seconds
[2020-11-05 13:28:27,046] {scheduler_job.py:155} INFO - Started process (PID=8118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:27,049] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:28:27,049] {logging_mixin.py:112} INFO - [2020-11-05 13:28:27,049] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:28,283] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:28,337] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:28:28,367] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:28:28,372] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.327 seconds
[2020-11-05 13:28:40,251] {scheduler_job.py:155} INFO - Started process (PID=8197) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:40,259] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:28:40,261] {logging_mixin.py:112} INFO - [2020-11-05 13:28:40,261] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:41,498] {logging_mixin.py:112} INFO - [2020-11-05 13:28:41,493] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 110, in <module>
    dt = {{ ds}}
NameError: name 'ds' is not defined
[2020-11-05 13:28:41,499] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:41,530] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-05 13:28:53,477] {scheduler_job.py:155} INFO - Started process (PID=8265) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:53,482] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:28:53,483] {logging_mixin.py:112} INFO - [2020-11-05 13:28:53,483] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:54,584] {logging_mixin.py:112} INFO - [2020-11-05 13:28:54,580] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 110, in <module>
    dt = {{ ds}}
NameError: name 'ds' is not defined
[2020-11-05 13:28:54,585] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:28:54,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.135 seconds
[2020-11-05 13:29:06,685] {scheduler_job.py:155} INFO - Started process (PID=8332) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:06,688] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:29:06,689] {logging_mixin.py:112} INFO - [2020-11-05 13:29:06,689] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:07,971] {logging_mixin.py:112} INFO - [2020-11-05 13:29:07,968] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 110, in <module>
    dt = {{ ds}}
NameError: name 'ds' is not defined
[2020-11-05 13:29:07,971] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:08,001] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.316 seconds
[2020-11-05 13:29:19,907] {scheduler_job.py:155} INFO - Started process (PID=8393) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:19,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:29:19,916] {logging_mixin.py:112} INFO - [2020-11-05 13:29:19,915] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:21,062] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:21,111] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:29:21,131] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:29:21,137] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.230 seconds
[2020-11-05 13:29:33,202] {scheduler_job.py:155} INFO - Started process (PID=8460) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:33,205] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:29:33,206] {logging_mixin.py:112} INFO - [2020-11-05 13:29:33,206] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:34,763] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:34,806] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:29:34,822] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:29:34,826] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.624 seconds
[2020-11-05 13:29:46,432] {scheduler_job.py:155} INFO - Started process (PID=8522) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:46,439] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:29:46,440] {logging_mixin.py:112} INFO - [2020-11-05 13:29:46,440] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:47,821] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:47,859] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:29:47,876] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:29:47,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.459 seconds
[2020-11-05 13:29:59,654] {scheduler_job.py:155} INFO - Started process (PID=8583) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:29:59,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:29:59,659] {logging_mixin.py:112} INFO - [2020-11-05 13:29:59,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:00,816] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:00,862] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:30:00,883] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:30:00,888] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.234 seconds
[2020-11-05 13:30:12,865] {scheduler_job.py:155} INFO - Started process (PID=8646) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:12,870] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:30:12,872] {logging_mixin.py:112} INFO - [2020-11-05 13:30:12,871] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:14,313] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:14,353] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:30:14,368] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:30:14,376] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.511 seconds
[2020-11-05 13:30:26,089] {scheduler_job.py:155} INFO - Started process (PID=8708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:26,095] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:30:26,099] {logging_mixin.py:112} INFO - [2020-11-05 13:30:26,096] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:27,328] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:27,371] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:30:27,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:30:27,404] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.315 seconds
[2020-11-05 13:30:39,267] {scheduler_job.py:155} INFO - Started process (PID=8766) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:39,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:30:39,277] {logging_mixin.py:112} INFO - [2020-11-05 13:30:39,276] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:40,447] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:40,488] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:30:40,508] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:30:40,512] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.245 seconds
[2020-11-05 13:30:52,444] {scheduler_job.py:155} INFO - Started process (PID=8823) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:52,449] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:30:52,450] {logging_mixin.py:112} INFO - [2020-11-05 13:30:52,449] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:53,850] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:30:53,942] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:30:53,972] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:30:53,990] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.546 seconds
[2020-11-05 13:31:05,719] {scheduler_job.py:155} INFO - Started process (PID=8881) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:05,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:31:05,725] {logging_mixin.py:112} INFO - [2020-11-05 13:31:05,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:06,976] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:07,025] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:31:07,043] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:31:07,048] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.329 seconds
[2020-11-05 13:31:18,895] {scheduler_job.py:155} INFO - Started process (PID=8941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:18,900] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:31:18,902] {logging_mixin.py:112} INFO - [2020-11-05 13:31:18,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:20,533] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:20,599] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:31:20,631] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:31:20,647] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.752 seconds
[2020-11-05 13:31:32,161] {scheduler_job.py:155} INFO - Started process (PID=8999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:32,166] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:31:32,167] {logging_mixin.py:112} INFO - [2020-11-05 13:31:32,166] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:33,647] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:33,711] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:31:33,730] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:31:33,737] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.576 seconds
[2020-11-05 13:31:45,334] {scheduler_job.py:155} INFO - Started process (PID=9059) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:45,337] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:31:45,338] {logging_mixin.py:112} INFO - [2020-11-05 13:31:45,338] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:46,464] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:46,507] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:31:46,526] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:31:46,530] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.196 seconds
[2020-11-05 13:31:58,545] {scheduler_job.py:155} INFO - Started process (PID=9119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:58,549] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:31:58,549] {logging_mixin.py:112} INFO - [2020-11-05 13:31:58,549] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:59,603] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:31:59,655] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:31:59,686] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:31:59,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.145 seconds
[2020-11-05 13:32:11,808] {scheduler_job.py:155} INFO - Started process (PID=9181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:11,820] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:32:11,821] {logging_mixin.py:112} INFO - [2020-11-05 13:32:11,820] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:13,148] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:13,197] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:32:13,215] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:32:13,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.412 seconds
[2020-11-05 13:32:25,037] {scheduler_job.py:155} INFO - Started process (PID=9239) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:25,040] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:32:25,042] {logging_mixin.py:112} INFO - [2020-11-05 13:32:25,040] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:26,369] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:26,416] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:32:26,438] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:32:26,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.410 seconds
[2020-11-05 13:32:38,320] {scheduler_job.py:155} INFO - Started process (PID=9296) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:38,324] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:32:38,324] {logging_mixin.py:112} INFO - [2020-11-05 13:32:38,324] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:39,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:39,490] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:32:39,506] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:32:39,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.195 seconds
[2020-11-05 13:32:51,485] {scheduler_job.py:155} INFO - Started process (PID=9359) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:51,489] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:32:51,491] {logging_mixin.py:112} INFO - [2020-11-05 13:32:51,489] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:52,886] {logging_mixin.py:112} INFO - [2020-11-05 13:32:52,881] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 119, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{ ds}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:32:52,886] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:32:52,908] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.424 seconds
[2020-11-05 13:33:04,675] {scheduler_job.py:155} INFO - Started process (PID=9422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:04,679] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:33:04,679] {logging_mixin.py:112} INFO - [2020-11-05 13:33:04,679] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:05,910] {logging_mixin.py:112} INFO - [2020-11-05 13:33:05,907] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 119, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{ ds}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:33:05,910] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:05,932] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.257 seconds
[2020-11-05 13:33:17,890] {scheduler_job.py:155} INFO - Started process (PID=9487) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:17,894] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:33:17,894] {logging_mixin.py:112} INFO - [2020-11-05 13:33:17,894] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:19,015] {logging_mixin.py:112} INFO - [2020-11-05 13:33:19,012] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 119, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{ ds}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:33:19,016] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:19,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.154 seconds
[2020-11-05 13:33:31,134] {scheduler_job.py:155} INFO - Started process (PID=9555) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:31,152] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:33:31,155] {logging_mixin.py:112} INFO - [2020-11-05 13:33:31,155] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:32,629] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:32,693] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:33:32,719] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:33:32,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.594 seconds
[2020-11-05 13:33:44,317] {scheduler_job.py:155} INFO - Started process (PID=9623) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:44,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:33:44,322] {logging_mixin.py:112} INFO - [2020-11-05 13:33:44,322] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:45,490] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:45,526] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:33:45,543] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:33:45,547] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.230 seconds
[2020-11-05 13:33:57,574] {scheduler_job.py:155} INFO - Started process (PID=9685) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:57,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:33:57,582] {logging_mixin.py:112} INFO - [2020-11-05 13:33:57,582] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:58,731] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:33:58,783] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:33:58,802] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:33:58,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-05 13:34:10,800] {scheduler_job.py:155} INFO - Started process (PID=9745) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:10,812] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:34:10,813] {logging_mixin.py:112} INFO - [2020-11-05 13:34:10,813] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:12,004] {logging_mixin.py:112} INFO - [2020-11-05 13:34:12,001] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 119, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{ds}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:34:12,004] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:12,037] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.237 seconds
[2020-11-05 13:34:24,029] {scheduler_job.py:155} INFO - Started process (PID=9810) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:24,051] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:34:24,052] {logging_mixin.py:112} INFO - [2020-11-05 13:34:24,051] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:25,611] {logging_mixin.py:112} INFO - [2020-11-05 13:34:25,607] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 119, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{ds}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:34:25,611] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:25,633] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.604 seconds
[2020-11-05 13:34:37,213] {scheduler_job.py:155} INFO - Started process (PID=9869) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:37,217] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:34:37,218] {logging_mixin.py:112} INFO - [2020-11-05 13:34:37,218] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:38,437] {logging_mixin.py:112} INFO - [2020-11-05 13:34:38,432] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:34:38,438] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:38,463] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.250 seconds
[2020-11-05 13:34:50,403] {scheduler_job.py:155} INFO - Started process (PID=9936) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:50,407] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:34:50,407] {logging_mixin.py:112} INFO - [2020-11-05 13:34:50,407] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:52,039] {logging_mixin.py:112} INFO - [2020-11-05 13:34:52,035] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:34:52,039] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:34:52,065] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.662 seconds
[2020-11-05 13:35:03,612] {scheduler_job.py:155} INFO - Started process (PID=10000) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:03,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:35:03,625] {logging_mixin.py:112} INFO - [2020-11-05 13:35:03,625] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:04,848] {logging_mixin.py:112} INFO - [2020-11-05 13:35:04,846] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:35:04,848] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:04,875] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.263 seconds
[2020-11-05 13:35:16,822] {scheduler_job.py:155} INFO - Started process (PID=10065) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:16,835] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:35:16,836] {logging_mixin.py:112} INFO - [2020-11-05 13:35:16,836] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:18,174] {logging_mixin.py:112} INFO - [2020-11-05 13:35:18,172] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:35:18,174] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:18,197] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.375 seconds
[2020-11-05 13:35:30,036] {scheduler_job.py:155} INFO - Started process (PID=10132) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:30,038] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:35:30,039] {logging_mixin.py:112} INFO - [2020-11-05 13:35:30,039] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:31,288] {logging_mixin.py:112} INFO - [2020-11-05 13:35:31,286] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:35:31,289] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:31,309] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.274 seconds
[2020-11-05 13:35:43,271] {scheduler_job.py:155} INFO - Started process (PID=10190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:43,276] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:35:43,278] {logging_mixin.py:112} INFO - [2020-11-05 13:35:43,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:44,610] {logging_mixin.py:112} INFO - [2020-11-05 13:35:44,607] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:35:44,610] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:44,651] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.380 seconds
[2020-11-05 13:35:56,530] {scheduler_job.py:155} INFO - Started process (PID=10264) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:56,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:35:56,538] {logging_mixin.py:112} INFO - [2020-11-05 13:35:56,538] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:57,669] {logging_mixin.py:112} INFO - [2020-11-05 13:35:57,667] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:35:57,669] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:35:57,696] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.166 seconds
[2020-11-05 13:36:09,755] {scheduler_job.py:155} INFO - Started process (PID=10330) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:09,758] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:36:09,759] {logging_mixin.py:112} INFO - [2020-11-05 13:36:09,759] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:10,946] {logging_mixin.py:112} INFO - [2020-11-05 13:36:10,941] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:36:10,946] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:10,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.228 seconds
[2020-11-05 13:36:22,971] {scheduler_job.py:155} INFO - Started process (PID=10388) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:22,975] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:36:22,976] {logging_mixin.py:112} INFO - [2020-11-05 13:36:22,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:24,228] {logging_mixin.py:112} INFO - [2020-11-05 13:36:24,224] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:36:24,229] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:24,253] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.282 seconds
[2020-11-05 13:36:36,211] {scheduler_job.py:155} INFO - Started process (PID=10451) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:36,215] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:36:36,216] {logging_mixin.py:112} INFO - [2020-11-05 13:36:36,216] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:37,515] {logging_mixin.py:112} INFO - [2020-11-05 13:36:37,511] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:36:37,515] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:37,543] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.332 seconds
[2020-11-05 13:36:49,396] {scheduler_job.py:155} INFO - Started process (PID=10512) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:49,400] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:36:49,400] {logging_mixin.py:112} INFO - [2020-11-05 13:36:49,400] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:50,843] {logging_mixin.py:112} INFO - [2020-11-05 13:36:50,838] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 116, in <module>
    "timedate": dt,
NameError: name 'dt' is not defined
[2020-11-05 13:36:50,844] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:36:50,882] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.486 seconds
[2020-11-05 13:37:02,637] {scheduler_job.py:155} INFO - Started process (PID=10576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:02,641] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:37:02,642] {logging_mixin.py:112} INFO - [2020-11-05 13:37:02,642] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:03,888] {logging_mixin.py:112} INFO - [2020-11-05 13:37:03,885] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 119, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{ds}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:37:03,888] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:03,912] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.274 seconds
[2020-11-05 13:37:15,881] {scheduler_job.py:155} INFO - Started process (PID=10635) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:15,888] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:37:15,891] {logging_mixin.py:112} INFO - [2020-11-05 13:37:15,889] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:17,063] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:17,101] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:37:17,118] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:37:17,124] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.243 seconds
[2020-11-05 13:37:29,088] {scheduler_job.py:155} INFO - Started process (PID=10700) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:29,094] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:37:29,095] {logging_mixin.py:112} INFO - [2020-11-05 13:37:29,095] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:30,348] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:30,391] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:37:30,415] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:37:30,423] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.335 seconds
[2020-11-05 13:37:42,350] {scheduler_job.py:155} INFO - Started process (PID=10759) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:42,353] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:37:42,354] {logging_mixin.py:112} INFO - [2020-11-05 13:37:42,353] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:43,684] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:43,731] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:37:43,752] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:37:43,757] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.407 seconds
[2020-11-05 13:37:55,554] {scheduler_job.py:155} INFO - Started process (PID=10833) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:55,559] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:37:55,560] {logging_mixin.py:112} INFO - [2020-11-05 13:37:55,560] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:56,762] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:37:56,814] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:37:56,846] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:37:56,851] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.297 seconds
[2020-11-05 13:38:08,830] {scheduler_job.py:155} INFO - Started process (PID=10893) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:08,834] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:38:08,835] {logging_mixin.py:112} INFO - [2020-11-05 13:38:08,835] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:10,121] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:10,191] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:38:10,224] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:38:10,232] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.402 seconds
[2020-11-05 13:38:22,049] {scheduler_job.py:155} INFO - Started process (PID=10953) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:22,067] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:38:22,071] {logging_mixin.py:112} INFO - [2020-11-05 13:38:22,068] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:23,210] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:23,255] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:38:23,280] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:38:23,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.237 seconds
[2020-11-05 13:38:35,279] {scheduler_job.py:155} INFO - Started process (PID=11013) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:35,283] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:38:35,283] {logging_mixin.py:112} INFO - [2020-11-05 13:38:35,283] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:36,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:36,493] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:38:36,509] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:38:36,513] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.234 seconds
[2020-11-05 13:38:48,481] {scheduler_job.py:155} INFO - Started process (PID=11075) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:48,487] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:38:48,487] {logging_mixin.py:112} INFO - [2020-11-05 13:38:48,487] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:49,812] {logging_mixin.py:112} INFO - [2020-11-05 13:38:49,809] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:38:49,813] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:38:49,844] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.363 seconds
[2020-11-05 13:39:01,664] {scheduler_job.py:155} INFO - Started process (PID=11137) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:01,667] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:39:01,667] {logging_mixin.py:112} INFO - [2020-11-05 13:39:01,667] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:02,906] {logging_mixin.py:112} INFO - [2020-11-05 13:39:02,901] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:39:02,906] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:02,945] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.281 seconds
[2020-11-05 13:39:14,899] {scheduler_job.py:155} INFO - Started process (PID=11208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:14,902] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:39:14,903] {logging_mixin.py:112} INFO - [2020-11-05 13:39:14,903] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:16,127] {logging_mixin.py:112} INFO - [2020-11-05 13:39:16,123] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:39:16,127] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:16,151] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.252 seconds
[2020-11-05 13:39:28,116] {scheduler_job.py:155} INFO - Started process (PID=11275) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:28,119] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:39:28,120] {logging_mixin.py:112} INFO - [2020-11-05 13:39:28,120] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:29,360] {logging_mixin.py:112} INFO - [2020-11-05 13:39:29,357] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:39:29,360] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:29,392] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.276 seconds
[2020-11-05 13:39:41,335] {scheduler_job.py:155} INFO - Started process (PID=11336) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:41,338] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:39:41,338] {logging_mixin.py:112} INFO - [2020-11-05 13:39:41,338] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:42,599] {logging_mixin.py:112} INFO - [2020-11-05 13:39:42,592] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:39:42,599] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:42,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.306 seconds
[2020-11-05 13:39:54,547] {scheduler_job.py:155} INFO - Started process (PID=11398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:54,550] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:39:54,551] {logging_mixin.py:112} INFO - [2020-11-05 13:39:54,551] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:55,602] {logging_mixin.py:112} INFO - [2020-11-05 13:39:55,599] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:39:55,602] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:39:55,624] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.077 seconds
[2020-11-05 13:40:07,855] {scheduler_job.py:155} INFO - Started process (PID=11460) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:07,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:40:07,863] {logging_mixin.py:112} INFO - [2020-11-05 13:40:07,862] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:08,960] {logging_mixin.py:112} INFO - [2020-11-05 13:40:08,957] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:40:08,960] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:08,990] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.135 seconds
[2020-11-05 13:40:21,109] {scheduler_job.py:155} INFO - Started process (PID=11518) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:21,113] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:40:21,113] {logging_mixin.py:112} INFO - [2020-11-05 13:40:21,113] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:22,243] {logging_mixin.py:112} INFO - [2020-11-05 13:40:22,238] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:40:22,243] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:22,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.162 seconds
[2020-11-05 13:40:34,357] {scheduler_job.py:155} INFO - Started process (PID=11578) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:34,364] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:40:34,365] {logging_mixin.py:112} INFO - [2020-11-05 13:40:34,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:35,563] {logging_mixin.py:112} INFO - [2020-11-05 13:40:35,559] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:40:35,563] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:35,590] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.234 seconds
[2020-11-05 13:40:47,547] {scheduler_job.py:155} INFO - Started process (PID=11636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:47,550] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:40:47,551] {logging_mixin.py:112} INFO - [2020-11-05 13:40:47,551] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:48,857] {logging_mixin.py:112} INFO - [2020-11-05 13:40:48,851] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 120, in <module>
    dag=dag
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 89, in __init__
    super(PythonOperator, self).__init__(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/decorators.py", line 98, in wrapper
    result = func(*args, **kwargs)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/baseoperator.py", line 342, in __init__
    validate_key(task_id)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/utils/helpers.py", line 68, in validate_key
    "dots and underscores exclusively".format(k=k))
airflow.exceptions.AirflowException: The key (simulate_task_{{execution_date}}) has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2020-11-05 13:40:48,858] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:40:48,884] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.337 seconds
[2020-11-05 13:41:00,743] {scheduler_job.py:155} INFO - Started process (PID=11697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:00,747] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:41:00,747] {logging_mixin.py:112} INFO - [2020-11-05 13:41:00,747] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:01,828] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:01,891] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:41:01,911] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:41:01,916] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.173 seconds
[2020-11-05 13:41:13,998] {scheduler_job.py:155} INFO - Started process (PID=11764) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:14,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:41:14,002] {logging_mixin.py:112} INFO - [2020-11-05 13:41:14,002] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:15,173] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:15,250] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:41:15,281] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:41:15,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.293 seconds
[2020-11-05 13:41:27,191] {scheduler_job.py:155} INFO - Started process (PID=11823) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:27,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:41:27,198] {logging_mixin.py:112} INFO - [2020-11-05 13:41:27,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:28,588] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:28,634] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:41:28,665] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:41:28,670] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.479 seconds
[2020-11-05 13:41:40,380] {scheduler_job.py:155} INFO - Started process (PID=11883) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:40,385] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:41:40,386] {logging_mixin.py:112} INFO - [2020-11-05 13:41:40,385] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:41,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:41,987] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:41:42,004] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:41:42,007] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.628 seconds
[2020-11-05 13:41:53,584] {scheduler_job.py:155} INFO - Started process (PID=11953) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:53,588] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:41:53,588] {logging_mixin.py:112} INFO - [2020-11-05 13:41:53,588] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:54,825] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:41:54,877] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:41:54,894] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:41:54,898] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.315 seconds
[2020-11-05 13:42:06,853] {scheduler_job.py:155} INFO - Started process (PID=12016) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:06,857] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:42:06,858] {logging_mixin.py:112} INFO - [2020-11-05 13:42:06,857] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:08,146] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:08,207] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:42:08,247] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:42:08,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.406 seconds
[2020-11-05 13:42:20,093] {scheduler_job.py:155} INFO - Started process (PID=12077) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:20,100] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:42:20,103] {logging_mixin.py:112} INFO - [2020-11-05 13:42:20,101] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:21,680] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:21,751] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:42:21,773] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:42:21,777] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.684 seconds
[2020-11-05 13:42:33,321] {scheduler_job.py:155} INFO - Started process (PID=12139) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:33,336] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:42:33,338] {logging_mixin.py:112} INFO - [2020-11-05 13:42:33,337] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:34,560] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:34,626] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:42:34,642] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:42:34,647] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.326 seconds
[2020-11-05 13:42:46,586] {scheduler_job.py:155} INFO - Started process (PID=12197) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:46,590] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:42:46,596] {logging_mixin.py:112} INFO - [2020-11-05 13:42:46,591] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:47,893] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:47,939] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:42:47,954] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:42:47,959] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.373 seconds
[2020-11-05 13:42:59,837] {scheduler_job.py:155} INFO - Started process (PID=12255) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:42:59,842] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:42:59,842] {logging_mixin.py:112} INFO - [2020-11-05 13:42:59,842] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:01,078] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:01,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:43:01,138] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:43:01,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.308 seconds
[2020-11-05 13:43:13,065] {scheduler_job.py:155} INFO - Started process (PID=12316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:13,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:43:13,078] {logging_mixin.py:112} INFO - [2020-11-05 13:43:13,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:14,434] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:14,489] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:43:14,507] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:43:14,511] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.446 seconds
[2020-11-05 13:43:26,333] {scheduler_job.py:155} INFO - Started process (PID=12386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:26,336] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:43:26,336] {logging_mixin.py:112} INFO - [2020-11-05 13:43:26,336] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:27,397] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:27,449] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:43:27,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:43:27,478] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.145 seconds
[2020-11-05 13:43:39,548] {scheduler_job.py:155} INFO - Started process (PID=12446) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:39,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:43:39,552] {logging_mixin.py:112} INFO - [2020-11-05 13:43:39,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:40,755] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:40,831] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:43:40,882] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:43:40,894] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-05 13:43:52,759] {scheduler_job.py:155} INFO - Started process (PID=12508) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:52,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:43:52,766] {logging_mixin.py:112} INFO - [2020-11-05 13:43:52,763] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:53,984] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:43:54,026] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:43:54,047] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:43:54,050] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.292 seconds
[2020-11-05 13:44:05,982] {scheduler_job.py:155} INFO - Started process (PID=12570) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:05,987] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:44:05,988] {logging_mixin.py:112} INFO - [2020-11-05 13:44:05,987] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:07,126] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:07,166] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:44:07,186] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:44:07,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.208 seconds
[2020-11-05 13:44:19,231] {scheduler_job.py:155} INFO - Started process (PID=12627) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:19,234] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:44:19,235] {logging_mixin.py:112} INFO - [2020-11-05 13:44:19,235] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:20,485] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:20,523] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:44:20,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:44:20,556] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.326 seconds
[2020-11-05 13:44:32,421] {scheduler_job.py:155} INFO - Started process (PID=12696) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:32,425] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:44:32,426] {logging_mixin.py:112} INFO - [2020-11-05 13:44:32,426] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:33,770] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:33,836] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:44:33,865] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:44:33,870] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.449 seconds
[2020-11-05 13:44:45,684] {scheduler_job.py:155} INFO - Started process (PID=12755) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:45,689] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:44:45,691] {logging_mixin.py:112} INFO - [2020-11-05 13:44:45,690] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:47,308] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:47,384] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:44:47,414] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:44:47,422] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.739 seconds
[2020-11-05 13:44:58,910] {scheduler_job.py:155} INFO - Started process (PID=12817) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:44:58,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:44:58,916] {logging_mixin.py:112} INFO - [2020-11-05 13:44:58,916] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:00,035] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:00,108] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:45:00,132] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:45:00,136] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.226 seconds
[2020-11-05 13:45:12,118] {scheduler_job.py:155} INFO - Started process (PID=12884) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:12,125] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:45:12,131] {logging_mixin.py:112} INFO - [2020-11-05 13:45:12,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:13,553] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:13,651] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:45:13,675] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:45:13,681] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.563 seconds
[2020-11-05 13:45:25,328] {scheduler_job.py:155} INFO - Started process (PID=12946) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:25,334] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:45:25,335] {logging_mixin.py:112} INFO - [2020-11-05 13:45:25,334] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:26,487] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:26,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:45:26,555] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:45:26,560] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-05 13:45:38,536] {scheduler_job.py:155} INFO - Started process (PID=13016) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:38,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:45:38,541] {logging_mixin.py:112} INFO - [2020-11-05 13:45:38,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:39,702] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:39,742] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:45:39,758] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:45:39,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.226 seconds
[2020-11-05 13:45:51,788] {scheduler_job.py:155} INFO - Started process (PID=13079) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:51,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:45:51,795] {logging_mixin.py:112} INFO - [2020-11-05 13:45:51,795] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:53,162] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:45:53,209] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:45:53,226] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:45:53,232] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.444 seconds
[2020-11-05 13:46:05,017] {scheduler_job.py:155} INFO - Started process (PID=13143) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:05,023] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:46:05,024] {logging_mixin.py:112} INFO - [2020-11-05 13:46:05,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:06,355] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:06,397] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:46:06,429] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:46:06,434] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.416 seconds
[2020-11-05 13:46:18,276] {scheduler_job.py:155} INFO - Started process (PID=13208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:18,284] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:46:18,286] {logging_mixin.py:112} INFO - [2020-11-05 13:46:18,286] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:20,040] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:20,099] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:46:20,122] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:46:20,126] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.850 seconds
[2020-11-05 13:46:31,480] {scheduler_job.py:155} INFO - Started process (PID=13288) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:31,483] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:46:31,484] {logging_mixin.py:112} INFO - [2020-11-05 13:46:31,483] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:32,775] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:32,832] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:46:32,852] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:46:32,857] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.377 seconds
[2020-11-05 13:46:44,729] {scheduler_job.py:155} INFO - Started process (PID=13348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:44,744] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:46:44,745] {logging_mixin.py:112} INFO - [2020-11-05 13:46:44,745] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:46,006] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:46,051] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:46:46,067] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:46:46,071] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.342 seconds
[2020-11-05 13:46:57,901] {scheduler_job.py:155} INFO - Started process (PID=13410) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:57,906] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:46:57,907] {logging_mixin.py:112} INFO - [2020-11-05 13:46:57,907] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:59,056] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:46:59,096] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:46:59,113] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:46:59,117] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.216 seconds
[2020-11-05 13:47:11,136] {scheduler_job.py:155} INFO - Started process (PID=13470) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:11,144] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:47:11,146] {logging_mixin.py:112} INFO - [2020-11-05 13:47:11,144] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:12,294] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:12,356] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:47:12,379] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:47:12,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.246 seconds
[2020-11-05 13:47:24,374] {scheduler_job.py:155} INFO - Started process (PID=13528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:24,378] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:47:24,379] {logging_mixin.py:112} INFO - [2020-11-05 13:47:24,379] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:25,568] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:25,609] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:47:25,625] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:47:25,629] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.255 seconds
[2020-11-05 13:47:37,617] {scheduler_job.py:155} INFO - Started process (PID=13589) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:37,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:47:37,622] {logging_mixin.py:112} INFO - [2020-11-05 13:47:37,622] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:38,790] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:38,834] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 13:47:38,850] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 13:47:38,854] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.238 seconds
[2020-11-05 13:47:50,814] {scheduler_job.py:155} INFO - Started process (PID=13648) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:50,830] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:47:50,831] {logging_mixin.py:112} INFO - [2020-11-05 13:47:50,831] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:52,081] {logging_mixin.py:112} INFO - [2020-11-05 13:47:52,076] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:47:52,081] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:47:52,107] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.294 seconds
[2020-11-05 13:48:04,002] {scheduler_job.py:155} INFO - Started process (PID=13708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:04,006] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:48:04,006] {logging_mixin.py:112} INFO - [2020-11-05 13:48:04,006] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:05,177] {logging_mixin.py:112} INFO - [2020-11-05 13:48:05,174] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:48:05,178] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:05,201] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.199 seconds
[2020-11-05 13:48:17,229] {scheduler_job.py:155} INFO - Started process (PID=13775) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:17,234] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:48:17,235] {logging_mixin.py:112} INFO - [2020-11-05 13:48:17,235] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:18,503] {logging_mixin.py:112} INFO - [2020-11-05 13:48:18,500] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:48:18,503] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:18,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.295 seconds
[2020-11-05 13:48:30,411] {scheduler_job.py:155} INFO - Started process (PID=13857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:30,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:48:30,415] {logging_mixin.py:112} INFO - [2020-11-05 13:48:30,415] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:31,494] {logging_mixin.py:112} INFO - [2020-11-05 13:48:31,491] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:48:31,494] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:31,516] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.106 seconds
[2020-11-05 13:48:43,632] {scheduler_job.py:155} INFO - Started process (PID=13927) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:43,635] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:48:43,636] {logging_mixin.py:112} INFO - [2020-11-05 13:48:43,636] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:44,850] {logging_mixin.py:112} INFO - [2020-11-05 13:48:44,848] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:48:44,850] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:44,872] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.240 seconds
[2020-11-05 13:48:56,836] {scheduler_job.py:155} INFO - Started process (PID=13996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:56,840] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:48:56,841] {logging_mixin.py:112} INFO - [2020-11-05 13:48:56,841] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:57,960] {logging_mixin.py:112} INFO - [2020-11-05 13:48:57,956] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:48:57,960] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:48:57,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.146 seconds
[2020-11-05 13:49:10,003] {scheduler_job.py:155} INFO - Started process (PID=14058) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:10,007] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:49:10,008] {logging_mixin.py:112} INFO - [2020-11-05 13:49:10,007] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:11,139] {logging_mixin.py:112} INFO - [2020-11-05 13:49:11,137] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:49:11,139] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:11,162] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.159 seconds
[2020-11-05 13:49:23,228] {scheduler_job.py:155} INFO - Started process (PID=14121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:23,232] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:49:23,233] {logging_mixin.py:112} INFO - [2020-11-05 13:49:23,232] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:24,307] {logging_mixin.py:112} INFO - [2020-11-05 13:49:24,305] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:49:24,307] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:24,333] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.105 seconds
[2020-11-05 13:49:36,402] {scheduler_job.py:155} INFO - Started process (PID=14179) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:36,406] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:49:36,406] {logging_mixin.py:112} INFO - [2020-11-05 13:49:36,406] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:37,739] {logging_mixin.py:112} INFO - [2020-11-05 13:49:37,734] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:49:37,739] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:37,767] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.364 seconds
[2020-11-05 13:49:49,651] {scheduler_job.py:155} INFO - Started process (PID=14240) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:49,654] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:49:49,655] {logging_mixin.py:112} INFO - [2020-11-05 13:49:49,655] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:50,773] {logging_mixin.py:112} INFO - [2020-11-05 13:49:50,769] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = datetime.strptime(dt, '%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:49:50,773] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:49:50,803] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.152 seconds
[2020-11-05 13:50:02,837] {scheduler_job.py:155} INFO - Started process (PID=14304) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:02,840] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:50:02,841] {logging_mixin.py:112} INFO - [2020-11-05 13:50:02,841] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:04,057] {logging_mixin.py:112} INFO - [2020-11-05 13:50:04,053] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = dt.strftime('%Y-%m-%d')
  File "/usr/lib/python3.6/_strptime.py", line 565, in _strptime_datetime
    tt, fraction = _strptime(data_string, format)
  File "/usr/lib/python3.6/_strptime.py", line 362, in _strptime
    (data_string, format))
ValueError: time data "{{ execution_date.strftime('%Y-%m-%d') }}" does not match format '%Y-%m-%d'
[2020-11-05 13:50:04,058] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:04,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.252 seconds
[2020-11-05 13:50:16,085] {scheduler_job.py:155} INFO - Started process (PID=14360) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:16,092] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:50:16,093] {logging_mixin.py:112} INFO - [2020-11-05 13:50:16,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:17,261] {logging_mixin.py:112} INFO - [2020-11-05 13:50:17,259] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:50:17,262] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:17,283] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.198 seconds
[2020-11-05 13:50:29,270] {scheduler_job.py:155} INFO - Started process (PID=14432) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:29,274] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:50:29,275] {logging_mixin.py:112} INFO - [2020-11-05 13:50:29,275] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:30,352] {logging_mixin.py:112} INFO - [2020-11-05 13:50:30,349] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:50:30,352] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:30,375] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.105 seconds
[2020-11-05 13:50:42,505] {scheduler_job.py:155} INFO - Started process (PID=14492) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:42,510] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:50:42,511] {logging_mixin.py:112} INFO - [2020-11-05 13:50:42,511] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:43,793] {logging_mixin.py:112} INFO - [2020-11-05 13:50:43,791] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 113, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:50:43,793] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:43,819] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.314 seconds
[2020-11-05 13:50:55,701] {scheduler_job.py:155} INFO - Started process (PID=14553) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:55,705] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:50:55,705] {logging_mixin.py:112} INFO - [2020-11-05 13:50:55,705] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:56,865] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:50:56,868] {logging_mixin.py:112} INFO - [2020-11-05 13:50:56,866] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:50:56,869] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:50:56,894] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.193 seconds
[2020-11-05 13:51:08,938] {scheduler_job.py:155} INFO - Started process (PID=14612) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:08,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:51:08,941] {logging_mixin.py:112} INFO - [2020-11-05 13:51:08,941] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:10,257] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:51:10,261] {logging_mixin.py:112} INFO - [2020-11-05 13:51:10,257] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:51:10,261] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:10,301] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.364 seconds
[2020-11-05 13:51:22,189] {scheduler_job.py:155} INFO - Started process (PID=14680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:22,192] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:51:22,193] {logging_mixin.py:112} INFO - [2020-11-05 13:51:22,193] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:23,256] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:51:23,259] {logging_mixin.py:112} INFO - [2020-11-05 13:51:23,257] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:51:23,259] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:23,278] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.089 seconds
[2020-11-05 13:51:35,382] {scheduler_job.py:155} INFO - Started process (PID=14744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:35,386] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:51:35,386] {logging_mixin.py:112} INFO - [2020-11-05 13:51:35,386] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:36,459] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:51:36,461] {logging_mixin.py:112} INFO - [2020-11-05 13:51:36,459] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:51:36,462] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:36,485] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.103 seconds
[2020-11-05 13:51:48,644] {scheduler_job.py:155} INFO - Started process (PID=14807) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:48,647] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:51:48,647] {logging_mixin.py:112} INFO - [2020-11-05 13:51:48,647] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:49,799] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:51:49,802] {logging_mixin.py:112} INFO - [2020-11-05 13:51:49,799] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:51:49,802] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:51:49,844] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.201 seconds
[2020-11-05 13:52:01,835] {scheduler_job.py:155} INFO - Started process (PID=14874) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:01,839] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:52:01,840] {logging_mixin.py:112} INFO - [2020-11-05 13:52:01,840] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:02,938] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:52:02,941] {logging_mixin.py:112} INFO - [2020-11-05 13:52:02,938] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:52:02,942] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:02,966] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.131 seconds
[2020-11-05 13:52:15,051] {scheduler_job.py:155} INFO - Started process (PID=14938) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:15,066] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:52:15,067] {logging_mixin.py:112} INFO - [2020-11-05 13:52:15,066] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:16,697] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:52:16,701] {logging_mixin.py:112} INFO - [2020-11-05 13:52:16,698] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:52:16,701] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:16,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.676 seconds
[2020-11-05 13:52:28,251] {scheduler_job.py:155} INFO - Started process (PID=14998) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:28,255] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:52:28,257] {logging_mixin.py:112} INFO - [2020-11-05 13:52:28,256] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:29,331] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:52:29,333] {logging_mixin.py:112} INFO - [2020-11-05 13:52:29,331] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:52:29,334] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:29,367] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.116 seconds
[2020-11-05 13:52:41,454] {scheduler_job.py:155} INFO - Started process (PID=15057) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:41,464] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:52:41,465] {logging_mixin.py:112} INFO - [2020-11-05 13:52:41,465] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:42,627] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:52:42,630] {logging_mixin.py:112} INFO - [2020-11-05 13:52:42,628] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:52:42,630] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:42,671] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.217 seconds
[2020-11-05 13:52:54,757] {scheduler_job.py:155} INFO - Started process (PID=15116) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:54,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:52:54,762] {logging_mixin.py:112} INFO - [2020-11-05 13:52:54,762] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:56,132] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:52:56,136] {logging_mixin.py:112} INFO - [2020-11-05 13:52:56,133] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:52:56,136] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:52:56,163] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.406 seconds
[2020-11-05 13:53:08,019] {scheduler_job.py:155} INFO - Started process (PID=15184) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:08,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:53:08,030] {logging_mixin.py:112} INFO - [2020-11-05 13:53:08,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:09,700] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:53:09,704] {logging_mixin.py:112} INFO - [2020-11-05 13:53:09,701] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:53:09,704] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:09,730] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.711 seconds
[2020-11-05 13:53:21,266] {scheduler_job.py:155} INFO - Started process (PID=15246) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:21,270] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:53:21,272] {logging_mixin.py:112} INFO - [2020-11-05 13:53:21,271] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:22,325] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:53:22,328] {logging_mixin.py:112} INFO - [2020-11-05 13:53:22,326] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:53:22,329] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:22,350] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.084 seconds
[2020-11-05 13:53:34,463] {scheduler_job.py:155} INFO - Started process (PID=15310) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:34,467] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:53:34,468] {logging_mixin.py:112} INFO - [2020-11-05 13:53:34,468] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:35,771] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:53:35,789] {logging_mixin.py:112} INFO - [2020-11-05 13:53:35,771] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:53:35,790] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:35,820] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.357 seconds
[2020-11-05 13:53:47,692] {scheduler_job.py:155} INFO - Started process (PID=15368) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:47,698] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:53:47,699] {logging_mixin.py:112} INFO - [2020-11-05 13:53:47,699] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:48,828] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:53:48,831] {logging_mixin.py:112} INFO - [2020-11-05 13:53:48,829] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:53:48,832] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:53:48,855] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.163 seconds
[2020-11-05 13:54:00,948] {scheduler_job.py:155} INFO - Started process (PID=15430) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:00,952] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:54:00,953] {logging_mixin.py:112} INFO - [2020-11-05 13:54:00,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:02,119] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:54:02,124] {logging_mixin.py:112} INFO - [2020-11-05 13:54:02,120] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:54:02,125] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:02,160] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.213 seconds
[2020-11-05 13:54:14,163] {scheduler_job.py:155} INFO - Started process (PID=15495) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:14,166] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:54:14,167] {logging_mixin.py:112} INFO - [2020-11-05 13:54:14,167] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:15,240] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:54:15,242] {logging_mixin.py:112} INFO - [2020-11-05 13:54:15,240] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:54:15,242] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:15,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.108 seconds
[2020-11-05 13:54:27,402] {scheduler_job.py:155} INFO - Started process (PID=15559) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:27,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:54:27,414] {logging_mixin.py:112} INFO - [2020-11-05 13:54:27,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:28,507] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:54:28,510] {logging_mixin.py:112} INFO - [2020-11-05 13:54:28,507] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:54:28,510] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:28,545] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.143 seconds
[2020-11-05 13:54:40,605] {scheduler_job.py:155} INFO - Started process (PID=15618) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:40,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:54:40,610] {logging_mixin.py:112} INFO - [2020-11-05 13:54:40,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:41,669] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:54:41,671] {logging_mixin.py:112} INFO - [2020-11-05 13:54:41,669] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:54:41,671] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:41,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.097 seconds
[2020-11-05 13:54:53,792] {scheduler_job.py:155} INFO - Started process (PID=15680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:53,796] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:54:53,796] {logging_mixin.py:112} INFO - [2020-11-05 13:54:53,796] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:54,981] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:54:54,991] {logging_mixin.py:112} INFO - [2020-11-05 13:54:54,981] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:54:54,992] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:54:55,019] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.227 seconds
[2020-11-05 13:55:06,995] {scheduler_job.py:155} INFO - Started process (PID=15744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:07,000] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:55:07,006] {logging_mixin.py:112} INFO - [2020-11-05 13:55:07,005] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:08,197] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:55:08,202] {logging_mixin.py:112} INFO - [2020-11-05 13:55:08,198] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:55:08,202] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:08,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.232 seconds
[2020-11-05 13:55:20,182] {scheduler_job.py:155} INFO - Started process (PID=15808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:20,187] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:55:20,188] {logging_mixin.py:112} INFO - [2020-11-05 13:55:20,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:21,267] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:55:21,271] {logging_mixin.py:112} INFO - [2020-11-05 13:55:21,269] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:55:21,271] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:21,297] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.115 seconds
[2020-11-05 13:55:33,431] {scheduler_job.py:155} INFO - Started process (PID=15870) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:33,437] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:55:33,438] {logging_mixin.py:112} INFO - [2020-11-05 13:55:33,437] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:34,699] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:55:34,702] {logging_mixin.py:112} INFO - [2020-11-05 13:55:34,700] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:55:34,702] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:34,724] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.293 seconds
[2020-11-05 13:55:46,624] {scheduler_job.py:155} INFO - Started process (PID=15932) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:46,628] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:55:46,629] {logging_mixin.py:112} INFO - [2020-11-05 13:55:46,629] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:48,432] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:55:48,435] {logging_mixin.py:112} INFO - [2020-11-05 13:55:48,432] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:55:48,435] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:48,464] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.840 seconds
[2020-11-05 13:55:59,932] {scheduler_job.py:155} INFO - Started process (PID=15995) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:55:59,936] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:55:59,941] {logging_mixin.py:112} INFO - [2020-11-05 13:55:59,937] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:01,850] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:56:01,858] {logging_mixin.py:112} INFO - [2020-11-05 13:56:01,851] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:56:01,859] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:01,900] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.969 seconds
[2020-11-05 13:56:13,192] {scheduler_job.py:155} INFO - Started process (PID=16051) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:13,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:56:13,198] {logging_mixin.py:112} INFO - [2020-11-05 13:56:13,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:14,809] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:56:14,813] {logging_mixin.py:112} INFO - [2020-11-05 13:56:14,810] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:56:14,813] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:14,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.646 seconds
[2020-11-05 13:56:26,394] {scheduler_job.py:155} INFO - Started process (PID=16109) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:26,398] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:56:26,399] {logging_mixin.py:112} INFO - [2020-11-05 13:56:26,399] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:27,668] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:56:27,671] {logging_mixin.py:112} INFO - [2020-11-05 13:56:27,669] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:56:27,672] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:27,699] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.306 seconds
[2020-11-05 13:56:39,646] {scheduler_job.py:155} INFO - Started process (PID=16168) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:39,651] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:56:39,652] {logging_mixin.py:112} INFO - [2020-11-05 13:56:39,651] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:41,190] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:56:41,198] {logging_mixin.py:112} INFO - [2020-11-05 13:56:41,190] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:56:41,198] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:41,240] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.594 seconds
[2020-11-05 13:56:52,849] {scheduler_job.py:155} INFO - Started process (PID=16227) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:52,852] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:56:52,853] {logging_mixin.py:112} INFO - [2020-11-05 13:56:52,853] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:54,004] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:56:54,007] {logging_mixin.py:112} INFO - [2020-11-05 13:56:54,004] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:56:54,007] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:56:54,028] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.180 seconds
[2020-11-05 13:57:06,048] {scheduler_job.py:155} INFO - Started process (PID=16290) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:06,052] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:57:06,053] {logging_mixin.py:112} INFO - [2020-11-05 13:57:06,052] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:07,219] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:57:07,224] {logging_mixin.py:112} INFO - [2020-11-05 13:57:07,219] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:57:07,224] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:07,249] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.201 seconds
[2020-11-05 13:57:19,253] {scheduler_job.py:155} INFO - Started process (PID=16349) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:19,257] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:57:19,259] {logging_mixin.py:112} INFO - [2020-11-05 13:57:19,258] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:20,493] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:57:20,497] {logging_mixin.py:112} INFO - [2020-11-05 13:57:20,494] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:57:20,497] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:20,522] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.270 seconds
[2020-11-05 13:57:32,540] {scheduler_job.py:155} INFO - Started process (PID=16406) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:32,545] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:57:32,546] {logging_mixin.py:112} INFO - [2020-11-05 13:57:32,545] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:33,949] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:57:33,952] {logging_mixin.py:112} INFO - [2020-11-05 13:57:33,950] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:57:33,953] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:33,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.435 seconds
[2020-11-05 13:57:45,804] {scheduler_job.py:155} INFO - Started process (PID=16466) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:45,809] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:57:45,809] {logging_mixin.py:112} INFO - [2020-11-05 13:57:45,809] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:47,350] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:57:47,352] {logging_mixin.py:112} INFO - [2020-11-05 13:57:47,350] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:57:47,353] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:47,383] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.580 seconds
[2020-11-05 13:57:59,043] {scheduler_job.py:155} INFO - Started process (PID=16520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:57:59,049] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:57:59,049] {logging_mixin.py:112} INFO - [2020-11-05 13:57:59,049] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:00,165] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:58:00,170] {logging_mixin.py:112} INFO - [2020-11-05 13:58:00,165] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:58:00,170] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:00,198] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.154 seconds
[2020-11-05 13:58:12,278] {scheduler_job.py:155} INFO - Started process (PID=16573) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:12,281] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:58:12,282] {logging_mixin.py:112} INFO - [2020-11-05 13:58:12,282] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:13,326] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:58:13,329] {logging_mixin.py:112} INFO - [2020-11-05 13:58:13,326] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:58:13,329] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:13,350] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.072 seconds
[2020-11-05 13:58:25,458] {scheduler_job.py:155} INFO - Started process (PID=16632) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:25,463] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:58:25,463] {logging_mixin.py:112} INFO - [2020-11-05 13:58:25,463] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:26,551] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:58:26,553] {logging_mixin.py:112} INFO - [2020-11-05 13:58:26,551] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:58:26,553] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:26,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.120 seconds
[2020-11-05 13:58:38,672] {scheduler_job.py:155} INFO - Started process (PID=16684) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:38,676] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:58:38,677] {logging_mixin.py:112} INFO - [2020-11-05 13:58:38,677] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:39,749] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:58:39,751] {logging_mixin.py:112} INFO - [2020-11-05 13:58:39,749] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:58:39,751] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:39,772] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.100 seconds
[2020-11-05 13:58:51,878] {scheduler_job.py:155} INFO - Started process (PID=16736) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:51,883] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:58:51,885] {logging_mixin.py:112} INFO - [2020-11-05 13:58:51,884] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:52,950] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:58:52,953] {logging_mixin.py:112} INFO - [2020-11-05 13:58:52,950] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:58:52,953] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:58:52,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.095 seconds
[2020-11-05 13:59:05,069] {scheduler_job.py:155} INFO - Started process (PID=16791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:05,073] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:59:05,074] {logging_mixin.py:112} INFO - [2020-11-05 13:59:05,074] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:06,149] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:59:06,151] {logging_mixin.py:112} INFO - [2020-11-05 13:59:06,149] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:59:06,151] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:06,173] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.104 seconds
[2020-11-05 13:59:18,260] {scheduler_job.py:155} INFO - Started process (PID=16843) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:18,264] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:59:18,265] {logging_mixin.py:112} INFO - [2020-11-05 13:59:18,265] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:19,387] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:59:19,393] {logging_mixin.py:112} INFO - [2020-11-05 13:59:19,388] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:59:19,393] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:19,425] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.165 seconds
[2020-11-05 13:59:31,450] {scheduler_job.py:155} INFO - Started process (PID=16896) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:31,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:59:31,453] {logging_mixin.py:112} INFO - [2020-11-05 13:59:31,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:32,577] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:59:32,579] {logging_mixin.py:112} INFO - [2020-11-05 13:59:32,577] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:59:32,580] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:32,602] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.153 seconds
[2020-11-05 13:59:44,633] {scheduler_job.py:155} INFO - Started process (PID=16948) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:44,637] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:59:44,638] {logging_mixin.py:112} INFO - [2020-11-05 13:59:44,638] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:45,902] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:59:45,905] {logging_mixin.py:112} INFO - [2020-11-05 13:59:45,902] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:59:45,906] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:45,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.304 seconds
[2020-11-05 13:59:57,815] {scheduler_job.py:155} INFO - Started process (PID=17002) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:57,821] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 13:59:57,822] {logging_mixin.py:112} INFO - [2020-11-05 13:59:57,821] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:59,111] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 13:59:59,113] {logging_mixin.py:112} INFO - [2020-11-05 13:59:59,111] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 13:59:59,114] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 13:59:59,144] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.329 seconds
[2020-11-05 14:00:11,014] {scheduler_job.py:155} INFO - Started process (PID=17081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:11,018] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:00:11,020] {logging_mixin.py:112} INFO - [2020-11-05 14:00:11,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:12,777] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:00:12,783] {logging_mixin.py:112} INFO - [2020-11-05 14:00:12,778] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:00:12,783] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:12,824] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.811 seconds
[2020-11-05 14:00:24,216] {scheduler_job.py:155} INFO - Started process (PID=17141) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:24,222] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:00:24,223] {logging_mixin.py:112} INFO - [2020-11-05 14:00:24,222] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:25,399] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:00:25,403] {logging_mixin.py:112} INFO - [2020-11-05 14:00:25,399] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:00:25,403] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:25,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.215 seconds
[2020-11-05 14:00:37,431] {scheduler_job.py:155} INFO - Started process (PID=17200) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:37,435] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:00:37,437] {logging_mixin.py:112} INFO - [2020-11-05 14:00:37,436] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:38,537] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:00:38,539] {logging_mixin.py:112} INFO - [2020-11-05 14:00:38,537] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:00:38,539] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:38,573] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.142 seconds
[2020-11-05 14:00:50,664] {scheduler_job.py:155} INFO - Started process (PID=17258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:50,672] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:00:50,687] {logging_mixin.py:112} INFO - [2020-11-05 14:00:50,674] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:52,189] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:00:52,193] {logging_mixin.py:112} INFO - [2020-11-05 14:00:52,190] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:00:52,193] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:00:52,231] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.568 seconds
[2020-11-05 14:01:03,870] {scheduler_job.py:155} INFO - Started process (PID=17317) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:03,873] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:01:03,873] {logging_mixin.py:112} INFO - [2020-11-05 14:01:03,873] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:05,100] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:01:05,104] {logging_mixin.py:112} INFO - [2020-11-05 14:01:05,101] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:01:05,105] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:05,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.279 seconds
[2020-11-05 14:01:17,126] {scheduler_job.py:155} INFO - Started process (PID=17375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:17,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:01:17,130] {logging_mixin.py:112} INFO - [2020-11-05 14:01:17,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:18,515] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:01:18,517] {logging_mixin.py:112} INFO - [2020-11-05 14:01:18,515] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:01:18,518] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:18,539] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.413 seconds
[2020-11-05 14:01:30,355] {scheduler_job.py:155} INFO - Started process (PID=17454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:30,359] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:01:30,360] {logging_mixin.py:112} INFO - [2020-11-05 14:01:30,359] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:31,601] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:01:31,607] {logging_mixin.py:112} INFO - [2020-11-05 14:01:31,602] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:01:31,607] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:31,637] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.282 seconds
[2020-11-05 14:01:43,521] {scheduler_job.py:155} INFO - Started process (PID=17512) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:43,526] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:01:43,527] {logging_mixin.py:112} INFO - [2020-11-05 14:01:43,527] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:44,623] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:01:44,626] {logging_mixin.py:112} INFO - [2020-11-05 14:01:44,623] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:01:44,626] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:44,653] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.133 seconds
[2020-11-05 14:01:56,736] {scheduler_job.py:155} INFO - Started process (PID=17572) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:56,740] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:01:56,740] {logging_mixin.py:112} INFO - [2020-11-05 14:01:56,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:58,105] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:01:58,108] {logging_mixin.py:112} INFO - [2020-11-05 14:01:58,106] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:01:58,108] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:01:58,134] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.398 seconds
[2020-11-05 14:02:09,952] {scheduler_job.py:155} INFO - Started process (PID=17635) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:09,957] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:02:09,958] {logging_mixin.py:112} INFO - [2020-11-05 14:02:09,958] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:11,123] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:02:11,125] {logging_mixin.py:112} INFO - [2020-11-05 14:02:11,123] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:02:11,125] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:11,150] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.198 seconds
[2020-11-05 14:02:23,163] {scheduler_job.py:155} INFO - Started process (PID=17696) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:23,172] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:02:23,174] {logging_mixin.py:112} INFO - [2020-11-05 14:02:23,172] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:24,292] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:02:24,294] {logging_mixin.py:112} INFO - [2020-11-05 14:02:24,292] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:02:24,294] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:24,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.154 seconds
[2020-11-05 14:02:36,383] {scheduler_job.py:155} INFO - Started process (PID=17755) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:36,386] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:02:36,386] {logging_mixin.py:112} INFO - [2020-11-05 14:02:36,386] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:37,579] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:02:37,583] {logging_mixin.py:112} INFO - [2020-11-05 14:02:37,580] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:02:37,583] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:37,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.230 seconds
[2020-11-05 14:02:49,578] {scheduler_job.py:155} INFO - Started process (PID=17819) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:49,581] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:02:49,582] {logging_mixin.py:112} INFO - [2020-11-05 14:02:49,581] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:51,003] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:02:51,006] {logging_mixin.py:112} INFO - [2020-11-05 14:02:51,003] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:02:51,007] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:02:51,032] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.454 seconds
[2020-11-05 14:03:02,775] {scheduler_job.py:155} INFO - Started process (PID=17882) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:02,780] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:03:02,780] {logging_mixin.py:112} INFO - [2020-11-05 14:03:02,780] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:03,929] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:03:03,931] {logging_mixin.py:112} INFO - [2020-11-05 14:03:03,929] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:03:03,933] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:03,954] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.178 seconds
[2020-11-05 14:03:16,015] {scheduler_job.py:155} INFO - Started process (PID=17942) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:16,021] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:03:16,022] {logging_mixin.py:112} INFO - [2020-11-05 14:03:16,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:17,666] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:03:17,668] {logging_mixin.py:112} INFO - [2020-11-05 14:03:17,666] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:03:17,668] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:17,694] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.680 seconds
[2020-11-05 14:03:29,236] {scheduler_job.py:155} INFO - Started process (PID=18007) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:29,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:03:29,242] {logging_mixin.py:112} INFO - [2020-11-05 14:03:29,242] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:30,360] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:03:30,363] {logging_mixin.py:112} INFO - [2020-11-05 14:03:30,360] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:03:30,363] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:30,393] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.158 seconds
[2020-11-05 14:03:42,424] {scheduler_job.py:155} INFO - Started process (PID=18067) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:42,427] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:03:42,428] {logging_mixin.py:112} INFO - [2020-11-05 14:03:42,427] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:43,528] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:03:43,531] {logging_mixin.py:112} INFO - [2020-11-05 14:03:43,528] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:03:43,531] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:43,565] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.141 seconds
[2020-11-05 14:03:55,624] {scheduler_job.py:155} INFO - Started process (PID=18130) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:55,630] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:03:55,630] {logging_mixin.py:112} INFO - [2020-11-05 14:03:55,630] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:57,066] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:03:57,068] {logging_mixin.py:112} INFO - [2020-11-05 14:03:57,066] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:03:57,068] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:03:57,094] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.470 seconds
[2020-11-05 14:04:08,883] {scheduler_job.py:155} INFO - Started process (PID=18194) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:08,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:04:08,888] {logging_mixin.py:112} INFO - [2020-11-05 14:04:08,888] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:09,985] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:04:09,988] {logging_mixin.py:112} INFO - [2020-11-05 14:04:09,986] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:04:09,988] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:10,031] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.148 seconds
[2020-11-05 14:04:22,036] {scheduler_job.py:155} INFO - Started process (PID=18254) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:22,040] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:04:22,041] {logging_mixin.py:112} INFO - [2020-11-05 14:04:22,041] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:23,170] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:04:23,172] {logging_mixin.py:112} INFO - [2020-11-05 14:04:23,170] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:04:23,173] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:23,201] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.165 seconds
[2020-11-05 14:04:35,252] {scheduler_job.py:155} INFO - Started process (PID=18316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:35,255] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:04:35,256] {logging_mixin.py:112} INFO - [2020-11-05 14:04:35,256] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:36,342] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:04:36,344] {logging_mixin.py:112} INFO - [2020-11-05 14:04:36,342] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:04:36,345] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:36,387] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.135 seconds
[2020-11-05 14:04:48,471] {scheduler_job.py:155} INFO - Started process (PID=18377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:48,482] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:04:48,483] {logging_mixin.py:112} INFO - [2020-11-05 14:04:48,482] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:49,690] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:04:49,694] {logging_mixin.py:112} INFO - [2020-11-05 14:04:49,690] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:04:49,695] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:04:49,720] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.249 seconds
[2020-11-05 14:05:01,652] {scheduler_job.py:155} INFO - Started process (PID=18440) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:01,656] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:05:01,657] {logging_mixin.py:112} INFO - [2020-11-05 14:05:01,657] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:02,911] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:05:02,913] {logging_mixin.py:112} INFO - [2020-11-05 14:05:02,911] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:05:02,913] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:02,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.282 seconds
[2020-11-05 14:05:14,852] {scheduler_job.py:155} INFO - Started process (PID=18502) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:14,854] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:05:14,855] {logging_mixin.py:112} INFO - [2020-11-05 14:05:14,855] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:16,136] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:05:16,139] {logging_mixin.py:112} INFO - [2020-11-05 14:05:16,136] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:05:16,140] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:16,164] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.312 seconds
[2020-11-05 14:05:28,027] {scheduler_job.py:155} INFO - Started process (PID=18568) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:28,032] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:05:28,033] {logging_mixin.py:112} INFO - [2020-11-05 14:05:28,033] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:29,179] {logging_mixin.py:112} INFO - {{ execution_date }}
[2020-11-05 14:05:29,181] {logging_mixin.py:112} INFO - [2020-11-05 14:05:29,179] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 114, in <module>
    dt = dt.strftime('%Y-%m-%d')
AttributeError: 'str' object has no attribute 'strftime'
[2020-11-05 14:05:29,182] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:29,208] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.181 seconds
[2020-11-05 14:05:41,256] {scheduler_job.py:155} INFO - Started process (PID=18627) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:41,264] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:05:41,264] {logging_mixin.py:112} INFO - [2020-11-05 14:05:41,264] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:41,494] {logging_mixin.py:112} INFO - [2020-11-05 14:05:41,493] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:05:41,494] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:41,520] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 14:05:54,468] {scheduler_job.py:155} INFO - Started process (PID=18689) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:54,487] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:05:54,488] {logging_mixin.py:112} INFO - [2020-11-05 14:05:54,488] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:54,692] {logging_mixin.py:112} INFO - [2020-11-05 14:05:54,690] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:05:54,692] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:05:54,718] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-05 14:06:07,687] {scheduler_job.py:155} INFO - Started process (PID=18753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:07,692] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:06:07,693] {logging_mixin.py:112} INFO - [2020-11-05 14:06:07,693] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:07,856] {logging_mixin.py:112} INFO - [2020-11-05 14:06:07,855] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:06:07,857] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:07,886] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.198 seconds
[2020-11-05 14:06:20,890] {scheduler_job.py:155} INFO - Started process (PID=18819) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:20,899] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:06:20,899] {logging_mixin.py:112} INFO - [2020-11-05 14:06:20,899] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:21,082] {logging_mixin.py:112} INFO - [2020-11-05 14:06:21,081] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:06:21,082] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:21,115] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.226 seconds
[2020-11-05 14:06:34,119] {scheduler_job.py:155} INFO - Started process (PID=18882) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:34,124] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:06:34,125] {logging_mixin.py:112} INFO - [2020-11-05 14:06:34,125] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:34,265] {logging_mixin.py:112} INFO - [2020-11-05 14:06:34,260] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:06:34,266] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:34,290] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-05 14:06:47,351] {scheduler_job.py:155} INFO - Started process (PID=18939) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:47,359] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:06:47,364] {logging_mixin.py:112} INFO - [2020-11-05 14:06:47,363] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:47,585] {logging_mixin.py:112} INFO - [2020-11-05 14:06:47,584] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:06:47,587] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:06:47,652] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.301 seconds
[2020-11-05 14:07:00,599] {scheduler_job.py:155} INFO - Started process (PID=18998) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:00,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:07:00,611] {logging_mixin.py:112} INFO - [2020-11-05 14:07:00,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:00,719] {logging_mixin.py:112} INFO - [2020-11-05 14:07:00,718] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:07:00,719] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:00,743] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-05 14:07:13,791] {scheduler_job.py:155} INFO - Started process (PID=19056) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:13,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:07:13,798] {logging_mixin.py:112} INFO - [2020-11-05 14:07:13,796] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:13,900] {logging_mixin.py:112} INFO - [2020-11-05 14:07:13,899] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate
ImportError: cannot import name 'simulate'
[2020-11-05 14:07:13,900] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:13,926] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:07:26,988] {scheduler_job.py:155} INFO - Started process (PID=19117) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:26,993] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:07:26,994] {logging_mixin.py:112} INFO - [2020-11-05 14:07:26,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:27,124] {logging_mixin.py:112} INFO - [2020-11-05 14:07:27,123] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:07:27,125] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:27,153] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-05 14:07:40,241] {scheduler_job.py:155} INFO - Started process (PID=19188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:40,254] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:07:40,255] {logging_mixin.py:112} INFO - [2020-11-05 14:07:40,255] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:40,388] {logging_mixin.py:112} INFO - [2020-11-05 14:07:40,387] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:07:40,388] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:40,413] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-05 14:07:53,467] {scheduler_job.py:155} INFO - Started process (PID=19247) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:53,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:07:53,471] {logging_mixin.py:112} INFO - [2020-11-05 14:07:53,470] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:53,574] {logging_mixin.py:112} INFO - [2020-11-05 14:07:53,573] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:07:53,575] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:07:53,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 14:08:06,713] {scheduler_job.py:155} INFO - Started process (PID=19308) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:06,719] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:08:06,720] {logging_mixin.py:112} INFO - [2020-11-05 14:08:06,719] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:06,840] {logging_mixin.py:112} INFO - [2020-11-05 14:08:06,839] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:08:06,840] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:06,866] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 14:08:19,946] {scheduler_job.py:155} INFO - Started process (PID=19377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:19,951] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:08:19,952] {logging_mixin.py:112} INFO - [2020-11-05 14:08:19,952] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:20,143] {logging_mixin.py:112} INFO - [2020-11-05 14:08:20,141] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:08:20,143] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:20,177] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-05 14:08:33,147] {scheduler_job.py:155} INFO - Started process (PID=19438) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:33,151] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:08:33,153] {logging_mixin.py:112} INFO - [2020-11-05 14:08:33,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:33,296] {logging_mixin.py:112} INFO - [2020-11-05 14:08:33,295] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:08:33,296] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:33,316] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 14:08:46,328] {scheduler_job.py:155} INFO - Started process (PID=19497) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:46,334] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:08:46,335] {logging_mixin.py:112} INFO - [2020-11-05 14:08:46,335] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:46,484] {logging_mixin.py:112} INFO - [2020-11-05 14:08:46,483] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:08:46,484] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:46,516] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 14:08:59,551] {scheduler_job.py:155} INFO - Started process (PID=19558) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:59,554] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:08:59,554] {logging_mixin.py:112} INFO - [2020-11-05 14:08:59,554] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:59,660] {logging_mixin.py:112} INFO - [2020-11-05 14:08:59,658] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:08:59,661] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:08:59,694] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 14:09:12,788] {scheduler_job.py:155} INFO - Started process (PID=19619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:12,802] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:09:12,803] {logging_mixin.py:112} INFO - [2020-11-05 14:09:12,803] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:12,946] {logging_mixin.py:112} INFO - [2020-11-05 14:09:12,944] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:09:12,946] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:12,972] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.184 seconds
[2020-11-05 14:09:25,999] {scheduler_job.py:155} INFO - Started process (PID=19680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:26,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:09:26,002] {logging_mixin.py:112} INFO - [2020-11-05 14:09:26,002] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:26,119] {logging_mixin.py:112} INFO - [2020-11-05 14:09:26,118] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:09:26,120] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:26,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 14:09:39,315] {scheduler_job.py:155} INFO - Started process (PID=19739) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:39,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:09:39,320] {logging_mixin.py:112} INFO - [2020-11-05 14:09:39,319] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:39,487] {logging_mixin.py:112} INFO - [2020-11-05 14:09:39,486] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:09:39,487] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:39,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 14:09:52,569] {scheduler_job.py:155} INFO - Started process (PID=19798) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:52,579] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:09:52,579] {logging_mixin.py:112} INFO - [2020-11-05 14:09:52,579] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:52,855] {logging_mixin.py:112} INFO - [2020-11-05 14:09:52,853] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:09:52,856] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:09:52,901] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.332 seconds
[2020-11-05 14:10:05,785] {scheduler_job.py:155} INFO - Started process (PID=19857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:05,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:10:05,795] {logging_mixin.py:112} INFO - [2020-11-05 14:10:05,794] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:05,919] {logging_mixin.py:112} INFO - [2020-11-05 14:10:05,918] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:10:05,919] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:05,962] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 14:10:18,998] {scheduler_job.py:155} INFO - Started process (PID=19918) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:19,004] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:10:19,005] {logging_mixin.py:112} INFO - [2020-11-05 14:10:19,005] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:19,138] {logging_mixin.py:112} INFO - [2020-11-05 14:10:19,137] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:10:19,138] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:19,161] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 14:10:32,222] {scheduler_job.py:155} INFO - Started process (PID=19978) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:32,226] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:10:32,227] {logging_mixin.py:112} INFO - [2020-11-05 14:10:32,227] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:32,382] {logging_mixin.py:112} INFO - [2020-11-05 14:10:32,381] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:10:32,384] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:32,412] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 14:10:45,454] {scheduler_job.py:155} INFO - Started process (PID=20036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:45,458] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:10:45,459] {logging_mixin.py:112} INFO - [2020-11-05 14:10:45,458] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:45,569] {logging_mixin.py:112} INFO - [2020-11-05 14:10:45,568] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:10:45,570] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:45,599] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 14:10:58,654] {scheduler_job.py:155} INFO - Started process (PID=20099) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:58,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:10:58,661] {logging_mixin.py:112} INFO - [2020-11-05 14:10:58,660] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:58,894] {logging_mixin.py:112} INFO - [2020-11-05 14:10:58,893] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:10:58,894] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:10:58,923] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.269 seconds
[2020-11-05 14:11:11,857] {scheduler_job.py:155} INFO - Started process (PID=20158) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:11,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:11:11,862] {logging_mixin.py:112} INFO - [2020-11-05 14:11:11,862] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:11,960] {logging_mixin.py:112} INFO - [2020-11-05 14:11:11,959] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:11:11,961] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:11,998] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 14:11:25,061] {scheduler_job.py:155} INFO - Started process (PID=20219) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:25,064] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:11:25,065] {logging_mixin.py:112} INFO - [2020-11-05 14:11:25,064] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:25,183] {logging_mixin.py:112} INFO - [2020-11-05 14:11:25,181] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:11:25,183] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:25,214] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-05 14:11:38,279] {scheduler_job.py:155} INFO - Started process (PID=20281) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:38,286] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:11:38,287] {logging_mixin.py:112} INFO - [2020-11-05 14:11:38,287] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:38,402] {logging_mixin.py:112} INFO - [2020-11-05 14:11:38,401] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:11:38,403] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:38,428] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 14:11:51,461] {scheduler_job.py:155} INFO - Started process (PID=20345) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:51,465] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:11:51,465] {logging_mixin.py:112} INFO - [2020-11-05 14:11:51,465] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:51,593] {logging_mixin.py:112} INFO - [2020-11-05 14:11:51,592] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:11:51,593] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:11:51,619] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 14:12:04,701] {scheduler_job.py:155} INFO - Started process (PID=20405) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:04,708] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:12:04,710] {logging_mixin.py:112} INFO - [2020-11-05 14:12:04,710] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:04,862] {logging_mixin.py:112} INFO - [2020-11-05 14:12:04,860] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:12:04,862] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:04,901] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.200 seconds
[2020-11-05 14:12:17,948] {scheduler_job.py:155} INFO - Started process (PID=20464) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:17,954] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:12:17,956] {logging_mixin.py:112} INFO - [2020-11-05 14:12:17,955] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:18,147] {logging_mixin.py:112} INFO - [2020-11-05 14:12:18,145] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:12:18,147] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:18,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.232 seconds
[2020-11-05 14:12:31,149] {scheduler_job.py:155} INFO - Started process (PID=20529) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:31,157] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:12:31,158] {logging_mixin.py:112} INFO - [2020-11-05 14:12:31,158] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:31,337] {logging_mixin.py:112} INFO - [2020-11-05 14:12:31,334] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:12:31,338] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:31,373] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 14:12:44,418] {scheduler_job.py:155} INFO - Started process (PID=20590) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:44,421] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:12:44,423] {logging_mixin.py:112} INFO - [2020-11-05 14:12:44,423] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:44,589] {logging_mixin.py:112} INFO - [2020-11-05 14:12:44,588] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:12:44,590] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:44,620] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 14:12:57,625] {scheduler_job.py:155} INFO - Started process (PID=20649) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:57,629] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:12:57,631] {logging_mixin.py:112} INFO - [2020-11-05 14:12:57,630] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:57,759] {logging_mixin.py:112} INFO - [2020-11-05 14:12:57,758] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:12:57,759] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:12:57,786] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 14:13:10,874] {scheduler_job.py:155} INFO - Started process (PID=20705) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:10,878] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:13:10,878] {logging_mixin.py:112} INFO - [2020-11-05 14:13:10,878] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:10,979] {logging_mixin.py:112} INFO - [2020-11-05 14:13:10,979] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:13:10,980] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:11,005] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 14:13:24,111] {scheduler_job.py:155} INFO - Started process (PID=20765) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:24,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:13:24,118] {logging_mixin.py:112} INFO - [2020-11-05 14:13:24,118] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:24,284] {logging_mixin.py:112} INFO - [2020-11-05 14:13:24,282] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:13:24,285] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:24,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 14:13:37,330] {scheduler_job.py:155} INFO - Started process (PID=20824) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:37,334] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:13:37,335] {logging_mixin.py:112} INFO - [2020-11-05 14:13:37,335] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:37,475] {logging_mixin.py:112} INFO - [2020-11-05 14:13:37,474] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:13:37,475] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:37,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-05 14:13:50,572] {scheduler_job.py:155} INFO - Started process (PID=20883) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:50,604] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:13:50,605] {logging_mixin.py:112} INFO - [2020-11-05 14:13:50,604] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:50,837] {logging_mixin.py:112} INFO - [2020-11-05 14:13:50,835] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:13:50,837] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:13:50,859] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.287 seconds
[2020-11-05 14:14:03,778] {scheduler_job.py:155} INFO - Started process (PID=20948) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:03,782] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:14:03,782] {logging_mixin.py:112} INFO - [2020-11-05 14:14:03,782] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:03,915] {logging_mixin.py:112} INFO - [2020-11-05 14:14:03,914] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:14:03,915] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:03,945] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-05 14:14:16,997] {scheduler_job.py:155} INFO - Started process (PID=21008) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:17,001] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:14:17,002] {logging_mixin.py:112} INFO - [2020-11-05 14:14:17,002] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:17,136] {logging_mixin.py:112} INFO - [2020-11-05 14:14:17,135] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:14:17,137] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:17,160] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 14:14:30,218] {scheduler_job.py:155} INFO - Started process (PID=21070) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:30,222] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:14:30,223] {logging_mixin.py:112} INFO - [2020-11-05 14:14:30,222] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:30,371] {logging_mixin.py:112} INFO - [2020-11-05 14:14:30,370] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:14:30,371] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:30,397] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 14:14:43,431] {scheduler_job.py:155} INFO - Started process (PID=21135) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:43,434] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:14:43,445] {logging_mixin.py:112} INFO - [2020-11-05 14:14:43,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:43,579] {logging_mixin.py:112} INFO - [2020-11-05 14:14:43,578] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:14:43,580] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:43,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 14:14:56,600] {scheduler_job.py:155} INFO - Started process (PID=21203) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:56,622] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:14:56,623] {logging_mixin.py:112} INFO - [2020-11-05 14:14:56,623] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:56,755] {logging_mixin.py:112} INFO - [2020-11-05 14:14:56,754] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:14:56,756] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:14:56,785] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-05 14:15:09,806] {scheduler_job.py:155} INFO - Started process (PID=21269) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:09,809] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:15:09,810] {logging_mixin.py:112} INFO - [2020-11-05 14:15:09,810] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:09,928] {logging_mixin.py:112} INFO - [2020-11-05 14:15:09,926] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:15:09,929] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:09,957] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 14:15:23,044] {scheduler_job.py:155} INFO - Started process (PID=21330) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:23,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:15:23,078] {logging_mixin.py:112} INFO - [2020-11-05 14:15:23,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:23,289] {logging_mixin.py:112} INFO - [2020-11-05 14:15:23,277] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:15:23,290] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:23,349] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.305 seconds
[2020-11-05 14:15:36,221] {scheduler_job.py:155} INFO - Started process (PID=21389) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:36,225] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:15:36,225] {logging_mixin.py:112} INFO - [2020-11-05 14:15:36,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:36,378] {logging_mixin.py:112} INFO - [2020-11-05 14:15:36,376] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:15:36,378] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:36,409] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 14:15:49,520] {scheduler_job.py:155} INFO - Started process (PID=21445) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:49,523] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:15:49,523] {logging_mixin.py:112} INFO - [2020-11-05 14:15:49,523] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:49,615] {logging_mixin.py:112} INFO - [2020-11-05 14:15:49,614] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:15:49,616] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:15:49,656] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 14:16:02,721] {scheduler_job.py:155} INFO - Started process (PID=21506) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:02,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:16:02,724] {logging_mixin.py:112} INFO - [2020-11-05 14:16:02,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:02,830] {logging_mixin.py:112} INFO - [2020-11-05 14:16:02,828] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:16:02,830] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:02,854] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 14:16:15,964] {scheduler_job.py:155} INFO - Started process (PID=21568) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:15,969] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:16:15,970] {logging_mixin.py:112} INFO - [2020-11-05 14:16:15,969] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:16,129] {logging_mixin.py:112} INFO - [2020-11-05 14:16:16,121] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:16:16,130] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:16,171] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.207 seconds
[2020-11-05 14:16:29,159] {scheduler_job.py:155} INFO - Started process (PID=21628) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:29,163] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:16:29,164] {logging_mixin.py:112} INFO - [2020-11-05 14:16:29,164] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:29,298] {logging_mixin.py:112} INFO - [2020-11-05 14:16:29,297] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:16:29,298] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:29,335] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 14:16:42,417] {scheduler_job.py:155} INFO - Started process (PID=21690) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:42,421] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:16:42,421] {logging_mixin.py:112} INFO - [2020-11-05 14:16:42,421] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:42,542] {logging_mixin.py:112} INFO - [2020-11-05 14:16:42,541] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:16:42,542] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:42,573] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 14:16:55,677] {scheduler_job.py:155} INFO - Started process (PID=21750) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:55,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:16:55,684] {logging_mixin.py:112} INFO - [2020-11-05 14:16:55,684] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:55,772] {logging_mixin.py:112} INFO - [2020-11-05 14:16:55,771] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:16:55,772] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:16:55,797] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.120 seconds
[2020-11-05 14:17:08,875] {scheduler_job.py:155} INFO - Started process (PID=21819) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:08,880] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:17:08,881] {logging_mixin.py:112} INFO - [2020-11-05 14:17:08,881] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:08,999] {logging_mixin.py:112} INFO - [2020-11-05 14:17:08,998] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:17:09,000] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:09,037] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 14:17:22,109] {scheduler_job.py:155} INFO - Started process (PID=21878) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:22,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:17:22,121] {logging_mixin.py:112} INFO - [2020-11-05 14:17:22,118] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:22,369] {logging_mixin.py:112} INFO - [2020-11-05 14:17:22,368] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:17:22,370] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:22,403] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.294 seconds
[2020-11-05 14:17:35,360] {scheduler_job.py:155} INFO - Started process (PID=21941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:35,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:17:35,365] {logging_mixin.py:112} INFO - [2020-11-05 14:17:35,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:35,535] {logging_mixin.py:112} INFO - [2020-11-05 14:17:35,533] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:17:35,535] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:35,585] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 14:17:48,674] {scheduler_job.py:155} INFO - Started process (PID=21999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:48,680] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:17:48,681] {logging_mixin.py:112} INFO - [2020-11-05 14:17:48,681] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:48,823] {logging_mixin.py:112} INFO - [2020-11-05 14:17:48,821] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:17:48,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:17:48,853] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 14:18:01,919] {scheduler_job.py:155} INFO - Started process (PID=22059) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:01,922] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:18:01,923] {logging_mixin.py:112} INFO - [2020-11-05 14:18:01,922] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:02,018] {logging_mixin.py:112} INFO - [2020-11-05 14:18:02,017] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:18:02,018] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:02,045] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-05 14:18:15,159] {scheduler_job.py:155} INFO - Started process (PID=22121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:15,162] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:18:15,163] {logging_mixin.py:112} INFO - [2020-11-05 14:18:15,163] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:15,279] {logging_mixin.py:112} INFO - [2020-11-05 14:18:15,278] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:18:15,280] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:15,305] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 14:18:28,438] {scheduler_job.py:155} INFO - Started process (PID=22185) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:28,444] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:18:28,445] {logging_mixin.py:112} INFO - [2020-11-05 14:18:28,445] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:28,604] {logging_mixin.py:112} INFO - [2020-11-05 14:18:28,602] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:18:28,605] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:28,629] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 14:18:41,737] {scheduler_job.py:155} INFO - Started process (PID=22247) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:41,741] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:18:41,742] {logging_mixin.py:112} INFO - [2020-11-05 14:18:41,742] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:41,862] {logging_mixin.py:112} INFO - [2020-11-05 14:18:41,861] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:18:41,863] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:41,888] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 14:18:54,990] {scheduler_job.py:155} INFO - Started process (PID=22310) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:54,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:18:54,996] {logging_mixin.py:112} INFO - [2020-11-05 14:18:54,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:55,155] {logging_mixin.py:112} INFO - [2020-11-05 14:18:55,154] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:18:55,155] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:18:55,203] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.213 seconds
[2020-11-05 14:19:08,445] {scheduler_job.py:155} INFO - Started process (PID=22375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:08,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:19:08,452] {logging_mixin.py:112} INFO - [2020-11-05 14:19:08,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:08,596] {logging_mixin.py:112} INFO - [2020-11-05 14:19:08,594] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:19:08,597] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:08,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 14:19:21,783] {scheduler_job.py:155} INFO - Started process (PID=22429) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:21,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:19:21,790] {logging_mixin.py:112} INFO - [2020-11-05 14:19:21,789] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:21,955] {logging_mixin.py:112} INFO - [2020-11-05 14:19:21,953] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:19:21,955] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:22,007] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.225 seconds
[2020-11-05 14:19:35,027] {scheduler_job.py:155} INFO - Started process (PID=22485) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:35,032] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:19:35,032] {logging_mixin.py:112} INFO - [2020-11-05 14:19:35,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:35,209] {logging_mixin.py:112} INFO - [2020-11-05 14:19:35,208] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:19:35,209] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:35,246] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.220 seconds
[2020-11-05 14:19:48,311] {scheduler_job.py:155} INFO - Started process (PID=22541) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:48,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:19:48,328] {logging_mixin.py:112} INFO - [2020-11-05 14:19:48,328] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:48,497] {logging_mixin.py:112} INFO - [2020-11-05 14:19:48,495] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:19:48,503] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:19:48,554] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.243 seconds
[2020-11-05 14:20:01,569] {scheduler_job.py:155} INFO - Started process (PID=22596) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:01,574] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:20:01,574] {logging_mixin.py:112} INFO - [2020-11-05 14:20:01,574] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:01,727] {logging_mixin.py:112} INFO - [2020-11-05 14:20:01,726] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:20:01,728] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:01,763] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.193 seconds
[2020-11-05 14:20:14,800] {scheduler_job.py:155} INFO - Started process (PID=22658) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:14,803] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:20:14,804] {logging_mixin.py:112} INFO - [2020-11-05 14:20:14,804] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:14,906] {logging_mixin.py:112} INFO - [2020-11-05 14:20:14,905] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:20:14,906] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:14,941] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 14:20:28,042] {scheduler_job.py:155} INFO - Started process (PID=22720) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:28,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:20:28,047] {logging_mixin.py:112} INFO - [2020-11-05 14:20:28,046] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:28,160] {logging_mixin.py:112} INFO - [2020-11-05 14:20:28,159] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:20:28,160] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:28,182] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 14:20:41,241] {scheduler_job.py:155} INFO - Started process (PID=22779) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:41,244] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:20:41,245] {logging_mixin.py:112} INFO - [2020-11-05 14:20:41,245] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:41,365] {logging_mixin.py:112} INFO - [2020-11-05 14:20:41,363] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:20:41,365] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:41,393] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 14:20:54,518] {scheduler_job.py:155} INFO - Started process (PID=22838) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:54,522] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:20:54,523] {logging_mixin.py:112} INFO - [2020-11-05 14:20:54,523] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:54,642] {logging_mixin.py:112} INFO - [2020-11-05 14:20:54,640] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:20:54,642] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:20:54,675] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 14:21:07,712] {scheduler_job.py:155} INFO - Started process (PID=22898) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:07,718] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:21:07,719] {logging_mixin.py:112} INFO - [2020-11-05 14:21:07,719] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:07,961] {logging_mixin.py:112} INFO - [2020-11-05 14:21:07,960] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:21:07,970] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:08,018] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.307 seconds
[2020-11-05 14:21:20,990] {scheduler_job.py:155} INFO - Started process (PID=22957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:20,993] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:21:20,994] {logging_mixin.py:112} INFO - [2020-11-05 14:21:20,994] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:21,145] {logging_mixin.py:112} INFO - [2020-11-05 14:21:21,144] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:21:21,145] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:21,173] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-05 14:21:34,193] {scheduler_job.py:155} INFO - Started process (PID=23020) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:34,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:21:34,198] {logging_mixin.py:112} INFO - [2020-11-05 14:21:34,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:34,393] {logging_mixin.py:112} INFO - [2020-11-05 14:21:34,392] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:21:34,394] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:34,429] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-05 14:21:47,461] {scheduler_job.py:155} INFO - Started process (PID=23079) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:47,465] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:21:47,465] {logging_mixin.py:112} INFO - [2020-11-05 14:21:47,465] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:47,583] {logging_mixin.py:112} INFO - [2020-11-05 14:21:47,582] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:21:47,583] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:21:47,607] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 14:22:00,698] {scheduler_job.py:155} INFO - Started process (PID=23133) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:00,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:22:00,703] {logging_mixin.py:112} INFO - [2020-11-05 14:22:00,703] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:00,904] {logging_mixin.py:112} INFO - [2020-11-05 14:22:00,902] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:22:00,905] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:00,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 14:22:13,956] {scheduler_job.py:155} INFO - Started process (PID=23187) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:13,974] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:22:13,975] {logging_mixin.py:112} INFO - [2020-11-05 14:22:13,975] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:14,147] {logging_mixin.py:112} INFO - [2020-11-05 14:22:14,146] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:22:14,148] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:14,189] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.234 seconds
[2020-11-05 14:22:27,288] {scheduler_job.py:155} INFO - Started process (PID=23247) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:27,293] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:22:27,294] {logging_mixin.py:112} INFO - [2020-11-05 14:22:27,294] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:27,398] {logging_mixin.py:112} INFO - [2020-11-05 14:22:27,396] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:22:27,398] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:27,428] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 14:22:40,523] {scheduler_job.py:155} INFO - Started process (PID=23307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:40,529] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:22:40,530] {logging_mixin.py:112} INFO - [2020-11-05 14:22:40,530] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:40,673] {logging_mixin.py:112} INFO - [2020-11-05 14:22:40,671] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:22:40,674] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:40,704] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-05 14:22:53,828] {scheduler_job.py:155} INFO - Started process (PID=23368) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:53,838] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:22:53,840] {logging_mixin.py:112} INFO - [2020-11-05 14:22:53,840] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:53,981] {logging_mixin.py:112} INFO - [2020-11-05 14:22:53,980] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:22:53,981] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:22:54,005] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 14:23:07,024] {scheduler_job.py:155} INFO - Started process (PID=23433) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:07,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:23:07,029] {logging_mixin.py:112} INFO - [2020-11-05 14:23:07,029] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:07,140] {logging_mixin.py:112} INFO - [2020-11-05 14:23:07,138] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:23:07,140] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:07,164] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 14:23:20,258] {scheduler_job.py:155} INFO - Started process (PID=23503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:20,262] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:23:20,263] {logging_mixin.py:112} INFO - [2020-11-05 14:23:20,263] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:20,376] {logging_mixin.py:112} INFO - [2020-11-05 14:23:20,375] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:23:20,377] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:20,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 14:23:33,443] {scheduler_job.py:155} INFO - Started process (PID=23566) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:33,447] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:23:33,447] {logging_mixin.py:112} INFO - [2020-11-05 14:23:33,447] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:33,556] {logging_mixin.py:112} INFO - [2020-11-05 14:23:33,555] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:23:33,556] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:33,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:23:46,729] {scheduler_job.py:155} INFO - Started process (PID=23627) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:46,733] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:23:46,734] {logging_mixin.py:112} INFO - [2020-11-05 14:23:46,734] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:47,024] {logging_mixin.py:112} INFO - [2020-11-05 14:23:47,020] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:23:47,024] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:23:47,061] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.331 seconds
[2020-11-05 14:24:00,163] {scheduler_job.py:155} INFO - Started process (PID=23682) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:00,171] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:24:00,171] {logging_mixin.py:112} INFO - [2020-11-05 14:24:00,171] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:00,316] {logging_mixin.py:112} INFO - [2020-11-05 14:24:00,315] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:24:00,317] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:00,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.201 seconds
[2020-11-05 14:24:13,401] {scheduler_job.py:155} INFO - Started process (PID=23744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:13,404] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:24:13,404] {logging_mixin.py:112} INFO - [2020-11-05 14:24:13,404] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:13,504] {logging_mixin.py:112} INFO - [2020-11-05 14:24:13,503] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:24:13,505] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:13,532] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 14:24:26,625] {scheduler_job.py:155} INFO - Started process (PID=23806) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:26,630] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:24:26,630] {logging_mixin.py:112} INFO - [2020-11-05 14:24:26,630] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:26,742] {logging_mixin.py:112} INFO - [2020-11-05 14:24:26,741] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:24:26,742] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:26,764] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-05 14:24:39,861] {scheduler_job.py:155} INFO - Started process (PID=23868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:39,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:24:39,866] {logging_mixin.py:112} INFO - [2020-11-05 14:24:39,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:39,955] {logging_mixin.py:112} INFO - [2020-11-05 14:24:39,954] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:24:39,956] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:39,981] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.120 seconds
[2020-11-05 14:24:53,108] {scheduler_job.py:155} INFO - Started process (PID=23932) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:53,112] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:24:53,112] {logging_mixin.py:112} INFO - [2020-11-05 14:24:53,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:53,204] {logging_mixin.py:112} INFO - [2020-11-05 14:24:53,203] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:24:53,204] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:24:53,225] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.118 seconds
[2020-11-05 14:25:06,340] {scheduler_job.py:155} INFO - Started process (PID=23997) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:06,344] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:25:06,345] {logging_mixin.py:112} INFO - [2020-11-05 14:25:06,345] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:06,478] {logging_mixin.py:112} INFO - [2020-11-05 14:25:06,477] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:25:06,479] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:06,515] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-05 14:25:19,627] {scheduler_job.py:155} INFO - Started process (PID=24056) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:19,632] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:25:19,632] {logging_mixin.py:112} INFO - [2020-11-05 14:25:19,632] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:19,739] {logging_mixin.py:112} INFO - [2020-11-05 14:25:19,738] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:25:19,740] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:19,767] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 14:25:32,870] {scheduler_job.py:155} INFO - Started process (PID=24120) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:32,875] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:25:32,876] {logging_mixin.py:112} INFO - [2020-11-05 14:25:32,875] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:33,041] {logging_mixin.py:112} INFO - [2020-11-05 14:25:33,040] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:25:33,041] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:33,068] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-05 14:25:46,099] {scheduler_job.py:155} INFO - Started process (PID=24180) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:46,104] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:25:46,104] {logging_mixin.py:112} INFO - [2020-11-05 14:25:46,104] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:46,273] {logging_mixin.py:112} INFO - [2020-11-05 14:25:46,272] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:25:46,274] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:46,318] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.218 seconds
[2020-11-05 14:25:59,311] {scheduler_job.py:155} INFO - Started process (PID=24240) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:59,316] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:25:59,317] {logging_mixin.py:112} INFO - [2020-11-05 14:25:59,317] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:59,439] {logging_mixin.py:112} INFO - [2020-11-05 14:25:59,438] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:25:59,440] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:25:59,465] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 14:26:12,592] {scheduler_job.py:155} INFO - Started process (PID=24296) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:12,597] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:26:12,598] {logging_mixin.py:112} INFO - [2020-11-05 14:26:12,598] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:12,707] {logging_mixin.py:112} INFO - [2020-11-05 14:26:12,706] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:26:12,707] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:12,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:26:25,818] {scheduler_job.py:155} INFO - Started process (PID=24361) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:25,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:26:25,828] {logging_mixin.py:112} INFO - [2020-11-05 14:26:25,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:25,944] {logging_mixin.py:112} INFO - [2020-11-05 14:26:25,943] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:26:25,944] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:25,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 14:26:39,060] {scheduler_job.py:155} INFO - Started process (PID=24422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:39,069] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:26:39,070] {logging_mixin.py:112} INFO - [2020-11-05 14:26:39,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:39,213] {logging_mixin.py:112} INFO - [2020-11-05 14:26:39,212] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:26:39,213] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:39,247] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.187 seconds
[2020-11-05 14:26:52,297] {scheduler_job.py:155} INFO - Started process (PID=24485) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:52,301] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:26:52,306] {logging_mixin.py:112} INFO - [2020-11-05 14:26:52,305] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:52,478] {logging_mixin.py:112} INFO - [2020-11-05 14:26:52,476] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:26:52,478] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:26:52,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.232 seconds
[2020-11-05 14:27:05,529] {scheduler_job.py:155} INFO - Started process (PID=24543) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:05,533] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:27:05,535] {logging_mixin.py:112} INFO - [2020-11-05 14:27:05,535] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:05,781] {logging_mixin.py:112} INFO - [2020-11-05 14:27:05,780] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:27:05,782] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:05,821] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.293 seconds
[2020-11-05 14:27:18,698] {scheduler_job.py:155} INFO - Started process (PID=24602) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:18,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:27:18,713] {logging_mixin.py:112} INFO - [2020-11-05 14:27:18,703] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:18,842] {logging_mixin.py:112} INFO - [2020-11-05 14:27:18,841] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:27:18,842] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:18,866] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-05 14:27:32,013] {scheduler_job.py:155} INFO - Started process (PID=24664) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:32,017] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:27:32,020] {logging_mixin.py:112} INFO - [2020-11-05 14:27:32,019] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:32,122] {logging_mixin.py:112} INFO - [2020-11-05 14:27:32,121] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:27:32,122] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:32,156] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 14:27:45,287] {scheduler_job.py:155} INFO - Started process (PID=24723) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:45,291] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:27:45,292] {logging_mixin.py:112} INFO - [2020-11-05 14:27:45,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:45,411] {logging_mixin.py:112} INFO - [2020-11-05 14:27:45,411] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:27:45,412] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:45,445] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 14:27:58,492] {scheduler_job.py:155} INFO - Started process (PID=24783) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:58,495] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:27:58,495] {logging_mixin.py:112} INFO - [2020-11-05 14:27:58,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:58,641] {logging_mixin.py:112} INFO - [2020-11-05 14:27:58,640] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:27:58,642] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:27:58,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-05 14:28:11,739] {scheduler_job.py:155} INFO - Started process (PID=24849) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:11,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:28:11,742] {logging_mixin.py:112} INFO - [2020-11-05 14:28:11,742] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:11,853] {logging_mixin.py:112} INFO - [2020-11-05 14:28:11,852] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:28:11,853] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:11,888] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 14:28:24,985] {scheduler_job.py:155} INFO - Started process (PID=24912) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:24,989] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:28:24,990] {logging_mixin.py:112} INFO - [2020-11-05 14:28:24,990] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:25,100] {logging_mixin.py:112} INFO - [2020-11-05 14:28:25,099] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:28:25,100] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:25,122] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:28:38,237] {scheduler_job.py:155} INFO - Started process (PID=24973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:38,245] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:28:38,247] {logging_mixin.py:112} INFO - [2020-11-05 14:28:38,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:38,391] {logging_mixin.py:112} INFO - [2020-11-05 14:28:38,389] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:28:38,391] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:38,418] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 14:28:51,426] {scheduler_job.py:155} INFO - Started process (PID=25038) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:51,432] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:28:51,433] {logging_mixin.py:112} INFO - [2020-11-05 14:28:51,432] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:51,567] {logging_mixin.py:112} INFO - [2020-11-05 14:28:51,566] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:28:51,567] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:28:51,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-05 14:29:04,645] {scheduler_job.py:155} INFO - Started process (PID=25104) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:04,648] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:29:04,649] {logging_mixin.py:112} INFO - [2020-11-05 14:29:04,649] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:04,753] {logging_mixin.py:112} INFO - [2020-11-05 14:29:04,752] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:29:04,753] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:04,776] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-05 14:29:17,882] {scheduler_job.py:155} INFO - Started process (PID=25161) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:17,895] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:29:17,896] {logging_mixin.py:112} INFO - [2020-11-05 14:29:17,896] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:18,082] {logging_mixin.py:112} INFO - [2020-11-05 14:29:18,081] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:29:18,083] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:18,128] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.246 seconds
[2020-11-05 14:29:31,103] {scheduler_job.py:155} INFO - Started process (PID=25227) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:31,106] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:29:31,106] {logging_mixin.py:112} INFO - [2020-11-05 14:29:31,106] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:31,200] {logging_mixin.py:112} INFO - [2020-11-05 14:29:31,199] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:29:31,200] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:31,228] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-05 14:29:44,306] {scheduler_job.py:155} INFO - Started process (PID=25284) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:44,310] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:29:44,311] {logging_mixin.py:112} INFO - [2020-11-05 14:29:44,310] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:44,405] {logging_mixin.py:112} INFO - [2020-11-05 14:29:44,404] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:29:44,406] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:44,429] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.123 seconds
[2020-11-05 14:29:57,570] {scheduler_job.py:155} INFO - Started process (PID=25346) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:57,575] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:29:57,576] {logging_mixin.py:112} INFO - [2020-11-05 14:29:57,575] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:57,825] {logging_mixin.py:112} INFO - [2020-11-05 14:29:57,822] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:29:57,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:29:57,898] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.327 seconds
[2020-11-05 14:30:10,820] {scheduler_job.py:155} INFO - Started process (PID=25420) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:10,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:30:10,825] {logging_mixin.py:112} INFO - [2020-11-05 14:30:10,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:10,934] {logging_mixin.py:112} INFO - [2020-11-05 14:30:10,933] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:30:10,937] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:10,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 14:30:24,023] {scheduler_job.py:155} INFO - Started process (PID=25485) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:24,026] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:30:24,027] {logging_mixin.py:112} INFO - [2020-11-05 14:30:24,026] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:24,191] {logging_mixin.py:112} INFO - [2020-11-05 14:30:24,190] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:30:24,191] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:24,229] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.206 seconds
[2020-11-05 14:30:37,310] {scheduler_job.py:155} INFO - Started process (PID=25550) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:37,313] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:30:37,314] {logging_mixin.py:112} INFO - [2020-11-05 14:30:37,313] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:37,421] {logging_mixin.py:112} INFO - [2020-11-05 14:30:37,420] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:30:37,421] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:37,445] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 14:30:50,556] {scheduler_job.py:155} INFO - Started process (PID=25610) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:50,562] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:30:50,563] {logging_mixin.py:112} INFO - [2020-11-05 14:30:50,562] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:50,730] {logging_mixin.py:112} INFO - [2020-11-05 14:30:50,727] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:30:50,730] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:30:50,751] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.196 seconds
[2020-11-05 14:31:03,802] {scheduler_job.py:155} INFO - Started process (PID=25666) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:03,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:31:03,816] {logging_mixin.py:112} INFO - [2020-11-05 14:31:03,815] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:03,986] {logging_mixin.py:112} INFO - [2020-11-05 14:31:03,983] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:31:03,988] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:04,033] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-05 14:31:17,099] {scheduler_job.py:155} INFO - Started process (PID=25725) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:17,103] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:31:17,104] {logging_mixin.py:112} INFO - [2020-11-05 14:31:17,103] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:17,233] {logging_mixin.py:112} INFO - [2020-11-05 14:31:17,232] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:31:17,234] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:17,264] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-05 14:31:30,316] {scheduler_job.py:155} INFO - Started process (PID=25786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:30,320] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:31:30,321] {logging_mixin.py:112} INFO - [2020-11-05 14:31:30,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:30,450] {logging_mixin.py:112} INFO - [2020-11-05 14:31:30,449] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:31:30,451] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:30,479] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 14:31:43,587] {scheduler_job.py:155} INFO - Started process (PID=25846) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:43,592] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:31:43,593] {logging_mixin.py:112} INFO - [2020-11-05 14:31:43,593] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:43,701] {logging_mixin.py:112} INFO - [2020-11-05 14:31:43,700] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:31:43,701] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:43,739] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 14:31:56,810] {scheduler_job.py:155} INFO - Started process (PID=25907) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:56,814] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:31:56,814] {logging_mixin.py:112} INFO - [2020-11-05 14:31:56,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:56,928] {logging_mixin.py:112} INFO - [2020-11-05 14:31:56,926] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:31:56,929] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:31:56,958] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 14:32:10,047] {scheduler_job.py:155} INFO - Started process (PID=25971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:10,050] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:32:10,051] {logging_mixin.py:112} INFO - [2020-11-05 14:32:10,051] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:10,156] {logging_mixin.py:112} INFO - [2020-11-05 14:32:10,155] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:32:10,157] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:10,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 14:32:23,267] {scheduler_job.py:155} INFO - Started process (PID=26032) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:23,272] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:32:23,275] {logging_mixin.py:112} INFO - [2020-11-05 14:32:23,273] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:23,398] {logging_mixin.py:112} INFO - [2020-11-05 14:32:23,397] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:32:23,398] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:23,424] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 14:32:36,547] {scheduler_job.py:155} INFO - Started process (PID=26095) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:36,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:32:36,553] {logging_mixin.py:112} INFO - [2020-11-05 14:32:36,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:36,667] {logging_mixin.py:112} INFO - [2020-11-05 14:32:36,666] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:32:36,668] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:36,693] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 14:32:49,739] {scheduler_job.py:155} INFO - Started process (PID=26154) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:49,749] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:32:49,750] {logging_mixin.py:112} INFO - [2020-11-05 14:32:49,750] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:50,016] {logging_mixin.py:112} INFO - [2020-11-05 14:32:50,015] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:32:50,016] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:32:50,053] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.315 seconds
[2020-11-05 14:33:03,014] {scheduler_job.py:155} INFO - Started process (PID=26212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:03,019] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:33:03,020] {logging_mixin.py:112} INFO - [2020-11-05 14:33:03,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:03,151] {logging_mixin.py:112} INFO - [2020-11-05 14:33:03,151] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:33:03,152] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:03,184] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 14:33:16,274] {scheduler_job.py:155} INFO - Started process (PID=26272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:16,288] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:33:16,292] {logging_mixin.py:112} INFO - [2020-11-05 14:33:16,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:16,507] {logging_mixin.py:112} INFO - [2020-11-05 14:33:16,506] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:33:16,507] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:16,543] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-05 14:33:29,474] {scheduler_job.py:155} INFO - Started process (PID=26341) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:29,478] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:33:29,478] {logging_mixin.py:112} INFO - [2020-11-05 14:33:29,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:29,604] {logging_mixin.py:112} INFO - [2020-11-05 14:33:29,603] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:33:29,604] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:29,624] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 14:33:42,724] {scheduler_job.py:155} INFO - Started process (PID=26401) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:42,731] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:33:42,731] {logging_mixin.py:112} INFO - [2020-11-05 14:33:42,731] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:42,869] {logging_mixin.py:112} INFO - [2020-11-05 14:33:42,864] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:33:42,870] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:42,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 14:33:55,932] {scheduler_job.py:155} INFO - Started process (PID=26466) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:55,938] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:33:55,939] {logging_mixin.py:112} INFO - [2020-11-05 14:33:55,938] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:56,113] {logging_mixin.py:112} INFO - [2020-11-05 14:33:56,111] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:33:56,113] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:33:56,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.217 seconds
[2020-11-05 14:34:09,184] {scheduler_job.py:155} INFO - Started process (PID=26527) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:09,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:34:09,200] {logging_mixin.py:112} INFO - [2020-11-05 14:34:09,199] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:09,396] {logging_mixin.py:112} INFO - [2020-11-05 14:34:09,395] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:34:09,397] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:09,441] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-05 14:34:22,435] {scheduler_job.py:155} INFO - Started process (PID=26582) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:22,439] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:34:22,440] {logging_mixin.py:112} INFO - [2020-11-05 14:34:22,440] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:22,573] {logging_mixin.py:112} INFO - [2020-11-05 14:34:22,572] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:34:22,573] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:22,612] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 14:34:35,678] {scheduler_job.py:155} INFO - Started process (PID=26641) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:35,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:34:35,684] {logging_mixin.py:112} INFO - [2020-11-05 14:34:35,683] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:35,780] {logging_mixin.py:112} INFO - [2020-11-05 14:34:35,779] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:34:35,781] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:35,820] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 14:34:48,918] {scheduler_job.py:155} INFO - Started process (PID=26699) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:48,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:34:48,926] {logging_mixin.py:112} INFO - [2020-11-05 14:34:48,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:49,132] {logging_mixin.py:112} INFO - [2020-11-05 14:34:49,131] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:34:49,133] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:34:49,157] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.238 seconds
[2020-11-05 14:35:02,163] {scheduler_job.py:155} INFO - Started process (PID=26762) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:02,170] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:35:02,170] {logging_mixin.py:112} INFO - [2020-11-05 14:35:02,170] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:02,328] {logging_mixin.py:112} INFO - [2020-11-05 14:35:02,327] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:35:02,329] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:02,366] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 14:35:15,378] {scheduler_job.py:155} INFO - Started process (PID=26817) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:15,382] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:35:15,383] {logging_mixin.py:112} INFO - [2020-11-05 14:35:15,383] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:15,566] {logging_mixin.py:112} INFO - [2020-11-05 14:35:15,563] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:35:15,567] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:15,605] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.228 seconds
[2020-11-05 14:35:28,587] {scheduler_job.py:155} INFO - Started process (PID=26878) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:28,590] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:35:28,591] {logging_mixin.py:112} INFO - [2020-11-05 14:35:28,591] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:28,688] {logging_mixin.py:112} INFO - [2020-11-05 14:35:28,687] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:35:28,689] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:28,741] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-05 14:35:41,871] {scheduler_job.py:155} INFO - Started process (PID=26935) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:41,875] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:35:41,879] {logging_mixin.py:112} INFO - [2020-11-05 14:35:41,879] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:42,100] {logging_mixin.py:112} INFO - [2020-11-05 14:35:42,099] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:35:42,100] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:42,135] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.264 seconds
[2020-11-05 14:35:55,093] {scheduler_job.py:155} INFO - Started process (PID=26994) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:55,096] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:35:55,097] {logging_mixin.py:112} INFO - [2020-11-05 14:35:55,097] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:55,265] {logging_mixin.py:112} INFO - [2020-11-05 14:35:55,264] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:35:55,265] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:35:55,288] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.195 seconds
[2020-11-05 14:36:08,361] {scheduler_job.py:155} INFO - Started process (PID=27051) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:08,378] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:36:08,378] {logging_mixin.py:112} INFO - [2020-11-05 14:36:08,378] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:08,540] {logging_mixin.py:112} INFO - [2020-11-05 14:36:08,539] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:36:08,541] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:08,566] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.205 seconds
[2020-11-05 14:36:21,642] {scheduler_job.py:155} INFO - Started process (PID=27106) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:21,651] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:36:21,652] {logging_mixin.py:112} INFO - [2020-11-05 14:36:21,652] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:21,822] {logging_mixin.py:112} INFO - [2020-11-05 14:36:21,819] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:36:21,822] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:21,852] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-05 14:36:34,894] {scheduler_job.py:155} INFO - Started process (PID=27162) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:34,899] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:36:34,899] {logging_mixin.py:112} INFO - [2020-11-05 14:36:34,899] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:35,141] {logging_mixin.py:112} INFO - [2020-11-05 14:36:35,140] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:36:35,141] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:35,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.286 seconds
[2020-11-05 14:36:48,121] {scheduler_job.py:155} INFO - Started process (PID=27221) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:48,131] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:36:48,131] {logging_mixin.py:112} INFO - [2020-11-05 14:36:48,131] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:48,355] {logging_mixin.py:112} INFO - [2020-11-05 14:36:48,354] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:36:48,356] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:36:48,385] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 14:37:01,384] {scheduler_job.py:155} INFO - Started process (PID=27283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:01,387] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:37:01,388] {logging_mixin.py:112} INFO - [2020-11-05 14:37:01,388] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:01,510] {logging_mixin.py:112} INFO - [2020-11-05 14:37:01,509] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:37:01,510] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:01,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 14:37:14,701] {scheduler_job.py:155} INFO - Started process (PID=27337) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:14,705] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:37:14,710] {logging_mixin.py:112} INFO - [2020-11-05 14:37:14,707] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:14,913] {logging_mixin.py:112} INFO - [2020-11-05 14:37:14,912] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:37:14,913] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:14,934] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-05 14:37:27,868] {scheduler_job.py:155} INFO - Started process (PID=27398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:27,873] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:37:27,873] {logging_mixin.py:112} INFO - [2020-11-05 14:37:27,873] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:28,018] {logging_mixin.py:112} INFO - [2020-11-05 14:37:28,016] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:37:28,018] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:28,060] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.193 seconds
[2020-11-05 14:37:41,110] {scheduler_job.py:155} INFO - Started process (PID=27458) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:41,114] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:37:41,115] {logging_mixin.py:112} INFO - [2020-11-05 14:37:41,115] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:41,230] {logging_mixin.py:112} INFO - [2020-11-05 14:37:41,230] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:37:41,231] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:41,258] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 14:37:54,339] {scheduler_job.py:155} INFO - Started process (PID=27515) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:54,346] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:37:54,347] {logging_mixin.py:112} INFO - [2020-11-05 14:37:54,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:54,452] {logging_mixin.py:112} INFO - [2020-11-05 14:37:54,451] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:37:54,452] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:37:54,475] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:38:07,588] {scheduler_job.py:155} INFO - Started process (PID=27575) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:07,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:38:07,596] {logging_mixin.py:112} INFO - [2020-11-05 14:38:07,596] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:07,725] {logging_mixin.py:112} INFO - [2020-11-05 14:38:07,723] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:38:07,725] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:07,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 14:38:20,810] {scheduler_job.py:155} INFO - Started process (PID=27638) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:20,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:38:20,814] {logging_mixin.py:112} INFO - [2020-11-05 14:38:20,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:20,943] {logging_mixin.py:112} INFO - [2020-11-05 14:38:20,942] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:38:20,943] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:20,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 14:38:34,024] {scheduler_job.py:155} INFO - Started process (PID=27699) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:34,035] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:38:34,035] {logging_mixin.py:112} INFO - [2020-11-05 14:38:34,035] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:34,203] {logging_mixin.py:112} INFO - [2020-11-05 14:38:34,202] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:38:34,204] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:34,234] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.210 seconds
[2020-11-05 14:38:47,364] {scheduler_job.py:155} INFO - Started process (PID=27760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:47,368] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:38:47,389] {logging_mixin.py:112} INFO - [2020-11-05 14:38:47,368] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:47,575] {logging_mixin.py:112} INFO - [2020-11-05 14:38:47,574] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:38:47,576] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:38:47,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-05 14:39:00,618] {scheduler_job.py:155} INFO - Started process (PID=27820) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:00,622] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:39:00,623] {logging_mixin.py:112} INFO - [2020-11-05 14:39:00,622] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:00,728] {logging_mixin.py:112} INFO - [2020-11-05 14:39:00,727] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:39:00,728] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:00,769] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 14:39:13,980] {scheduler_job.py:155} INFO - Started process (PID=27879) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:13,984] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:39:13,984] {logging_mixin.py:112} INFO - [2020-11-05 14:39:13,984] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:14,087] {logging_mixin.py:112} INFO - [2020-11-05 14:39:14,086] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:39:14,087] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:14,117] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 14:39:27,190] {scheduler_job.py:155} INFO - Started process (PID=27938) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:27,200] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:39:27,201] {logging_mixin.py:112} INFO - [2020-11-05 14:39:27,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:27,337] {logging_mixin.py:112} INFO - [2020-11-05 14:39:27,335] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:39:27,338] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:27,361] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-05 14:39:40,475] {scheduler_job.py:155} INFO - Started process (PID=27996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:40,479] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:39:40,480] {logging_mixin.py:112} INFO - [2020-11-05 14:39:40,479] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:40,658] {logging_mixin.py:112} INFO - [2020-11-05 14:39:40,656] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:39:40,658] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:40,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.227 seconds
[2020-11-05 14:39:53,707] {scheduler_job.py:155} INFO - Started process (PID=28056) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:53,711] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:39:53,711] {logging_mixin.py:112} INFO - [2020-11-05 14:39:53,711] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:53,807] {logging_mixin.py:112} INFO - [2020-11-05 14:39:53,806] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:39:53,808] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:39:53,844] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:40:06,930] {scheduler_job.py:155} INFO - Started process (PID=28118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:06,936] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:40:06,938] {logging_mixin.py:112} INFO - [2020-11-05 14:40:06,937] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:07,054] {logging_mixin.py:112} INFO - [2020-11-05 14:40:07,053] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:40:07,054] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:07,080] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 14:40:20,159] {scheduler_job.py:155} INFO - Started process (PID=28176) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:20,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:40:20,165] {logging_mixin.py:112} INFO - [2020-11-05 14:40:20,165] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:20,282] {logging_mixin.py:112} INFO - [2020-11-05 14:40:20,281] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:40:20,282] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:20,308] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 14:40:33,381] {scheduler_job.py:155} INFO - Started process (PID=28235) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:33,385] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:40:33,386] {logging_mixin.py:112} INFO - [2020-11-05 14:40:33,386] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:33,505] {logging_mixin.py:112} INFO - [2020-11-05 14:40:33,504] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:40:33,505] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:33,532] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 14:40:46,578] {scheduler_job.py:155} INFO - Started process (PID=28294) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:46,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:40:46,582] {logging_mixin.py:112} INFO - [2020-11-05 14:40:46,582] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:46,735] {logging_mixin.py:112} INFO - [2020-11-05 14:40:46,729] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:40:46,735] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:46,767] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.189 seconds
[2020-11-05 14:40:59,757] {scheduler_job.py:155} INFO - Started process (PID=28356) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:59,760] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:40:59,761] {logging_mixin.py:112} INFO - [2020-11-05 14:40:59,761] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:59,870] {logging_mixin.py:112} INFO - [2020-11-05 14:40:59,870] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:40:59,871] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:40:59,899] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 14:41:13,039] {scheduler_job.py:155} INFO - Started process (PID=28422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:13,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:41:13,048] {logging_mixin.py:112} INFO - [2020-11-05 14:41:13,048] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:13,465] {logging_mixin.py:112} INFO - [2020-11-05 14:41:13,460] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:41:13,466] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:13,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.469 seconds
[2020-11-05 14:41:26,318] {scheduler_job.py:155} INFO - Started process (PID=28503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:26,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:41:26,342] {logging_mixin.py:112} INFO - [2020-11-05 14:41:26,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:26,573] {logging_mixin.py:112} INFO - [2020-11-05 14:41:26,571] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:41:26,573] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:26,620] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.302 seconds
[2020-11-05 14:41:39,572] {scheduler_job.py:155} INFO - Started process (PID=28560) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:39,577] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:41:39,578] {logging_mixin.py:112} INFO - [2020-11-05 14:41:39,578] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:39,712] {logging_mixin.py:112} INFO - [2020-11-05 14:41:39,711] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:41:39,712] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:39,748] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-05 14:41:52,786] {scheduler_job.py:155} INFO - Started process (PID=28621) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:52,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:41:52,805] {logging_mixin.py:112} INFO - [2020-11-05 14:41:52,805] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:52,935] {logging_mixin.py:112} INFO - [2020-11-05 14:41:52,934] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:41:52,936] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:41:52,964] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 14:42:05,944] {scheduler_job.py:155} INFO - Started process (PID=28687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:05,949] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:42:05,949] {logging_mixin.py:112} INFO - [2020-11-05 14:42:05,949] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:06,246] {logging_mixin.py:112} INFO - [2020-11-05 14:42:06,239] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:42:06,247] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:06,280] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.336 seconds
[2020-11-05 14:42:19,225] {scheduler_job.py:155} INFO - Started process (PID=28752) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:19,228] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:42:19,243] {logging_mixin.py:112} INFO - [2020-11-05 14:42:19,242] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:19,538] {logging_mixin.py:112} INFO - [2020-11-05 14:42:19,537] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:42:19,538] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:19,586] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.361 seconds
[2020-11-05 14:42:32,426] {scheduler_job.py:155} INFO - Started process (PID=28810) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:32,436] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:42:32,437] {logging_mixin.py:112} INFO - [2020-11-05 14:42:32,436] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:32,594] {logging_mixin.py:112} INFO - [2020-11-05 14:42:32,593] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:42:32,594] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:32,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.189 seconds
[2020-11-05 14:42:45,655] {scheduler_job.py:155} INFO - Started process (PID=28871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:45,670] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:42:45,671] {logging_mixin.py:112} INFO - [2020-11-05 14:42:45,671] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:45,864] {logging_mixin.py:112} INFO - [2020-11-05 14:42:45,863] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:42:45,864] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:45,905] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.249 seconds
[2020-11-05 14:42:58,876] {scheduler_job.py:155} INFO - Started process (PID=28931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:58,880] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:42:58,881] {logging_mixin.py:112} INFO - [2020-11-05 14:42:58,881] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:59,023] {logging_mixin.py:112} INFO - [2020-11-05 14:42:59,021] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:42:59,023] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:42:59,047] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-05 14:43:12,092] {scheduler_job.py:155} INFO - Started process (PID=28992) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:12,096] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:43:12,097] {logging_mixin.py:112} INFO - [2020-11-05 14:43:12,096] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:12,187] {logging_mixin.py:112} INFO - [2020-11-05 14:43:12,186] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:43:12,188] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:12,218] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-05 14:43:25,294] {scheduler_job.py:155} INFO - Started process (PID=29059) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:25,299] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:43:25,300] {logging_mixin.py:112} INFO - [2020-11-05 14:43:25,299] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:25,396] {logging_mixin.py:112} INFO - [2020-11-05 14:43:25,395] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:43:25,397] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:25,435] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 14:43:38,517] {scheduler_job.py:155} INFO - Started process (PID=29121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:38,521] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:43:38,522] {logging_mixin.py:112} INFO - [2020-11-05 14:43:38,521] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:38,631] {logging_mixin.py:112} INFO - [2020-11-05 14:43:38,630] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:43:38,631] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:38,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 14:43:51,721] {scheduler_job.py:155} INFO - Started process (PID=29190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:51,725] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:43:51,725] {logging_mixin.py:112} INFO - [2020-11-05 14:43:51,725] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:51,845] {logging_mixin.py:112} INFO - [2020-11-05 14:43:51,844] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:43:51,846] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:43:51,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 14:44:05,000] {scheduler_job.py:155} INFO - Started process (PID=29256) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:05,003] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:44:05,003] {logging_mixin.py:112} INFO - [2020-11-05 14:44:05,003] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:05,108] {logging_mixin.py:112} INFO - [2020-11-05 14:44:05,107] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:44:05,108] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:05,136] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:44:18,219] {scheduler_job.py:155} INFO - Started process (PID=29316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:18,224] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:44:18,225] {logging_mixin.py:112} INFO - [2020-11-05 14:44:18,225] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:18,390] {logging_mixin.py:112} INFO - [2020-11-05 14:44:18,388] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:44:18,390] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:18,433] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 14:44:31,482] {scheduler_job.py:155} INFO - Started process (PID=29375) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:31,486] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:44:31,487] {logging_mixin.py:112} INFO - [2020-11-05 14:44:31,487] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:31,618] {logging_mixin.py:112} INFO - [2020-11-05 14:44:31,617] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:44:31,619] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:31,640] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 14:44:44,729] {scheduler_job.py:155} INFO - Started process (PID=29430) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:44,733] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:44:44,734] {logging_mixin.py:112} INFO - [2020-11-05 14:44:44,733] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:44,827] {logging_mixin.py:112} INFO - [2020-11-05 14:44:44,826] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:44:44,828] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:44,899] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 14:44:57,988] {scheduler_job.py:155} INFO - Started process (PID=29506) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:57,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:44:58,009] {logging_mixin.py:112} INFO - [2020-11-05 14:44:58,009] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:58,237] {logging_mixin.py:112} INFO - [2020-11-05 14:44:58,235] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:44:58,238] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:44:58,269] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.282 seconds
[2020-11-05 14:45:11,266] {scheduler_job.py:155} INFO - Started process (PID=29583) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:11,269] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:45:11,270] {logging_mixin.py:112} INFO - [2020-11-05 14:45:11,269] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:11,365] {logging_mixin.py:112} INFO - [2020-11-05 14:45:11,364] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:45:11,365] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:11,387] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.121 seconds
[2020-11-05 14:45:24,546] {scheduler_job.py:155} INFO - Started process (PID=29644) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:24,549] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:45:24,550] {logging_mixin.py:112} INFO - [2020-11-05 14:45:24,550] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:24,722] {logging_mixin.py:112} INFO - [2020-11-05 14:45:24,721] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:45:24,723] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:24,758] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.212 seconds
[2020-11-05 14:45:37,833] {scheduler_job.py:155} INFO - Started process (PID=29705) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:37,839] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:45:37,840] {logging_mixin.py:112} INFO - [2020-11-05 14:45:37,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:37,966] {logging_mixin.py:112} INFO - [2020-11-05 14:45:37,964] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:45:37,966] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:37,994] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 14:45:51,088] {scheduler_job.py:155} INFO - Started process (PID=29794) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:51,092] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:45:51,092] {logging_mixin.py:112} INFO - [2020-11-05 14:45:51,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:51,185] {logging_mixin.py:112} INFO - [2020-11-05 14:45:51,184] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:45:51,186] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:45:51,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 14:46:04,337] {scheduler_job.py:155} INFO - Started process (PID=29855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:04,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:46:04,344] {logging_mixin.py:112} INFO - [2020-11-05 14:46:04,344] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:04,458] {logging_mixin.py:112} INFO - [2020-11-05 14:46:04,457] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:46:04,458] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:04,488] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 14:46:17,562] {scheduler_job.py:155} INFO - Started process (PID=29918) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:17,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:46:17,567] {logging_mixin.py:112} INFO - [2020-11-05 14:46:17,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:17,694] {logging_mixin.py:112} INFO - [2020-11-05 14:46:17,693] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:46:17,694] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:17,719] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 14:46:30,773] {scheduler_job.py:155} INFO - Started process (PID=29983) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:30,777] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:46:30,778] {logging_mixin.py:112} INFO - [2020-11-05 14:46:30,778] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:30,945] {logging_mixin.py:112} INFO - [2020-11-05 14:46:30,942] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:46:30,946] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:30,968] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.196 seconds
[2020-11-05 14:46:44,038] {scheduler_job.py:155} INFO - Started process (PID=30038) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:44,043] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:46:44,045] {logging_mixin.py:112} INFO - [2020-11-05 14:46:44,044] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:44,208] {logging_mixin.py:112} INFO - [2020-11-05 14:46:44,206] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:46:44,208] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:44,245] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.208 seconds
[2020-11-05 14:46:57,341] {scheduler_job.py:155} INFO - Started process (PID=30098) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:57,345] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:46:57,350] {logging_mixin.py:112} INFO - [2020-11-05 14:46:57,349] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:57,465] {logging_mixin.py:112} INFO - [2020-11-05 14:46:57,464] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:46:57,465] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:46:57,509] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.168 seconds
[2020-11-05 14:47:10,565] {scheduler_job.py:155} INFO - Started process (PID=30159) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:10,568] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:47:10,568] {logging_mixin.py:112} INFO - [2020-11-05 14:47:10,568] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:10,669] {logging_mixin.py:112} INFO - [2020-11-05 14:47:10,668] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:47:10,669] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:10,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 14:47:23,852] {scheduler_job.py:155} INFO - Started process (PID=30227) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:23,855] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:47:23,857] {logging_mixin.py:112} INFO - [2020-11-05 14:47:23,856] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:23,981] {logging_mixin.py:112} INFO - [2020-11-05 14:47:23,980] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:47:23,981] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:24,015] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 14:47:37,065] {scheduler_job.py:155} INFO - Started process (PID=30290) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:37,068] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:47:37,068] {logging_mixin.py:112} INFO - [2020-11-05 14:47:37,068] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:37,176] {logging_mixin.py:112} INFO - [2020-11-05 14:47:37,175] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:47:37,177] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:37,200] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 14:47:50,305] {scheduler_job.py:155} INFO - Started process (PID=30352) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:50,308] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:47:50,309] {logging_mixin.py:112} INFO - [2020-11-05 14:47:50,309] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:50,418] {logging_mixin.py:112} INFO - [2020-11-05 14:47:50,416] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:47:50,418] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:47:50,448] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 14:48:03,486] {scheduler_job.py:155} INFO - Started process (PID=30416) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:03,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:48:03,494] {logging_mixin.py:112} INFO - [2020-11-05 14:48:03,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:03,603] {logging_mixin.py:112} INFO - [2020-11-05 14:48:03,603] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:48:03,604] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:03,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 14:48:16,739] {scheduler_job.py:155} INFO - Started process (PID=30481) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:16,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:48:16,743] {logging_mixin.py:112} INFO - [2020-11-05 14:48:16,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:16,874] {logging_mixin.py:112} INFO - [2020-11-05 14:48:16,873] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:48:16,874] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:16,898] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 14:48:29,966] {scheduler_job.py:155} INFO - Started process (PID=30548) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:29,970] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:48:29,970] {logging_mixin.py:112} INFO - [2020-11-05 14:48:29,970] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:30,183] {logging_mixin.py:112} INFO - [2020-11-05 14:48:30,182] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:48:30,184] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:30,208] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.243 seconds
[2020-11-05 14:48:43,183] {scheduler_job.py:155} INFO - Started process (PID=30608) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:43,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:48:43,189] {logging_mixin.py:112} INFO - [2020-11-05 14:48:43,188] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:43,397] {logging_mixin.py:112} INFO - [2020-11-05 14:48:43,396] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:48:43,398] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:43,422] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-05 14:48:56,422] {scheduler_job.py:155} INFO - Started process (PID=30670) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:56,426] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:48:56,427] {logging_mixin.py:112} INFO - [2020-11-05 14:48:56,427] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:56,517] {logging_mixin.py:112} INFO - [2020-11-05 14:48:56,516] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:48:56,518] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:48:56,545] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.122 seconds
[2020-11-05 14:49:09,623] {scheduler_job.py:155} INFO - Started process (PID=30733) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:09,628] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:49:09,628] {logging_mixin.py:112} INFO - [2020-11-05 14:49:09,628] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:09,726] {logging_mixin.py:112} INFO - [2020-11-05 14:49:09,724] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:49:09,726] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:09,755] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 14:49:22,841] {scheduler_job.py:155} INFO - Started process (PID=30797) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:22,845] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:49:22,846] {logging_mixin.py:112} INFO - [2020-11-05 14:49:22,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:22,958] {logging_mixin.py:112} INFO - [2020-11-05 14:49:22,957] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:49:22,958] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:22,985] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 14:49:36,132] {scheduler_job.py:155} INFO - Started process (PID=30861) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:36,135] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:49:36,136] {logging_mixin.py:112} INFO - [2020-11-05 14:49:36,136] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:36,275] {logging_mixin.py:112} INFO - [2020-11-05 14:49:36,274] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:49:36,276] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:36,302] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 14:49:49,354] {scheduler_job.py:155} INFO - Started process (PID=30922) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:49,358] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:49:49,359] {logging_mixin.py:112} INFO - [2020-11-05 14:49:49,359] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:49,457] {logging_mixin.py:112} INFO - [2020-11-05 14:49:49,456] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:49:49,457] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:49:49,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.129 seconds
[2020-11-05 14:50:02,584] {scheduler_job.py:155} INFO - Started process (PID=30993) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:02,594] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:50:02,601] {logging_mixin.py:112} INFO - [2020-11-05 14:50:02,601] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:02,768] {logging_mixin.py:112} INFO - [2020-11-05 14:50:02,767] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:50:02,769] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:02,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.209 seconds
[2020-11-05 14:50:15,831] {scheduler_job.py:155} INFO - Started process (PID=31053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:15,834] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:50:15,835] {logging_mixin.py:112} INFO - [2020-11-05 14:50:15,835] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:15,937] {logging_mixin.py:112} INFO - [2020-11-05 14:50:15,936] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:50:15,938] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:15,967] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 14:50:29,069] {scheduler_job.py:155} INFO - Started process (PID=31113) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:29,074] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:50:29,075] {logging_mixin.py:112} INFO - [2020-11-05 14:50:29,075] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:29,179] {logging_mixin.py:112} INFO - [2020-11-05 14:50:29,178] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:50:29,181] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:29,203] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 14:50:42,265] {scheduler_job.py:155} INFO - Started process (PID=31175) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:42,268] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:50:42,269] {logging_mixin.py:112} INFO - [2020-11-05 14:50:42,268] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:42,380] {logging_mixin.py:112} INFO - [2020-11-05 14:50:42,377] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:50:42,381] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:42,407] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 14:50:55,480] {scheduler_job.py:155} INFO - Started process (PID=31237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:55,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:50:55,495] {logging_mixin.py:112} INFO - [2020-11-05 14:50:55,494] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:55,590] {logging_mixin.py:112} INFO - [2020-11-05 14:50:55,589] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:50:55,590] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:50:55,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 14:51:08,780] {scheduler_job.py:155} INFO - Started process (PID=31301) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:08,784] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:51:08,785] {logging_mixin.py:112} INFO - [2020-11-05 14:51:08,785] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:08,891] {logging_mixin.py:112} INFO - [2020-11-05 14:51:08,890] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:51:08,891] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:08,915] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 14:51:22,059] {scheduler_job.py:155} INFO - Started process (PID=31367) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:22,063] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:51:22,063] {logging_mixin.py:112} INFO - [2020-11-05 14:51:22,063] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:22,164] {logging_mixin.py:112} INFO - [2020-11-05 14:51:22,162] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:51:22,164] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:22,197] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 14:51:35,313] {scheduler_job.py:155} INFO - Started process (PID=31433) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:35,318] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:51:35,319] {logging_mixin.py:112} INFO - [2020-11-05 14:51:35,319] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:35,507] {logging_mixin.py:112} INFO - [2020-11-05 14:51:35,506] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:51:35,508] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:35,536] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.223 seconds
[2020-11-05 14:51:48,525] {scheduler_job.py:155} INFO - Started process (PID=31495) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:48,529] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:51:48,530] {logging_mixin.py:112} INFO - [2020-11-05 14:51:48,530] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:48,615] {logging_mixin.py:112} INFO - [2020-11-05 14:51:48,614] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:51:48,615] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:51:48,646] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.121 seconds
[2020-11-05 14:52:01,805] {scheduler_job.py:155} INFO - Started process (PID=31556) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:01,809] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:52:01,810] {logging_mixin.py:112} INFO - [2020-11-05 14:52:01,810] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:01,946] {logging_mixin.py:112} INFO - [2020-11-05 14:52:01,941] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:52:01,946] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:01,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-05 14:52:15,024] {scheduler_job.py:155} INFO - Started process (PID=31621) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:15,038] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:52:15,038] {logging_mixin.py:112} INFO - [2020-11-05 14:52:15,038] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:15,175] {logging_mixin.py:112} INFO - [2020-11-05 14:52:15,173] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:52:15,176] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:15,210] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-05 14:52:28,302] {scheduler_job.py:155} INFO - Started process (PID=31685) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:28,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:52:28,307] {logging_mixin.py:112} INFO - [2020-11-05 14:52:28,306] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:28,426] {logging_mixin.py:112} INFO - [2020-11-05 14:52:28,425] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:52:28,426] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:28,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 14:52:41,539] {scheduler_job.py:155} INFO - Started process (PID=31746) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:41,542] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:52:41,543] {logging_mixin.py:112} INFO - [2020-11-05 14:52:41,543] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:41,640] {logging_mixin.py:112} INFO - [2020-11-05 14:52:41,639] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:52:41,641] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:41,666] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 14:52:54,794] {scheduler_job.py:155} INFO - Started process (PID=31806) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:54,798] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:52:54,799] {logging_mixin.py:112} INFO - [2020-11-05 14:52:54,799] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:55,036] {logging_mixin.py:112} INFO - [2020-11-05 14:52:55,033] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:52:55,037] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:52:55,071] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.277 seconds
[2020-11-05 14:53:07,985] {scheduler_job.py:155} INFO - Started process (PID=31871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:07,988] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:53:07,989] {logging_mixin.py:112} INFO - [2020-11-05 14:53:07,989] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:08,092] {logging_mixin.py:112} INFO - [2020-11-05 14:53:08,091] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:53:08,092] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:08,112] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 14:53:21,188] {scheduler_job.py:155} INFO - Started process (PID=31933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:21,191] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:53:21,192] {logging_mixin.py:112} INFO - [2020-11-05 14:53:21,192] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:21,300] {logging_mixin.py:112} INFO - [2020-11-05 14:53:21,299] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:53:21,300] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:21,330] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 14:53:34,468] {scheduler_job.py:155} INFO - Started process (PID=31996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:34,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:53:34,472] {logging_mixin.py:112} INFO - [2020-11-05 14:53:34,472] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:34,646] {logging_mixin.py:112} INFO - [2020-11-05 14:53:34,645] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:53:34,647] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:34,672] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.205 seconds
[2020-11-05 14:53:47,808] {scheduler_job.py:155} INFO - Started process (PID=32061) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:47,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:53:47,814] {logging_mixin.py:112} INFO - [2020-11-05 14:53:47,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:47,919] {logging_mixin.py:112} INFO - [2020-11-05 14:53:47,917] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:53:47,919] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:53:47,942] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 14:54:01,067] {scheduler_job.py:155} INFO - Started process (PID=32125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:01,071] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:54:01,072] {logging_mixin.py:112} INFO - [2020-11-05 14:54:01,072] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:01,190] {logging_mixin.py:112} INFO - [2020-11-05 14:54:01,189] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:54:01,190] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:01,218] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 14:54:14,304] {scheduler_job.py:155} INFO - Started process (PID=32190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:14,309] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:54:14,309] {logging_mixin.py:112} INFO - [2020-11-05 14:54:14,309] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:14,421] {logging_mixin.py:112} INFO - [2020-11-05 14:54:14,419] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:54:14,421] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:14,453] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 14:54:27,604] {scheduler_job.py:155} INFO - Started process (PID=32255) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:27,608] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:54:27,609] {logging_mixin.py:112} INFO - [2020-11-05 14:54:27,609] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:27,730] {logging_mixin.py:112} INFO - [2020-11-05 14:54:27,729] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:54:27,731] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:27,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 14:54:40,824] {scheduler_job.py:155} INFO - Started process (PID=32317) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:40,828] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:54:40,829] {logging_mixin.py:112} INFO - [2020-11-05 14:54:40,829] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:40,927] {logging_mixin.py:112} INFO - [2020-11-05 14:54:40,926] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:54:40,927] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:40,959] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 14:54:54,094] {scheduler_job.py:155} INFO - Started process (PID=32378) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:54,137] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:54:54,138] {logging_mixin.py:112} INFO - [2020-11-05 14:54:54,138] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:54,252] {logging_mixin.py:112} INFO - [2020-11-05 14:54:54,251] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:54:54,252] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:54:54,274] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 14:55:07,326] {scheduler_job.py:155} INFO - Started process (PID=32448) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:07,330] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:55:07,331] {logging_mixin.py:112} INFO - [2020-11-05 14:55:07,331] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:07,529] {logging_mixin.py:112} INFO - [2020-11-05 14:55:07,528] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:55:07,530] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:07,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-05 14:55:20,532] {scheduler_job.py:155} INFO - Started process (PID=32509) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:20,535] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:55:20,535] {logging_mixin.py:112} INFO - [2020-11-05 14:55:20,535] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:20,646] {logging_mixin.py:112} INFO - [2020-11-05 14:55:20,645] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:55:20,646] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:20,669] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 14:55:33,833] {scheduler_job.py:155} INFO - Started process (PID=32572) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:33,836] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:55:33,836] {logging_mixin.py:112} INFO - [2020-11-05 14:55:33,836] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:33,950] {logging_mixin.py:112} INFO - [2020-11-05 14:55:33,949] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:55:33,950] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:33,971] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 14:55:47,044] {scheduler_job.py:155} INFO - Started process (PID=32637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:47,047] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:55:47,047] {logging_mixin.py:112} INFO - [2020-11-05 14:55:47,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:47,154] {logging_mixin.py:112} INFO - [2020-11-05 14:55:47,153] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:55:47,154] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:55:47,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 14:56:00,377] {scheduler_job.py:155} INFO - Started process (PID=32701) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:00,380] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:56:00,381] {logging_mixin.py:112} INFO - [2020-11-05 14:56:00,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:00,500] {logging_mixin.py:112} INFO - [2020-11-05 14:56:00,499] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:56:00,500] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:00,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 14:56:13,719] {scheduler_job.py:155} INFO - Started process (PID=32762) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:13,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:56:13,725] {logging_mixin.py:112} INFO - [2020-11-05 14:56:13,725] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:13,845] {logging_mixin.py:112} INFO - [2020-11-05 14:56:13,844] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:56:13,845] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:13,870] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 14:56:26,938] {scheduler_job.py:155} INFO - Started process (PID=365) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:26,942] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:56:26,942] {logging_mixin.py:112} INFO - [2020-11-05 14:56:26,942] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:27,039] {logging_mixin.py:112} INFO - [2020-11-05 14:56:27,038] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:56:27,039] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:27,069] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 14:56:40,158] {scheduler_job.py:155} INFO - Started process (PID=432) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:40,165] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:56:40,166] {logging_mixin.py:112} INFO - [2020-11-05 14:56:40,165] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:40,332] {logging_mixin.py:112} INFO - [2020-11-05 14:56:40,331] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:56:40,332] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:40,360] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 14:56:53,392] {scheduler_job.py:155} INFO - Started process (PID=501) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:53,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:56:53,397] {logging_mixin.py:112} INFO - [2020-11-05 14:56:53,396] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:53,498] {logging_mixin.py:112} INFO - [2020-11-05 14:56:53,497] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:56:53,498] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:56:53,525] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 14:57:06,755] {scheduler_job.py:155} INFO - Started process (PID=584) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:06,759] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:57:06,760] {logging_mixin.py:112} INFO - [2020-11-05 14:57:06,760] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:06,873] {logging_mixin.py:112} INFO - [2020-11-05 14:57:06,872] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:57:06,874] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:06,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 14:57:19,953] {scheduler_job.py:155} INFO - Started process (PID=662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:19,957] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:57:19,959] {logging_mixin.py:112} INFO - [2020-11-05 14:57:19,957] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:20,055] {logging_mixin.py:112} INFO - [2020-11-05 14:57:20,054] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:57:20,055] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:20,079] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 14:57:33,293] {scheduler_job.py:155} INFO - Started process (PID=737) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:33,297] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:57:33,298] {logging_mixin.py:112} INFO - [2020-11-05 14:57:33,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:33,400] {logging_mixin.py:112} INFO - [2020-11-05 14:57:33,399] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:57:33,400] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:33,423] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-05 14:57:46,614] {scheduler_job.py:155} INFO - Started process (PID=803) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:46,618] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:57:46,618] {logging_mixin.py:112} INFO - [2020-11-05 14:57:46,618] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:46,739] {logging_mixin.py:112} INFO - [2020-11-05 14:57:46,737] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:57:46,739] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:46,763] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 14:57:59,856] {scheduler_job.py:155} INFO - Started process (PID=867) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:57:59,860] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:57:59,861] {logging_mixin.py:112} INFO - [2020-11-05 14:57:59,860] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:00,053] {logging_mixin.py:112} INFO - [2020-11-05 14:58:00,051] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:58:00,053] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:00,090] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-05 14:58:13,092] {scheduler_job.py:155} INFO - Started process (PID=933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:13,096] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:58:13,097] {logging_mixin.py:112} INFO - [2020-11-05 14:58:13,096] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:13,201] {logging_mixin.py:112} INFO - [2020-11-05 14:58:13,200] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:58:13,201] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:13,226] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 14:58:26,339] {scheduler_job.py:155} INFO - Started process (PID=1005) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:26,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:58:26,342] {logging_mixin.py:112} INFO - [2020-11-05 14:58:26,342] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:26,450] {logging_mixin.py:112} INFO - [2020-11-05 14:58:26,449] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:58:26,450] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:26,473] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 14:58:39,631] {scheduler_job.py:155} INFO - Started process (PID=1069) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:39,634] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:58:39,635] {logging_mixin.py:112} INFO - [2020-11-05 14:58:39,634] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:39,758] {logging_mixin.py:112} INFO - [2020-11-05 14:58:39,756] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:58:39,758] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:39,793] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 14:58:52,813] {scheduler_job.py:155} INFO - Started process (PID=1132) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:52,819] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:58:52,819] {logging_mixin.py:112} INFO - [2020-11-05 14:58:52,819] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:52,908] {logging_mixin.py:112} INFO - [2020-11-05 14:58:52,907] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:58:52,909] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:58:52,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.119 seconds
[2020-11-05 14:59:06,013] {scheduler_job.py:155} INFO - Started process (PID=1196) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:06,022] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:59:06,023] {logging_mixin.py:112} INFO - [2020-11-05 14:59:06,022] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:06,121] {logging_mixin.py:112} INFO - [2020-11-05 14:59:06,120] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:59:06,121] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:06,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 14:59:19,237] {scheduler_job.py:155} INFO - Started process (PID=1258) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:19,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:59:19,240] {logging_mixin.py:112} INFO - [2020-11-05 14:59:19,240] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:19,338] {logging_mixin.py:112} INFO - [2020-11-05 14:59:19,336] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:59:19,338] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:19,362] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 14:59:32,489] {scheduler_job.py:155} INFO - Started process (PID=1322) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:32,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:59:32,493] {logging_mixin.py:112} INFO - [2020-11-05 14:59:32,493] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:32,591] {logging_mixin.py:112} INFO - [2020-11-05 14:59:32,590] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:59:32,591] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:32,614] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 14:59:45,658] {scheduler_job.py:155} INFO - Started process (PID=1384) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:45,661] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:59:45,662] {logging_mixin.py:112} INFO - [2020-11-05 14:59:45,662] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:45,769] {logging_mixin.py:112} INFO - [2020-11-05 14:59:45,767] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:59:45,769] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:45,809] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 14:59:58,899] {scheduler_job.py:155} INFO - Started process (PID=1452) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:58,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 14:59:58,905] {logging_mixin.py:112} INFO - [2020-11-05 14:59:58,905] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:59,023] {logging_mixin.py:112} INFO - [2020-11-05 14:59:59,022] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 14:59:59,023] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 14:59:59,057] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 15:00:12,170] {scheduler_job.py:155} INFO - Started process (PID=1521) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:12,178] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:00:12,185] {logging_mixin.py:112} INFO - [2020-11-05 15:00:12,185] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:12,312] {logging_mixin.py:112} INFO - [2020-11-05 15:00:12,311] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:00:12,313] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:12,342] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.171 seconds
[2020-11-05 15:00:25,433] {scheduler_job.py:155} INFO - Started process (PID=1615) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:25,437] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:00:25,437] {logging_mixin.py:112} INFO - [2020-11-05 15:00:25,437] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:25,535] {logging_mixin.py:112} INFO - [2020-11-05 15:00:25,534] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:00:25,535] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:25,560] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-05 15:00:38,742] {scheduler_job.py:155} INFO - Started process (PID=1687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:38,745] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:00:38,746] {logging_mixin.py:112} INFO - [2020-11-05 15:00:38,746] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:38,864] {logging_mixin.py:112} INFO - [2020-11-05 15:00:38,863] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:00:38,865] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:38,889] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-05 15:00:51,977] {scheduler_job.py:155} INFO - Started process (PID=1792) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:51,982] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:00:51,982] {logging_mixin.py:112} INFO - [2020-11-05 15:00:51,982] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:52,084] {logging_mixin.py:112} INFO - [2020-11-05 15:00:52,083] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:00:52,085] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:00:52,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 15:01:05,227] {scheduler_job.py:155} INFO - Started process (PID=1987) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:05,229] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:01:05,230] {logging_mixin.py:112} INFO - [2020-11-05 15:01:05,230] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:05,326] {logging_mixin.py:112} INFO - [2020-11-05 15:01:05,325] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:01:05,326] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:05,348] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.122 seconds
[2020-11-05 15:01:18,452] {scheduler_job.py:155} INFO - Started process (PID=2077) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:18,456] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:01:18,456] {logging_mixin.py:112} INFO - [2020-11-05 15:01:18,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:18,565] {logging_mixin.py:112} INFO - [2020-11-05 15:01:18,563] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:01:18,566] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:18,588] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 15:01:31,775] {scheduler_job.py:155} INFO - Started process (PID=2152) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:31,779] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:01:31,779] {logging_mixin.py:112} INFO - [2020-11-05 15:01:31,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:31,874] {logging_mixin.py:112} INFO - [2020-11-05 15:01:31,873] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:01:31,874] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:31,896] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.121 seconds
[2020-11-05 15:01:44,943] {scheduler_job.py:155} INFO - Started process (PID=2239) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:44,953] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:01:44,954] {logging_mixin.py:112} INFO - [2020-11-05 15:01:44,953] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:45,171] {logging_mixin.py:112} INFO - [2020-11-05 15:01:45,170] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:01:45,172] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:45,216] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.273 seconds
[2020-11-05 15:01:58,147] {scheduler_job.py:155} INFO - Started process (PID=2340) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:58,152] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:01:58,153] {logging_mixin.py:112} INFO - [2020-11-05 15:01:58,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:58,259] {logging_mixin.py:112} INFO - [2020-11-05 15:01:58,258] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:01:58,259] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:01:58,282] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 15:02:11,363] {scheduler_job.py:155} INFO - Started process (PID=2426) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:11,366] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:02:11,367] {logging_mixin.py:112} INFO - [2020-11-05 15:02:11,367] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:11,480] {logging_mixin.py:112} INFO - [2020-11-05 15:02:11,479] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:02:11,481] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:11,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 15:02:24,584] {scheduler_job.py:155} INFO - Started process (PID=2530) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:24,588] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:02:24,590] {logging_mixin.py:112} INFO - [2020-11-05 15:02:24,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:24,685] {logging_mixin.py:112} INFO - [2020-11-05 15:02:24,684] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:02:24,685] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:24,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-05 15:02:37,781] {scheduler_job.py:155} INFO - Started process (PID=2602) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:37,785] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:02:37,786] {logging_mixin.py:112} INFO - [2020-11-05 15:02:37,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:37,872] {logging_mixin.py:112} INFO - [2020-11-05 15:02:37,871] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:02:37,872] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:37,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.114 seconds
[2020-11-05 15:02:50,994] {scheduler_job.py:155} INFO - Started process (PID=2663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:50,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:02:50,998] {logging_mixin.py:112} INFO - [2020-11-05 15:02:50,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:51,091] {logging_mixin.py:112} INFO - [2020-11-05 15:02:51,090] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:02:51,092] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:02:51,115] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.121 seconds
[2020-11-05 15:03:04,347] {scheduler_job.py:155} INFO - Started process (PID=2733) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:04,351] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:03:04,352] {logging_mixin.py:112} INFO - [2020-11-05 15:03:04,352] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:04,444] {logging_mixin.py:112} INFO - [2020-11-05 15:03:04,444] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:03:04,445] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:04,469] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.121 seconds
[2020-11-05 15:03:17,538] {scheduler_job.py:155} INFO - Started process (PID=2805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:17,542] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:03:17,543] {logging_mixin.py:112} INFO - [2020-11-05 15:03:17,543] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:17,638] {logging_mixin.py:112} INFO - [2020-11-05 15:03:17,637] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:03:17,638] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:17,670] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 15:03:30,762] {scheduler_job.py:155} INFO - Started process (PID=2867) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:30,768] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:03:30,769] {logging_mixin.py:112} INFO - [2020-11-05 15:03:30,769] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:30,875] {logging_mixin.py:112} INFO - [2020-11-05 15:03:30,874] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:03:30,875] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:30,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 15:03:43,955] {scheduler_job.py:155} INFO - Started process (PID=2933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:43,958] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:03:43,959] {logging_mixin.py:112} INFO - [2020-11-05 15:03:43,959] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:44,133] {logging_mixin.py:112} INFO - [2020-11-05 15:03:44,132] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:03:44,133] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:44,159] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.205 seconds
[2020-11-05 15:03:57,140] {scheduler_job.py:155} INFO - Started process (PID=2996) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:57,145] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:03:57,145] {logging_mixin.py:112} INFO - [2020-11-05 15:03:57,145] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:57,248] {logging_mixin.py:112} INFO - [2020-11-05 15:03:57,247] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:03:57,248] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:03:57,274] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 15:04:10,349] {scheduler_job.py:155} INFO - Started process (PID=3057) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:10,352] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:04:10,353] {logging_mixin.py:112} INFO - [2020-11-05 15:04:10,353] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:10,446] {logging_mixin.py:112} INFO - [2020-11-05 15:04:10,445] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:04:10,446] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:10,471] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.121 seconds
[2020-11-05 15:04:23,570] {scheduler_job.py:155} INFO - Started process (PID=3119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:23,573] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:04:23,574] {logging_mixin.py:112} INFO - [2020-11-05 15:04:23,574] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:23,672] {logging_mixin.py:112} INFO - [2020-11-05 15:04:23,671] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:04:23,673] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:23,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 15:04:36,809] {scheduler_job.py:155} INFO - Started process (PID=3181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:36,812] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:04:36,812] {logging_mixin.py:112} INFO - [2020-11-05 15:04:36,812] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:36,927] {logging_mixin.py:112} INFO - [2020-11-05 15:04:36,927] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:04:36,928] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:36,951] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 15:04:50,037] {scheduler_job.py:155} INFO - Started process (PID=3252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:50,040] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:04:50,040] {logging_mixin.py:112} INFO - [2020-11-05 15:04:50,040] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:50,155] {logging_mixin.py:112} INFO - [2020-11-05 15:04:50,153] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:04:50,155] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:04:50,193] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 15:05:03,316] {scheduler_job.py:155} INFO - Started process (PID=3333) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:03,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:05:03,322] {logging_mixin.py:112} INFO - [2020-11-05 15:05:03,322] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:03,438] {logging_mixin.py:112} INFO - [2020-11-05 15:05:03,437] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:05:03,438] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:03,461] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-05 15:05:16,839] {scheduler_job.py:155} INFO - Started process (PID=3399) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:16,850] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:05:16,850] {logging_mixin.py:112} INFO - [2020-11-05 15:05:16,850] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:16,991] {logging_mixin.py:112} INFO - [2020-11-05 15:05:16,990] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:05:16,991] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:17,011] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.172 seconds
[2020-11-05 15:05:30,042] {scheduler_job.py:155} INFO - Started process (PID=3462) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:30,046] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:05:30,047] {logging_mixin.py:112} INFO - [2020-11-05 15:05:30,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:30,141] {logging_mixin.py:112} INFO - [2020-11-05 15:05:30,140] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:05:30,141] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:30,164] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.123 seconds
[2020-11-05 15:05:43,376] {scheduler_job.py:155} INFO - Started process (PID=3530) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:43,379] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:05:43,380] {logging_mixin.py:112} INFO - [2020-11-05 15:05:43,379] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:43,488] {logging_mixin.py:112} INFO - [2020-11-05 15:05:43,487] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:05:43,488] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:43,510] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 15:05:56,566] {scheduler_job.py:155} INFO - Started process (PID=3590) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:56,570] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:05:56,571] {logging_mixin.py:112} INFO - [2020-11-05 15:05:56,570] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:56,688] {logging_mixin.py:112} INFO - [2020-11-05 15:05:56,687] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:05:56,689] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:05:56,710] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-05 15:06:09,784] {scheduler_job.py:155} INFO - Started process (PID=3657) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:09,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:06:09,787] {logging_mixin.py:112} INFO - [2020-11-05 15:06:09,787] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:09,903] {logging_mixin.py:112} INFO - [2020-11-05 15:06:09,902] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:06:09,903] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:09,925] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 15:06:23,034] {scheduler_job.py:155} INFO - Started process (PID=3719) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:23,039] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:06:23,040] {logging_mixin.py:112} INFO - [2020-11-05 15:06:23,040] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:23,194] {logging_mixin.py:112} INFO - [2020-11-05 15:06:23,193] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:06:23,195] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:23,231] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-05 15:06:36,395] {scheduler_job.py:155} INFO - Started process (PID=3780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:36,402] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:06:36,403] {logging_mixin.py:112} INFO - [2020-11-05 15:06:36,403] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:36,503] {logging_mixin.py:112} INFO - [2020-11-05 15:06:36,503] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:06:36,504] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:36,525] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-05 15:06:49,640] {scheduler_job.py:155} INFO - Started process (PID=3851) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:49,644] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:06:49,648] {logging_mixin.py:112} INFO - [2020-11-05 15:06:49,647] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:49,840] {logging_mixin.py:112} INFO - [2020-11-05 15:06:49,839] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:06:49,841] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:06:49,878] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.238 seconds
[2020-11-05 15:07:02,849] {scheduler_job.py:155} INFO - Started process (PID=3914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:02,854] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:07:02,855] {logging_mixin.py:112} INFO - [2020-11-05 15:07:02,854] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:02,952] {logging_mixin.py:112} INFO - [2020-11-05 15:07:02,951] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:07:02,953] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:02,975] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 15:07:16,112] {scheduler_job.py:155} INFO - Started process (PID=3975) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:16,115] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:07:16,116] {logging_mixin.py:112} INFO - [2020-11-05 15:07:16,115] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:16,218] {logging_mixin.py:112} INFO - [2020-11-05 15:07:16,217] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:07:16,218] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:16,241] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.129 seconds
[2020-11-05 15:07:29,311] {scheduler_job.py:155} INFO - Started process (PID=4040) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:29,315] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:07:29,316] {logging_mixin.py:112} INFO - [2020-11-05 15:07:29,316] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:29,423] {logging_mixin.py:112} INFO - [2020-11-05 15:07:29,422] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:07:29,424] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:29,454] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 15:07:42,640] {scheduler_job.py:155} INFO - Started process (PID=4119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:42,654] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:07:42,654] {logging_mixin.py:112} INFO - [2020-11-05 15:07:42,654] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:42,873] {logging_mixin.py:112} INFO - [2020-11-05 15:07:42,871] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:07:42,875] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:42,906] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.266 seconds
[2020-11-05 15:07:55,897] {scheduler_job.py:155} INFO - Started process (PID=4197) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:55,914] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:07:55,915] {logging_mixin.py:112} INFO - [2020-11-05 15:07:55,914] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:56,225] {logging_mixin.py:112} INFO - [2020-11-05 15:07:56,224] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:07:56,225] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:07:56,255] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.358 seconds
[2020-11-05 15:08:09,126] {scheduler_job.py:155} INFO - Started process (PID=4254) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:09,137] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:08:09,137] {logging_mixin.py:112} INFO - [2020-11-05 15:08:09,137] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:09,338] {logging_mixin.py:112} INFO - [2020-11-05 15:08:09,336] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:08:09,338] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:09,388] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.262 seconds
[2020-11-05 15:08:22,349] {scheduler_job.py:155} INFO - Started process (PID=4317) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:22,354] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:08:22,355] {logging_mixin.py:112} INFO - [2020-11-05 15:08:22,355] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:22,514] {logging_mixin.py:112} INFO - [2020-11-05 15:08:22,513] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:08:22,514] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:22,553] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-05 15:08:35,545] {scheduler_job.py:155} INFO - Started process (PID=4376) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:35,549] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:08:35,550] {logging_mixin.py:112} INFO - [2020-11-05 15:08:35,550] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:35,684] {logging_mixin.py:112} INFO - [2020-11-05 15:08:35,683] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:08:35,685] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:35,725] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 15:08:48,796] {scheduler_job.py:155} INFO - Started process (PID=4433) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:48,799] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:08:48,800] {logging_mixin.py:112} INFO - [2020-11-05 15:08:48,800] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:48,901] {logging_mixin.py:112} INFO - [2020-11-05 15:08:48,900] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:08:48,901] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:08:48,924] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 15:09:02,018] {scheduler_job.py:155} INFO - Started process (PID=4497) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:02,026] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:09:02,029] {logging_mixin.py:112} INFO - [2020-11-05 15:09:02,029] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:02,286] {logging_mixin.py:112} INFO - [2020-11-05 15:09:02,284] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:09:02,286] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:02,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.311 seconds
[2020-11-05 15:09:15,305] {scheduler_job.py:155} INFO - Started process (PID=4557) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:15,309] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:09:15,310] {logging_mixin.py:112} INFO - [2020-11-05 15:09:15,310] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:15,487] {logging_mixin.py:112} INFO - [2020-11-05 15:09:15,486] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:09:15,487] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:15,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.218 seconds
[2020-11-05 15:09:28,530] {scheduler_job.py:155} INFO - Started process (PID=4616) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:28,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:09:28,537] {logging_mixin.py:112} INFO - [2020-11-05 15:09:28,536] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:28,645] {logging_mixin.py:112} INFO - [2020-11-05 15:09:28,644] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:09:28,646] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:28,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-05 15:09:41,761] {scheduler_job.py:155} INFO - Started process (PID=4673) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:41,764] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:09:41,765] {logging_mixin.py:112} INFO - [2020-11-05 15:09:41,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:41,887] {logging_mixin.py:112} INFO - [2020-11-05 15:09:41,886] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import smulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'smulate'
[2020-11-05 15:09:41,887] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:41,922] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 15:09:54,959] {scheduler_job.py:155} INFO - Started process (PID=4740) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:54,962] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:09:54,963] {logging_mixin.py:112} INFO - [2020-11-05 15:09:54,962] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:55,073] {logging_mixin.py:112} INFO - [2020-11-05 15:09:55,072] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import smulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'smulate'
[2020-11-05 15:09:55,073] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:09:55,096] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 15:10:08,161] {scheduler_job.py:155} INFO - Started process (PID=4813) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:08,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:10:08,164] {logging_mixin.py:112} INFO - [2020-11-05 15:10:08,164] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:08,286] {logging_mixin.py:112} INFO - [2020-11-05 15:10:08,285] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:10:08,287] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:08,318] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 15:10:21,423] {scheduler_job.py:155} INFO - Started process (PID=4876) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:21,427] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:10:21,427] {logging_mixin.py:112} INFO - [2020-11-05 15:10:21,427] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:21,560] {logging_mixin.py:112} INFO - [2020-11-05 15:10:21,559] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:10:21,560] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:21,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 15:10:34,647] {scheduler_job.py:155} INFO - Started process (PID=4937) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:34,650] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:10:34,651] {logging_mixin.py:112} INFO - [2020-11-05 15:10:34,651] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:34,748] {logging_mixin.py:112} INFO - [2020-11-05 15:10:34,747] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:10:34,748] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:34,773] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-05 15:10:47,867] {scheduler_job.py:155} INFO - Started process (PID=4999) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:47,872] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:10:47,873] {logging_mixin.py:112} INFO - [2020-11-05 15:10:47,873] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:48,036] {logging_mixin.py:112} INFO - [2020-11-05 15:10:48,035] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:10:48,037] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:10:48,074] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.207 seconds
[2020-11-05 15:11:01,086] {scheduler_job.py:155} INFO - Started process (PID=5057) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:01,090] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:11:01,091] {logging_mixin.py:112} INFO - [2020-11-05 15:11:01,090] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:01,215] {logging_mixin.py:112} INFO - [2020-11-05 15:11:01,213] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:11:01,215] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:01,242] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 15:11:14,305] {scheduler_job.py:155} INFO - Started process (PID=5120) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:14,308] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:11:14,309] {logging_mixin.py:112} INFO - [2020-11-05 15:11:14,309] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:14,422] {logging_mixin.py:112} INFO - [2020-11-05 15:11:14,420] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:11:14,422] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:14,462] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 15:11:27,488] {scheduler_job.py:155} INFO - Started process (PID=5181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:27,493] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:11:27,493] {logging_mixin.py:112} INFO - [2020-11-05 15:11:27,493] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:27,602] {logging_mixin.py:112} INFO - [2020-11-05 15:11:27,601] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:11:27,602] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:27,623] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 15:11:40,667] {scheduler_job.py:155} INFO - Started process (PID=5240) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:40,671] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:11:40,672] {logging_mixin.py:112} INFO - [2020-11-05 15:11:40,672] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:40,765] {logging_mixin.py:112} INFO - [2020-11-05 15:11:40,763] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:11:40,766] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:40,792] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 15:11:53,904] {scheduler_job.py:155} INFO - Started process (PID=5299) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:53,911] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:11:53,918] {logging_mixin.py:112} INFO - [2020-11-05 15:11:53,918] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:54,048] {logging_mixin.py:112} INFO - [2020-11-05 15:11:54,045] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:11:54,048] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:11:54,074] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 15:12:07,147] {scheduler_job.py:155} INFO - Started process (PID=5362) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:07,150] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:12:07,151] {logging_mixin.py:112} INFO - [2020-11-05 15:12:07,150] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:07,244] {logging_mixin.py:112} INFO - [2020-11-05 15:12:07,243] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:12:07,245] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:07,280] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 15:12:20,582] {scheduler_job.py:155} INFO - Started process (PID=5422) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:20,588] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:12:20,590] {logging_mixin.py:112} INFO - [2020-11-05 15:12:20,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:20,698] {logging_mixin.py:112} INFO - [2020-11-05 15:12:20,698] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:12:20,699] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:20,723] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 15:12:33,759] {scheduler_job.py:155} INFO - Started process (PID=5484) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:33,763] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:12:33,764] {logging_mixin.py:112} INFO - [2020-11-05 15:12:33,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:33,953] {logging_mixin.py:112} INFO - [2020-11-05 15:12:33,952] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:12:33,954] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:33,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 15:12:47,007] {scheduler_job.py:155} INFO - Started process (PID=5544) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:47,019] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:12:47,019] {logging_mixin.py:112} INFO - [2020-11-05 15:12:47,019] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:47,244] {logging_mixin.py:112} INFO - [2020-11-05 15:12:47,242] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:12:47,247] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:12:47,292] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.285 seconds
[2020-11-05 15:13:00,219] {scheduler_job.py:155} INFO - Started process (PID=5613) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:00,225] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:13:00,226] {logging_mixin.py:112} INFO - [2020-11-05 15:13:00,226] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:00,352] {logging_mixin.py:112} INFO - [2020-11-05 15:13:00,351] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:13:00,352] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:00,383] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-05 15:13:13,416] {scheduler_job.py:155} INFO - Started process (PID=5679) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:13,420] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:13:13,421] {logging_mixin.py:112} INFO - [2020-11-05 15:13:13,420] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:13,525] {logging_mixin.py:112} INFO - [2020-11-05 15:13:13,523] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:13:13,526] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:13,550] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 15:13:26,652] {scheduler_job.py:155} INFO - Started process (PID=5743) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:26,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:13:26,659] {logging_mixin.py:112} INFO - [2020-11-05 15:13:26,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:26,783] {logging_mixin.py:112} INFO - [2020-11-05 15:13:26,782] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:13:26,784] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:26,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 15:13:39,845] {scheduler_job.py:155} INFO - Started process (PID=5814) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:39,848] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:13:39,848] {logging_mixin.py:112} INFO - [2020-11-05 15:13:39,848] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:39,966] {logging_mixin.py:112} INFO - [2020-11-05 15:13:39,965] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:13:39,966] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:39,995] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 15:13:53,098] {scheduler_job.py:155} INFO - Started process (PID=5883) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:53,102] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:13:53,103] {logging_mixin.py:112} INFO - [2020-11-05 15:13:53,102] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:53,208] {logging_mixin.py:112} INFO - [2020-11-05 15:13:53,207] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:13:53,208] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:13:53,234] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 15:14:06,344] {scheduler_job.py:155} INFO - Started process (PID=5948) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:06,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:14:06,349] {logging_mixin.py:112} INFO - [2020-11-05 15:14:06,349] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:06,464] {logging_mixin.py:112} INFO - [2020-11-05 15:14:06,464] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:14:06,465] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:06,487] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-05 15:14:19,570] {scheduler_job.py:155} INFO - Started process (PID=6011) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:19,573] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:14:19,574] {logging_mixin.py:112} INFO - [2020-11-05 15:14:19,574] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:19,674] {logging_mixin.py:112} INFO - [2020-11-05 15:14:19,673] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:14:19,674] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:19,701] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 15:14:32,777] {scheduler_job.py:155} INFO - Started process (PID=6080) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:32,784] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:14:32,785] {logging_mixin.py:112} INFO - [2020-11-05 15:14:32,785] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:32,895] {logging_mixin.py:112} INFO - [2020-11-05 15:14:32,894] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:14:32,895] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:32,927] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 15:14:46,049] {scheduler_job.py:155} INFO - Started process (PID=6137) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:46,054] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:14:46,054] {logging_mixin.py:112} INFO - [2020-11-05 15:14:46,054] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:46,173] {logging_mixin.py:112} INFO - [2020-11-05 15:14:46,171] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:14:46,174] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:46,230] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 15:14:59,315] {scheduler_job.py:155} INFO - Started process (PID=6205) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:59,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:14:59,320] {logging_mixin.py:112} INFO - [2020-11-05 15:14:59,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:59,496] {logging_mixin.py:112} INFO - [2020-11-05 15:14:59,495] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:14:59,496] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:14:59,533] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.219 seconds
[2020-11-05 15:15:12,513] {scheduler_job.py:155} INFO - Started process (PID=6267) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:12,527] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:15:12,528] {logging_mixin.py:112} INFO - [2020-11-05 15:15:12,527] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:12,665] {logging_mixin.py:112} INFO - [2020-11-05 15:15:12,664] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:15:12,665] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:12,704] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 15:15:25,790] {scheduler_job.py:155} INFO - Started process (PID=6346) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:25,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:15:25,795] {logging_mixin.py:112} INFO - [2020-11-05 15:15:25,795] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:25,914] {logging_mixin.py:112} INFO - [2020-11-05 15:15:25,913] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:15:25,914] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:25,941] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 15:15:39,016] {scheduler_job.py:155} INFO - Started process (PID=6408) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:39,018] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:15:39,019] {logging_mixin.py:112} INFO - [2020-11-05 15:15:39,019] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:39,129] {logging_mixin.py:112} INFO - [2020-11-05 15:15:39,128] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:15:39,129] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:39,152] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 15:15:52,244] {scheduler_job.py:155} INFO - Started process (PID=6472) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:52,252] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:15:52,258] {logging_mixin.py:112} INFO - [2020-11-05 15:15:52,255] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:52,419] {logging_mixin.py:112} INFO - [2020-11-05 15:15:52,418] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:15:52,419] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:15:52,443] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 15:16:05,485] {scheduler_job.py:155} INFO - Started process (PID=6537) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:05,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:16:05,492] {logging_mixin.py:112} INFO - [2020-11-05 15:16:05,491] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:05,659] {logging_mixin.py:112} INFO - [2020-11-05 15:16:05,657] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:16:05,659] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:05,694] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.209 seconds
[2020-11-05 15:16:18,755] {scheduler_job.py:155} INFO - Started process (PID=6603) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:18,759] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:16:18,759] {logging_mixin.py:112} INFO - [2020-11-05 15:16:18,759] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:18,869] {logging_mixin.py:112} INFO - [2020-11-05 15:16:18,869] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:16:18,870] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:18,893] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 15:16:31,987] {scheduler_job.py:155} INFO - Started process (PID=6663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:31,991] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:16:31,992] {logging_mixin.py:112} INFO - [2020-11-05 15:16:31,992] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:32,084] {logging_mixin.py:112} INFO - [2020-11-05 15:16:32,083] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:16:32,085] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:32,116] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 15:16:45,215] {scheduler_job.py:155} INFO - Started process (PID=6724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:45,218] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:16:45,219] {logging_mixin.py:112} INFO - [2020-11-05 15:16:45,218] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:45,397] {logging_mixin.py:112} INFO - [2020-11-05 15:16:45,396] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:16:45,398] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:45,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.215 seconds
[2020-11-05 15:16:58,503] {scheduler_job.py:155} INFO - Started process (PID=6786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:58,508] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:16:58,509] {logging_mixin.py:112} INFO - [2020-11-05 15:16:58,509] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:58,629] {logging_mixin.py:112} INFO - [2020-11-05 15:16:58,628] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:16:58,629] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:16:58,659] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 15:17:11,706] {scheduler_job.py:155} INFO - Started process (PID=6850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:11,710] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:17:11,710] {logging_mixin.py:112} INFO - [2020-11-05 15:17:11,710] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:11,816] {logging_mixin.py:112} INFO - [2020-11-05 15:17:11,815] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:17:11,817] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:11,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 15:17:24,919] {scheduler_job.py:155} INFO - Started process (PID=6912) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:24,925] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:17:24,928] {logging_mixin.py:112} INFO - [2020-11-05 15:17:24,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:25,037] {logging_mixin.py:112} INFO - [2020-11-05 15:17:25,036] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:17:25,037] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:25,063] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 15:17:38,110] {scheduler_job.py:155} INFO - Started process (PID=6974) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:38,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:17:38,118] {logging_mixin.py:112} INFO - [2020-11-05 15:17:38,118] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:38,221] {logging_mixin.py:112} INFO - [2020-11-05 15:17:38,220] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:17:38,221] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:38,245] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 15:17:51,361] {scheduler_job.py:155} INFO - Started process (PID=7043) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:51,384] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:17:51,385] {logging_mixin.py:112} INFO - [2020-11-05 15:17:51,385] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:51,585] {logging_mixin.py:112} INFO - [2020-11-05 15:17:51,583] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:17:51,585] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:17:51,632] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.271 seconds
[2020-11-05 15:18:04,555] {scheduler_job.py:155} INFO - Started process (PID=7106) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:04,558] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:18:04,559] {logging_mixin.py:112} INFO - [2020-11-05 15:18:04,558] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:04,670] {logging_mixin.py:112} INFO - [2020-11-05 15:18:04,669] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:18:04,670] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:04,695] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 15:18:17,757] {scheduler_job.py:155} INFO - Started process (PID=7169) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:17,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:18:17,763] {logging_mixin.py:112} INFO - [2020-11-05 15:18:17,762] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:17,895] {logging_mixin.py:112} INFO - [2020-11-05 15:18:17,894] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:18:17,896] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:17,916] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 15:18:31,032] {scheduler_job.py:155} INFO - Started process (PID=7231) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:31,056] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:18:31,057] {logging_mixin.py:112} INFO - [2020-11-05 15:18:31,056] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:31,182] {logging_mixin.py:112} INFO - [2020-11-05 15:18:31,181] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:18:31,182] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:31,206] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 15:18:44,209] {scheduler_job.py:155} INFO - Started process (PID=7297) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:44,214] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:18:44,214] {logging_mixin.py:112} INFO - [2020-11-05 15:18:44,214] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:44,343] {logging_mixin.py:112} INFO - [2020-11-05 15:18:44,341] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:18:44,343] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:44,373] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 15:18:57,447] {scheduler_job.py:155} INFO - Started process (PID=7365) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:57,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:18:57,452] {logging_mixin.py:112} INFO - [2020-11-05 15:18:57,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:57,656] {logging_mixin.py:112} INFO - [2020-11-05 15:18:57,655] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:18:57,656] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:18:57,678] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-05 15:19:10,693] {scheduler_job.py:155} INFO - Started process (PID=7430) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:10,697] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:19:10,698] {logging_mixin.py:112} INFO - [2020-11-05 15:19:10,698] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:10,798] {logging_mixin.py:112} INFO - [2020-11-05 15:19:10,797] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:19:10,798] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:10,822] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.129 seconds
[2020-11-05 15:19:23,946] {scheduler_job.py:155} INFO - Started process (PID=7491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:23,950] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:19:23,950] {logging_mixin.py:112} INFO - [2020-11-05 15:19:23,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:24,062] {logging_mixin.py:112} INFO - [2020-11-05 15:19:24,061] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:19:24,062] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:24,084] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-05 15:19:37,138] {scheduler_job.py:155} INFO - Started process (PID=7553) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:37,147] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:19:37,148] {logging_mixin.py:112} INFO - [2020-11-05 15:19:37,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:37,313] {logging_mixin.py:112} INFO - [2020-11-05 15:19:37,312] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:19:37,314] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:37,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 15:19:50,339] {scheduler_job.py:155} INFO - Started process (PID=7615) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:50,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:19:50,349] {logging_mixin.py:112} INFO - [2020-11-05 15:19:50,349] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:50,437] {logging_mixin.py:112} INFO - [2020-11-05 15:19:50,436] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:19:50,438] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:19:50,467] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 15:20:03,572] {scheduler_job.py:155} INFO - Started process (PID=7677) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:03,575] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:20:03,575] {logging_mixin.py:112} INFO - [2020-11-05 15:20:03,575] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:03,681] {logging_mixin.py:112} INFO - [2020-11-05 15:20:03,675] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:20:03,681] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:03,722] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 15:20:16,792] {scheduler_job.py:155} INFO - Started process (PID=7735) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:16,799] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:20:16,799] {logging_mixin.py:112} INFO - [2020-11-05 15:20:16,799] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:16,910] {logging_mixin.py:112} INFO - [2020-11-05 15:20:16,909] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:20:16,911] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:16,944] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-05 15:20:30,026] {scheduler_job.py:155} INFO - Started process (PID=7798) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:30,031] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:20:30,032] {logging_mixin.py:112} INFO - [2020-11-05 15:20:30,032] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:30,160] {logging_mixin.py:112} INFO - [2020-11-05 15:20:30,159] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:20:30,160] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:30,186] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 15:20:43,222] {scheduler_job.py:155} INFO - Started process (PID=7855) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:43,227] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:20:43,228] {logging_mixin.py:112} INFO - [2020-11-05 15:20:43,227] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:43,350] {logging_mixin.py:112} INFO - [2020-11-05 15:20:43,340] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:20:43,350] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:43,385] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 15:20:56,487] {scheduler_job.py:155} INFO - Started process (PID=7916) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:56,495] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:20:56,496] {logging_mixin.py:112} INFO - [2020-11-05 15:20:56,496] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:56,632] {logging_mixin.py:112} INFO - [2020-11-05 15:20:56,632] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:20:56,633] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:20:56,656] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-05 15:21:09,701] {scheduler_job.py:155} INFO - Started process (PID=7975) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:09,706] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:21:09,707] {logging_mixin.py:112} INFO - [2020-11-05 15:21:09,706] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:09,843] {logging_mixin.py:112} INFO - [2020-11-05 15:21:09,842] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:21:09,843] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:09,879] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 15:21:22,946] {scheduler_job.py:155} INFO - Started process (PID=8036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:22,950] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:21:22,950] {logging_mixin.py:112} INFO - [2020-11-05 15:21:22,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:23,076] {logging_mixin.py:112} INFO - [2020-11-05 15:21:23,075] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:21:23,076] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:23,106] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 15:21:36,189] {scheduler_job.py:155} INFO - Started process (PID=8097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:36,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:21:36,201] {logging_mixin.py:112} INFO - [2020-11-05 15:21:36,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:36,336] {logging_mixin.py:112} INFO - [2020-11-05 15:21:36,335] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:21:36,336] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:36,367] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 15:21:49,397] {scheduler_job.py:155} INFO - Started process (PID=8157) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:49,401] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:21:49,402] {logging_mixin.py:112} INFO - [2020-11-05 15:21:49,402] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:49,511] {logging_mixin.py:112} INFO - [2020-11-05 15:21:49,510] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:21:49,511] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:21:49,538] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 15:22:02,633] {scheduler_job.py:155} INFO - Started process (PID=8237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:02,637] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:22:02,638] {logging_mixin.py:112} INFO - [2020-11-05 15:22:02,637] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:02,787] {logging_mixin.py:112} INFO - [2020-11-05 15:22:02,786] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:22:02,787] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:02,815] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-05 15:22:15,867] {scheduler_job.py:155} INFO - Started process (PID=8307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:15,871] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:22:15,871] {logging_mixin.py:112} INFO - [2020-11-05 15:22:15,871] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:15,974] {logging_mixin.py:112} INFO - [2020-11-05 15:22:15,972] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:22:15,974] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:16,010] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-05 15:22:29,129] {scheduler_job.py:155} INFO - Started process (PID=8371) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:29,132] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:22:29,134] {logging_mixin.py:112} INFO - [2020-11-05 15:22:29,132] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:29,261] {logging_mixin.py:112} INFO - [2020-11-05 15:22:29,257] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:22:29,261] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:29,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 15:22:42,301] {scheduler_job.py:155} INFO - Started process (PID=8430) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:42,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:22:42,306] {logging_mixin.py:112} INFO - [2020-11-05 15:22:42,306] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:42,430] {logging_mixin.py:112} INFO - [2020-11-05 15:22:42,428] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:22:42,430] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:42,461] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 15:22:55,615] {scheduler_job.py:155} INFO - Started process (PID=8491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:55,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:22:55,620] {logging_mixin.py:112} INFO - [2020-11-05 15:22:55,620] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:55,733] {logging_mixin.py:112} INFO - [2020-11-05 15:22:55,732] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:22:55,734] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:22:55,757] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 15:23:08,887] {scheduler_job.py:155} INFO - Started process (PID=8556) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:08,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:23:08,894] {logging_mixin.py:112} INFO - [2020-11-05 15:23:08,894] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:09,012] {logging_mixin.py:112} INFO - [2020-11-05 15:23:09,011] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:23:09,014] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:09,040] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.153 seconds
[2020-11-05 15:23:22,082] {scheduler_job.py:155} INFO - Started process (PID=8619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:22,088] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:23:22,089] {logging_mixin.py:112} INFO - [2020-11-05 15:23:22,089] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:22,183] {logging_mixin.py:112} INFO - [2020-11-05 15:23:22,183] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:23:22,184] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:22,207] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 15:23:35,367] {scheduler_job.py:155} INFO - Started process (PID=8680) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:35,375] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:23:35,376] {logging_mixin.py:112} INFO - [2020-11-05 15:23:35,376] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:35,508] {logging_mixin.py:112} INFO - [2020-11-05 15:23:35,507] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:23:35,508] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:35,534] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-05 15:23:48,744] {scheduler_job.py:155} INFO - Started process (PID=8742) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:48,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:23:48,753] {logging_mixin.py:112} INFO - [2020-11-05 15:23:48,752] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:48,886] {logging_mixin.py:112} INFO - [2020-11-05 15:23:48,885] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:23:48,887] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:23:48,910] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.166 seconds
[2020-11-05 15:24:01,999] {scheduler_job.py:155} INFO - Started process (PID=8807) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:02,004] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:24:02,018] {logging_mixin.py:112} INFO - [2020-11-05 15:24:02,017] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:02,223] {logging_mixin.py:112} INFO - [2020-11-05 15:24:02,222] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:24:02,224] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:02,263] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.263 seconds
[2020-11-05 15:24:15,196] {scheduler_job.py:155} INFO - Started process (PID=8866) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:15,200] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:24:15,201] {logging_mixin.py:112} INFO - [2020-11-05 15:24:15,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:15,303] {logging_mixin.py:112} INFO - [2020-11-05 15:24:15,303] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:24:15,304] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:15,341] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 15:24:28,441] {scheduler_job.py:155} INFO - Started process (PID=8927) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:28,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:24:28,446] {logging_mixin.py:112} INFO - [2020-11-05 15:24:28,446] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:28,569] {logging_mixin.py:112} INFO - [2020-11-05 15:24:28,568] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:24:28,569] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:28,603] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 15:24:41,693] {scheduler_job.py:155} INFO - Started process (PID=8990) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:41,698] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:24:41,699] {logging_mixin.py:112} INFO - [2020-11-05 15:24:41,698] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:41,827] {logging_mixin.py:112} INFO - [2020-11-05 15:24:41,826] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:24:41,827] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:41,853] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 15:24:54,923] {scheduler_job.py:155} INFO - Started process (PID=9050) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:54,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:24:54,926] {logging_mixin.py:112} INFO - [2020-11-05 15:24:54,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:55,039] {logging_mixin.py:112} INFO - [2020-11-05 15:24:55,038] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:24:55,039] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:24:55,071] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 15:25:08,209] {scheduler_job.py:155} INFO - Started process (PID=9115) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:08,213] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:25:08,214] {logging_mixin.py:112} INFO - [2020-11-05 15:25:08,214] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:08,324] {logging_mixin.py:112} INFO - [2020-11-05 15:25:08,323] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:25:08,324] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:08,356] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 15:25:21,416] {scheduler_job.py:155} INFO - Started process (PID=9175) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:21,423] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:25:21,431] {logging_mixin.py:112} INFO - [2020-11-05 15:25:21,430] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:21,596] {logging_mixin.py:112} INFO - [2020-11-05 15:25:21,595] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:25:21,596] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:21,618] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 15:25:34,621] {scheduler_job.py:155} INFO - Started process (PID=9243) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:34,624] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:25:34,625] {logging_mixin.py:112} INFO - [2020-11-05 15:25:34,625] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:34,778] {logging_mixin.py:112} INFO - [2020-11-05 15:25:34,777] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:25:34,778] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:34,805] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.184 seconds
[2020-11-05 15:25:47,830] {scheduler_job.py:155} INFO - Started process (PID=9305) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:47,833] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:25:47,834] {logging_mixin.py:112} INFO - [2020-11-05 15:25:47,834] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:47,954] {logging_mixin.py:112} INFO - [2020-11-05 15:25:47,952] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:25:47,954] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:25:47,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 15:26:01,074] {scheduler_job.py:155} INFO - Started process (PID=9366) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:01,077] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:26:01,077] {logging_mixin.py:112} INFO - [2020-11-05 15:26:01,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:01,187] {logging_mixin.py:112} INFO - [2020-11-05 15:26:01,185] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:26:01,187] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:01,223] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 15:26:14,302] {scheduler_job.py:155} INFO - Started process (PID=9425) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:14,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:26:14,306] {logging_mixin.py:112} INFO - [2020-11-05 15:26:14,306] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:14,426] {logging_mixin.py:112} INFO - [2020-11-05 15:26:14,425] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:26:14,426] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:14,467] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-05 15:26:27,547] {scheduler_job.py:155} INFO - Started process (PID=9490) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:27,555] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:26:27,555] {logging_mixin.py:112} INFO - [2020-11-05 15:26:27,555] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:27,581] {logging_mixin.py:112} INFO - [2020-11-05 15:26:27,578] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 60
    bucket_key_template = 's3://{bucket_name}/{source}/{}.csv'.
                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:26:27,581] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:27,654] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.108 seconds
[2020-11-05 15:26:40,721] {scheduler_job.py:155} INFO - Started process (PID=9552) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:40,724] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:26:40,724] {logging_mixin.py:112} INFO - [2020-11-05 15:26:40,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:40,729] {logging_mixin.py:112} INFO - [2020-11-05 15:26:40,727] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 60
    bucket_key_template = 's3://{bucket_name}/{source}/{}.csv'.
                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:26:40,729] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:40,767] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.047 seconds
[2020-11-05 15:26:53,869] {scheduler_job.py:155} INFO - Started process (PID=9615) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:53,872] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:26:53,873] {logging_mixin.py:112} INFO - [2020-11-05 15:26:53,873] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:53,884] {logging_mixin.py:112} INFO - [2020-11-05 15:26:53,882] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 60
    bucket_key_template = 's3://{bucket_name}/{source}/{}.csv'.
                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:26:53,884] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:26:53,917] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.048 seconds
[2020-11-05 15:27:07,131] {scheduler_job.py:155} INFO - Started process (PID=9676) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:07,137] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:27:07,138] {logging_mixin.py:112} INFO - [2020-11-05 15:27:07,137] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:07,145] {logging_mixin.py:112} INFO - [2020-11-05 15:27:07,142] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 60
    bucket_key_template = 's3://{bucket_name}/{source}/{}.csv'.
                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:27:07,146] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:07,179] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.048 seconds
[2020-11-05 15:27:20,292] {scheduler_job.py:155} INFO - Started process (PID=9739) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:20,297] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:27:20,299] {logging_mixin.py:112} INFO - [2020-11-05 15:27:20,297] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:20,303] {logging_mixin.py:112} INFO - [2020-11-05 15:27:20,301] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 60
    bucket_key_template = 's3://{bucket_name}/{source}/{}.csv'.
                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:27:20,303] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:20,338] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.046 seconds
[2020-11-05 15:27:33,553] {scheduler_job.py:155} INFO - Started process (PID=9801) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:33,557] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:27:33,557] {logging_mixin.py:112} INFO - [2020-11-05 15:27:33,557] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:33,561] {logging_mixin.py:112} INFO - [2020-11-05 15:27:33,560] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 60
    bucket_key_template = 's3://{bucket_name}/{source}/{}.csv'.
                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:27:33,561] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:33,588] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.035 seconds
[2020-11-05 15:27:46,803] {scheduler_job.py:155} INFO - Started process (PID=9861) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:46,807] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:27:46,807] {logging_mixin.py:112} INFO - [2020-11-05 15:27:46,807] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:46,811] {logging_mixin.py:112} INFO - [2020-11-05 15:27:46,810] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/*'.
                                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:27:46,812] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:27:46,847] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.045 seconds
[2020-11-05 15:28:00,029] {scheduler_job.py:155} INFO - Started process (PID=9924) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:00,033] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:28:00,034] {logging_mixin.py:112} INFO - [2020-11-05 15:28:00,033] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:00,038] {logging_mixin.py:112} INFO - [2020-11-05 15:28:00,037] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/*'.
                                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:28:00,040] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:00,090] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.061 seconds
[2020-11-05 15:28:13,224] {scheduler_job.py:155} INFO - Started process (PID=9988) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:13,229] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:28:13,230] {logging_mixin.py:112} INFO - [2020-11-05 15:28:13,230] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:13,235] {logging_mixin.py:112} INFO - [2020-11-05 15:28:13,234] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/*'.
                                                                              ^
SyntaxError: invalid syntax
[2020-11-05 15:28:13,236] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:13,297] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.073 seconds
[2020-11-05 15:28:26,466] {scheduler_job.py:155} INFO - Started process (PID=10053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:26,471] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:28:26,472] {logging_mixin.py:112} INFO - [2020-11-05 15:28:26,472] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:26,477] {logging_mixin.py:112} INFO - [2020-11-05 15:28:26,475] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:28:26,477] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:26,535] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.069 seconds
[2020-11-05 15:28:39,710] {scheduler_job.py:155} INFO - Started process (PID=10118) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:39,716] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:28:39,718] {logging_mixin.py:112} INFO - [2020-11-05 15:28:39,717] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:39,722] {logging_mixin.py:112} INFO - [2020-11-05 15:28:39,720] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:28:39,722] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:39,748] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.038 seconds
[2020-11-05 15:28:52,891] {scheduler_job.py:155} INFO - Started process (PID=10181) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:52,896] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:28:52,896] {logging_mixin.py:112} INFO - [2020-11-05 15:28:52,896] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:52,904] {logging_mixin.py:112} INFO - [2020-11-05 15:28:52,903] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:28:52,905] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:28:52,948] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.056 seconds
[2020-11-05 15:29:06,160] {scheduler_job.py:155} INFO - Started process (PID=10247) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:06,165] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:29:06,167] {logging_mixin.py:112} INFO - [2020-11-05 15:29:06,167] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:06,170] {logging_mixin.py:112} INFO - [2020-11-05 15:29:06,169] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:29:06,171] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:06,197] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.037 seconds
[2020-11-05 15:29:19,379] {scheduler_job.py:155} INFO - Started process (PID=10304) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:19,382] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:29:19,383] {logging_mixin.py:112} INFO - [2020-11-05 15:29:19,383] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:19,387] {logging_mixin.py:112} INFO - [2020-11-05 15:29:19,386] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:29:19,388] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:19,420] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.041 seconds
[2020-11-05 15:29:32,683] {scheduler_job.py:155} INFO - Started process (PID=10363) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:32,686] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:29:32,687] {logging_mixin.py:112} INFO - [2020-11-05 15:29:32,687] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:32,691] {logging_mixin.py:112} INFO - [2020-11-05 15:29:32,690] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:29:32,692] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:32,717] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.035 seconds
[2020-11-05 15:29:45,858] {scheduler_job.py:155} INFO - Started process (PID=10425) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:45,867] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:29:45,870] {logging_mixin.py:112} INFO - [2020-11-05 15:29:45,868] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:45,878] {logging_mixin.py:112} INFO - [2020-11-05 15:29:45,877] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:29:45,879] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:45,909] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.051 seconds
[2020-11-05 15:29:59,071] {scheduler_job.py:155} INFO - Started process (PID=10486) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:59,074] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:29:59,074] {logging_mixin.py:112} INFO - [2020-11-05 15:29:59,074] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:59,080] {logging_mixin.py:112} INFO - [2020-11-05 15:29:59,077] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:29:59,082] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:29:59,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.043 seconds
[2020-11-05 15:30:12,274] {scheduler_job.py:155} INFO - Started process (PID=10545) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:12,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:30:12,280] {logging_mixin.py:112} INFO - [2020-11-05 15:30:12,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:12,284] {logging_mixin.py:112} INFO - [2020-11-05 15:30:12,283] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:30:12,285] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:12,313] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.039 seconds
[2020-11-05 15:30:25,496] {scheduler_job.py:155} INFO - Started process (PID=10607) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:25,499] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:30:25,499] {logging_mixin.py:112} INFO - [2020-11-05 15:30:25,499] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:25,502] {logging_mixin.py:112} INFO - [2020-11-05 15:30:25,501] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:30:25,503] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:25,537] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.041 seconds
[2020-11-05 15:30:38,875] {scheduler_job.py:155} INFO - Started process (PID=10675) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:38,882] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:30:38,884] {logging_mixin.py:112} INFO - [2020-11-05 15:30:38,883] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:38,891] {logging_mixin.py:112} INFO - [2020-11-05 15:30:38,889] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:30:38,892] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:38,920] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.046 seconds
[2020-11-05 15:30:52,079] {scheduler_job.py:155} INFO - Started process (PID=10734) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:52,083] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:30:52,084] {logging_mixin.py:112} INFO - [2020-11-05 15:30:52,084] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:52,088] {logging_mixin.py:112} INFO - [2020-11-05 15:30:52,087] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:30:52,089] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:30:52,129] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.049 seconds
[2020-11-05 15:31:05,532] {scheduler_job.py:155} INFO - Started process (PID=10798) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:05,536] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:31:05,537] {logging_mixin.py:112} INFO - [2020-11-05 15:31:05,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:05,541] {logging_mixin.py:112} INFO - [2020-11-05 15:31:05,539] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:31:05,541] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:05,574] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.042 seconds
[2020-11-05 15:31:18,696] {scheduler_job.py:155} INFO - Started process (PID=10858) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:18,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:31:18,703] {logging_mixin.py:112} INFO - [2020-11-05 15:31:18,703] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:18,708] {logging_mixin.py:112} INFO - [2020-11-05 15:31:18,707] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:31:18,708] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:18,732] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.036 seconds
[2020-11-05 15:31:31,896] {scheduler_job.py:155} INFO - Started process (PID=10921) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:31,899] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:31:31,899] {logging_mixin.py:112} INFO - [2020-11-05 15:31:31,899] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:31,903] {logging_mixin.py:112} INFO - [2020-11-05 15:31:31,902] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:31:31,903] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:31,928] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.032 seconds
[2020-11-05 15:31:45,163] {scheduler_job.py:155} INFO - Started process (PID=10987) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:45,167] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:31:45,168] {logging_mixin.py:112} INFO - [2020-11-05 15:31:45,168] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:45,173] {logging_mixin.py:112} INFO - [2020-11-05 15:31:45,171] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:31:45,174] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:45,218] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.055 seconds
[2020-11-05 15:31:58,374] {scheduler_job.py:155} INFO - Started process (PID=11050) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:58,386] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:31:58,387] {logging_mixin.py:112} INFO - [2020-11-05 15:31:58,387] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:58,392] {logging_mixin.py:112} INFO - [2020-11-05 15:31:58,390] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:31:58,392] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:31:58,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.045 seconds
[2020-11-05 15:32:11,604] {scheduler_job.py:155} INFO - Started process (PID=11113) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:11,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:32:11,609] {logging_mixin.py:112} INFO - [2020-11-05 15:32:11,609] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:11,613] {logging_mixin.py:112} INFO - [2020-11-05 15:32:11,612] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:32:11,613] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:11,638] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.034 seconds
[2020-11-05 15:32:24,811] {scheduler_job.py:155} INFO - Started process (PID=11174) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:24,814] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:32:24,815] {logging_mixin.py:112} INFO - [2020-11-05 15:32:24,815] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:24,819] {logging_mixin.py:112} INFO - [2020-11-05 15:32:24,818] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:32:24,820] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:24,851] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.040 seconds
[2020-11-05 15:32:38,025] {scheduler_job.py:155} INFO - Started process (PID=11238) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:38,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:32:38,029] {logging_mixin.py:112} INFO - [2020-11-05 15:32:38,029] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:38,032] {logging_mixin.py:112} INFO - [2020-11-05 15:32:38,031] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:32:38,033] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:38,054] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.030 seconds
[2020-11-05 15:32:51,230] {scheduler_job.py:155} INFO - Started process (PID=11301) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:51,233] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:32:51,234] {logging_mixin.py:112} INFO - [2020-11-05 15:32:51,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:51,238] {logging_mixin.py:112} INFO - [2020-11-05 15:32:51,237] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:32:51,239] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:32:51,277] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.048 seconds
[2020-11-05 15:33:04,469] {scheduler_job.py:155} INFO - Started process (PID=11364) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:04,471] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:33:04,472] {logging_mixin.py:112} INFO - [2020-11-05 15:33:04,472] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:04,475] {logging_mixin.py:112} INFO - [2020-11-05 15:33:04,474] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:33:04,475] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:04,507] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.038 seconds
[2020-11-05 15:33:17,676] {scheduler_job.py:155} INFO - Started process (PID=11429) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:17,681] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:33:17,683] {logging_mixin.py:112} INFO - [2020-11-05 15:33:17,682] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:17,689] {logging_mixin.py:112} INFO - [2020-11-05 15:33:17,686] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:33:17,689] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:17,716] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.040 seconds
[2020-11-05 15:33:30,870] {scheduler_job.py:155} INFO - Started process (PID=11491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:30,875] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:33:30,876] {logging_mixin.py:112} INFO - [2020-11-05 15:33:30,875] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:30,880] {logging_mixin.py:112} INFO - [2020-11-05 15:33:30,879] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:33:30,881] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:30,921] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.051 seconds
[2020-11-05 15:33:44,092] {scheduler_job.py:155} INFO - Started process (PID=11551) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:44,098] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:33:44,098] {logging_mixin.py:112} INFO - [2020-11-05 15:33:44,098] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:44,104] {logging_mixin.py:112} INFO - [2020-11-05 15:33:44,103] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:33:44,105] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:44,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.053 seconds
[2020-11-05 15:33:57,278] {scheduler_job.py:155} INFO - Started process (PID=11617) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:57,281] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:33:57,281] {logging_mixin.py:112} INFO - [2020-11-05 15:33:57,281] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:57,286] {logging_mixin.py:112} INFO - [2020-11-05 15:33:57,284] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:33:57,286] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:33:57,309] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.031 seconds
[2020-11-05 15:34:10,488] {scheduler_job.py:155} INFO - Started process (PID=11683) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:10,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:34:10,492] {logging_mixin.py:112} INFO - [2020-11-05 15:34:10,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:10,496] {logging_mixin.py:112} INFO - [2020-11-05 15:34:10,494] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:34:10,496] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:10,525] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.037 seconds
[2020-11-05 15:34:23,716] {scheduler_job.py:155} INFO - Started process (PID=11744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:23,720] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:34:23,721] {logging_mixin.py:112} INFO - [2020-11-05 15:34:23,720] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:23,726] {logging_mixin.py:112} INFO - [2020-11-05 15:34:23,724] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:34:23,726] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:23,757] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.041 seconds
[2020-11-05 15:34:36,968] {scheduler_job.py:155} INFO - Started process (PID=11805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:36,983] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:34:36,984] {logging_mixin.py:112} INFO - [2020-11-05 15:34:36,984] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:36,998] {logging_mixin.py:112} INFO - [2020-11-05 15:34:36,987] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:34:36,998] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:37,073] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.105 seconds
[2020-11-05 15:34:50,206] {scheduler_job.py:155} INFO - Started process (PID=11864) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:50,211] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:34:50,211] {logging_mixin.py:112} INFO - [2020-11-05 15:34:50,211] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:50,215] {logging_mixin.py:112} INFO - [2020-11-05 15:34:50,214] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:34:50,215] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:34:50,257] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.051 seconds
[2020-11-05 15:35:03,451] {scheduler_job.py:155} INFO - Started process (PID=11938) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:03,468] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:35:03,472] {logging_mixin.py:112} INFO - [2020-11-05 15:35:03,468] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:03,476] {logging_mixin.py:112} INFO - [2020-11-05 15:35:03,475] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:35:03,480] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:03,526] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.075 seconds
[2020-11-05 15:35:16,795] {scheduler_job.py:155} INFO - Started process (PID=12018) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:16,815] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:35:16,822] {logging_mixin.py:112} INFO - [2020-11-05 15:35:16,821] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:16,837] {logging_mixin.py:112} INFO - [2020-11-05 15:35:16,834] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:35:16,840] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:16,889] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.094 seconds
[2020-11-05 15:35:30,059] {scheduler_job.py:155} INFO - Started process (PID=12074) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:30,073] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:35:30,074] {logging_mixin.py:112} INFO - [2020-11-05 15:35:30,073] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:30,084] {logging_mixin.py:112} INFO - [2020-11-05 15:35:30,083] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:35:30,084] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:30,142] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.084 seconds
[2020-11-05 15:35:43,348] {scheduler_job.py:155} INFO - Started process (PID=12134) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:43,387] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:35:43,394] {logging_mixin.py:112} INFO - [2020-11-05 15:35:43,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:43,434] {logging_mixin.py:112} INFO - [2020-11-05 15:35:43,421] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:35:43,435] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:43,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 15:35:56,528] {scheduler_job.py:155} INFO - Started process (PID=12191) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:56,533] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:35:56,534] {logging_mixin.py:112} INFO - [2020-11-05 15:35:56,534] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:56,539] {logging_mixin.py:112} INFO - [2020-11-05 15:35:56,537] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:35:56,539] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:35:56,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.040 seconds
[2020-11-05 15:36:09,885] {scheduler_job.py:155} INFO - Started process (PID=12254) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:09,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:36:09,902] {logging_mixin.py:112} INFO - [2020-11-05 15:36:09,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:09,928] {logging_mixin.py:112} INFO - [2020-11-05 15:36:09,908] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:36:09,930] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:10,008] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.124 seconds
[2020-11-05 15:36:23,111] {scheduler_job.py:155} INFO - Started process (PID=12309) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:23,114] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:36:23,115] {logging_mixin.py:112} INFO - [2020-11-05 15:36:23,115] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:23,119] {logging_mixin.py:112} INFO - [2020-11-05 15:36:23,118] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:36:23,119] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:23,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.038 seconds
[2020-11-05 15:36:36,375] {scheduler_job.py:155} INFO - Started process (PID=12373) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:36,381] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:36:36,382] {logging_mixin.py:112} INFO - [2020-11-05 15:36:36,382] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:36,387] {logging_mixin.py:112} INFO - [2020-11-05 15:36:36,384] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:36:36,387] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:36,410] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.035 seconds
[2020-11-05 15:36:49,802] {scheduler_job.py:155} INFO - Started process (PID=12435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:49,827] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:36:49,828] {logging_mixin.py:112} INFO - [2020-11-05 15:36:49,827] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:49,851] {logging_mixin.py:112} INFO - [2020-11-05 15:36:49,848] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:36:49,852] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:36:49,901] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.100 seconds
[2020-11-05 15:37:03,005] {scheduler_job.py:155} INFO - Started process (PID=12499) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:03,009] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:37:03,010] {logging_mixin.py:112} INFO - [2020-11-05 15:37:03,010] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:03,020] {logging_mixin.py:112} INFO - [2020-11-05 15:37:03,016] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 61
    bucket_key_template = 's3://{bucket_name}/{source}/{year}/{month}/{day}/ypsource.json'.
                                                                                          ^
SyntaxError: invalid syntax
[2020-11-05 15:37:03,021] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:03,056] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.052 seconds
[2020-11-05 15:37:16,332] {scheduler_job.py:155} INFO - Started process (PID=12565) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:16,346] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:37:16,346] {logging_mixin.py:112} INFO - [2020-11-05 15:37:16,346] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:16,512] {logging_mixin.py:112} INFO - [2020-11-05 15:37:16,511] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:37:16,512] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:16,546] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 15:37:29,745] {scheduler_job.py:155} INFO - Started process (PID=12641) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:29,765] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:37:29,766] {logging_mixin.py:112} INFO - [2020-11-05 15:37:29,766] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:29,937] {logging_mixin.py:112} INFO - [2020-11-05 15:37:29,936] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:37:29,938] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:29,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.219 seconds
[2020-11-05 15:37:43,123] {scheduler_job.py:155} INFO - Started process (PID=12701) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:43,132] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:37:43,132] {logging_mixin.py:112} INFO - [2020-11-05 15:37:43,132] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:43,256] {logging_mixin.py:112} INFO - [2020-11-05 15:37:43,255] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:37:43,257] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:43,280] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 15:37:56,398] {scheduler_job.py:155} INFO - Started process (PID=12763) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:56,403] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:37:56,404] {logging_mixin.py:112} INFO - [2020-11-05 15:37:56,404] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:56,521] {logging_mixin.py:112} INFO - [2020-11-05 15:37:56,520] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:37:56,521] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:37:56,547] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 15:38:09,596] {scheduler_job.py:155} INFO - Started process (PID=12825) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:09,604] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:38:09,604] {logging_mixin.py:112} INFO - [2020-11-05 15:38:09,604] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:09,714] {logging_mixin.py:112} INFO - [2020-11-05 15:38:09,713] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:38:09,715] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:09,739] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 15:38:22,820] {scheduler_job.py:155} INFO - Started process (PID=12896) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:22,824] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:38:22,825] {logging_mixin.py:112} INFO - [2020-11-05 15:38:22,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:22,978] {logging_mixin.py:112} INFO - [2020-11-05 15:38:22,977] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:38:22,979] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:23,011] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 15:38:36,176] {scheduler_job.py:155} INFO - Started process (PID=12957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:36,186] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:38:36,193] {logging_mixin.py:112} INFO - [2020-11-05 15:38:36,193] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:36,399] {logging_mixin.py:112} INFO - [2020-11-05 15:38:36,398] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:38:36,401] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:36,440] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.264 seconds
[2020-11-05 15:38:49,390] {scheduler_job.py:155} INFO - Started process (PID=13030) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:49,394] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:38:49,396] {logging_mixin.py:112} INFO - [2020-11-05 15:38:49,395] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:49,698] {logging_mixin.py:112} INFO - [2020-11-05 15:38:49,697] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:38:49,698] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:38:49,743] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.353 seconds
[2020-11-05 15:39:02,719] {scheduler_job.py:155} INFO - Started process (PID=13114) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:02,730] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:39:02,740] {logging_mixin.py:112} INFO - [2020-11-05 15:39:02,739] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:02,963] {logging_mixin.py:112} INFO - [2020-11-05 15:39:02,962] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:39:02,963] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:02,984] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 15:39:15,983] {scheduler_job.py:155} INFO - Started process (PID=13190) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:15,990] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:39:15,991] {logging_mixin.py:112} INFO - [2020-11-05 15:39:15,991] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:16,186] {logging_mixin.py:112} INFO - [2020-11-05 15:39:16,185] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:39:16,187] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:16,220] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.237 seconds
[2020-11-05 15:39:29,363] {scheduler_job.py:155} INFO - Started process (PID=13267) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:29,373] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:39:29,374] {logging_mixin.py:112} INFO - [2020-11-05 15:39:29,373] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:29,531] {logging_mixin.py:112} INFO - [2020-11-05 15:39:29,529] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:39:29,531] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:29,572] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.209 seconds
[2020-11-05 15:39:42,655] {scheduler_job.py:155} INFO - Started process (PID=13326) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:42,674] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:39:42,675] {logging_mixin.py:112} INFO - [2020-11-05 15:39:42,674] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:42,824] {logging_mixin.py:112} INFO - [2020-11-05 15:39:42,823] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:39:42,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:42,848] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.193 seconds
[2020-11-05 15:39:55,843] {scheduler_job.py:155} INFO - Started process (PID=13386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:55,847] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:39:55,848] {logging_mixin.py:112} INFO - [2020-11-05 15:39:55,848] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:55,985] {logging_mixin.py:112} INFO - [2020-11-05 15:39:55,982] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:39:55,986] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:39:56,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 15:40:09,086] {scheduler_job.py:155} INFO - Started process (PID=13454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:09,091] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:40:09,092] {logging_mixin.py:112} INFO - [2020-11-05 15:40:09,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:09,394] {logging_mixin.py:112} INFO - [2020-11-05 15:40:09,389] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:40:09,394] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:09,441] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.355 seconds
[2020-11-05 15:40:22,306] {scheduler_job.py:155} INFO - Started process (PID=13511) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:22,310] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:40:22,311] {logging_mixin.py:112} INFO - [2020-11-05 15:40:22,310] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:22,402] {logging_mixin.py:112} INFO - [2020-11-05 15:40:22,399] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:40:22,402] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:22,435] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.129 seconds
[2020-11-05 15:40:35,498] {scheduler_job.py:155} INFO - Started process (PID=13573) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:35,502] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:40:35,502] {logging_mixin.py:112} INFO - [2020-11-05 15:40:35,502] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:35,601] {logging_mixin.py:112} INFO - [2020-11-05 15:40:35,600] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:40:35,602] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:35,625] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-05 15:40:48,701] {scheduler_job.py:155} INFO - Started process (PID=13633) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:48,706] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:40:48,707] {logging_mixin.py:112} INFO - [2020-11-05 15:40:48,706] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:48,824] {logging_mixin.py:112} INFO - [2020-11-05 15:40:48,823] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:40:48,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:40:48,858] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 15:41:01,976] {scheduler_job.py:155} INFO - Started process (PID=13696) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:01,987] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:41:01,991] {logging_mixin.py:112} INFO - [2020-11-05 15:41:01,991] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:02,130] {logging_mixin.py:112} INFO - [2020-11-05 15:41:02,130] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:41:02,131] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:02,174] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.198 seconds
[2020-11-05 15:41:15,222] {scheduler_job.py:155} INFO - Started process (PID=13760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:15,226] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:41:15,227] {logging_mixin.py:112} INFO - [2020-11-05 15:41:15,227] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:15,322] {logging_mixin.py:112} INFO - [2020-11-05 15:41:15,321] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:41:15,322] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:15,347] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 15:41:28,430] {scheduler_job.py:155} INFO - Started process (PID=13822) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:28,443] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:41:28,444] {logging_mixin.py:112} INFO - [2020-11-05 15:41:28,444] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:28,641] {logging_mixin.py:112} INFO - [2020-11-05 15:41:28,639] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:41:28,641] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:28,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-05 15:41:41,694] {scheduler_job.py:155} INFO - Started process (PID=13901) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:41,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:41:41,710] {logging_mixin.py:112} INFO - [2020-11-05 15:41:41,709] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:42,022] {logging_mixin.py:112} INFO - [2020-11-05 15:41:42,012] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:41:42,023] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:42,060] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.366 seconds
[2020-11-05 15:41:54,968] {scheduler_job.py:155} INFO - Started process (PID=13956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:54,974] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:41:54,975] {logging_mixin.py:112} INFO - [2020-11-05 15:41:54,975] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:55,155] {logging_mixin.py:112} INFO - [2020-11-05 15:41:55,151] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:41:55,156] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:41:55,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.222 seconds
[2020-11-05 15:42:08,170] {scheduler_job.py:155} INFO - Started process (PID=14019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:08,176] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:42:08,178] {logging_mixin.py:112} INFO - [2020-11-05 15:42:08,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:08,416] {logging_mixin.py:112} INFO - [2020-11-05 15:42:08,414] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:42:08,416] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:08,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.285 seconds
[2020-11-05 15:42:21,432] {scheduler_job.py:155} INFO - Started process (PID=14077) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:21,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:42:21,447] {logging_mixin.py:112} INFO - [2020-11-05 15:42:21,446] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:21,629] {logging_mixin.py:112} INFO - [2020-11-05 15:42:21,628] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:42:21,629] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:21,663] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.230 seconds
[2020-11-05 15:42:34,616] {scheduler_job.py:155} INFO - Started process (PID=14138) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:34,623] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:42:34,625] {logging_mixin.py:112} INFO - [2020-11-05 15:42:34,624] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:34,743] {logging_mixin.py:112} INFO - [2020-11-05 15:42:34,742] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:42:34,744] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:34,765] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 15:42:47,884] {scheduler_job.py:155} INFO - Started process (PID=14202) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:47,892] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:42:47,892] {logging_mixin.py:112} INFO - [2020-11-05 15:42:47,892] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:48,119] {logging_mixin.py:112} INFO - [2020-11-05 15:42:48,117] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:42:48,119] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:42:48,187] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.304 seconds
[2020-11-05 15:43:01,098] {scheduler_job.py:155} INFO - Started process (PID=14259) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:01,101] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:43:01,102] {logging_mixin.py:112} INFO - [2020-11-05 15:43:01,102] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:01,228] {logging_mixin.py:112} INFO - [2020-11-05 15:43:01,227] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:43:01,228] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:01,271] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-05 15:43:14,288] {scheduler_job.py:155} INFO - Started process (PID=14323) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:14,303] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:43:14,305] {logging_mixin.py:112} INFO - [2020-11-05 15:43:14,305] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:14,418] {logging_mixin.py:112} INFO - [2020-11-05 15:43:14,417] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:43:14,419] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:14,450] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 15:43:27,497] {scheduler_job.py:155} INFO - Started process (PID=14384) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:27,501] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:43:27,502] {logging_mixin.py:112} INFO - [2020-11-05 15:43:27,501] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:27,616] {logging_mixin.py:112} INFO - [2020-11-05 15:43:27,615] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:43:27,616] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:27,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-05 15:43:40,689] {scheduler_job.py:155} INFO - Started process (PID=14443) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:40,694] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:43:40,695] {logging_mixin.py:112} INFO - [2020-11-05 15:43:40,695] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:40,806] {logging_mixin.py:112} INFO - [2020-11-05 15:43:40,804] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:43:40,806] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:40,837] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-05 15:43:53,944] {scheduler_job.py:155} INFO - Started process (PID=14505) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:53,948] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:43:53,948] {logging_mixin.py:112} INFO - [2020-11-05 15:43:53,948] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:54,042] {logging_mixin.py:112} INFO - [2020-11-05 15:43:54,041] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:43:54,042] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:43:54,070] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.125 seconds
[2020-11-05 15:44:07,174] {scheduler_job.py:155} INFO - Started process (PID=14567) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:07,178] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:44:07,179] {logging_mixin.py:112} INFO - [2020-11-05 15:44:07,179] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:07,418] {logging_mixin.py:112} INFO - [2020-11-05 15:44:07,417] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:44:07,419] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:07,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-05 15:44:20,502] {scheduler_job.py:155} INFO - Started process (PID=14629) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:20,507] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:44:20,508] {logging_mixin.py:112} INFO - [2020-11-05 15:44:20,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:20,628] {logging_mixin.py:112} INFO - [2020-11-05 15:44:20,627] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:44:20,628] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:20,663] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 15:44:33,894] {scheduler_job.py:155} INFO - Started process (PID=14687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:33,899] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:44:33,900] {logging_mixin.py:112} INFO - [2020-11-05 15:44:33,899] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:34,175] {logging_mixin.py:112} INFO - [2020-11-05 15:44:34,173] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:44:34,175] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:34,229] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.335 seconds
[2020-11-05 15:44:47,130] {scheduler_job.py:155} INFO - Started process (PID=14748) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:47,144] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:44:47,145] {logging_mixin.py:112} INFO - [2020-11-05 15:44:47,145] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:47,473] {logging_mixin.py:112} INFO - [2020-11-05 15:44:47,471] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:44:47,474] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:44:47,521] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.391 seconds
[2020-11-05 15:45:00,494] {scheduler_job.py:155} INFO - Started process (PID=14808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:00,497] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:45:00,498] {logging_mixin.py:112} INFO - [2020-11-05 15:45:00,498] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:00,607] {logging_mixin.py:112} INFO - [2020-11-05 15:45:00,606] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:45:00,608] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:00,636] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 15:45:13,686] {scheduler_job.py:155} INFO - Started process (PID=14870) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:13,705] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:45:13,705] {logging_mixin.py:112} INFO - [2020-11-05 15:45:13,705] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:13,913] {logging_mixin.py:112} INFO - [2020-11-05 15:45:13,911] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:45:13,913] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:13,943] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-05 15:45:27,009] {scheduler_job.py:155} INFO - Started process (PID=14929) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:27,016] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:45:27,017] {logging_mixin.py:112} INFO - [2020-11-05 15:45:27,016] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:27,298] {logging_mixin.py:112} INFO - [2020-11-05 15:45:27,275] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:45:27,299] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:27,354] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.354 seconds
[2020-11-05 15:45:40,276] {scheduler_job.py:155} INFO - Started process (PID=14988) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:40,281] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:45:40,282] {logging_mixin.py:112} INFO - [2020-11-05 15:45:40,282] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:40,431] {logging_mixin.py:112} INFO - [2020-11-05 15:45:40,430] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:45:40,432] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:40,452] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 15:45:53,573] {scheduler_job.py:155} INFO - Started process (PID=15048) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:53,577] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:45:53,578] {logging_mixin.py:112} INFO - [2020-11-05 15:45:53,578] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:53,755] {logging_mixin.py:112} INFO - [2020-11-05 15:45:53,753] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:45:53,758] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:45:53,799] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.226 seconds
[2020-11-05 15:46:06,775] {scheduler_job.py:155} INFO - Started process (PID=15108) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:06,778] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:46:06,779] {logging_mixin.py:112} INFO - [2020-11-05 15:46:06,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:07,024] {logging_mixin.py:112} INFO - [2020-11-05 15:46:07,023] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:46:07,025] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:07,060] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.285 seconds
[2020-11-05 15:46:20,071] {scheduler_job.py:155} INFO - Started process (PID=15174) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:20,079] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:46:20,081] {logging_mixin.py:112} INFO - [2020-11-05 15:46:20,081] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:20,351] {logging_mixin.py:112} INFO - [2020-11-05 15:46:20,350] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:46:20,352] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:20,405] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.334 seconds
[2020-11-05 15:46:33,344] {scheduler_job.py:155} INFO - Started process (PID=15229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:33,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:46:33,349] {logging_mixin.py:112} INFO - [2020-11-05 15:46:33,349] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:33,467] {logging_mixin.py:112} INFO - [2020-11-05 15:46:33,464] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:46:33,468] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:33,499] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 15:46:46,591] {scheduler_job.py:155} INFO - Started process (PID=15290) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:46,610] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:46:46,611] {logging_mixin.py:112} INFO - [2020-11-05 15:46:46,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:46,796] {logging_mixin.py:112} INFO - [2020-11-05 15:46:46,794] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:46:46,796] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:46,834] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.244 seconds
[2020-11-05 15:46:59,898] {scheduler_job.py:155} INFO - Started process (PID=15348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:46:59,909] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:46:59,910] {logging_mixin.py:112} INFO - [2020-11-05 15:46:59,910] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:00,112] {logging_mixin.py:112} INFO - [2020-11-05 15:47:00,110] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:47:00,113] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:00,151] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.253 seconds
[2020-11-05 15:47:13,115] {scheduler_job.py:155} INFO - Started process (PID=15405) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:13,119] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:47:13,120] {logging_mixin.py:112} INFO - [2020-11-05 15:47:13,120] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:13,289] {logging_mixin.py:112} INFO - [2020-11-05 15:47:13,288] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:47:13,290] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:13,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 15:47:26,471] {scheduler_job.py:155} INFO - Started process (PID=15470) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:26,475] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:47:26,476] {logging_mixin.py:112} INFO - [2020-11-05 15:47:26,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:26,744] {logging_mixin.py:112} INFO - [2020-11-05 15:47:26,740] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:47:26,744] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:26,778] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.307 seconds
[2020-11-05 15:47:39,720] {scheduler_job.py:155} INFO - Started process (PID=15524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:39,728] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:47:39,730] {logging_mixin.py:112} INFO - [2020-11-05 15:47:39,729] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:39,907] {logging_mixin.py:112} INFO - [2020-11-05 15:47:39,906] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:47:39,908] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:39,958] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-05 15:47:53,204] {scheduler_job.py:155} INFO - Started process (PID=15585) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:53,223] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:47:53,231] {logging_mixin.py:112} INFO - [2020-11-05 15:47:53,231] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:53,484] {logging_mixin.py:112} INFO - [2020-11-05 15:47:53,480] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:47:53,485] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:47:53,523] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.319 seconds
[2020-11-05 15:48:06,464] {scheduler_job.py:155} INFO - Started process (PID=15639) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:06,467] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:48:06,468] {logging_mixin.py:112} INFO - [2020-11-05 15:48:06,467] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:06,585] {logging_mixin.py:112} INFO - [2020-11-05 15:48:06,583] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:48:06,586] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:06,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 15:48:19,713] {scheduler_job.py:155} INFO - Started process (PID=15700) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:19,735] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:48:19,740] {logging_mixin.py:112} INFO - [2020-11-05 15:48:19,739] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:19,981] {logging_mixin.py:112} INFO - [2020-11-05 15:48:19,980] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:48:19,984] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:20,020] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.307 seconds
[2020-11-05 15:48:32,962] {scheduler_job.py:155} INFO - Started process (PID=15759) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:32,974] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:48:32,983] {logging_mixin.py:112} INFO - [2020-11-05 15:48:32,983] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:33,190] {logging_mixin.py:112} INFO - [2020-11-05 15:48:33,189] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:48:33,191] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:33,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.266 seconds
[2020-11-05 15:48:46,202] {scheduler_job.py:155} INFO - Started process (PID=15815) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:46,208] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:48:46,210] {logging_mixin.py:112} INFO - [2020-11-05 15:48:46,209] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:46,377] {logging_mixin.py:112} INFO - [2020-11-05 15:48:46,375] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:48:46,377] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:46,403] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.201 seconds
[2020-11-05 15:48:59,485] {scheduler_job.py:155} INFO - Started process (PID=15877) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:59,489] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:48:59,490] {logging_mixin.py:112} INFO - [2020-11-05 15:48:59,489] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:59,609] {logging_mixin.py:112} INFO - [2020-11-05 15:48:59,608] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:48:59,609] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:48:59,648] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 15:49:12,661] {scheduler_job.py:155} INFO - Started process (PID=15938) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:12,665] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:49:12,666] {logging_mixin.py:112} INFO - [2020-11-05 15:49:12,665] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:12,870] {logging_mixin.py:112} INFO - [2020-11-05 15:49:12,869] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:49:12,871] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:12,907] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.246 seconds
[2020-11-05 15:49:25,903] {scheduler_job.py:155} INFO - Started process (PID=15998) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:25,911] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:49:25,919] {logging_mixin.py:112} INFO - [2020-11-05 15:49:25,918] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:26,088] {logging_mixin.py:112} INFO - [2020-11-05 15:49:26,087] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:49:26,089] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:26,129] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.227 seconds
[2020-11-05 15:49:39,087] {scheduler_job.py:155} INFO - Started process (PID=16059) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:39,102] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:49:39,103] {logging_mixin.py:112} INFO - [2020-11-05 15:49:39,102] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:39,279] {logging_mixin.py:112} INFO - [2020-11-05 15:49:39,277] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:49:39,279] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:39,302] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 15:49:52,301] {scheduler_job.py:155} INFO - Started process (PID=16120) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:52,317] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:49:52,320] {logging_mixin.py:112} INFO - [2020-11-05 15:49:52,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:52,642] {logging_mixin.py:112} INFO - [2020-11-05 15:49:52,633] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:49:52,643] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:49:52,688] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.387 seconds
[2020-11-05 15:50:05,547] {scheduler_job.py:155} INFO - Started process (PID=16177) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:05,556] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:50:05,557] {logging_mixin.py:112} INFO - [2020-11-05 15:50:05,556] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:05,756] {logging_mixin.py:112} INFO - [2020-11-05 15:50:05,753] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:50:05,757] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:05,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.294 seconds
[2020-11-05 15:50:18,843] {scheduler_job.py:155} INFO - Started process (PID=16232) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:18,851] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:50:18,852] {logging_mixin.py:112} INFO - [2020-11-05 15:50:18,852] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:19,006] {logging_mixin.py:112} INFO - [2020-11-05 15:50:19,005] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:50:19,006] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:19,033] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 15:50:32,115] {scheduler_job.py:155} INFO - Started process (PID=16298) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:32,119] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:50:32,120] {logging_mixin.py:112} INFO - [2020-11-05 15:50:32,120] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:32,258] {logging_mixin.py:112} INFO - [2020-11-05 15:50:32,257] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:50:32,258] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:32,283] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.168 seconds
[2020-11-05 15:50:45,290] {scheduler_job.py:155} INFO - Started process (PID=16357) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:45,293] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:50:45,294] {logging_mixin.py:112} INFO - [2020-11-05 15:50:45,294] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:45,421] {logging_mixin.py:112} INFO - [2020-11-05 15:50:45,419] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:50:45,421] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:45,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-05 15:50:58,532] {scheduler_job.py:155} INFO - Started process (PID=16416) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:58,538] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:50:58,539] {logging_mixin.py:112} INFO - [2020-11-05 15:50:58,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:58,774] {logging_mixin.py:112} INFO - [2020-11-05 15:50:58,773] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:50:58,774] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:50:58,801] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.269 seconds
[2020-11-05 15:51:11,755] {scheduler_job.py:155} INFO - Started process (PID=16477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:11,758] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:51:11,759] {logging_mixin.py:112} INFO - [2020-11-05 15:51:11,759] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:11,953] {logging_mixin.py:112} INFO - [2020-11-05 15:51:11,951] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:51:11,953] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:11,996] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.241 seconds
[2020-11-05 15:51:24,950] {scheduler_job.py:155} INFO - Started process (PID=16536) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:24,954] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:51:24,955] {logging_mixin.py:112} INFO - [2020-11-05 15:51:24,955] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:25,090] {logging_mixin.py:112} INFO - [2020-11-05 15:51:25,088] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:51:25,091] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:25,120] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 15:51:38,179] {scheduler_job.py:155} INFO - Started process (PID=16598) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:38,183] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:51:38,185] {logging_mixin.py:112} INFO - [2020-11-05 15:51:38,185] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:38,303] {logging_mixin.py:112} INFO - [2020-11-05 15:51:38,302] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:51:38,303] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:38,329] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 15:51:51,361] {scheduler_job.py:155} INFO - Started process (PID=16657) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:51,368] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:51:51,369] {logging_mixin.py:112} INFO - [2020-11-05 15:51:51,368] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:51,500] {logging_mixin.py:112} INFO - [2020-11-05 15:51:51,499] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:51:51,500] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:51:51,533] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.172 seconds
[2020-11-05 15:52:04,576] {scheduler_job.py:155} INFO - Started process (PID=16728) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:04,582] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:52:04,583] {logging_mixin.py:112} INFO - [2020-11-05 15:52:04,583] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:04,725] {logging_mixin.py:112} INFO - [2020-11-05 15:52:04,724] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:52:04,726] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:04,750] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-05 15:52:17,782] {scheduler_job.py:155} INFO - Started process (PID=16788) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:17,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:52:17,790] {logging_mixin.py:112} INFO - [2020-11-05 15:52:17,789] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:17,914] {logging_mixin.py:112} INFO - [2020-11-05 15:52:17,913] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:52:17,914] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:17,938] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 15:52:30,998] {scheduler_job.py:155} INFO - Started process (PID=16851) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:31,003] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:52:31,006] {logging_mixin.py:112} INFO - [2020-11-05 15:52:31,004] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:31,115] {logging_mixin.py:112} INFO - [2020-11-05 15:52:31,115] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:52:31,116] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:31,138] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 15:52:44,235] {scheduler_job.py:155} INFO - Started process (PID=16913) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:44,240] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:52:44,241] {logging_mixin.py:112} INFO - [2020-11-05 15:52:44,241] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:44,354] {logging_mixin.py:112} INFO - [2020-11-05 15:52:44,353] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:52:44,354] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:44,386] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 15:52:57,483] {scheduler_job.py:155} INFO - Started process (PID=16976) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:57,490] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:52:57,490] {logging_mixin.py:112} INFO - [2020-11-05 15:52:57,490] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:57,610] {logging_mixin.py:112} INFO - [2020-11-05 15:52:57,609] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:52:57,610] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:52:57,631] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 15:53:10,715] {scheduler_job.py:155} INFO - Started process (PID=17038) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:10,720] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:53:10,720] {logging_mixin.py:112} INFO - [2020-11-05 15:53:10,720] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:10,864] {logging_mixin.py:112} INFO - [2020-11-05 15:53:10,863] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:53:10,864] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:10,890] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 15:53:24,019] {scheduler_job.py:155} INFO - Started process (PID=17115) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:24,025] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:53:24,026] {logging_mixin.py:112} INFO - [2020-11-05 15:53:24,026] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:24,152] {logging_mixin.py:112} INFO - [2020-11-05 15:53:24,150] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:53:24,153] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:24,198] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 15:53:37,333] {scheduler_job.py:155} INFO - Started process (PID=17184) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:37,337] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:53:37,338] {logging_mixin.py:112} INFO - [2020-11-05 15:53:37,337] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:37,463] {logging_mixin.py:112} INFO - [2020-11-05 15:53:37,461] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:53:37,463] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:37,493] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 15:53:50,562] {scheduler_job.py:155} INFO - Started process (PID=17251) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:50,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:53:50,566] {logging_mixin.py:112} INFO - [2020-11-05 15:53:50,566] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:50,671] {logging_mixin.py:112} INFO - [2020-11-05 15:53:50,668] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:53:50,672] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:53:50,707] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 15:54:03,824] {scheduler_job.py:155} INFO - Started process (PID=17322) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:03,828] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:54:03,830] {logging_mixin.py:112} INFO - [2020-11-05 15:54:03,829] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:04,013] {logging_mixin.py:112} INFO - [2020-11-05 15:54:04,012] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:54:04,014] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:04,046] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.222 seconds
[2020-11-05 15:54:17,030] {scheduler_job.py:155} INFO - Started process (PID=17382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:17,035] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:54:17,042] {logging_mixin.py:112} INFO - [2020-11-05 15:54:17,042] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:17,197] {logging_mixin.py:112} INFO - [2020-11-05 15:54:17,196] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:54:17,197] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:17,225] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.195 seconds
[2020-11-05 15:54:30,234] {scheduler_job.py:155} INFO - Started process (PID=17442) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:30,238] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:54:30,239] {logging_mixin.py:112} INFO - [2020-11-05 15:54:30,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:30,359] {logging_mixin.py:112} INFO - [2020-11-05 15:54:30,358] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:54:30,359] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:30,395] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 15:54:43,434] {scheduler_job.py:155} INFO - Started process (PID=17503) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:43,438] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:54:43,439] {logging_mixin.py:112} INFO - [2020-11-05 15:54:43,439] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:43,566] {logging_mixin.py:112} INFO - [2020-11-05 15:54:43,564] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:54:43,568] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:43,595] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 15:54:56,613] {scheduler_job.py:155} INFO - Started process (PID=17573) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:56,618] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:54:56,619] {logging_mixin.py:112} INFO - [2020-11-05 15:54:56,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:56,717] {logging_mixin.py:112} INFO - [2020-11-05 15:54:56,715] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:54:56,718] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:54:56,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 15:55:09,846] {scheduler_job.py:155} INFO - Started process (PID=17644) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:09,853] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:55:09,853] {logging_mixin.py:112} INFO - [2020-11-05 15:55:09,853] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:10,074] {logging_mixin.py:112} INFO - [2020-11-05 15:55:10,073] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:55:10,075] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:10,103] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-05 15:55:23,137] {scheduler_job.py:155} INFO - Started process (PID=17720) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:23,150] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:55:23,151] {logging_mixin.py:112} INFO - [2020-11-05 15:55:23,151] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:23,387] {logging_mixin.py:112} INFO - [2020-11-05 15:55:23,385] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:55:23,388] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:23,424] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.287 seconds
[2020-11-05 15:55:36,361] {scheduler_job.py:155} INFO - Started process (PID=17782) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:36,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:55:36,366] {logging_mixin.py:112} INFO - [2020-11-05 15:55:36,366] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:36,471] {logging_mixin.py:112} INFO - [2020-11-05 15:55:36,470] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:55:36,471] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:36,495] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 15:55:49,586] {scheduler_job.py:155} INFO - Started process (PID=17843) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:49,594] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:55:49,595] {logging_mixin.py:112} INFO - [2020-11-05 15:55:49,595] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:49,794] {logging_mixin.py:112} INFO - [2020-11-05 15:55:49,793] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:55:49,795] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:55:49,832] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.246 seconds
[2020-11-05 15:56:02,776] {scheduler_job.py:155} INFO - Started process (PID=17900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:02,780] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:56:02,781] {logging_mixin.py:112} INFO - [2020-11-05 15:56:02,781] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:02,885] {logging_mixin.py:112} INFO - [2020-11-05 15:56:02,884] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:56:02,886] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:02,907] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 15:56:16,046] {scheduler_job.py:155} INFO - Started process (PID=17962) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:16,060] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:56:16,071] {logging_mixin.py:112} INFO - [2020-11-05 15:56:16,071] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:16,367] {logging_mixin.py:112} INFO - [2020-11-05 15:56:16,363] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:56:16,368] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:16,415] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.369 seconds
[2020-11-05 15:56:29,247] {scheduler_job.py:155} INFO - Started process (PID=18020) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:29,259] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:56:29,259] {logging_mixin.py:112} INFO - [2020-11-05 15:56:29,259] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:29,471] {logging_mixin.py:112} INFO - [2020-11-05 15:56:29,468] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:56:29,472] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:29,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-05 15:56:42,554] {scheduler_job.py:155} INFO - Started process (PID=18087) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:42,571] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:56:42,573] {logging_mixin.py:112} INFO - [2020-11-05 15:56:42,572] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:42,724] {logging_mixin.py:112} INFO - [2020-11-05 15:56:42,722] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:56:42,725] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:42,757] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 15:56:55,724] {scheduler_job.py:155} INFO - Started process (PID=18154) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:55,729] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:56:55,731] {logging_mixin.py:112} INFO - [2020-11-05 15:56:55,730] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:55,853] {logging_mixin.py:112} INFO - [2020-11-05 15:56:55,852] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:56:55,854] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:56:55,882] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 15:57:08,993] {scheduler_job.py:155} INFO - Started process (PID=18216) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:08,998] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:57:08,999] {logging_mixin.py:112} INFO - [2020-11-05 15:57:08,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:09,137] {logging_mixin.py:112} INFO - [2020-11-05 15:57:09,136] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:57:09,140] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:09,166] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-05 15:57:22,240] {scheduler_job.py:155} INFO - Started process (PID=18274) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:22,245] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:57:22,248] {logging_mixin.py:112} INFO - [2020-11-05 15:57:22,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:22,398] {logging_mixin.py:112} INFO - [2020-11-05 15:57:22,396] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:57:22,398] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:22,437] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-05 15:57:35,527] {scheduler_job.py:155} INFO - Started process (PID=18335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:35,531] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:57:35,532] {logging_mixin.py:112} INFO - [2020-11-05 15:57:35,532] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:35,643] {logging_mixin.py:112} INFO - [2020-11-05 15:57:35,641] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:57:35,643] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:35,673] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 15:57:48,739] {scheduler_job.py:155} INFO - Started process (PID=18393) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:48,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:57:48,742] {logging_mixin.py:112} INFO - [2020-11-05 15:57:48,742] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:48,883] {logging_mixin.py:112} INFO - [2020-11-05 15:57:48,882] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:57:48,884] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:57:48,908] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 15:58:01,915] {scheduler_job.py:155} INFO - Started process (PID=18456) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:01,919] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:58:01,919] {logging_mixin.py:112} INFO - [2020-11-05 15:58:01,919] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:02,068] {logging_mixin.py:112} INFO - [2020-11-05 15:58:02,065] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:58:02,069] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:02,105] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 15:58:15,219] {scheduler_job.py:155} INFO - Started process (PID=18516) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:15,223] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:58:15,224] {logging_mixin.py:112} INFO - [2020-11-05 15:58:15,224] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:15,405] {logging_mixin.py:112} INFO - [2020-11-05 15:58:15,402] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:58:15,405] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:15,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-05 15:58:28,416] {scheduler_job.py:155} INFO - Started process (PID=18592) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:28,422] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:58:28,423] {logging_mixin.py:112} INFO - [2020-11-05 15:58:28,423] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:28,552] {logging_mixin.py:112} INFO - [2020-11-05 15:58:28,551] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:58:28,552] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:28,576] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 15:58:41,611] {scheduler_job.py:155} INFO - Started process (PID=18655) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:41,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:58:41,617] {logging_mixin.py:112} INFO - [2020-11-05 15:58:41,616] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:41,841] {logging_mixin.py:112} INFO - [2020-11-05 15:58:41,840] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:58:41,842] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:41,865] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.253 seconds
[2020-11-05 15:58:54,879] {scheduler_job.py:155} INFO - Started process (PID=18712) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:54,893] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:58:54,894] {logging_mixin.py:112} INFO - [2020-11-05 15:58:54,894] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:55,215] {logging_mixin.py:112} INFO - [2020-11-05 15:58:55,210] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:58:55,216] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:58:55,302] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.424 seconds
[2020-11-05 15:59:08,069] {scheduler_job.py:155} INFO - Started process (PID=18776) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:08,072] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:59:08,073] {logging_mixin.py:112} INFO - [2020-11-05 15:59:08,073] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:08,189] {logging_mixin.py:112} INFO - [2020-11-05 15:59:08,188] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:59:08,189] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:08,211] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 15:59:21,262] {scheduler_job.py:155} INFO - Started process (PID=18835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:21,266] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:59:21,267] {logging_mixin.py:112} INFO - [2020-11-05 15:59:21,267] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:21,386] {logging_mixin.py:112} INFO - [2020-11-05 15:59:21,385] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:59:21,386] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:21,429] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-05 15:59:34,466] {scheduler_job.py:155} INFO - Started process (PID=18897) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:34,478] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:59:34,478] {logging_mixin.py:112} INFO - [2020-11-05 15:59:34,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:34,666] {logging_mixin.py:112} INFO - [2020-11-05 15:59:34,665] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:59:34,666] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:34,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.223 seconds
[2020-11-05 15:59:47,662] {scheduler_job.py:155} INFO - Started process (PID=18957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:47,667] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 15:59:47,668] {logging_mixin.py:112} INFO - [2020-11-05 15:59:47,668] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:47,776] {logging_mixin.py:112} INFO - [2020-11-05 15:59:47,775] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 15:59:47,777] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 15:59:47,802] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 16:00:00,929] {scheduler_job.py:155} INFO - Started process (PID=19019) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:00,933] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:00:00,934] {logging_mixin.py:112} INFO - [2020-11-05 16:00:00,933] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:01,049] {logging_mixin.py:112} INFO - [2020-11-05 16:00:01,048] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:00:01,049] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:01,090] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 16:00:14,160] {scheduler_job.py:155} INFO - Started process (PID=19084) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:14,169] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:00:14,178] {logging_mixin.py:112} INFO - [2020-11-05 16:00:14,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:14,344] {logging_mixin.py:112} INFO - [2020-11-05 16:00:14,343] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:00:14,345] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:14,369] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.208 seconds
[2020-11-05 16:00:27,393] {scheduler_job.py:155} INFO - Started process (PID=19147) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:27,396] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:00:27,397] {logging_mixin.py:112} INFO - [2020-11-05 16:00:27,397] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:27,518] {logging_mixin.py:112} INFO - [2020-11-05 16:00:27,517] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:00:27,519] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:27,549] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:00:40,609] {scheduler_job.py:155} INFO - Started process (PID=19208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:40,613] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:00:40,614] {logging_mixin.py:112} INFO - [2020-11-05 16:00:40,614] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:40,720] {logging_mixin.py:112} INFO - [2020-11-05 16:00:40,719] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:00:40,721] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:40,751] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 16:00:53,773] {scheduler_job.py:155} INFO - Started process (PID=19276) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:53,776] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:00:53,777] {logging_mixin.py:112} INFO - [2020-11-05 16:00:53,777] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:53,883] {logging_mixin.py:112} INFO - [2020-11-05 16:00:53,875] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:00:53,885] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:00:53,933] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 16:01:06,978] {scheduler_job.py:155} INFO - Started process (PID=19339) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:06,982] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:01:06,982] {logging_mixin.py:112} INFO - [2020-11-05 16:01:06,982] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:07,092] {logging_mixin.py:112} INFO - [2020-11-05 16:01:07,091] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:01:07,093] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:07,124] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 16:01:20,177] {scheduler_job.py:155} INFO - Started process (PID=19411) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:20,183] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:01:20,184] {logging_mixin.py:112} INFO - [2020-11-05 16:01:20,184] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:20,339] {logging_mixin.py:112} INFO - [2020-11-05 16:01:20,335] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:01:20,339] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:20,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 16:01:33,417] {scheduler_job.py:155} INFO - Started process (PID=19470) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:33,433] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:01:33,435] {logging_mixin.py:112} INFO - [2020-11-05 16:01:33,435] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:33,588] {logging_mixin.py:112} INFO - [2020-11-05 16:01:33,587] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:01:33,588] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:33,614] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-05 16:01:46,636] {scheduler_job.py:155} INFO - Started process (PID=19524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:46,642] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:01:46,643] {logging_mixin.py:112} INFO - [2020-11-05 16:01:46,643] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:46,786] {logging_mixin.py:112} INFO - [2020-11-05 16:01:46,784] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:01:46,788] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:46,816] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 16:01:59,854] {scheduler_job.py:155} INFO - Started process (PID=19584) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:01:59,859] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:01:59,860] {logging_mixin.py:112} INFO - [2020-11-05 16:01:59,860] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:00,147] {logging_mixin.py:112} INFO - [2020-11-05 16:02:00,145] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:02:00,148] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:00,185] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.331 seconds
[2020-11-05 16:02:13,131] {scheduler_job.py:155} INFO - Started process (PID=19637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:13,134] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:02:13,134] {logging_mixin.py:112} INFO - [2020-11-05 16:02:13,134] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:13,328] {logging_mixin.py:112} INFO - [2020-11-05 16:02:13,326] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:02:13,328] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:13,386] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.255 seconds
[2020-11-05 16:02:26,382] {scheduler_job.py:155} INFO - Started process (PID=19695) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:26,388] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:02:26,388] {logging_mixin.py:112} INFO - [2020-11-05 16:02:26,388] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:26,546] {logging_mixin.py:112} INFO - [2020-11-05 16:02:26,546] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:02:26,547] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:26,576] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.194 seconds
[2020-11-05 16:02:39,765] {scheduler_job.py:155} INFO - Started process (PID=19751) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:39,776] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:02:39,777] {logging_mixin.py:112} INFO - [2020-11-05 16:02:39,777] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:39,990] {logging_mixin.py:112} INFO - [2020-11-05 16:02:39,989] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:02:39,991] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:40,026] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.261 seconds
[2020-11-05 16:02:52,987] {scheduler_job.py:155} INFO - Started process (PID=19813) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:52,994] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:02:52,995] {logging_mixin.py:112} INFO - [2020-11-05 16:02:52,994] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:53,256] {logging_mixin.py:112} INFO - [2020-11-05 16:02:53,255] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:02:53,257] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:02:53,314] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.327 seconds
[2020-11-05 16:03:06,193] {scheduler_job.py:155} INFO - Started process (PID=19868) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:06,197] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:03:06,197] {logging_mixin.py:112} INFO - [2020-11-05 16:03:06,197] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:06,369] {logging_mixin.py:112} INFO - [2020-11-05 16:03:06,367] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:03:06,370] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:06,416] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 16:03:19,405] {scheduler_job.py:155} INFO - Started process (PID=19925) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:19,418] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:03:19,418] {logging_mixin.py:112} INFO - [2020-11-05 16:03:19,418] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:19,510] {logging_mixin.py:112} INFO - [2020-11-05 16:03:19,509] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:03:19,510] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:19,533] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 16:03:32,622] {scheduler_job.py:155} INFO - Started process (PID=19983) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:32,628] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:03:32,635] {logging_mixin.py:112} INFO - [2020-11-05 16:03:32,628] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:32,769] {logging_mixin.py:112} INFO - [2020-11-05 16:03:32,768] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:03:32,769] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:32,806] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-05 16:03:45,851] {scheduler_job.py:155} INFO - Started process (PID=20040) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:45,856] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:03:45,858] {logging_mixin.py:112} INFO - [2020-11-05 16:03:45,857] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:46,063] {logging_mixin.py:112} INFO - [2020-11-05 16:03:46,061] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:03:46,063] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:46,098] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-05 16:03:59,073] {scheduler_job.py:155} INFO - Started process (PID=20100) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:59,076] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:03:59,077] {logging_mixin.py:112} INFO - [2020-11-05 16:03:59,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:59,198] {logging_mixin.py:112} INFO - [2020-11-05 16:03:59,197] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:03:59,198] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:03:59,229] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 16:04:12,276] {scheduler_job.py:155} INFO - Started process (PID=20163) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:12,280] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:04:12,281] {logging_mixin.py:112} INFO - [2020-11-05 16:04:12,281] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:12,431] {logging_mixin.py:112} INFO - [2020-11-05 16:04:12,429] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:04:12,431] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:12,451] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 16:04:25,507] {scheduler_job.py:155} INFO - Started process (PID=20224) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:25,515] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:04:25,516] {logging_mixin.py:112} INFO - [2020-11-05 16:04:25,515] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:25,652] {logging_mixin.py:112} INFO - [2020-11-05 16:04:25,651] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:04:25,653] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:25,683] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.176 seconds
[2020-11-05 16:04:38,693] {scheduler_job.py:155} INFO - Started process (PID=20283) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:38,696] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:04:38,697] {logging_mixin.py:112} INFO - [2020-11-05 16:04:38,697] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:38,827] {logging_mixin.py:112} INFO - [2020-11-05 16:04:38,823] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:04:38,827] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:38,855] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 16:04:51,928] {scheduler_job.py:155} INFO - Started process (PID=20343) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:51,936] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:04:51,945] {logging_mixin.py:112} INFO - [2020-11-05 16:04:51,936] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:52,056] {logging_mixin.py:112} INFO - [2020-11-05 16:04:52,055] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:04:52,057] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:04:52,085] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 16:05:05,104] {scheduler_job.py:155} INFO - Started process (PID=20409) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:05,109] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:05:05,110] {logging_mixin.py:112} INFO - [2020-11-05 16:05:05,110] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:05,251] {logging_mixin.py:112} INFO - [2020-11-05 16:05:05,248] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:05:05,252] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:05,289] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-05 16:05:18,309] {scheduler_job.py:155} INFO - Started process (PID=20476) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:18,314] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:05:18,315] {logging_mixin.py:112} INFO - [2020-11-05 16:05:18,314] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:18,432] {logging_mixin.py:112} INFO - [2020-11-05 16:05:18,432] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:05:18,433] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:18,461] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 16:05:31,489] {scheduler_job.py:155} INFO - Started process (PID=20538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:31,495] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:05:31,496] {logging_mixin.py:112} INFO - [2020-11-05 16:05:31,495] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:31,654] {logging_mixin.py:112} INFO - [2020-11-05 16:05:31,652] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:05:31,654] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:31,681] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-05 16:05:44,866] {scheduler_job.py:155} INFO - Started process (PID=20598) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:44,871] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:05:44,872] {logging_mixin.py:112} INFO - [2020-11-05 16:05:44,872] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:45,079] {logging_mixin.py:112} INFO - [2020-11-05 16:05:45,078] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:05:45,080] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:45,134] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-05 16:05:58,085] {scheduler_job.py:155} INFO - Started process (PID=20658) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:58,089] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:05:58,090] {logging_mixin.py:112} INFO - [2020-11-05 16:05:58,090] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:58,238] {logging_mixin.py:112} INFO - [2020-11-05 16:05:58,237] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:05:58,238] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:05:58,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 16:06:11,305] {scheduler_job.py:155} INFO - Started process (PID=20724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:11,310] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:06:11,311] {logging_mixin.py:112} INFO - [2020-11-05 16:06:11,311] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:11,420] {logging_mixin.py:112} INFO - [2020-11-05 16:06:11,419] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:06:11,421] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:11,460] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:06:24,539] {scheduler_job.py:155} INFO - Started process (PID=20780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:24,544] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:06:24,545] {logging_mixin.py:112} INFO - [2020-11-05 16:06:24,544] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:24,681] {logging_mixin.py:112} INFO - [2020-11-05 16:06:24,680] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:06:24,685] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:24,711] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.172 seconds
[2020-11-05 16:06:37,717] {scheduler_job.py:155} INFO - Started process (PID=20841) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:37,721] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:06:37,722] {logging_mixin.py:112} INFO - [2020-11-05 16:06:37,721] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:37,824] {logging_mixin.py:112} INFO - [2020-11-05 16:06:37,822] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:06:37,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:37,849] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 16:06:50,979] {scheduler_job.py:155} INFO - Started process (PID=20907) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:50,983] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:06:50,984] {logging_mixin.py:112} INFO - [2020-11-05 16:06:50,983] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:51,079] {logging_mixin.py:112} INFO - [2020-11-05 16:06:51,078] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:06:51,079] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:06:51,111] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 16:07:04,185] {scheduler_job.py:155} INFO - Started process (PID=20974) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:04,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:07:04,189] {logging_mixin.py:112} INFO - [2020-11-05 16:07:04,189] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:04,280] {logging_mixin.py:112} INFO - [2020-11-05 16:07:04,279] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:07:04,280] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:04,302] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.118 seconds
[2020-11-05 16:07:17,399] {scheduler_job.py:155} INFO - Started process (PID=21031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:17,404] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:07:17,405] {logging_mixin.py:112} INFO - [2020-11-05 16:07:17,405] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:17,555] {logging_mixin.py:112} INFO - [2020-11-05 16:07:17,554] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:07:17,556] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:17,579] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 16:07:30,622] {scheduler_job.py:155} INFO - Started process (PID=21090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:30,630] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:07:30,633] {logging_mixin.py:112} INFO - [2020-11-05 16:07:30,631] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:30,768] {logging_mixin.py:112} INFO - [2020-11-05 16:07:30,767] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:07:30,769] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:30,792] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 16:07:43,864] {scheduler_job.py:155} INFO - Started process (PID=21149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:43,873] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:07:43,873] {logging_mixin.py:112} INFO - [2020-11-05 16:07:43,873] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:43,961] {logging_mixin.py:112} INFO - [2020-11-05 16:07:43,960] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:07:43,961] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:43,991] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.127 seconds
[2020-11-05 16:07:57,046] {scheduler_job.py:155} INFO - Started process (PID=21209) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:57,052] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:07:57,053] {logging_mixin.py:112} INFO - [2020-11-05 16:07:57,052] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:57,224] {logging_mixin.py:112} INFO - [2020-11-05 16:07:57,223] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:07:57,225] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:07:57,257] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-05 16:08:10,240] {scheduler_job.py:155} INFO - Started process (PID=21274) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:10,245] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:08:10,246] {logging_mixin.py:112} INFO - [2020-11-05 16:08:10,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:10,374] {logging_mixin.py:112} INFO - [2020-11-05 16:08:10,373] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:08:10,375] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:10,403] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 16:08:23,564] {scheduler_job.py:155} INFO - Started process (PID=21336) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:23,567] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:08:23,567] {logging_mixin.py:112} INFO - [2020-11-05 16:08:23,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:23,697] {logging_mixin.py:112} INFO - [2020-11-05 16:08:23,696] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:08:23,697] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:23,719] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:08:36,764] {scheduler_job.py:155} INFO - Started process (PID=21395) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:36,768] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:08:36,769] {logging_mixin.py:112} INFO - [2020-11-05 16:08:36,769] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:36,891] {logging_mixin.py:112} INFO - [2020-11-05 16:08:36,890] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:08:36,891] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:36,923] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.158 seconds
[2020-11-05 16:08:49,971] {scheduler_job.py:155} INFO - Started process (PID=21456) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:49,977] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:08:49,978] {logging_mixin.py:112} INFO - [2020-11-05 16:08:49,978] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:50,101] {logging_mixin.py:112} INFO - [2020-11-05 16:08:50,098] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:08:50,102] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:08:50,132] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 16:09:03,158] {scheduler_job.py:155} INFO - Started process (PID=21526) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:03,163] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:09:03,164] {logging_mixin.py:112} INFO - [2020-11-05 16:09:03,164] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:03,258] {logging_mixin.py:112} INFO - [2020-11-05 16:09:03,257] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:09:03,258] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:03,284] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-05 16:09:16,406] {scheduler_job.py:155} INFO - Started process (PID=21586) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:16,413] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:09:16,413] {logging_mixin.py:112} INFO - [2020-11-05 16:09:16,413] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:16,539] {logging_mixin.py:112} INFO - [2020-11-05 16:09:16,538] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:09:16,540] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:16,562] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:09:29,585] {scheduler_job.py:155} INFO - Started process (PID=21654) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:29,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:09:29,590] {logging_mixin.py:112} INFO - [2020-11-05 16:09:29,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:29,769] {logging_mixin.py:112} INFO - [2020-11-05 16:09:29,767] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:09:29,769] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:29,814] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-05 16:09:42,775] {scheduler_job.py:155} INFO - Started process (PID=21714) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:42,779] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:09:42,779] {logging_mixin.py:112} INFO - [2020-11-05 16:09:42,779] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:42,902] {logging_mixin.py:112} INFO - [2020-11-05 16:09:42,901] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:09:42,902] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:42,929] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:09:56,036] {scheduler_job.py:155} INFO - Started process (PID=21771) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:56,040] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:09:56,041] {logging_mixin.py:112} INFO - [2020-11-05 16:09:56,041] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:56,208] {logging_mixin.py:112} INFO - [2020-11-05 16:09:56,207] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:09:56,208] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:09:56,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 16:10:09,400] {scheduler_job.py:155} INFO - Started process (PID=21835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:09,403] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:10:09,404] {logging_mixin.py:112} INFO - [2020-11-05 16:10:09,403] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:09,564] {logging_mixin.py:112} INFO - [2020-11-05 16:10:09,563] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:10:09,565] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:09,587] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 16:10:22,709] {scheduler_job.py:155} INFO - Started process (PID=21893) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:22,728] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:10:22,730] {logging_mixin.py:112} INFO - [2020-11-05 16:10:22,729] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:22,971] {logging_mixin.py:112} INFO - [2020-11-05 16:10:22,969] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:10:22,972] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:23,025] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.316 seconds
[2020-11-05 16:10:35,968] {scheduler_job.py:155} INFO - Started process (PID=21950) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:35,972] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:10:35,974] {logging_mixin.py:112} INFO - [2020-11-05 16:10:35,973] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:36,318] {logging_mixin.py:112} INFO - [2020-11-05 16:10:36,317] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:10:36,318] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:36,351] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.383 seconds
[2020-11-05 16:10:49,288] {scheduler_job.py:155} INFO - Started process (PID=22011) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:49,294] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:10:49,296] {logging_mixin.py:112} INFO - [2020-11-05 16:10:49,295] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:49,411] {logging_mixin.py:112} INFO - [2020-11-05 16:10:49,408] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:10:49,412] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:10:49,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 16:11:02,566] {scheduler_job.py:155} INFO - Started process (PID=22078) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:02,572] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:11:02,576] {logging_mixin.py:112} INFO - [2020-11-05 16:11:02,576] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:02,740] {logging_mixin.py:112} INFO - [2020-11-05 16:11:02,739] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:11:02,740] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:02,776] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.210 seconds
[2020-11-05 16:11:15,763] {scheduler_job.py:155} INFO - Started process (PID=22145) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:15,768] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:11:15,769] {logging_mixin.py:112} INFO - [2020-11-05 16:11:15,768] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:16,042] {logging_mixin.py:112} INFO - [2020-11-05 16:11:16,041] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:11:16,043] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:16,103] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.340 seconds
[2020-11-05 16:11:29,137] {scheduler_job.py:155} INFO - Started process (PID=22207) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:29,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:11:29,172] {logging_mixin.py:112} INFO - [2020-11-05 16:11:29,172] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:29,369] {logging_mixin.py:112} INFO - [2020-11-05 16:11:29,367] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:11:29,369] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:29,427] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.290 seconds
[2020-11-05 16:11:42,405] {scheduler_job.py:155} INFO - Started process (PID=22267) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:42,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:11:42,416] {logging_mixin.py:112} INFO - [2020-11-05 16:11:42,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:42,619] {logging_mixin.py:112} INFO - [2020-11-05 16:11:42,618] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:11:42,620] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:42,670] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 16:11:55,674] {scheduler_job.py:155} INFO - Started process (PID=22322) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:55,682] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:11:55,682] {logging_mixin.py:112} INFO - [2020-11-05 16:11:55,682] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:55,823] {logging_mixin.py:112} INFO - [2020-11-05 16:11:55,818] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:11:55,823] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:11:55,853] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 16:12:08,971] {scheduler_job.py:155} INFO - Started process (PID=22384) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:08,975] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:12:08,976] {logging_mixin.py:112} INFO - [2020-11-05 16:12:08,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:09,092] {logging_mixin.py:112} INFO - [2020-11-05 16:12:09,091] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:12:09,093] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:09,124] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.154 seconds
[2020-11-05 16:12:22,292] {scheduler_job.py:155} INFO - Started process (PID=22452) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:22,304] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:12:22,305] {logging_mixin.py:112} INFO - [2020-11-05 16:12:22,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:22,502] {logging_mixin.py:112} INFO - [2020-11-05 16:12:22,500] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:12:22,502] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:22,544] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.252 seconds
[2020-11-05 16:12:35,491] {scheduler_job.py:155} INFO - Started process (PID=22510) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:35,496] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:12:35,498] {logging_mixin.py:112} INFO - [2020-11-05 16:12:35,497] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:35,616] {logging_mixin.py:112} INFO - [2020-11-05 16:12:35,615] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:12:35,616] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:35,651] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 16:12:48,745] {scheduler_job.py:155} INFO - Started process (PID=22569) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:48,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:12:48,751] {logging_mixin.py:112} INFO - [2020-11-05 16:12:48,751] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:48,867] {logging_mixin.py:112} INFO - [2020-11-05 16:12:48,866] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:12:48,867] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:12:48,888] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 16:13:01,974] {scheduler_job.py:155} INFO - Started process (PID=22628) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:01,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:13:01,987] {logging_mixin.py:112} INFO - [2020-11-05 16:13:01,987] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:02,206] {logging_mixin.py:112} INFO - [2020-11-05 16:13:02,205] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:13:02,207] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:02,243] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.269 seconds
[2020-11-05 16:13:15,211] {scheduler_job.py:155} INFO - Started process (PID=22685) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:15,227] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:13:15,228] {logging_mixin.py:112} INFO - [2020-11-05 16:13:15,228] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:15,369] {logging_mixin.py:112} INFO - [2020-11-05 16:13:15,368] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:13:15,369] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:15,401] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 16:13:28,918] {scheduler_job.py:155} INFO - Started process (PID=22753) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:28,923] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:13:28,933] {logging_mixin.py:112} INFO - [2020-11-05 16:13:28,933] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:29,287] {logging_mixin.py:112} INFO - [2020-11-05 16:13:29,286] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:13:29,288] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:29,383] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.466 seconds
[2020-11-05 16:13:42,208] {scheduler_job.py:155} INFO - Started process (PID=22806) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:42,213] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:13:42,214] {logging_mixin.py:112} INFO - [2020-11-05 16:13:42,214] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:42,381] {logging_mixin.py:112} INFO - [2020-11-05 16:13:42,380] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:13:42,382] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:42,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-05 16:13:55,432] {scheduler_job.py:155} INFO - Started process (PID=22865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:55,440] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:13:55,441] {logging_mixin.py:112} INFO - [2020-11-05 16:13:55,440] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:55,580] {logging_mixin.py:112} INFO - [2020-11-05 16:13:55,579] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:13:55,580] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:13:55,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-05 16:14:08,685] {scheduler_job.py:155} INFO - Started process (PID=22931) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:08,688] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:14:08,693] {logging_mixin.py:112} INFO - [2020-11-05 16:14:08,693] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:08,916] {logging_mixin.py:112} INFO - [2020-11-05 16:14:08,915] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:14:08,916] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:08,939] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.254 seconds
[2020-11-05 16:14:21,971] {scheduler_job.py:155} INFO - Started process (PID=22985) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:21,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:14:21,977] {logging_mixin.py:112} INFO - [2020-11-05 16:14:21,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:22,114] {logging_mixin.py:112} INFO - [2020-11-05 16:14:22,112] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:14:22,116] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:22,140] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-05 16:14:35,313] {scheduler_job.py:155} INFO - Started process (PID=23042) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:35,317] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:14:35,319] {logging_mixin.py:112} INFO - [2020-11-05 16:14:35,318] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:35,546] {logging_mixin.py:112} INFO - [2020-11-05 16:14:35,545] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:14:35,547] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:35,586] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.273 seconds
[2020-11-05 16:14:48,533] {scheduler_job.py:155} INFO - Started process (PID=23100) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:48,537] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:14:48,538] {logging_mixin.py:112} INFO - [2020-11-05 16:14:48,538] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:48,673] {logging_mixin.py:112} INFO - [2020-11-05 16:14:48,672] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:14:48,673] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:14:48,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-05 16:15:01,835] {scheduler_job.py:155} INFO - Started process (PID=23161) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:01,844] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:15:01,845] {logging_mixin.py:112} INFO - [2020-11-05 16:15:01,845] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:01,985] {logging_mixin.py:112} INFO - [2020-11-05 16:15:01,983] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:15:01,986] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:02,008] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 16:15:15,083] {scheduler_job.py:155} INFO - Started process (PID=23223) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:15,087] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:15:15,088] {logging_mixin.py:112} INFO - [2020-11-05 16:15:15,088] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:15,286] {logging_mixin.py:112} INFO - [2020-11-05 16:15:15,285] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:15:15,286] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:15,325] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.242 seconds
[2020-11-05 16:15:28,318] {scheduler_job.py:155} INFO - Started process (PID=23285) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:28,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:15:28,321] {logging_mixin.py:112} INFO - [2020-11-05 16:15:28,321] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:28,461] {logging_mixin.py:112} INFO - [2020-11-05 16:15:28,461] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:15:28,462] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:28,492] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 16:15:41,572] {scheduler_job.py:155} INFO - Started process (PID=23341) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:41,577] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:15:41,578] {logging_mixin.py:112} INFO - [2020-11-05 16:15:41,577] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:41,746] {logging_mixin.py:112} INFO - [2020-11-05 16:15:41,745] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:15:41,746] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:41,778] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.206 seconds
[2020-11-05 16:15:54,877] {scheduler_job.py:155} INFO - Started process (PID=23406) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:54,881] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:15:54,882] {logging_mixin.py:112} INFO - [2020-11-05 16:15:54,882] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:55,116] {logging_mixin.py:112} INFO - [2020-11-05 16:15:55,115] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:15:55,117] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:15:55,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.290 seconds
[2020-11-05 16:16:08,060] {scheduler_job.py:155} INFO - Started process (PID=23467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:08,066] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:16:08,066] {logging_mixin.py:112} INFO - [2020-11-05 16:16:08,066] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:08,220] {logging_mixin.py:112} INFO - [2020-11-05 16:16:08,219] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:16:08,220] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:08,242] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.182 seconds
[2020-11-05 16:16:21,306] {scheduler_job.py:155} INFO - Started process (PID=23520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:21,310] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:16:21,310] {logging_mixin.py:112} INFO - [2020-11-05 16:16:21,310] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:21,550] {logging_mixin.py:112} INFO - [2020-11-05 16:16:21,549] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:16:21,551] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:21,587] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.281 seconds
[2020-11-05 16:16:33,818] {scheduler_job.py:155} INFO - Started process (PID=23574) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:33,825] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:16:33,826] {logging_mixin.py:112} INFO - [2020-11-05 16:16:33,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:34,078] {logging_mixin.py:112} INFO - [2020-11-05 16:16:34,077] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:16:34,079] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:34,126] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.308 seconds
[2020-11-05 16:16:45,910] {scheduler_job.py:155} INFO - Started process (PID=23627) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:45,915] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:16:45,916] {logging_mixin.py:112} INFO - [2020-11-05 16:16:45,915] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:46,158] {logging_mixin.py:112} INFO - [2020-11-05 16:16:46,157] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:16:46,159] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:46,208] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.299 seconds
[2020-11-05 16:16:59,269] {scheduler_job.py:155} INFO - Started process (PID=23683) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:59,272] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:16:59,273] {logging_mixin.py:112} INFO - [2020-11-05 16:16:59,273] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:59,400] {logging_mixin.py:112} INFO - [2020-11-05 16:16:59,398] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:16:59,401] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:16:59,443] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 16:17:12,537] {scheduler_job.py:155} INFO - Started process (PID=23743) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:12,540] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:17:12,541] {logging_mixin.py:112} INFO - [2020-11-05 16:17:12,541] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:12,675] {logging_mixin.py:112} INFO - [2020-11-05 16:17:12,675] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:17:12,676] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:12,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-05 16:17:25,809] {scheduler_job.py:155} INFO - Started process (PID=23801) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:25,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:17:25,815] {logging_mixin.py:112} INFO - [2020-11-05 16:17:25,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:26,050] {logging_mixin.py:112} INFO - [2020-11-05 16:17:26,049] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:17:26,051] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:26,082] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.273 seconds
[2020-11-05 16:17:39,133] {scheduler_job.py:155} INFO - Started process (PID=23856) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:39,138] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:17:39,139] {logging_mixin.py:112} INFO - [2020-11-05 16:17:39,139] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:39,310] {logging_mixin.py:112} INFO - [2020-11-05 16:17:39,309] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:17:39,311] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:39,347] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 16:17:52,422] {scheduler_job.py:155} INFO - Started process (PID=23910) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:52,430] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:17:52,434] {logging_mixin.py:112} INFO - [2020-11-05 16:17:52,431] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:52,640] {logging_mixin.py:112} INFO - [2020-11-05 16:17:52,639] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:17:52,641] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:17:52,687] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 16:18:05,804] {scheduler_job.py:155} INFO - Started process (PID=23965) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:05,811] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:18:05,813] {logging_mixin.py:112} INFO - [2020-11-05 16:18:05,812] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:06,007] {logging_mixin.py:112} INFO - [2020-11-05 16:18:06,004] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:18:06,008] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:06,051] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-05 16:18:18,999] {scheduler_job.py:155} INFO - Started process (PID=24027) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:19,002] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:18:19,003] {logging_mixin.py:112} INFO - [2020-11-05 16:18:19,003] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:19,122] {logging_mixin.py:112} INFO - [2020-11-05 16:18:19,121] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:18:19,123] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:19,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 16:18:32,310] {scheduler_job.py:155} INFO - Started process (PID=24093) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:32,315] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:18:32,316] {logging_mixin.py:112} INFO - [2020-11-05 16:18:32,315] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:32,434] {logging_mixin.py:112} INFO - [2020-11-05 16:18:32,433] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:18:32,434] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:32,459] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 16:18:45,525] {scheduler_job.py:155} INFO - Started process (PID=24148) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:45,528] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:18:45,529] {logging_mixin.py:112} INFO - [2020-11-05 16:18:45,529] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:45,664] {logging_mixin.py:112} INFO - [2020-11-05 16:18:45,663] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:18:45,665] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:45,701] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 16:18:58,817] {scheduler_job.py:155} INFO - Started process (PID=24205) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:58,825] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:18:58,826] {logging_mixin.py:112} INFO - [2020-11-05 16:18:58,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:58,975] {logging_mixin.py:112} INFO - [2020-11-05 16:18:58,973] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:18:58,975] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:18:59,001] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-05 16:19:12,054] {scheduler_job.py:155} INFO - Started process (PID=24268) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:12,064] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:19:12,066] {logging_mixin.py:112} INFO - [2020-11-05 16:19:12,065] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:12,231] {logging_mixin.py:112} INFO - [2020-11-05 16:19:12,230] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:19:12,233] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:12,267] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 16:19:25,315] {scheduler_job.py:155} INFO - Started process (PID=24329) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:25,327] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:19:25,327] {logging_mixin.py:112} INFO - [2020-11-05 16:19:25,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:25,541] {logging_mixin.py:112} INFO - [2020-11-05 16:19:25,532] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:19:25,543] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:25,585] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-05 16:19:38,607] {scheduler_job.py:155} INFO - Started process (PID=24387) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:38,610] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:19:38,611] {logging_mixin.py:112} INFO - [2020-11-05 16:19:38,611] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:38,736] {logging_mixin.py:112} INFO - [2020-11-05 16:19:38,735] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:19:38,737] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:38,770] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 16:19:51,882] {scheduler_job.py:155} INFO - Started process (PID=24442) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:51,890] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:19:51,891] {logging_mixin.py:112} INFO - [2020-11-05 16:19:51,891] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:52,087] {logging_mixin.py:112} INFO - [2020-11-05 16:19:52,086] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:19:52,087] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:19:52,110] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.228 seconds
[2020-11-05 16:20:05,115] {scheduler_job.py:155} INFO - Started process (PID=24502) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:05,118] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:20:05,119] {logging_mixin.py:112} INFO - [2020-11-05 16:20:05,119] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:05,235] {logging_mixin.py:112} INFO - [2020-11-05 16:20:05,233] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:20:05,236] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:05,296] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 16:20:18,336] {scheduler_job.py:155} INFO - Started process (PID=24561) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:18,341] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:20:18,342] {logging_mixin.py:112} INFO - [2020-11-05 16:20:18,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:18,463] {logging_mixin.py:112} INFO - [2020-11-05 16:20:18,461] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:20:18,463] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:18,486] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 16:20:31,606] {scheduler_job.py:155} INFO - Started process (PID=24618) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:31,616] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:20:31,618] {logging_mixin.py:112} INFO - [2020-11-05 16:20:31,618] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:31,788] {logging_mixin.py:112} INFO - [2020-11-05 16:20:31,787] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:20:31,788] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:31,814] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.208 seconds
[2020-11-05 16:20:43,835] {scheduler_job.py:155} INFO - Started process (PID=24672) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:43,839] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:20:43,839] {logging_mixin.py:112} INFO - [2020-11-05 16:20:43,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:43,945] {logging_mixin.py:112} INFO - [2020-11-05 16:20:43,944] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:20:43,945] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:43,978] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 16:20:57,033] {scheduler_job.py:155} INFO - Started process (PID=24731) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:57,036] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:20:57,037] {logging_mixin.py:112} INFO - [2020-11-05 16:20:57,037] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:57,191] {logging_mixin.py:112} INFO - [2020-11-05 16:20:57,190] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:20:57,191] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:20:57,213] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 16:21:10,280] {scheduler_job.py:155} INFO - Started process (PID=24786) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:10,283] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:21:10,284] {logging_mixin.py:112} INFO - [2020-11-05 16:21:10,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:10,381] {logging_mixin.py:112} INFO - [2020-11-05 16:21:10,380] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:21:10,381] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:10,408] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 16:21:23,595] {scheduler_job.py:155} INFO - Started process (PID=24853) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:23,598] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:21:23,599] {logging_mixin.py:112} INFO - [2020-11-05 16:21:23,598] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:23,709] {logging_mixin.py:112} INFO - [2020-11-05 16:21:23,708] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:21:23,709] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:23,742] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 16:21:36,845] {scheduler_job.py:155} INFO - Started process (PID=24921) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:36,851] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:21:36,852] {logging_mixin.py:112} INFO - [2020-11-05 16:21:36,852] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:36,956] {logging_mixin.py:112} INFO - [2020-11-05 16:21:36,955] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:21:36,957] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:36,988] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.143 seconds
[2020-11-05 16:21:50,072] {scheduler_job.py:155} INFO - Started process (PID=24978) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:50,075] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:21:50,076] {logging_mixin.py:112} INFO - [2020-11-05 16:21:50,076] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:50,202] {logging_mixin.py:112} INFO - [2020-11-05 16:21:50,200] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:21:50,203] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:21:50,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:22:03,320] {scheduler_job.py:155} INFO - Started process (PID=25046) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:03,323] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:22:03,324] {logging_mixin.py:112} INFO - [2020-11-05 16:22:03,323] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:03,451] {logging_mixin.py:112} INFO - [2020-11-05 16:22:03,450] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:22:03,451] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:03,481] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 16:22:16,547] {scheduler_job.py:155} INFO - Started process (PID=25107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:16,552] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:22:16,554] {logging_mixin.py:112} INFO - [2020-11-05 16:22:16,553] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:16,718] {logging_mixin.py:112} INFO - [2020-11-05 16:22:16,717] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:22:16,719] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:16,745] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.198 seconds
[2020-11-05 16:22:29,736] {scheduler_job.py:155} INFO - Started process (PID=25175) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:29,740] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:22:29,741] {logging_mixin.py:112} INFO - [2020-11-05 16:22:29,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:29,847] {logging_mixin.py:112} INFO - [2020-11-05 16:22:29,846] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:22:29,848] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:29,871] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.135 seconds
[2020-11-05 16:22:42,976] {scheduler_job.py:155} INFO - Started process (PID=25235) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:42,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:22:42,980] {logging_mixin.py:112} INFO - [2020-11-05 16:22:42,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:43,079] {logging_mixin.py:112} INFO - [2020-11-05 16:22:43,078] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:22:43,080] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:43,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 16:22:56,187] {scheduler_job.py:155} INFO - Started process (PID=25295) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:56,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:22:56,198] {logging_mixin.py:112} INFO - [2020-11-05 16:22:56,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:56,290] {logging_mixin.py:112} INFO - [2020-11-05 16:22:56,289] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:22:56,290] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:22:56,315] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 16:23:09,434] {scheduler_job.py:155} INFO - Started process (PID=25357) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:09,438] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:23:09,438] {logging_mixin.py:112} INFO - [2020-11-05 16:23:09,438] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:09,564] {logging_mixin.py:112} INFO - [2020-11-05 16:23:09,562] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:23:09,564] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:09,590] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:23:22,620] {scheduler_job.py:155} INFO - Started process (PID=25417) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:22,624] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:23:22,624] {logging_mixin.py:112} INFO - [2020-11-05 16:23:22,624] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:22,723] {logging_mixin.py:112} INFO - [2020-11-05 16:23:22,722] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:23:22,724] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:22,751] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 16:23:35,893] {scheduler_job.py:155} INFO - Started process (PID=25477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:35,897] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:23:35,898] {logging_mixin.py:112} INFO - [2020-11-05 16:23:35,898] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:36,021] {logging_mixin.py:112} INFO - [2020-11-05 16:23:36,020] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:23:36,022] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:36,061] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.168 seconds
[2020-11-05 16:23:49,128] {scheduler_job.py:155} INFO - Started process (PID=25566) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:49,143] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:23:49,149] {logging_mixin.py:112} INFO - [2020-11-05 16:23:49,149] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:49,302] {logging_mixin.py:112} INFO - [2020-11-05 16:23:49,300] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:23:49,303] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:23:49,347] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.219 seconds
[2020-11-05 16:24:02,439] {scheduler_job.py:155} INFO - Started process (PID=25649) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:02,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:24:02,470] {logging_mixin.py:112} INFO - [2020-11-05 16:24:02,469] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:02,753] {logging_mixin.py:112} INFO - [2020-11-05 16:24:02,752] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:24:02,753] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:02,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.344 seconds
[2020-11-05 16:24:15,783] {scheduler_job.py:155} INFO - Started process (PID=25707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:15,789] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:24:15,790] {logging_mixin.py:112} INFO - [2020-11-05 16:24:15,790] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:16,024] {logging_mixin.py:112} INFO - [2020-11-05 16:24:16,021] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:24:16,025] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:16,080] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.297 seconds
[2020-11-05 16:24:29,031] {scheduler_job.py:155} INFO - Started process (PID=25769) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:29,043] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:24:29,043] {logging_mixin.py:112} INFO - [2020-11-05 16:24:29,043] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:29,212] {logging_mixin.py:112} INFO - [2020-11-05 16:24:29,211] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:24:29,213] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:29,235] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-05 16:24:42,242] {scheduler_job.py:155} INFO - Started process (PID=25828) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:42,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:24:42,247] {logging_mixin.py:112} INFO - [2020-11-05 16:24:42,247] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:42,372] {logging_mixin.py:112} INFO - [2020-11-05 16:24:42,371] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:24:42,374] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:42,399] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:24:55,437] {scheduler_job.py:155} INFO - Started process (PID=25889) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:55,442] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:24:55,443] {logging_mixin.py:112} INFO - [2020-11-05 16:24:55,443] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:55,624] {logging_mixin.py:112} INFO - [2020-11-05 16:24:55,620] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:24:55,625] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:24:55,680] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.243 seconds
[2020-11-05 16:25:08,735] {scheduler_job.py:155} INFO - Started process (PID=25957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:08,739] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:25:08,740] {logging_mixin.py:112} INFO - [2020-11-05 16:25:08,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:08,898] {logging_mixin.py:112} INFO - [2020-11-05 16:25:08,898] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:25:08,899] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:08,920] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-05 16:25:21,958] {scheduler_job.py:155} INFO - Started process (PID=26036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:21,964] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:25:21,965] {logging_mixin.py:112} INFO - [2020-11-05 16:25:21,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:22,150] {logging_mixin.py:112} INFO - [2020-11-05 16:25:22,148] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:25:22,150] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:22,176] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.218 seconds
[2020-11-05 16:25:35,156] {scheduler_job.py:155} INFO - Started process (PID=26101) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:35,161] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:25:35,162] {logging_mixin.py:112} INFO - [2020-11-05 16:25:35,162] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:35,308] {logging_mixin.py:112} INFO - [2020-11-05 16:25:35,307] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:25:35,308] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:35,335] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 16:25:48,371] {scheduler_job.py:155} INFO - Started process (PID=26157) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:48,394] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:25:48,395] {logging_mixin.py:112} INFO - [2020-11-05 16:25:48,394] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:48,583] {logging_mixin.py:112} INFO - [2020-11-05 16:25:48,580] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:25:48,583] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:25:48,622] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-05 16:26:01,604] {scheduler_job.py:155} INFO - Started process (PID=26216) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:01,618] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:26:01,619] {logging_mixin.py:112} INFO - [2020-11-05 16:26:01,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:01,777] {logging_mixin.py:112} INFO - [2020-11-05 16:26:01,776] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:26:01,777] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:01,813] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.209 seconds
[2020-11-05 16:26:14,911] {scheduler_job.py:155} INFO - Started process (PID=26272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:14,921] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:26:14,922] {logging_mixin.py:112} INFO - [2020-11-05 16:26:14,922] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:15,039] {logging_mixin.py:112} INFO - [2020-11-05 16:26:15,039] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:26:15,040] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:15,070] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 16:26:28,094] {scheduler_job.py:155} INFO - Started process (PID=26333) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:28,099] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:26:28,104] {logging_mixin.py:112} INFO - [2020-11-05 16:26:28,103] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:28,391] {logging_mixin.py:112} INFO - [2020-11-05 16:26:28,389] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:26:28,392] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:28,420] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.327 seconds
[2020-11-05 16:26:41,371] {scheduler_job.py:155} INFO - Started process (PID=26393) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:41,379] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:26:41,389] {logging_mixin.py:112} INFO - [2020-11-05 16:26:41,383] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:41,548] {logging_mixin.py:112} INFO - [2020-11-05 16:26:41,546] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:26:41,548] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:41,573] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 16:26:54,545] {scheduler_job.py:155} INFO - Started process (PID=26453) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:54,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:26:54,552] {logging_mixin.py:112} INFO - [2020-11-05 16:26:54,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:54,680] {logging_mixin.py:112} INFO - [2020-11-05 16:26:54,679] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:26:54,681] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:26:54,709] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 16:27:07,753] {scheduler_job.py:155} INFO - Started process (PID=26516) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:07,756] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:27:07,757] {logging_mixin.py:112} INFO - [2020-11-05 16:27:07,756] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:07,857] {logging_mixin.py:112} INFO - [2020-11-05 16:27:07,856] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:27:07,857] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:07,890] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 16:27:21,019] {scheduler_job.py:155} INFO - Started process (PID=26578) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:21,024] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:27:21,024] {logging_mixin.py:112} INFO - [2020-11-05 16:27:21,024] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:21,133] {logging_mixin.py:112} INFO - [2020-11-05 16:27:21,132] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:27:21,133] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:21,166] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.146 seconds
[2020-11-05 16:27:34,280] {scheduler_job.py:155} INFO - Started process (PID=26642) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:34,283] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:27:34,285] {logging_mixin.py:112} INFO - [2020-11-05 16:27:34,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:34,407] {logging_mixin.py:112} INFO - [2020-11-05 16:27:34,406] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:27:34,408] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:34,446] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.166 seconds
[2020-11-05 16:27:47,519] {scheduler_job.py:155} INFO - Started process (PID=26702) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:47,526] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:27:47,527] {logging_mixin.py:112} INFO - [2020-11-05 16:27:47,526] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:47,628] {logging_mixin.py:112} INFO - [2020-11-05 16:27:47,627] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:27:47,628] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:27:47,651] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.132 seconds
[2020-11-05 16:28:00,757] {scheduler_job.py:155} INFO - Started process (PID=26772) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:00,761] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:28:00,761] {logging_mixin.py:112} INFO - [2020-11-05 16:28:00,761] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:00,855] {logging_mixin.py:112} INFO - [2020-11-05 16:28:00,854] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:28:00,855] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:00,890] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 16:28:14,043] {scheduler_job.py:155} INFO - Started process (PID=26835) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:14,048] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:28:14,049] {logging_mixin.py:112} INFO - [2020-11-05 16:28:14,049] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:14,150] {logging_mixin.py:112} INFO - [2020-11-05 16:28:14,149] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:28:14,150] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:14,176] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 16:28:27,233] {scheduler_job.py:155} INFO - Started process (PID=26904) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:27,239] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:28:27,240] {logging_mixin.py:112} INFO - [2020-11-05 16:28:27,239] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:27,357] {logging_mixin.py:112} INFO - [2020-11-05 16:28:27,355] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:28:27,357] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:27,381] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 16:28:40,483] {scheduler_job.py:155} INFO - Started process (PID=26966) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:40,488] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:28:40,489] {logging_mixin.py:112} INFO - [2020-11-05 16:28:40,488] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:40,596] {logging_mixin.py:112} INFO - [2020-11-05 16:28:40,595] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:28:40,596] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:40,621] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 16:28:53,710] {scheduler_job.py:155} INFO - Started process (PID=27031) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:53,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:28:53,714] {logging_mixin.py:112} INFO - [2020-11-05 16:28:53,714] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:53,825] {logging_mixin.py:112} INFO - [2020-11-05 16:28:53,824] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:28:53,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:28:53,854] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.144 seconds
[2020-11-05 16:29:06,892] {scheduler_job.py:155} INFO - Started process (PID=27095) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:06,897] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:29:06,899] {logging_mixin.py:112} INFO - [2020-11-05 16:29:06,898] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:07,049] {logging_mixin.py:112} INFO - [2020-11-05 16:29:07,048] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:29:07,049] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:07,076] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-05 16:29:20,174] {scheduler_job.py:155} INFO - Started process (PID=27156) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:20,179] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:29:20,179] {logging_mixin.py:112} INFO - [2020-11-05 16:29:20,179] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:20,289] {logging_mixin.py:112} INFO - [2020-11-05 16:29:20,287] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:29:20,289] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:20,323] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 16:29:33,388] {scheduler_job.py:155} INFO - Started process (PID=27226) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:33,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:29:33,394] {logging_mixin.py:112} INFO - [2020-11-05 16:29:33,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:33,499] {logging_mixin.py:112} INFO - [2020-11-05 16:29:33,498] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:29:33,499] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:33,535] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-05 16:29:46,725] {scheduler_job.py:155} INFO - Started process (PID=27286) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:46,743] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:29:46,743] {logging_mixin.py:112} INFO - [2020-11-05 16:29:46,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:46,937] {logging_mixin.py:112} INFO - [2020-11-05 16:29:46,935] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:29:46,937] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:46,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.248 seconds
[2020-11-05 16:29:59,889] {scheduler_job.py:155} INFO - Started process (PID=27346) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:29:59,894] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:29:59,897] {logging_mixin.py:112} INFO - [2020-11-05 16:29:59,895] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:00,103] {logging_mixin.py:112} INFO - [2020-11-05 16:30:00,102] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:30:00,103] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:00,139] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.250 seconds
[2020-11-05 16:30:13,110] {scheduler_job.py:155} INFO - Started process (PID=27415) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:13,118] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:30:13,118] {logging_mixin.py:112} INFO - [2020-11-05 16:30:13,118] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:13,240] {logging_mixin.py:112} INFO - [2020-11-05 16:30:13,239] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:30:13,240] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:13,269] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 16:30:26,310] {scheduler_job.py:155} INFO - Started process (PID=27477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:26,314] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:30:26,315] {logging_mixin.py:112} INFO - [2020-11-05 16:30:26,315] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:26,412] {logging_mixin.py:112} INFO - [2020-11-05 16:30:26,411] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:30:26,412] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:26,440] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-05 16:30:39,555] {scheduler_job.py:155} INFO - Started process (PID=27538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:39,558] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:30:39,559] {logging_mixin.py:112} INFO - [2020-11-05 16:30:39,559] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:39,668] {logging_mixin.py:112} INFO - [2020-11-05 16:30:39,667] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:30:39,668] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:39,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 16:30:52,795] {scheduler_job.py:155} INFO - Started process (PID=27593) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:52,800] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:30:52,808] {logging_mixin.py:112} INFO - [2020-11-05 16:30:52,807] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:52,933] {logging_mixin.py:112} INFO - [2020-11-05 16:30:52,932] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:30:52,933] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:30:52,957] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 16:31:06,186] {scheduler_job.py:155} INFO - Started process (PID=27663) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:06,191] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:31:06,192] {logging_mixin.py:112} INFO - [2020-11-05 16:31:06,192] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:06,297] {logging_mixin.py:112} INFO - [2020-11-05 16:31:06,296] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:31:06,298] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:06,319] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.133 seconds
[2020-11-05 16:31:19,438] {scheduler_job.py:155} INFO - Started process (PID=27724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:19,452] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:31:19,453] {logging_mixin.py:112} INFO - [2020-11-05 16:31:19,453] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:19,702] {logging_mixin.py:112} INFO - [2020-11-05 16:31:19,700] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:31:19,702] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:19,735] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.296 seconds
[2020-11-05 16:31:32,660] {scheduler_job.py:155} INFO - Started process (PID=27784) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:32,663] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:31:32,663] {logging_mixin.py:112} INFO - [2020-11-05 16:31:32,663] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:32,766] {logging_mixin.py:112} INFO - [2020-11-05 16:31:32,765] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:31:32,766] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:32,797] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 16:31:45,921] {scheduler_job.py:155} INFO - Started process (PID=27844) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:45,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:31:45,926] {logging_mixin.py:112} INFO - [2020-11-05 16:31:45,926] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:46,132] {logging_mixin.py:112} INFO - [2020-11-05 16:31:46,131] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:31:46,133] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:46,167] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-05 16:31:59,123] {scheduler_job.py:155} INFO - Started process (PID=27905) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:59,127] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:31:59,128] {logging_mixin.py:112} INFO - [2020-11-05 16:31:59,128] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:59,247] {logging_mixin.py:112} INFO - [2020-11-05 16:31:59,246] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:31:59,247] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:31:59,279] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:32:12,343] {scheduler_job.py:155} INFO - Started process (PID=27967) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:12,346] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:32:12,347] {logging_mixin.py:112} INFO - [2020-11-05 16:32:12,347] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:12,483] {logging_mixin.py:112} INFO - [2020-11-05 16:32:12,482] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:32:12,484] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:12,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 16:32:25,636] {scheduler_job.py:155} INFO - Started process (PID=28027) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:25,640] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:32:25,640] {logging_mixin.py:112} INFO - [2020-11-05 16:32:25,640] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:25,743] {logging_mixin.py:112} INFO - [2020-11-05 16:32:25,742] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:32:25,743] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:25,766] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.130 seconds
[2020-11-05 16:32:38,882] {scheduler_job.py:155} INFO - Started process (PID=28090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:38,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:32:38,889] {logging_mixin.py:112} INFO - [2020-11-05 16:32:38,888] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:39,037] {logging_mixin.py:112} INFO - [2020-11-05 16:32:39,034] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:32:39,037] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:39,088] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.206 seconds
[2020-11-05 16:32:52,169] {scheduler_job.py:155} INFO - Started process (PID=28149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:52,172] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:32:52,173] {logging_mixin.py:112} INFO - [2020-11-05 16:32:52,173] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:52,313] {logging_mixin.py:112} INFO - [2020-11-05 16:32:52,311] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:32:52,313] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:32:52,359] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 16:33:05,387] {scheduler_job.py:155} INFO - Started process (PID=28210) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:05,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:33:05,392] {logging_mixin.py:112} INFO - [2020-11-05 16:33:05,392] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:05,531] {logging_mixin.py:112} INFO - [2020-11-05 16:33:05,526] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:33:05,531] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:05,555] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.168 seconds
[2020-11-05 16:33:18,638] {scheduler_job.py:155} INFO - Started process (PID=28271) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:18,642] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:33:18,643] {logging_mixin.py:112} INFO - [2020-11-05 16:33:18,643] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:18,770] {logging_mixin.py:112} INFO - [2020-11-05 16:33:18,769] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:33:18,771] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:18,808] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 16:33:31,863] {scheduler_job.py:155} INFO - Started process (PID=28336) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:31,868] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:33:31,869] {logging_mixin.py:112} INFO - [2020-11-05 16:33:31,869] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:31,972] {logging_mixin.py:112} INFO - [2020-11-05 16:33:31,971] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:33:31,972] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:32,019] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:33:45,099] {scheduler_job.py:155} INFO - Started process (PID=28398) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:45,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:33:45,112] {logging_mixin.py:112} INFO - [2020-11-05 16:33:45,111] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:45,238] {logging_mixin.py:112} INFO - [2020-11-05 16:33:45,237] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:33:45,238] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:45,265] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.166 seconds
[2020-11-05 16:33:58,291] {scheduler_job.py:155} INFO - Started process (PID=28467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:58,297] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:33:58,298] {logging_mixin.py:112} INFO - [2020-11-05 16:33:58,298] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:58,443] {logging_mixin.py:112} INFO - [2020-11-05 16:33:58,441] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:33:58,443] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:33:58,470] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 16:34:11,518] {scheduler_job.py:155} INFO - Started process (PID=28535) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:11,544] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:34:11,545] {logging_mixin.py:112} INFO - [2020-11-05 16:34:11,545] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:11,678] {logging_mixin.py:112} INFO - [2020-11-05 16:34:11,677] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:34:11,679] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:11,720] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 16:34:24,762] {scheduler_job.py:155} INFO - Started process (PID=28596) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:24,767] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:34:24,768] {logging_mixin.py:112} INFO - [2020-11-05 16:34:24,768] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:24,950] {logging_mixin.py:112} INFO - [2020-11-05 16:34:24,948] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:34:24,950] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:24,979] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.217 seconds
[2020-11-05 16:34:37,998] {scheduler_job.py:155} INFO - Started process (PID=28664) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:38,022] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:34:38,023] {logging_mixin.py:112} INFO - [2020-11-05 16:34:38,023] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:38,216] {logging_mixin.py:112} INFO - [2020-11-05 16:34:38,215] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:34:38,216] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:38,237] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-05 16:34:51,320] {scheduler_job.py:155} INFO - Started process (PID=28741) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:51,333] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:34:51,334] {logging_mixin.py:112} INFO - [2020-11-05 16:34:51,333] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:51,616] {logging_mixin.py:112} INFO - [2020-11-05 16:34:51,615] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:34:51,616] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:34:51,658] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.338 seconds
[2020-11-05 16:35:04,607] {scheduler_job.py:155} INFO - Started process (PID=28802) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:04,610] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:35:04,610] {logging_mixin.py:112} INFO - [2020-11-05 16:35:04,610] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:04,698] {logging_mixin.py:112} INFO - [2020-11-05 16:35:04,697] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:35:04,698] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:04,734] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.128 seconds
[2020-11-05 16:35:17,973] {scheduler_job.py:155} INFO - Started process (PID=28867) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:17,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:35:17,981] {logging_mixin.py:112} INFO - [2020-11-05 16:35:17,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:18,243] {logging_mixin.py:112} INFO - [2020-11-05 16:35:18,240] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:35:18,244] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:18,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.319 seconds
[2020-11-05 16:35:31,242] {scheduler_job.py:155} INFO - Started process (PID=28926) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:31,247] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:35:31,254] {logging_mixin.py:112} INFO - [2020-11-05 16:35:31,253] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:31,431] {logging_mixin.py:112} INFO - [2020-11-05 16:35:31,430] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:35:31,431] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:31,465] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.223 seconds
[2020-11-05 16:35:44,506] {scheduler_job.py:155} INFO - Started process (PID=28986) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:44,522] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:35:44,523] {logging_mixin.py:112} INFO - [2020-11-05 16:35:44,523] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:44,773] {logging_mixin.py:112} INFO - [2020-11-05 16:35:44,769] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:35:44,773] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:44,821] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.315 seconds
[2020-11-05 16:35:57,739] {scheduler_job.py:155} INFO - Started process (PID=29045) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:57,748] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:35:57,755] {logging_mixin.py:112} INFO - [2020-11-05 16:35:57,754] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:57,876] {logging_mixin.py:112} INFO - [2020-11-05 16:35:57,874] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:35:57,876] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:35:57,901] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.162 seconds
[2020-11-05 16:36:10,971] {scheduler_job.py:155} INFO - Started process (PID=29102) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:10,978] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:36:10,978] {logging_mixin.py:112} INFO - [2020-11-05 16:36:10,978] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:11,179] {logging_mixin.py:112} INFO - [2020-11-05 16:36:11,178] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:36:11,179] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:11,227] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.256 seconds
[2020-11-05 16:36:24,280] {scheduler_job.py:155} INFO - Started process (PID=29161) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:24,284] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:36:24,285] {logging_mixin.py:112} INFO - [2020-11-05 16:36:24,285] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:24,373] {logging_mixin.py:112} INFO - [2020-11-05 16:36:24,372] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:36:24,374] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:24,414] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 16:36:37,517] {scheduler_job.py:155} INFO - Started process (PID=29222) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:37,531] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:36:37,532] {logging_mixin.py:112} INFO - [2020-11-05 16:36:37,532] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:37,702] {logging_mixin.py:112} INFO - [2020-11-05 16:36:37,701] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:36:37,702] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:37,729] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.213 seconds
[2020-11-05 16:36:50,726] {scheduler_job.py:155} INFO - Started process (PID=29285) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:50,729] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:36:50,730] {logging_mixin.py:112} INFO - [2020-11-05 16:36:50,729] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:50,849] {logging_mixin.py:112} INFO - [2020-11-05 16:36:50,848] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:36:50,850] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:36:50,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 16:37:04,000] {scheduler_job.py:155} INFO - Started process (PID=29346) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:04,004] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:37:04,009] {logging_mixin.py:112} INFO - [2020-11-05 16:37:04,009] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:04,125] {logging_mixin.py:112} INFO - [2020-11-05 16:37:04,124] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:37:04,126] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:04,156] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 16:37:17,232] {scheduler_job.py:155} INFO - Started process (PID=29408) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:17,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:37:17,247] {logging_mixin.py:112} INFO - [2020-11-05 16:37:17,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:17,406] {logging_mixin.py:112} INFO - [2020-11-05 16:37:17,405] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:37:17,407] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:17,444] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.213 seconds
[2020-11-05 16:37:30,419] {scheduler_job.py:155} INFO - Started process (PID=29469) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:30,422] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:37:30,424] {logging_mixin.py:112} INFO - [2020-11-05 16:37:30,423] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:30,517] {logging_mixin.py:112} INFO - [2020-11-05 16:37:30,516] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:37:30,517] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:30,541] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.122 seconds
[2020-11-05 16:37:43,678] {scheduler_job.py:155} INFO - Started process (PID=29532) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:43,683] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:37:43,684] {logging_mixin.py:112} INFO - [2020-11-05 16:37:43,683] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:43,948] {logging_mixin.py:112} INFO - [2020-11-05 16:37:43,947] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:37:43,948] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:43,994] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.316 seconds
[2020-11-05 16:37:57,038] {scheduler_job.py:155} INFO - Started process (PID=29590) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:57,052] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:37:57,053] {logging_mixin.py:112} INFO - [2020-11-05 16:37:57,052] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:57,174] {logging_mixin.py:112} INFO - [2020-11-05 16:37:57,173] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:37:57,174] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:37:57,207] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.168 seconds
[2020-11-05 16:38:10,300] {scheduler_job.py:155} INFO - Started process (PID=29649) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:10,306] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:38:10,307] {logging_mixin.py:112} INFO - [2020-11-05 16:38:10,307] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:10,420] {logging_mixin.py:112} INFO - [2020-11-05 16:38:10,419] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:38:10,420] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:10,455] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:38:23,585] {scheduler_job.py:155} INFO - Started process (PID=29708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:23,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:38:23,590] {logging_mixin.py:112} INFO - [2020-11-05 16:38:23,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:23,724] {logging_mixin.py:112} INFO - [2020-11-05 16:38:23,723] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:38:23,724] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:23,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.164 seconds
[2020-11-05 16:38:36,766] {scheduler_job.py:155} INFO - Started process (PID=29789) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:36,770] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:38:36,771] {logging_mixin.py:112} INFO - [2020-11-05 16:38:36,771] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:36,871] {logging_mixin.py:112} INFO - [2020-11-05 16:38:36,870] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:38:36,871] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:36,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.131 seconds
[2020-11-05 16:38:49,980] {scheduler_job.py:155} INFO - Started process (PID=29849) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:49,984] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:38:49,986] {logging_mixin.py:112} INFO - [2020-11-05 16:38:49,986] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:50,131] {logging_mixin.py:112} INFO - [2020-11-05 16:38:50,130] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:38:50,131] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:38:50,154] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 16:39:03,160] {scheduler_job.py:155} INFO - Started process (PID=29917) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:03,164] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:39:03,165] {logging_mixin.py:112} INFO - [2020-11-05 16:39:03,164] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:03,281] {logging_mixin.py:112} INFO - [2020-11-05 16:39:03,280] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:39:03,282] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:03,319] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 16:39:16,422] {scheduler_job.py:155} INFO - Started process (PID=29978) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:16,426] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:39:16,427] {logging_mixin.py:112} INFO - [2020-11-05 16:39:16,426] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:16,649] {logging_mixin.py:112} INFO - [2020-11-05 16:39:16,647] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:39:16,649] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:16,689] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.267 seconds
[2020-11-05 16:39:29,694] {scheduler_job.py:155} INFO - Started process (PID=30036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:29,707] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:39:29,712] {logging_mixin.py:112} INFO - [2020-11-05 16:39:29,712] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:29,926] {logging_mixin.py:112} INFO - [2020-11-05 16:39:29,924] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:39:29,926] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:29,958] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.264 seconds
[2020-11-05 16:39:42,944] {scheduler_job.py:155} INFO - Started process (PID=30092) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:42,950] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:39:42,951] {logging_mixin.py:112} INFO - [2020-11-05 16:39:42,951] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:43,062] {logging_mixin.py:112} INFO - [2020-11-05 16:39:43,060] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:39:43,062] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:43,103] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 16:39:56,236] {scheduler_job.py:155} INFO - Started process (PID=30152) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:56,241] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:39:56,241] {logging_mixin.py:112} INFO - [2020-11-05 16:39:56,241] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:56,542] {logging_mixin.py:112} INFO - [2020-11-05 16:39:56,540] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:39:56,542] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:39:56,580] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.345 seconds
[2020-11-05 16:40:09,422] {scheduler_job.py:155} INFO - Started process (PID=30215) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:09,428] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:40:09,431] {logging_mixin.py:112} INFO - [2020-11-05 16:40:09,430] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:09,655] {logging_mixin.py:112} INFO - [2020-11-05 16:40:09,653] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:40:09,655] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:09,697] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.275 seconds
[2020-11-05 16:40:22,627] {scheduler_job.py:155} INFO - Started process (PID=30282) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:22,632] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:40:22,633] {logging_mixin.py:112} INFO - [2020-11-05 16:40:22,633] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:22,768] {logging_mixin.py:112} INFO - [2020-11-05 16:40:22,767] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:40:22,769] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:22,800] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-05 16:40:35,897] {scheduler_job.py:155} INFO - Started process (PID=30342) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:35,906] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:40:35,909] {logging_mixin.py:112} INFO - [2020-11-05 16:40:35,907] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:36,013] {logging_mixin.py:112} INFO - [2020-11-05 16:40:36,012] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:40:36,014] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:36,039] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 16:40:49,146] {scheduler_job.py:155} INFO - Started process (PID=30403) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:49,162] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:40:49,167] {logging_mixin.py:112} INFO - [2020-11-05 16:40:49,167] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:49,338] {logging_mixin.py:112} INFO - [2020-11-05 16:40:49,337] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:40:49,339] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:40:49,364] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.220 seconds
[2020-11-05 16:41:02,405] {scheduler_job.py:155} INFO - Started process (PID=30463) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:02,409] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:41:02,410] {logging_mixin.py:112} INFO - [2020-11-05 16:41:02,410] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:02,509] {logging_mixin.py:112} INFO - [2020-11-05 16:41:02,507] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:41:02,509] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:02,541] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 16:41:15,613] {scheduler_job.py:155} INFO - Started process (PID=30522) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:15,616] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:41:15,617] {logging_mixin.py:112} INFO - [2020-11-05 16:41:15,616] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:15,707] {logging_mixin.py:112} INFO - [2020-11-05 16:41:15,705] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:41:15,708] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:15,736] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.123 seconds
[2020-11-05 16:41:28,833] {scheduler_job.py:155} INFO - Started process (PID=30585) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:28,837] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:41:28,838] {logging_mixin.py:112} INFO - [2020-11-05 16:41:28,837] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:28,952] {logging_mixin.py:112} INFO - [2020-11-05 16:41:28,951] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:41:28,953] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:28,974] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 16:41:42,079] {scheduler_job.py:155} INFO - Started process (PID=30646) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:42,082] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:41:42,082] {logging_mixin.py:112} INFO - [2020-11-05 16:41:42,082] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:42,176] {logging_mixin.py:112} INFO - [2020-11-05 16:41:42,175] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:41:42,176] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:42,198] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.120 seconds
[2020-11-05 16:41:55,291] {scheduler_job.py:155} INFO - Started process (PID=30707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:55,294] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:41:55,294] {logging_mixin.py:112} INFO - [2020-11-05 16:41:55,294] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:55,400] {logging_mixin.py:112} INFO - [2020-11-05 16:41:55,399] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:41:55,401] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:41:55,433] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 16:42:08,533] {scheduler_job.py:155} INFO - Started process (PID=30774) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:08,539] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:42:08,540] {logging_mixin.py:112} INFO - [2020-11-05 16:42:08,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:08,729] {logging_mixin.py:112} INFO - [2020-11-05 16:42:08,728] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:42:08,729] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:08,763] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.230 seconds
[2020-11-05 16:42:21,776] {scheduler_job.py:155} INFO - Started process (PID=30856) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:21,788] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:42:21,798] {logging_mixin.py:112} INFO - [2020-11-05 16:42:21,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:22,065] {logging_mixin.py:112} INFO - [2020-11-05 16:42:22,063] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:42:22,066] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:22,114] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.338 seconds
[2020-11-05 16:42:35,168] {scheduler_job.py:155} INFO - Started process (PID=30916) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:35,172] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:42:35,174] {logging_mixin.py:112} INFO - [2020-11-05 16:42:35,173] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:35,345] {logging_mixin.py:112} INFO - [2020-11-05 16:42:35,343] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:42:35,345] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:35,390] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.222 seconds
[2020-11-05 16:42:48,405] {scheduler_job.py:155} INFO - Started process (PID=30980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:48,419] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:42:48,419] {logging_mixin.py:112} INFO - [2020-11-05 16:42:48,419] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:48,625] {logging_mixin.py:112} INFO - [2020-11-05 16:42:48,623] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:42:48,625] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:42:48,684] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.279 seconds
[2020-11-05 16:43:01,689] {scheduler_job.py:155} INFO - Started process (PID=31040) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:01,693] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:43:01,694] {logging_mixin.py:112} INFO - [2020-11-05 16:43:01,693] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:01,808] {logging_mixin.py:112} INFO - [2020-11-05 16:43:01,807] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:43:01,809] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:01,836] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-05 16:43:14,898] {scheduler_job.py:155} INFO - Started process (PID=31097) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:14,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:43:14,902] {logging_mixin.py:112} INFO - [2020-11-05 16:43:14,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:15,078] {logging_mixin.py:112} INFO - [2020-11-05 16:43:15,076] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:43:15,079] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:15,115] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.217 seconds
[2020-11-05 16:43:28,190] {scheduler_job.py:155} INFO - Started process (PID=31162) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:28,196] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:43:28,197] {logging_mixin.py:112} INFO - [2020-11-05 16:43:28,197] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:28,325] {logging_mixin.py:112} INFO - [2020-11-05 16:43:28,323] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:43:28,325] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:28,357] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-05 16:43:41,439] {scheduler_job.py:155} INFO - Started process (PID=31220) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:41,453] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:43:41,457] {logging_mixin.py:112} INFO - [2020-11-05 16:43:41,457] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:41,738] {logging_mixin.py:112} INFO - [2020-11-05 16:43:41,737] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:43:41,738] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:41,772] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.333 seconds
[2020-11-05 16:43:54,751] {scheduler_job.py:155} INFO - Started process (PID=31277) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:54,755] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:43:54,755] {logging_mixin.py:112} INFO - [2020-11-05 16:43:54,755] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:54,919] {logging_mixin.py:112} INFO - [2020-11-05 16:43:54,917] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:43:54,919] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:43:54,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.232 seconds
[2020-11-05 16:44:07,996] {scheduler_job.py:155} INFO - Started process (PID=31335) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:08,001] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:44:08,002] {logging_mixin.py:112} INFO - [2020-11-05 16:44:08,002] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:08,159] {logging_mixin.py:112} INFO - [2020-11-05 16:44:08,158] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:44:08,159] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:08,185] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.189 seconds
[2020-11-05 16:44:21,323] {scheduler_job.py:155} INFO - Started process (PID=31393) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:21,359] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:44:21,363] {logging_mixin.py:112} INFO - [2020-11-05 16:44:21,363] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:21,683] {logging_mixin.py:112} INFO - [2020-11-05 16:44:21,682] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:44:21,683] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:21,710] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.387 seconds
[2020-11-05 16:44:34,586] {scheduler_job.py:155} INFO - Started process (PID=31448) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:34,593] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:44:34,594] {logging_mixin.py:112} INFO - [2020-11-05 16:44:34,594] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:34,819] {logging_mixin.py:112} INFO - [2020-11-05 16:44:34,818] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:44:34,819] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:34,846] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-05 16:44:47,859] {scheduler_job.py:155} INFO - Started process (PID=31511) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:47,889] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:44:47,889] {logging_mixin.py:112} INFO - [2020-11-05 16:44:47,889] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:48,232] {logging_mixin.py:112} INFO - [2020-11-05 16:44:48,231] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:44:48,233] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:44:48,273] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.415 seconds
[2020-11-05 16:45:01,093] {scheduler_job.py:155} INFO - Started process (PID=31566) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:01,096] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:45:01,097] {logging_mixin.py:112} INFO - [2020-11-05 16:45:01,097] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:01,240] {logging_mixin.py:112} INFO - [2020-11-05 16:45:01,239] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:45:01,240] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:01,270] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 16:45:14,326] {scheduler_job.py:155} INFO - Started process (PID=31626) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:14,329] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:45:14,329] {logging_mixin.py:112} INFO - [2020-11-05 16:45:14,329] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:14,437] {logging_mixin.py:112} INFO - [2020-11-05 16:45:14,436] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:45:14,437] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:14,464] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.138 seconds
[2020-11-05 16:45:27,561] {scheduler_job.py:155} INFO - Started process (PID=31692) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:27,566] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:45:27,567] {logging_mixin.py:112} INFO - [2020-11-05 16:45:27,567] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:27,684] {logging_mixin.py:112} INFO - [2020-11-05 16:45:27,683] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:45:27,685] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:27,708] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.147 seconds
[2020-11-05 16:45:40,767] {scheduler_job.py:155} INFO - Started process (PID=31760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:40,770] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:45:40,771] {logging_mixin.py:112} INFO - [2020-11-05 16:45:40,771] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:40,859] {logging_mixin.py:112} INFO - [2020-11-05 16:45:40,858] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:45:40,860] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:40,893] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.126 seconds
[2020-11-05 16:45:54,023] {scheduler_job.py:155} INFO - Started process (PID=31821) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:54,041] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:45:54,042] {logging_mixin.py:112} INFO - [2020-11-05 16:45:54,042] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:54,250] {logging_mixin.py:112} INFO - [2020-11-05 16:45:54,248] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:45:54,250] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:45:54,285] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.262 seconds
[2020-11-05 16:46:07,327] {scheduler_job.py:155} INFO - Started process (PID=31876) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:07,340] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:46:07,341] {logging_mixin.py:112} INFO - [2020-11-05 16:46:07,341] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:07,473] {logging_mixin.py:112} INFO - [2020-11-05 16:46:07,472] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:46:07,474] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:07,510] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.184 seconds
[2020-11-05 16:46:20,530] {scheduler_job.py:155} INFO - Started process (PID=31935) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:20,533] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:46:20,534] {logging_mixin.py:112} INFO - [2020-11-05 16:46:20,534] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:20,638] {logging_mixin.py:112} INFO - [2020-11-05 16:46:20,637] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:46:20,640] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:20,667] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.137 seconds
[2020-11-05 16:46:33,741] {scheduler_job.py:155} INFO - Started process (PID=32009) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:33,744] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:46:33,744] {logging_mixin.py:112} INFO - [2020-11-05 16:46:33,744] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:33,828] {logging_mixin.py:112} INFO - [2020-11-05 16:46:33,827] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:46:33,828] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:33,852] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.111 seconds
[2020-11-05 16:46:46,966] {scheduler_job.py:155} INFO - Started process (PID=32092) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:46,970] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:46:46,971] {logging_mixin.py:112} INFO - [2020-11-05 16:46:46,970] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:47,158] {logging_mixin.py:112} INFO - [2020-11-05 16:46:47,155] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:46:47,158] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:46:47,190] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 16:47:00,229] {scheduler_job.py:155} INFO - Started process (PID=32152) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:00,236] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:47:00,237] {logging_mixin.py:112} INFO - [2020-11-05 16:47:00,236] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:00,382] {logging_mixin.py:112} INFO - [2020-11-05 16:47:00,380] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:47:00,383] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:00,419] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 16:47:13,436] {scheduler_job.py:155} INFO - Started process (PID=32212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:13,442] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:47:13,443] {logging_mixin.py:112} INFO - [2020-11-05 16:47:13,443] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:13,551] {logging_mixin.py:112} INFO - [2020-11-05 16:47:13,550] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:47:13,551] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:13,576] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.140 seconds
[2020-11-05 16:47:26,701] {scheduler_job.py:155} INFO - Started process (PID=32273) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:26,747] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:47:26,821] {logging_mixin.py:112} INFO - [2020-11-05 16:47:26,821] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:27,011] {logging_mixin.py:112} INFO - [2020-11-05 16:47:27,009] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:47:27,011] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:27,041] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.340 seconds
[2020-11-05 16:47:39,999] {scheduler_job.py:155} INFO - Started process (PID=32330) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:40,003] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:47:40,003] {logging_mixin.py:112} INFO - [2020-11-05 16:47:40,003] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:40,125] {logging_mixin.py:112} INFO - [2020-11-05 16:47:40,123] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:47:40,127] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:40,155] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 16:47:53,219] {scheduler_job.py:155} INFO - Started process (PID=32386) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:53,225] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:47:53,226] {logging_mixin.py:112} INFO - [2020-11-05 16:47:53,226] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:53,391] {logging_mixin.py:112} INFO - [2020-11-05 16:47:53,390] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:47:53,392] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:47:53,435] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.216 seconds
[2020-11-05 16:48:06,496] {scheduler_job.py:155} INFO - Started process (PID=32454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:06,500] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:48:06,501] {logging_mixin.py:112} INFO - [2020-11-05 16:48:06,501] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:06,617] {logging_mixin.py:112} INFO - [2020-11-05 16:48:06,615] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:48:06,617] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:06,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 16:48:19,723] {scheduler_job.py:155} INFO - Started process (PID=32515) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:19,726] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:48:19,727] {logging_mixin.py:112} INFO - [2020-11-05 16:48:19,727] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:19,839] {logging_mixin.py:112} INFO - [2020-11-05 16:48:19,838] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:48:19,839] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:19,868] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.145 seconds
[2020-11-05 16:48:32,987] {scheduler_job.py:155} INFO - Started process (PID=32576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:32,992] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:48:32,993] {logging_mixin.py:112} INFO - [2020-11-05 16:48:32,993] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:33,096] {logging_mixin.py:112} INFO - [2020-11-05 16:48:33,095] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:48:33,096] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:33,136] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 16:48:45,205] {scheduler_job.py:155} INFO - Started process (PID=32634) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:45,211] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:48:45,212] {logging_mixin.py:112} INFO - [2020-11-05 16:48:45,211] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:45,313] {logging_mixin.py:112} INFO - [2020-11-05 16:48:45,312] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:48:45,314] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:45,340] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.134 seconds
[2020-11-05 16:48:58,407] {scheduler_job.py:155} INFO - Started process (PID=32698) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:58,430] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:48:58,431] {logging_mixin.py:112} INFO - [2020-11-05 16:48:58,431] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:58,646] {logging_mixin.py:112} INFO - [2020-11-05 16:48:58,644] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:48:58,646] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:48:58,677] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-05 16:49:11,675] {scheduler_job.py:155} INFO - Started process (PID=32765) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:11,680] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:49:11,680] {logging_mixin.py:112} INFO - [2020-11-05 16:49:11,680] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:11,888] {logging_mixin.py:112} INFO - [2020-11-05 16:49:11,887] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:49:11,889] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:11,919] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.244 seconds
[2020-11-05 16:49:24,868] {scheduler_job.py:155} INFO - Started process (PID=364) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:24,873] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:49:24,874] {logging_mixin.py:112} INFO - [2020-11-05 16:49:24,874] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:24,982] {logging_mixin.py:112} INFO - [2020-11-05 16:49:24,981] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:49:24,982] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:25,010] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.142 seconds
[2020-11-05 16:49:38,115] {scheduler_job.py:155} INFO - Started process (PID=423) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:38,128] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:49:38,137] {logging_mixin.py:112} INFO - [2020-11-05 16:49:38,137] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:38,311] {logging_mixin.py:112} INFO - [2020-11-05 16:49:38,310] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:49:38,311] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:38,353] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.238 seconds
[2020-11-05 16:49:51,318] {scheduler_job.py:155} INFO - Started process (PID=480) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:51,323] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:49:51,324] {logging_mixin.py:112} INFO - [2020-11-05 16:49:51,323] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:51,466] {logging_mixin.py:112} INFO - [2020-11-05 16:49:51,465] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:49:51,466] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:49:51,507] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 16:50:04,572] {scheduler_job.py:155} INFO - Started process (PID=557) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:04,585] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:50:04,585] {logging_mixin.py:112} INFO - [2020-11-05 16:50:04,585] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:04,862] {logging_mixin.py:112} INFO - [2020-11-05 16:50:04,860] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:50:04,863] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:04,893] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.320 seconds
[2020-11-05 16:50:17,770] {scheduler_job.py:155} INFO - Started process (PID=636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:17,774] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:50:17,775] {logging_mixin.py:112} INFO - [2020-11-05 16:50:17,775] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:17,921] {logging_mixin.py:112} INFO - [2020-11-05 16:50:17,920] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:50:17,921] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:17,947] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 16:50:30,946] {scheduler_job.py:155} INFO - Started process (PID=710) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:30,950] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:50:30,951] {logging_mixin.py:112} INFO - [2020-11-05 16:50:30,950] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:31,124] {logging_mixin.py:112} INFO - [2020-11-05 16:50:31,123] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:50:31,124] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:31,154] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.209 seconds
[2020-11-05 16:50:44,232] {scheduler_job.py:155} INFO - Started process (PID=772) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:44,236] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:50:44,237] {logging_mixin.py:112} INFO - [2020-11-05 16:50:44,237] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:44,375] {logging_mixin.py:112} INFO - [2020-11-05 16:50:44,374] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:50:44,376] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:44,418] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-05 16:50:57,478] {scheduler_job.py:155} INFO - Started process (PID=828) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:57,486] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:50:57,487] {logging_mixin.py:112} INFO - [2020-11-05 16:50:57,486] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:57,818] {logging_mixin.py:112} INFO - [2020-11-05 16:50:57,815] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:50:57,818] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:50:57,855] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.377 seconds
[2020-11-05 16:51:10,764] {scheduler_job.py:155} INFO - Started process (PID=889) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:10,767] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:51:10,767] {logging_mixin.py:112} INFO - [2020-11-05 16:51:10,767] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:10,894] {logging_mixin.py:112} INFO - [2020-11-05 16:51:10,893] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:51:10,894] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:10,919] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:51:23,966] {scheduler_job.py:155} INFO - Started process (PID=947) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:23,971] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:51:23,971] {logging_mixin.py:112} INFO - [2020-11-05 16:51:23,971] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:24,109] {logging_mixin.py:112} INFO - [2020-11-05 16:51:24,108] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:51:24,110] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:24,160] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.194 seconds
[2020-11-05 16:51:37,238] {scheduler_job.py:155} INFO - Started process (PID=1011) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:37,246] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:51:37,246] {logging_mixin.py:112} INFO - [2020-11-05 16:51:37,246] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:37,392] {logging_mixin.py:112} INFO - [2020-11-05 16:51:37,391] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:51:37,396] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:37,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 16:51:50,434] {scheduler_job.py:155} INFO - Started process (PID=1071) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:50,442] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:51:50,444] {logging_mixin.py:112} INFO - [2020-11-05 16:51:50,444] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:50,598] {logging_mixin.py:112} INFO - [2020-11-05 16:51:50,597] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:51:50,598] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:51:50,632] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 16:52:03,654] {scheduler_job.py:155} INFO - Started process (PID=1138) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:03,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:52:03,660] {logging_mixin.py:112} INFO - [2020-11-05 16:52:03,660] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:03,821] {logging_mixin.py:112} INFO - [2020-11-05 16:52:03,819] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:52:03,825] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:03,857] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 16:52:16,890] {scheduler_job.py:155} INFO - Started process (PID=1197) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:16,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:52:16,902] {logging_mixin.py:112} INFO - [2020-11-05 16:52:16,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:17,038] {logging_mixin.py:112} INFO - [2020-11-05 16:52:17,037] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:52:17,038] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:17,065] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 16:52:30,107] {scheduler_job.py:155} INFO - Started process (PID=1274) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:30,111] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:52:30,112] {logging_mixin.py:112} INFO - [2020-11-05 16:52:30,112] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:30,340] {logging_mixin.py:112} INFO - [2020-11-05 16:52:30,339] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:52:30,340] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:30,391] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.284 seconds
[2020-11-05 16:52:43,426] {scheduler_job.py:155} INFO - Started process (PID=1348) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:43,431] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:52:43,432] {logging_mixin.py:112} INFO - [2020-11-05 16:52:43,432] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:43,640] {logging_mixin.py:112} INFO - [2020-11-05 16:52:43,639] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:52:43,641] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:43,681] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.255 seconds
[2020-11-05 16:52:56,714] {scheduler_job.py:155} INFO - Started process (PID=1407) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:56,718] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:52:56,723] {logging_mixin.py:112} INFO - [2020-11-05 16:52:56,723] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:56,893] {logging_mixin.py:112} INFO - [2020-11-05 16:52:56,892] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:52:56,893] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:52:56,917] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 16:53:10,040] {scheduler_job.py:155} INFO - Started process (PID=1470) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:10,047] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:53:10,049] {logging_mixin.py:112} INFO - [2020-11-05 16:53:10,049] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:10,196] {logging_mixin.py:112} INFO - [2020-11-05 16:53:10,195] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:53:10,196] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:10,235] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.196 seconds
[2020-11-05 16:53:23,222] {scheduler_job.py:155} INFO - Started process (PID=1546) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:23,226] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:53:23,227] {logging_mixin.py:112} INFO - [2020-11-05 16:53:23,226] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:23,424] {logging_mixin.py:112} INFO - [2020-11-05 16:53:23,423] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:53:23,425] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:23,465] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.242 seconds
[2020-11-05 16:53:36,613] {scheduler_job.py:155} INFO - Started process (PID=1628) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:36,620] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:53:36,630] {logging_mixin.py:112} INFO - [2020-11-05 16:53:36,629] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:36,937] {logging_mixin.py:112} INFO - [2020-11-05 16:53:36,936] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:53:36,938] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:36,984] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.371 seconds
[2020-11-05 16:53:49,846] {scheduler_job.py:155} INFO - Started process (PID=1687) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:49,868] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:53:49,869] {logging_mixin.py:112} INFO - [2020-11-05 16:53:49,869] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:50,040] {logging_mixin.py:112} INFO - [2020-11-05 16:53:50,039] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:53:50,041] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:53:50,080] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.234 seconds
[2020-11-05 16:54:03,085] {scheduler_job.py:155} INFO - Started process (PID=1799) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:03,091] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:54:03,092] {logging_mixin.py:112} INFO - [2020-11-05 16:54:03,091] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:03,233] {logging_mixin.py:112} INFO - [2020-11-05 16:54:03,232] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:54:03,233] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:03,286] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.200 seconds
[2020-11-05 16:54:16,298] {scheduler_job.py:155} INFO - Started process (PID=1986) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:16,303] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:54:16,304] {logging_mixin.py:112} INFO - [2020-11-05 16:54:16,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:16,431] {logging_mixin.py:112} INFO - [2020-11-05 16:54:16,431] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:54:16,432] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:16,458] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 16:54:29,527] {scheduler_job.py:155} INFO - Started process (PID=2065) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:29,530] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:54:29,531] {logging_mixin.py:112} INFO - [2020-11-05 16:54:29,531] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:29,637] {logging_mixin.py:112} INFO - [2020-11-05 16:54:29,636] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:54:29,637] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:29,663] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.136 seconds
[2020-11-05 16:54:42,825] {scheduler_job.py:155} INFO - Started process (PID=2140) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:42,828] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:54:42,828] {logging_mixin.py:112} INFO - [2020-11-05 16:54:42,828] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:42,919] {logging_mixin.py:112} INFO - [2020-11-05 16:54:42,918] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:54:42,919] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:42,947] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.123 seconds
[2020-11-05 16:54:56,055] {scheduler_job.py:155} INFO - Started process (PID=2234) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:56,059] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:54:56,060] {logging_mixin.py:112} INFO - [2020-11-05 16:54:56,060] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:56,182] {logging_mixin.py:112} INFO - [2020-11-05 16:54:56,181] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:54:56,182] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:54:56,210] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 16:55:09,376] {scheduler_job.py:155} INFO - Started process (PID=2332) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:09,380] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:55:09,381] {logging_mixin.py:112} INFO - [2020-11-05 16:55:09,381] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:09,633] {logging_mixin.py:112} INFO - [2020-11-05 16:55:09,631] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 16, in <module>
    from helpers import simulate, get_file_from_s3, flatten_data, upload_file_to_S3_with_hook
ImportError: cannot import name 'simulate'
[2020-11-05 16:55:09,633] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:09,694] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.319 seconds
[2020-11-05 16:55:33,719] {scheduler_job.py:155} INFO - Started process (PID=2482) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:33,723] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:55:33,724] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:33,843] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:33,870] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 16:55:33,888] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 16:55:33,894] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,894] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.simulate_task_ 2020-11-05 14:44:25.679993+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:33,895] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,895] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.get_from_S3 2020-11-05 14:44:25.679993+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:33,929] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 16:55:33,936] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,936] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.simulate_task_ 2020-11-05 15:35:38.074129+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:33,936] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,936] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.get_from_S3 2020-11-05 15:35:38.074129+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:33,963] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 16:55:33,970] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,969] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.simulate_task_ 2020-11-05 15:42:55.992029+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:33,970] {logging_mixin.py:112} INFO - [2020-11-05 16:55:33,970] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.get_from_S3 2020-11-05 15:42:55.992029+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,004] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 16:55:34,008] {logging_mixin.py:112} INFO - [2020-11-05 16:55:34,008] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.simulate_task_ 2020-11-05 15:45:37+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,009] {logging_mixin.py:112} INFO - [2020-11-05 16:55:34,009] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.get_from_S3 2020-11-05 15:45:37+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,038] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 16:55:34,043] {logging_mixin.py:112} INFO - [2020-11-05 16:55:34,043] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.simulate_task_ 2020-11-05 15:49:01+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,043] {logging_mixin.py:112} INFO - [2020-11-05 16:55:34,043] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.get_from_S3 2020-11-05 15:49:01+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,074] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 16:55:34,077] {logging_mixin.py:112} INFO - [2020-11-05 16:55:34,077] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.simulate_task_ 2020-11-05 15:50:30+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,078] {logging_mixin.py:112} INFO - [2020-11-05 16:55:34,077] {dagrun.py:378} WARNING - Failed to get task '<TaskInstance: S3_task.get_from_S3 2020-11-05 15:50:30+00:00 [None]>' for dag '<DAG: S3_task>'. Marking it as removed.
[2020-11-05 16:55:34,104] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 16:55:34,134] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 16:55:34,163] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 16:55:34,428] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 16:55:34,434] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 14:44:25.679993+00:00 [success]> in ORM
[2020-11-05 16:55:34,441] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 14:44:25.679993+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,447] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:35:38.074129+00:00 [success]> in ORM
[2020-11-05 16:55:34,454] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:35:38.074129+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,461] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:42:55.992029+00:00 [success]> in ORM
[2020-11-05 16:55:34,468] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:42:55.992029+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,475] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:45:37+00:00 [success]> in ORM
[2020-11-05 16:55:34,481] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:45:37+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,489] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:49:01+00:00 [success]> in ORM
[2020-11-05 16:55:34,496] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:49:01+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,505] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:50:30+00:00 [success]> in ORM
[2020-11-05 16:55:34,517] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:50:30+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,524] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:52:01+00:00 [success]> in ORM
[2020-11-05 16:55:34,530] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:52:01+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,539] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:53:12.067609+00:00 [success]> in ORM
[2020-11-05 16:55:34,550] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:53:12.067609+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,565] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:54:48+00:00 [success]> in ORM
[2020-11-05 16:55:34,573] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:54:48+00:00 [scheduled]> in ORM
[2020-11-05 16:55:34,600] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.881 seconds
[2020-11-05 16:55:48,297] {scheduler_job.py:155} INFO - Started process (PID=2630) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:48,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:55:48,349] {logging_mixin.py:112} INFO - [2020-11-05 16:55:48,329] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:48,785] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:55:48,861] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 16:55:48,956] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 16:55:49,021] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 16:55:49,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 16:55:49,133] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 16:55:49,194] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 16:55:49,253] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 16:55:49,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 16:55:49,335] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 16:55:49,417] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 16:55:49,781] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 16:55:49,800] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 14:44:25.679993+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,819] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:35:38.074129+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,849] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:42:55.992029+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,867] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:45:37+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,882] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:49:01+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,906] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:50:30+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,950] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:52:01+00:00 [scheduled]> in ORM
[2020-11-05 16:55:49,993] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:53:12.067609+00:00 [scheduled]> in ORM
[2020-11-05 16:55:50,009] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:54:48+00:00 [scheduled]> in ORM
[2020-11-05 16:55:50,052] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.756 seconds
[2020-11-05 16:56:01,824] {scheduler_job.py:155} INFO - Started process (PID=2729) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:01,845] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:56:01,846] {logging_mixin.py:112} INFO - [2020-11-05 16:56:01,846] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:02,183] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:02,240] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 16:56:02,279] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 16:56:02,340] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 16:56:02,401] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 16:56:02,489] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 16:56:02,551] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 16:56:02,630] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 16:56:02,701] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 16:56:02,801] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 16:56:02,871] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 16:56:03,103] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 16:56:03,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.289 seconds
[2020-11-05 16:56:40,863] {scheduler_job.py:155} INFO - Started process (PID=2886) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:40,867] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:56:40,867] {logging_mixin.py:112} INFO - [2020-11-05 16:56:40,867] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:40,971] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:40,992] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 16:56:41,009] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 16:56:41,030] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 16:56:41,053] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 16:56:41,076] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 16:56:41,105] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 16:56:41,137] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 16:56:41,156] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 16:56:41,178] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 16:56:41,203] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 16:56:41,390] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 16:56:41,397] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 14:44:25.679993+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,403] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 14:44:25.679993+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,410] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:35:38.074129+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,416] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:35:38.074129+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,423] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:42:55.992029+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,430] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:45:37+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,437] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:49:01+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,443] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:49:01+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,451] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:50:30+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,472] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:52:01+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,480] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:53:12.067609+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,488] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:54:48+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,499] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:54:48+00:00 [scheduled]> in ORM
[2020-11-05 16:56:41,518] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.655 seconds
[2020-11-05 16:56:54,284] {scheduler_job.py:155} INFO - Started process (PID=2987) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:54,288] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:56:54,288] {logging_mixin.py:112} INFO - [2020-11-05 16:56:54,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:54,502] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:56:54,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 16:56:54,553] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 16:56:54,577] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 16:56:54,598] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 16:56:54,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 16:56:54,643] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 16:56:54,668] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 16:56:54,696] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 16:56:54,717] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 16:56:54,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 16:56:54,810] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 16:56:54,813] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.530 seconds
[2020-11-05 16:57:07,539] {scheduler_job.py:155} INFO - Started process (PID=3055) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:57:07,544] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 16:57:07,544] {logging_mixin.py:112} INFO - [2020-11-05 16:57:07,544] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:57:07,723] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 16:57:07,761] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 16:57:07,805] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 16:57:07,853] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 16:57:07,893] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 16:57:07,953] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 16:57:08,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 16:57:08,061] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 16:57:08,107] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 16:57:08,154] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 16:57:08,187] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 16:57:08,264] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 16:57:08,268] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.729 seconds
[2020-11-05 17:01:39,809] {scheduler_job.py:155} INFO - Started process (PID=3883) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:01:39,813] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:01:39,814] {logging_mixin.py:112} INFO - [2020-11-05 17:01:39,814] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:01:39,974] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:01:39,998] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:01:40,020] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True>
[2020-11-05 17:01:40,045] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,044] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 14:44:25.679993+00:00: manual__2020-11-05T14:44:25.679993+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,053] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True>
[2020-11-05 17:01:40,078] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,077] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:35:38.074129+00:00: manual__2020-11-05T15:35:38.074129+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,084] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True>
[2020-11-05 17:01:40,104] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,104] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:42:55.992029+00:00: manual__2020-11-05T15:42:55.992029+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,108] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True>
[2020-11-05 17:01:40,126] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,126] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:45:37+00:00: manual__2020-11-05T15:45:37+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,138] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True>
[2020-11-05 17:01:40,155] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,155] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:49:01+00:00: manual__2020-11-05T15:49:01+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,159] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True>
[2020-11-05 17:01:40,181] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,180] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:50:30+00:00: manual__2020-11-05T15:50:30+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,184] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True>
[2020-11-05 17:01:40,209] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,209] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:52:01+00:00: manual__2020-11-05T15:52:01+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,213] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True>
[2020-11-05 17:01:40,231] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,231] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:53:12.067609+00:00: manual__2020-11-05T15:53:12.067609+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,237] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True>
[2020-11-05 17:01:40,252] {logging_mixin.py:112} INFO - [2020-11-05 17:01:40,251] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:54:48+00:00: manual__2020-11-05T15:54:48+00:00, externally triggered: True> successful
[2020-11-05 17:01:40,256] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:57:27.456483+00:00: manual__2020-11-05T15:57:27.456483+00:00, externally triggered: True>
[2020-11-05 17:01:40,313] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:01:40,328] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:57:27.456483+00:00 [success]> in ORM
[2020-11-05 17:01:40,357] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:57:27.456483+00:00 [scheduled]> in ORM
[2020-11-05 17:01:40,375] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.566 seconds
[2020-11-05 17:01:54,262] {scheduler_job.py:155} INFO - Started process (PID=3971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:01:54,270] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:01:54,271] {logging_mixin.py:112} INFO - [2020-11-05 17:01:54,271] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:01:54,425] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:01:54,450] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:01:54,467] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:57:27.456483+00:00: manual__2020-11-05T15:57:27.456483+00:00, externally triggered: True>
[2020-11-05 17:01:54,505] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:01:54,511] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 15:57:27.456483+00:00 [scheduled]> in ORM
[2020-11-05 17:01:54,521] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-05 17:02:06,675] {scheduler_job.py:155} INFO - Started process (PID=4055) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:02:06,679] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:02:06,679] {logging_mixin.py:112} INFO - [2020-11-05 17:02:06,679] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:02:06,826] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:02:06,856] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:02:06,873] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:57:27.456483+00:00: manual__2020-11-05T15:57:27.456483+00:00, externally triggered: True>
[2020-11-05 17:02:06,901] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:02:06,905] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.231 seconds
[2020-11-05 17:02:19,992] {scheduler_job.py:155} INFO - Started process (PID=4125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:02:20,012] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:02:20,022] {logging_mixin.py:112} INFO - [2020-11-05 17:02:20,021] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:02:20,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:02:20,188] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:02:20,219] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:57:27.456483+00:00: manual__2020-11-05T15:57:27.456483+00:00, externally triggered: True>
[2020-11-05 17:02:20,274] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:02:20,284] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.292 seconds
[2020-11-05 17:03:21,556] {scheduler_job.py:155} INFO - Started process (PID=4275) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:21,565] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:03:21,567] {logging_mixin.py:112} INFO - [2020-11-05 17:03:21,566] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:21,777] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:21,834] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:03:21,870] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:57:27.456483+00:00: manual__2020-11-05T15:57:27.456483+00:00, externally triggered: True>
[2020-11-05 17:03:21,898] {logging_mixin.py:112} INFO - [2020-11-05 17:03:21,898] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 15:57:27.456483+00:00: manual__2020-11-05T15:57:27.456483+00:00, externally triggered: True> successful
[2020-11-05 17:03:21,903] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:03:21,915] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.360 seconds
[2020-11-05 17:03:34,927] {scheduler_job.py:155} INFO - Started process (PID=4334) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:34,942] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:03:34,945] {logging_mixin.py:112} INFO - [2020-11-05 17:03:34,943] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:35,308] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:35,371] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:03:35,414] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:03:35,426] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.499 seconds
[2020-11-05 17:03:48,322] {scheduler_job.py:155} INFO - Started process (PID=4391) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:48,328] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:03:48,329] {logging_mixin.py:112} INFO - [2020-11-05 17:03:48,329] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:48,498] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:03:48,535] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:03:48,563] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:03:48,568] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.246 seconds
[2020-11-05 17:04:01,678] {scheduler_job.py:155} INFO - Started process (PID=4445) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:01,682] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:04:01,684] {logging_mixin.py:112} INFO - [2020-11-05 17:04:01,683] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:01,789] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:01,819] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:04:01,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:04:01,842] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-05 17:04:15,027] {scheduler_job.py:155} INFO - Started process (PID=4509) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:15,054] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:04:15,059] {logging_mixin.py:112} INFO - [2020-11-05 17:04:15,059] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:15,376] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:15,428] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:04:15,452] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:04:15,458] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.432 seconds
[2020-11-05 17:04:28,332] {scheduler_job.py:155} INFO - Started process (PID=4563) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:28,336] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:04:28,337] {logging_mixin.py:112} INFO - [2020-11-05 17:04:28,337] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:28,489] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:28,519] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:04:28,541] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:04:28,547] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 17:04:41,607] {scheduler_job.py:155} INFO - Started process (PID=4619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:41,611] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:04:41,612] {logging_mixin.py:112} INFO - [2020-11-05 17:04:41,612] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:41,737] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:41,765] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:04:41,788] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:04:41,796] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 17:04:54,992] {scheduler_job.py:155} INFO - Started process (PID=4682) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:54,996] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:04:54,997] {logging_mixin.py:112} INFO - [2020-11-05 17:04:54,997] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:55,119] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:04:55,146] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:04:55,162] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:04:55,166] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:05:08,321] {scheduler_job.py:155} INFO - Started process (PID=4747) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:08,328] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:05:08,329] {logging_mixin.py:112} INFO - [2020-11-05 17:05:08,329] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:08,482] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:08,529] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:05:08,574] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:05:08,581] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.261 seconds
[2020-11-05 17:05:21,736] {scheduler_job.py:155} INFO - Started process (PID=4808) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:21,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:05:21,743] {logging_mixin.py:112} INFO - [2020-11-05 17:05:21,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:21,950] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:21,984] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:05:22,014] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:05:22,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.287 seconds
[2020-11-05 17:05:35,011] {scheduler_job.py:155} INFO - Started process (PID=4865) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:35,015] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:05:35,015] {logging_mixin.py:112} INFO - [2020-11-05 17:05:35,015] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:35,161] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:35,199] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:05:35,230] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:05:23.525380+00:00: manual__2020-11-05T16:05:23.525380+00:00, externally triggered: True>
[2020-11-05 17:05:35,303] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:05:35,314] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 16:05:23.525380+00:00 [success]> in ORM
[2020-11-05 17:05:35,322] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 16:05:23.525380+00:00 [scheduled]> in ORM
[2020-11-05 17:05:35,336] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.326 seconds
[2020-11-05 17:05:48,425] {scheduler_job.py:155} INFO - Started process (PID=4956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:48,444] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:05:48,444] {logging_mixin.py:112} INFO - [2020-11-05 17:05:48,444] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:48,628] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:05:48,659] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:05:48,679] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:05:23.525380+00:00: manual__2020-11-05T16:05:23.525380+00:00, externally triggered: True>
[2020-11-05 17:05:48,731] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:05:48,738] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 16:05:23.525380+00:00 [scheduled]> in ORM
[2020-11-05 17:05:48,754] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.328 seconds
[2020-11-05 17:06:01,851] {scheduler_job.py:155} INFO - Started process (PID=5053) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:01,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:06:01,863] {logging_mixin.py:112} INFO - [2020-11-05 17:06:01,863] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:01,988] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:02,015] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:06:02,038] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:05:23.525380+00:00: manual__2020-11-05T16:05:23.525380+00:00, externally triggered: True>
[2020-11-05 17:06:02,078] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:06:02,084] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-05 17:06:15,171] {scheduler_job.py:155} INFO - Started process (PID=5125) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:15,184] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:06:15,186] {logging_mixin.py:112} INFO - [2020-11-05 17:06:15,185] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:15,379] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:15,405] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:06:15,424] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:05:23.525380+00:00: manual__2020-11-05T16:05:23.525380+00:00, externally triggered: True>
[2020-11-05 17:06:15,439] {logging_mixin.py:112} INFO - [2020-11-05 17:06:15,439] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 16:05:23.525380+00:00: manual__2020-11-05T16:05:23.525380+00:00, externally triggered: True> successful
[2020-11-05 17:06:15,443] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:06:15,447] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.275 seconds
[2020-11-05 17:06:28,468] {scheduler_job.py:155} INFO - Started process (PID=5188) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:28,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:06:28,474] {logging_mixin.py:112} INFO - [2020-11-05 17:06:28,473] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:28,630] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:28,657] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:06:28,685] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:06:28,688] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.220 seconds
[2020-11-05 17:06:41,763] {scheduler_job.py:155} INFO - Started process (PID=5249) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:41,769] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:06:41,770] {logging_mixin.py:112} INFO - [2020-11-05 17:06:41,769] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:41,937] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:41,974] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:06:42,009] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:06:42,014] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-05 17:06:55,300] {scheduler_job.py:155} INFO - Started process (PID=5307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:55,304] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:06:55,305] {logging_mixin.py:112} INFO - [2020-11-05 17:06:55,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:55,436] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:06:55,471] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:06:55,489] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:06:55,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.194 seconds
[2020-11-05 17:07:08,636] {scheduler_job.py:155} INFO - Started process (PID=5368) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:08,640] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:07:08,641] {logging_mixin.py:112} INFO - [2020-11-05 17:07:08,641] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:08,744] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:08,766] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:07:08,791] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:07:08,795] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.160 seconds
[2020-11-05 17:07:21,925] {scheduler_job.py:155} INFO - Started process (PID=5430) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:21,928] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:07:21,928] {logging_mixin.py:112} INFO - [2020-11-05 17:07:21,928] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:22,031] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:22,053] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:07:22,081] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:07:22,086] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.161 seconds
[2020-11-05 17:07:35,366] {scheduler_job.py:155} INFO - Started process (PID=5491) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:35,369] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:07:35,370] {logging_mixin.py:112} INFO - [2020-11-05 17:07:35,370] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:35,517] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:35,554] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:07:35,588] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:07:35,593] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.227 seconds
[2020-11-05 17:07:48,746] {scheduler_job.py:155} INFO - Started process (PID=5558) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:48,750] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:07:48,756] {logging_mixin.py:112} INFO - [2020-11-05 17:07:48,756] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:48,929] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:07:48,952] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:07:48,969] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:07:48,974] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.228 seconds
[2020-11-05 17:08:01,964] {scheduler_job.py:155} INFO - Started process (PID=5621) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:01,970] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:08:01,971] {logging_mixin.py:112} INFO - [2020-11-05 17:08:01,971] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:02,099] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:02,124] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:08:02,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:08:02,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 17:08:15,197] {scheduler_job.py:155} INFO - Started process (PID=5683) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:15,201] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:08:15,202] {logging_mixin.py:112} INFO - [2020-11-05 17:08:15,202] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:15,309] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:15,335] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:08:15,357] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:08:15,360] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.163 seconds
[2020-11-05 17:08:28,476] {scheduler_job.py:155} INFO - Started process (PID=5754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:28,479] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:08:28,479] {logging_mixin.py:112} INFO - [2020-11-05 17:08:28,479] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:28,574] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:28,603] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:08:28,625] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:08:28,630] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 17:08:41,767] {scheduler_job.py:155} INFO - Started process (PID=5815) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:41,771] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:08:41,771] {logging_mixin.py:112} INFO - [2020-11-05 17:08:41,771] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:41,871] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:41,894] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:08:41,913] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:08:41,916] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.150 seconds
[2020-11-05 17:08:55,209] {scheduler_job.py:155} INFO - Started process (PID=5888) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:55,215] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:08:55,216] {logging_mixin.py:112} INFO - [2020-11-05 17:08:55,216] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:55,503] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:08:55,547] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:08:55,582] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:08:55,588] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.379 seconds
[2020-11-05 17:09:08,414] {scheduler_job.py:155} INFO - Started process (PID=5949) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:08,417] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:09:08,418] {logging_mixin.py:112} INFO - [2020-11-05 17:09:08,417] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:08,511] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:08,531] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:09:08,547] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:09:08,552] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-05 17:09:21,779] {scheduler_job.py:155} INFO - Started process (PID=6016) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:21,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:09:21,790] {logging_mixin.py:112} INFO - [2020-11-05 17:09:21,789] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:21,977] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:21,997] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:09:22,015] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:09:22,018] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-05 17:09:35,066] {scheduler_job.py:155} INFO - Started process (PID=6075) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:35,070] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:09:35,070] {logging_mixin.py:112} INFO - [2020-11-05 17:09:35,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:35,176] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:35,213] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:09:35,230] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:09:35,236] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 17:09:48,340] {scheduler_job.py:155} INFO - Started process (PID=6135) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:48,344] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:09:48,345] {logging_mixin.py:112} INFO - [2020-11-05 17:09:48,344] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:48,452] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:09:48,476] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:09:48,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:09:48,496] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.156 seconds
[2020-11-05 17:10:01,651] {scheduler_job.py:155} INFO - Started process (PID=6197) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:01,654] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:10:01,656] {logging_mixin.py:112} INFO - [2020-11-05 17:10:01,655] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:01,828] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:01,864] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:10:01,884] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:10:01,889] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-05 17:10:14,898] {scheduler_job.py:155} INFO - Started process (PID=6256) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:14,901] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:10:14,902] {logging_mixin.py:112} INFO - [2020-11-05 17:10:14,902] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:15,018] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:15,045] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:10:15,068] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:10:15,072] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:10:28,207] {scheduler_job.py:155} INFO - Started process (PID=6336) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:28,212] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:10:28,213] {logging_mixin.py:112} INFO - [2020-11-05 17:10:28,212] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:28,353] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:28,392] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:10:28,425] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:10:28,432] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.225 seconds
[2020-11-05 17:10:41,453] {scheduler_job.py:155} INFO - Started process (PID=6396) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:41,456] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:10:41,457] {logging_mixin.py:112} INFO - [2020-11-05 17:10:41,457] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:41,583] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:41,607] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:10:41,636] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:10:41,640] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.187 seconds
[2020-11-05 17:10:54,748] {scheduler_job.py:155} INFO - Started process (PID=6459) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:54,752] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:10:54,753] {logging_mixin.py:112} INFO - [2020-11-05 17:10:54,753] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:54,906] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:10:54,937] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:10:54,955] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:10:54,960] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.212 seconds
[2020-11-05 17:11:08,016] {scheduler_job.py:155} INFO - Started process (PID=6526) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:08,020] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:11:08,020] {logging_mixin.py:112} INFO - [2020-11-05 17:11:08,020] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:08,117] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:08,141] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:11:08,160] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:11:08,168] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.152 seconds
[2020-11-05 17:11:21,243] {scheduler_job.py:155} INFO - Started process (PID=6590) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:21,249] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:11:21,249] {logging_mixin.py:112} INFO - [2020-11-05 17:11:21,249] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:21,438] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:21,497] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:11:21,527] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:11:21,531] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.288 seconds
[2020-11-05 17:11:34,655] {scheduler_job.py:155} INFO - Started process (PID=6649) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:34,668] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:11:34,678] {logging_mixin.py:112} INFO - [2020-11-05 17:11:34,678] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:34,834] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:34,870] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:11:34,898] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:11:34,905] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.250 seconds
[2020-11-05 17:11:47,980] {scheduler_job.py:155} INFO - Started process (PID=6712) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:47,984] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:11:47,985] {logging_mixin.py:112} INFO - [2020-11-05 17:11:47,985] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:48,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:11:48,158] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:11:48,174] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:11:48,179] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 17:12:01,524] {scheduler_job.py:155} INFO - Started process (PID=6780) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:01,537] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:12:01,537] {logging_mixin.py:112} INFO - [2020-11-05 17:12:01,537] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:01,777] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:01,808] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:12:01,851] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:12:01,857] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.333 seconds
[2020-11-05 17:12:14,876] {scheduler_job.py:155} INFO - Started process (PID=6834) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:14,881] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:12:14,881] {logging_mixin.py:112} INFO - [2020-11-05 17:12:14,881] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:15,047] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:15,103] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:12:15,129] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:12:15,135] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.259 seconds
[2020-11-05 17:12:28,346] {scheduler_job.py:155} INFO - Started process (PID=6892) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:28,355] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:12:28,356] {logging_mixin.py:112} INFO - [2020-11-05 17:12:28,356] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:28,575] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:28,617] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:12:28,645] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:12:21.784948+00:00: manual__2020-11-05T16:12:21.784948+00:00, externally triggered: True>
[2020-11-05 17:12:28,786] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:12:28,798] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 16:12:21.784948+00:00 [success]> in ORM
[2020-11-05 17:12:28,819] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 16:12:21.784948+00:00 [scheduled]> in ORM
[2020-11-05 17:12:28,847] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.502 seconds
[2020-11-05 17:12:41,837] {scheduler_job.py:155} INFO - Started process (PID=6977) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:41,840] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:12:41,854] {logging_mixin.py:112} INFO - [2020-11-05 17:12:41,854] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:42,057] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:42,083] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:12:42,111] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:12:21.784948+00:00: manual__2020-11-05T16:12:21.784948+00:00, externally triggered: True>
[2020-11-05 17:12:42,162] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:12:42,169] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 16:12:21.784948+00:00 [scheduled]> in ORM
[2020-11-05 17:12:42,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.345 seconds
[2020-11-05 17:12:55,208] {scheduler_job.py:155} INFO - Started process (PID=7067) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:55,216] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:12:55,216] {logging_mixin.py:112} INFO - [2020-11-05 17:12:55,216] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:55,408] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:12:55,436] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:12:55,462] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:12:21.784948+00:00: manual__2020-11-05T16:12:21.784948+00:00, externally triggered: True>
[2020-11-05 17:12:55,502] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:12:55,507] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.298 seconds
[2020-11-05 17:13:08,575] {scheduler_job.py:155} INFO - Started process (PID=7134) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:13:08,592] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:13:08,593] {logging_mixin.py:112} INFO - [2020-11-05 17:13:08,593] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:13:08,750] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:13:08,774] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:13:08,800] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:12:21.784948+00:00: manual__2020-11-05T16:12:21.784948+00:00, externally triggered: True>
[2020-11-05 17:13:08,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:13:08,843] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-05 17:14:05,854] {scheduler_job.py:155} INFO - Started process (PID=7257) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:05,859] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:14:05,867] {logging_mixin.py:112} INFO - [2020-11-05 17:14:05,866] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:06,020] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:06,057] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:14:06,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:12:21.784948+00:00: manual__2020-11-05T16:12:21.784948+00:00, externally triggered: True>
[2020-11-05 17:14:06,096] {logging_mixin.py:112} INFO - [2020-11-05 17:14:06,096] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 16:12:21.784948+00:00: manual__2020-11-05T16:12:21.784948+00:00, externally triggered: True> successful
[2020-11-05 17:14:06,100] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:14:06,103] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.250 seconds
[2020-11-05 17:14:20,154] {scheduler_job.py:155} INFO - Started process (PID=7316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:20,158] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:14:20,159] {logging_mixin.py:112} INFO - [2020-11-05 17:14:20,158] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:20,256] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:20,282] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:14:20,300] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:14:20,305] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.151 seconds
[2020-11-05 17:14:32,437] {scheduler_job.py:155} INFO - Started process (PID=7382) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:32,441] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:14:32,442] {logging_mixin.py:112} INFO - [2020-11-05 17:14:32,442] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:32,561] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:32,584] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:14:32,601] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:14:32,605] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.168 seconds
[2020-11-05 17:14:45,768] {scheduler_job.py:155} INFO - Started process (PID=7447) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:45,773] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:14:45,774] {logging_mixin.py:112} INFO - [2020-11-05 17:14:45,774] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:45,897] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:45,918] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:14:45,934] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:14:45,938] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 17:14:59,156] {scheduler_job.py:155} INFO - Started process (PID=7519) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:59,171] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:14:59,178] {logging_mixin.py:112} INFO - [2020-11-05 17:14:59,178] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:59,323] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:14:59,344] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:14:59,360] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:14:59,365] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.210 seconds
[2020-11-05 17:15:12,435] {scheduler_job.py:155} INFO - Started process (PID=7590) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:12,439] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:15:12,441] {logging_mixin.py:112} INFO - [2020-11-05 17:15:12,441] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:12,542] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:12,562] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:15:12,578] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:15:12,583] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.148 seconds
[2020-11-05 17:15:25,644] {scheduler_job.py:155} INFO - Started process (PID=7657) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:25,659] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:15:25,659] {logging_mixin.py:112} INFO - [2020-11-05 17:15:25,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:25,851] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:25,883] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:15:25,906] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:15:25,914] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.270 seconds
[2020-11-05 17:15:38,927] {scheduler_job.py:155} INFO - Started process (PID=7725) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:38,931] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:15:38,932] {logging_mixin.py:112} INFO - [2020-11-05 17:15:38,931] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:39,029] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:39,052] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:15:39,073] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:15:39,077] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 17:15:52,205] {scheduler_job.py:155} INFO - Started process (PID=7787) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:52,212] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:15:52,213] {logging_mixin.py:112} INFO - [2020-11-05 17:15:52,213] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:52,371] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:15:52,391] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:15:52,408] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:15:52,412] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.207 seconds
[2020-11-05 17:16:05,617] {scheduler_job.py:155} INFO - Started process (PID=7852) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:05,620] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:16:05,621] {logging_mixin.py:112} INFO - [2020-11-05 17:16:05,621] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:05,728] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:05,774] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:16:05,797] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:16:05,803] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.185 seconds
[2020-11-05 17:16:18,928] {scheduler_job.py:155} INFO - Started process (PID=7914) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:18,934] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:16:18,935] {logging_mixin.py:112} INFO - [2020-11-05 17:16:18,935] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:19,035] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:19,059] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:16:19,078] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:16:19,087] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.159 seconds
[2020-11-05 17:16:31,280] {scheduler_job.py:155} INFO - Started process (PID=7972) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:31,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:16:31,296] {logging_mixin.py:112} INFO - [2020-11-05 17:16:31,293] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:31,468] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:31,491] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:16:31,511] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:16:31,516] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-05 17:16:44,577] {scheduler_job.py:155} INFO - Started process (PID=8027) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:44,600] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:16:44,603] {logging_mixin.py:112} INFO - [2020-11-05 17:16:44,603] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:44,854] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:44,913] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:16:44,956] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:16:44,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.387 seconds
[2020-11-05 17:16:57,883] {scheduler_job.py:155} INFO - Started process (PID=8090) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:57,890] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:16:57,892] {logging_mixin.py:112} INFO - [2020-11-05 17:16:57,891] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:58,039] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:16:58,062] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:16:58,081] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:16:58,086] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 17:17:11,218] {scheduler_job.py:155} INFO - Started process (PID=8149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:11,234] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:17:11,234] {logging_mixin.py:112} INFO - [2020-11-05 17:17:11,234] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:11,425] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:11,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:17:11,480] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:17:11,497] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.279 seconds
[2020-11-05 17:17:24,545] {scheduler_job.py:155} INFO - Started process (PID=8225) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:24,562] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:17:24,563] {logging_mixin.py:112} INFO - [2020-11-05 17:17:24,563] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:24,749] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:24,793] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:17:24,819] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:17:22.672151+00:00: manual__2020-11-05T16:17:22.672151+00:00, externally triggered: True>
[2020-11-05 17:17:24,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:17:24,906] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 16:17:22.672151+00:00 [success]> in ORM
[2020-11-05 17:17:24,931] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 16:17:22.672151+00:00 [scheduled]> in ORM
[2020-11-05 17:17:24,952] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.407 seconds
[2020-11-05 17:17:38,279] {scheduler_job.py:155} INFO - Started process (PID=8324) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:38,302] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:17:38,306] {logging_mixin.py:112} INFO - [2020-11-05 17:17:38,305] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:38,550] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:38,583] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:17:38,609] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:17:22.672151+00:00: manual__2020-11-05T16:17:22.672151+00:00, externally triggered: True>
[2020-11-05 17:17:38,662] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:17:38,669] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 16:17:22.672151+00:00 [scheduled]> in ORM
[2020-11-05 17:17:38,679] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.400 seconds
[2020-11-05 17:17:51,781] {scheduler_job.py:155} INFO - Started process (PID=8421) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:51,793] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:17:51,794] {logging_mixin.py:112} INFO - [2020-11-05 17:17:51,794] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:51,919] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:17:51,948] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:17:51,981] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:17:22.672151+00:00: manual__2020-11-05T16:17:22.672151+00:00, externally triggered: True>
[2020-11-05 17:17:52,045] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:17:52,052] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.271 seconds
[2020-11-05 17:18:05,108] {scheduler_job.py:155} INFO - Started process (PID=8497) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:05,113] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:18:05,114] {logging_mixin.py:112} INFO - [2020-11-05 17:18:05,113] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:05,280] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:05,311] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:18:05,328] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:17:22.672151+00:00: manual__2020-11-05T16:17:22.672151+00:00, externally triggered: True>
[2020-11-05 17:18:05,359] {logging_mixin.py:112} INFO - [2020-11-05 17:18:05,358] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 16:17:22.672151+00:00: manual__2020-11-05T16:17:22.672151+00:00, externally triggered: True> successful
[2020-11-05 17:18:05,362] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:18:05,366] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.258 seconds
[2020-11-05 17:18:18,385] {scheduler_job.py:155} INFO - Started process (PID=8563) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:18,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:18:18,392] {logging_mixin.py:112} INFO - [2020-11-05 17:18:18,392] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:18,575] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:18,650] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:18:18,681] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:18:18,687] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.302 seconds
[2020-11-05 17:18:31,786] {scheduler_job.py:155} INFO - Started process (PID=8626) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:31,802] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:18:31,803] {logging_mixin.py:112} INFO - [2020-11-05 17:18:31,803] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:32,085] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:32,138] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:18:32,169] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:18:32,174] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.388 seconds
[2020-11-05 17:18:45,147] {scheduler_job.py:155} INFO - Started process (PID=8681) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:45,160] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:18:45,161] {logging_mixin.py:112} INFO - [2020-11-05 17:18:45,161] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:45,290] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:45,342] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:18:45,367] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:18:45,373] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.227 seconds
[2020-11-05 17:18:58,438] {scheduler_job.py:155} INFO - Started process (PID=8736) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:58,441] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:18:58,441] {logging_mixin.py:112} INFO - [2020-11-05 17:18:58,441] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:58,564] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:18:58,595] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:18:58,613] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:18:58,617] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.180 seconds
[2020-11-05 17:19:11,834] {scheduler_job.py:155} INFO - Started process (PID=8804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:11,861] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:19:11,862] {logging_mixin.py:112} INFO - [2020-11-05 17:19:11,861] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:12,019] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:12,054] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:19:12,083] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:19:12,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.255 seconds
[2020-11-05 17:19:25,137] {scheduler_job.py:155} INFO - Started process (PID=8857) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:25,144] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:19:25,144] {logging_mixin.py:112} INFO - [2020-11-05 17:19:25,144] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:25,335] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:25,372] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:19:25,407] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:19:25,423] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.286 seconds
[2020-11-05 17:19:38,605] {scheduler_job.py:155} INFO - Started process (PID=8915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:38,615] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:19:38,616] {logging_mixin.py:112} INFO - [2020-11-05 17:19:38,616] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:38,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:38,809] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:19:38,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:19:38,845] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.241 seconds
[2020-11-05 17:19:51,875] {scheduler_job.py:155} INFO - Started process (PID=8973) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:51,900] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:19:51,901] {logging_mixin.py:112} INFO - [2020-11-05 17:19:51,901] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:52,133] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:19:52,192] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:19:52,218] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:19:40.364321+00:00: manual__2020-11-05T16:19:40.364321+00:00, externally triggered: True>
[2020-11-05 17:19:52,302] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:19:52,312] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 16:19:40.364321+00:00 [success]> in ORM
[2020-11-05 17:19:52,320] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 16:19:40.364321+00:00 [scheduled]> in ORM
[2020-11-05 17:19:52,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.464 seconds
[2020-11-05 17:20:05,399] {scheduler_job.py:155} INFO - Started process (PID=9062) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:05,418] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:20:05,418] {logging_mixin.py:112} INFO - [2020-11-05 17:20:05,418] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:05,794] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:05,862] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:20:05,921] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:19:40.364321+00:00: manual__2020-11-05T16:19:40.364321+00:00, externally triggered: True>
[2020-11-05 17:20:05,991] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:20:06,002] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 16:19:40.364321+00:00 [scheduled]> in ORM
[2020-11-05 17:20:06,024] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.625 seconds
[2020-11-05 17:20:18,755] {scheduler_job.py:155} INFO - Started process (PID=9164) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:18,762] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:20:18,768] {logging_mixin.py:112} INFO - [2020-11-05 17:20:18,768] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:18,973] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:19,036] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:20:19,055] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 16:19:40.364321+00:00: manual__2020-11-05T16:19:40.364321+00:00, externally triggered: True>
[2020-11-05 17:20:19,075] {logging_mixin.py:112} INFO - [2020-11-05 17:20:19,075] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 16:19:40.364321+00:00: manual__2020-11-05T16:19:40.364321+00:00, externally triggered: True> successful
[2020-11-05 17:20:19,079] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:20:19,084] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.329 seconds
[2020-11-05 17:20:32,064] {scheduler_job.py:155} INFO - Started process (PID=9223) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:32,084] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:20:32,101] {logging_mixin.py:112} INFO - [2020-11-05 17:20:32,100] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:32,287] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:32,323] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:20:32,362] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:20:32,367] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.303 seconds
[2020-11-05 17:20:45,451] {scheduler_job.py:155} INFO - Started process (PID=9279) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:45,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:20:45,456] {logging_mixin.py:112} INFO - [2020-11-05 17:20:45,456] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:45,554] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:45,584] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:20:45,621] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:20:45,628] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 17:20:58,784] {scheduler_job.py:155} INFO - Started process (PID=9340) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:58,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:20:58,792] {logging_mixin.py:112} INFO - [2020-11-05 17:20:58,792] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:58,925] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:20:58,958] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:20:58,979] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:20:58,983] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 17:21:11,141] {scheduler_job.py:155} INFO - Started process (PID=9399) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:11,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:21:11,147] {logging_mixin.py:112} INFO - [2020-11-05 17:21:11,147] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:11,265] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:11,304] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:21:11,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:21:11,327] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.187 seconds
[2020-11-05 17:21:24,387] {scheduler_job.py:155} INFO - Started process (PID=9465) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:24,391] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:21:24,391] {logging_mixin.py:112} INFO - [2020-11-05 17:21:24,391] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:24,512] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:24,544] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:21:24,566] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:21:24,575] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 17:21:37,835] {scheduler_job.py:155} INFO - Started process (PID=9527) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:37,839] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:21:37,839] {logging_mixin.py:112} INFO - [2020-11-05 17:21:37,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:37,967] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:38,000] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:21:38,021] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:21:38,025] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 17:21:51,070] {scheduler_job.py:155} INFO - Started process (PID=9587) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:51,075] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:21:51,075] {logging_mixin.py:112} INFO - [2020-11-05 17:21:51,075] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:51,185] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:21:51,211] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:21:51,232] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:21:51,236] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.166 seconds
[2020-11-05 17:22:04,455] {scheduler_job.py:155} INFO - Started process (PID=9651) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:04,477] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:22:04,483] {logging_mixin.py:112} INFO - [2020-11-05 17:22:04,482] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:04,665] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:04,689] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:22:04,707] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:22:04,712] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-05 17:22:17,773] {scheduler_job.py:155} INFO - Started process (PID=9706) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:17,777] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:22:17,778] {logging_mixin.py:112} INFO - [2020-11-05 17:22:17,777] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:17,889] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:17,929] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:22:17,952] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:22:17,961] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 17:22:31,090] {scheduler_job.py:155} INFO - Started process (PID=9765) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:31,095] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:22:31,095] {logging_mixin.py:112} INFO - [2020-11-05 17:22:31,095] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:31,200] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:31,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:22:31,243] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:22:31,247] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.157 seconds
[2020-11-05 17:22:44,391] {scheduler_job.py:155} INFO - Started process (PID=9825) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:44,400] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:22:44,402] {logging_mixin.py:112} INFO - [2020-11-05 17:22:44,401] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:44,618] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:44,640] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:22:44,663] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:22:44,669] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.278 seconds
[2020-11-05 17:22:57,675] {scheduler_job.py:155} INFO - Started process (PID=9883) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:57,679] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:22:57,680] {logging_mixin.py:112} INFO - [2020-11-05 17:22:57,680] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:57,815] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:22:57,845] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:22:57,865] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:22:57,869] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.194 seconds
[2020-11-05 17:23:10,980] {scheduler_job.py:155} INFO - Started process (PID=9943) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:10,985] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:23:10,985] {logging_mixin.py:112} INFO - [2020-11-05 17:23:10,985] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:11,093] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:11,115] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:23:11,131] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:23:11,135] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.155 seconds
[2020-11-05 17:23:24,272] {scheduler_job.py:155} INFO - Started process (PID=10006) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:24,275] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:23:24,277] {logging_mixin.py:112} INFO - [2020-11-05 17:23:24,277] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:24,400] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:24,429] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:23:24,455] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:23:24,466] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.195 seconds
[2020-11-05 17:23:37,575] {scheduler_job.py:155} INFO - Started process (PID=10067) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:37,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:23:37,587] {logging_mixin.py:112} INFO - [2020-11-05 17:23:37,587] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:37,795] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:37,857] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:23:37,901] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:23:37,906] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.330 seconds
[2020-11-05 17:23:50,879] {scheduler_job.py:155} INFO - Started process (PID=10126) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:50,882] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:23:50,883] {logging_mixin.py:112} INFO - [2020-11-05 17:23:50,883] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:51,006] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:23:51,070] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:23:51,128] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:23:51,135] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-05 17:24:04,250] {scheduler_job.py:155} INFO - Started process (PID=10195) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:04,260] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:24:04,266] {logging_mixin.py:112} INFO - [2020-11-05 17:24:04,265] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:04,441] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:04,464] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:24:04,483] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:24:04,486] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-05 17:24:17,510] {scheduler_job.py:155} INFO - Started process (PID=10272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:17,514] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:24:17,515] {logging_mixin.py:112} INFO - [2020-11-05 17:24:17,514] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:17,694] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:17,715] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:24:17,739] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:24:17,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.239 seconds
[2020-11-05 17:24:30,785] {scheduler_job.py:155} INFO - Started process (PID=10353) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:30,790] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:24:30,790] {logging_mixin.py:112} INFO - [2020-11-05 17:24:30,790] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:30,913] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:30,939] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:24:30,956] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:24:30,960] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 17:24:44,088] {scheduler_job.py:155} INFO - Started process (PID=10412) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:44,092] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:24:44,093] {logging_mixin.py:112} INFO - [2020-11-05 17:24:44,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:44,209] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:44,233] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:24:44,250] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:24:44,254] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.166 seconds
[2020-11-05 17:24:57,343] {scheduler_job.py:155} INFO - Started process (PID=10470) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:57,347] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:24:57,347] {logging_mixin.py:112} INFO - [2020-11-05 17:24:57,347] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:57,461] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:24:57,482] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:24:57,513] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:24:57,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:25:10,648] {scheduler_job.py:155} INFO - Started process (PID=10532) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:10,655] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:25:10,656] {logging_mixin.py:112} INFO - [2020-11-05 17:25:10,656] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:10,840] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:10,879] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:25:10,908] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:25:10,913] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.265 seconds
[2020-11-05 17:25:23,966] {scheduler_job.py:155} INFO - Started process (PID=10590) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:23,969] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:25:23,970] {logging_mixin.py:112} INFO - [2020-11-05 17:25:23,969] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:24,098] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:24,127] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:25:24,144] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:25:24,149] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.183 seconds
[2020-11-05 17:25:37,216] {scheduler_job.py:155} INFO - Started process (PID=10647) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:37,219] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:25:37,220] {logging_mixin.py:112} INFO - [2020-11-05 17:25:37,220] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:37,385] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:37,418] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:25:37,445] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:25:37,450] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.234 seconds
[2020-11-05 17:25:50,528] {scheduler_job.py:155} INFO - Started process (PID=10707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:50,537] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:25:50,542] {logging_mixin.py:112} INFO - [2020-11-05 17:25:50,540] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:50,725] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:25:50,759] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:25:50,796] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:25:50,804] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.276 seconds
[2020-11-05 17:26:03,837] {scheduler_job.py:155} INFO - Started process (PID=10767) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:03,841] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:26:03,841] {logging_mixin.py:112} INFO - [2020-11-05 17:26:03,841] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:03,957] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:03,989] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:26:04,009] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:26:04,015] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 17:26:17,119] {scheduler_job.py:155} INFO - Started process (PID=10825) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:17,122] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:26:17,123] {logging_mixin.py:112} INFO - [2020-11-05 17:26:17,123] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:17,217] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:17,238] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:26:17,255] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:26:17,260] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.141 seconds
[2020-11-05 17:26:30,412] {scheduler_job.py:155} INFO - Started process (PID=10886) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:30,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:26:30,416] {logging_mixin.py:112} INFO - [2020-11-05 17:26:30,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:30,532] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:30,557] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:26:30,573] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:26:30,577] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.165 seconds
[2020-11-05 17:26:43,769] {scheduler_job.py:155} INFO - Started process (PID=10948) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:43,777] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:26:43,778] {logging_mixin.py:112} INFO - [2020-11-05 17:26:43,778] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:43,946] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:43,982] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:26:44,002] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:26:44,006] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.237 seconds
[2020-11-05 17:26:57,086] {scheduler_job.py:155} INFO - Started process (PID=11004) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:57,091] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:26:57,092] {logging_mixin.py:112} INFO - [2020-11-05 17:26:57,092] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:57,228] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:26:57,253] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:26:57,273] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:26:57,277] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-05 17:27:10,338] {scheduler_job.py:155} INFO - Started process (PID=11064) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:10,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:27:10,353] {logging_mixin.py:112} INFO - [2020-11-05 17:27:10,344] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:10,637] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:10,692] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:27:10,735] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:27:10,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.411 seconds
[2020-11-05 17:27:23,699] {scheduler_job.py:155} INFO - Started process (PID=11142) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:23,702] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:27:23,702] {logging_mixin.py:112} INFO - [2020-11-05 17:27:23,702] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:23,811] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:23,836] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:27:23,863] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:27:23,871] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.173 seconds
[2020-11-05 17:27:36,998] {scheduler_job.py:155} INFO - Started process (PID=11203) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:37,012] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:27:37,012] {logging_mixin.py:112} INFO - [2020-11-05 17:27:37,012] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:37,186] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:37,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:27:37,249] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:27:37,257] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-05 17:27:50,314] {scheduler_job.py:155} INFO - Started process (PID=11260) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:50,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:27:50,320] {logging_mixin.py:112} INFO - [2020-11-05 17:27:50,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:50,461] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:27:50,496] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:27:50,522] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:27:50,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.215 seconds
[2020-11-05 17:28:03,606] {scheduler_job.py:155} INFO - Started process (PID=11319) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:03,609] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:28:03,609] {logging_mixin.py:112} INFO - [2020-11-05 17:28:03,609] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:03,720] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:03,748] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:28:03,772] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:28:03,775] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.169 seconds
[2020-11-05 17:28:16,882] {scheduler_job.py:155} INFO - Started process (PID=11377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:16,885] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:28:16,886] {logging_mixin.py:112} INFO - [2020-11-05 17:28:16,886] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:17,038] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:17,060] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:28:17,077] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:28:17,082] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.200 seconds
[2020-11-05 17:28:30,143] {scheduler_job.py:155} INFO - Started process (PID=11435) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:30,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:28:30,146] {logging_mixin.py:112} INFO - [2020-11-05 17:28:30,146] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:30,262] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:30,301] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:28:30,323] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:28:30,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-05 17:28:43,574] {scheduler_job.py:155} INFO - Started process (PID=11504) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:43,587] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:28:43,588] {logging_mixin.py:112} INFO - [2020-11-05 17:28:43,587] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:43,767] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:43,811] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:28:43,835] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:28:43,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-05 17:28:57,005] {scheduler_job.py:155} INFO - Started process (PID=11576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:57,012] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:28:57,018] {logging_mixin.py:112} INFO - [2020-11-05 17:28:57,018] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:57,216] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:28:57,263] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:28:57,311] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:28:57,330] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.325 seconds
[2020-11-05 17:29:10,305] {scheduler_job.py:155} INFO - Started process (PID=11633) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:10,309] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:29:10,309] {logging_mixin.py:112} INFO - [2020-11-05 17:29:10,309] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:10,509] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:10,548] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:29:10,591] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:29:10,598] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.294 seconds
[2020-11-05 17:29:23,575] {scheduler_job.py:155} INFO - Started process (PID=11695) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:23,578] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:29:23,579] {logging_mixin.py:112} INFO - [2020-11-05 17:29:23,579] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:23,689] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:23,718] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:29:23,744] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:29:23,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:29:36,881] {scheduler_job.py:155} INFO - Started process (PID=11755) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:36,886] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:29:36,887] {logging_mixin.py:112} INFO - [2020-11-05 17:29:36,887] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:37,029] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:37,059] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:29:37,080] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:29:37,085] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-05 17:29:50,213] {scheduler_job.py:155} INFO - Started process (PID=11815) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:50,218] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:29:50,219] {logging_mixin.py:112} INFO - [2020-11-05 17:29:50,218] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:50,357] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:29:50,380] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:29:50,398] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:29:50,402] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.189 seconds
[2020-11-05 17:30:03,422] {scheduler_job.py:155} INFO - Started process (PID=11877) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:03,429] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:30:03,430] {logging_mixin.py:112} INFO - [2020-11-05 17:30:03,430] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:03,532] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:03,562] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:30:03,589] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:30:03,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.172 seconds
[2020-11-05 17:30:16,666] {scheduler_job.py:155} INFO - Started process (PID=11943) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:16,670] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:30:16,671] {logging_mixin.py:112} INFO - [2020-11-05 17:30:16,671] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:16,831] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:16,854] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:30:16,873] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:30:16,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-05 17:30:29,991] {scheduler_job.py:155} INFO - Started process (PID=12004) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:29,994] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:30:29,995] {logging_mixin.py:112} INFO - [2020-11-05 17:30:29,995] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:30,125] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:30,158] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:30:30,175] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:30:30,179] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 17:30:43,232] {scheduler_job.py:155} INFO - Started process (PID=12069) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:43,236] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:30:43,236] {logging_mixin.py:112} INFO - [2020-11-05 17:30:43,236] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:43,362] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:43,401] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:30:43,426] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:30:43,429] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.197 seconds
[2020-11-05 17:30:56,501] {scheduler_job.py:155} INFO - Started process (PID=12131) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:56,504] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:30:56,505] {logging_mixin.py:112} INFO - [2020-11-05 17:30:56,505] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:56,614] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:30:56,642] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:30:56,671] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:30:56,676] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 17:31:09,788] {scheduler_job.py:155} INFO - Started process (PID=12193) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:09,791] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:31:09,791] {logging_mixin.py:112} INFO - [2020-11-05 17:31:09,791] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:09,913] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:09,941] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:31:09,958] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:31:09,962] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:31:23,125] {scheduler_job.py:155} INFO - Started process (PID=12253) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:23,129] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:31:23,130] {logging_mixin.py:112} INFO - [2020-11-05 17:31:23,130] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:23,229] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:23,260] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:31:23,284] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:31:23,291] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.166 seconds
[2020-11-05 17:31:36,390] {scheduler_job.py:155} INFO - Started process (PID=12316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:36,393] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:31:36,393] {logging_mixin.py:112} INFO - [2020-11-05 17:31:36,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:36,514] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:36,538] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:31:36,563] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:31:36,567] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 17:31:49,683] {scheduler_job.py:155} INFO - Started process (PID=12379) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:49,687] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:31:49,687] {logging_mixin.py:112} INFO - [2020-11-05 17:31:49,687] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:49,785] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:31:49,808] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:31:49,828] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:31:49,832] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.149 seconds
[2020-11-05 17:32:03,058] {scheduler_job.py:155} INFO - Started process (PID=12442) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:03,062] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:32:03,063] {logging_mixin.py:112} INFO - [2020-11-05 17:32:03,062] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:03,197] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:03,227] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:32:03,250] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:32:03,258] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 17:32:16,327] {scheduler_job.py:155} INFO - Started process (PID=12506) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:16,342] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:32:16,343] {logging_mixin.py:112} INFO - [2020-11-05 17:32:16,343] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:16,451] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:16,488] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:32:16,538] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:32:16,551] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 17:32:29,802] {scheduler_job.py:155} INFO - Started process (PID=12583) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:29,808] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:32:29,811] {logging_mixin.py:112} INFO - [2020-11-05 17:32:29,811] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:30,047] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:30,094] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:32:30,121] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:32:30,129] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.326 seconds
[2020-11-05 17:32:43,476] {scheduler_job.py:155} INFO - Started process (PID=12636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:43,479] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:32:43,480] {logging_mixin.py:112} INFO - [2020-11-05 17:32:43,480] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:43,594] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:43,627] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:32:43,647] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:32:43,650] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.175 seconds
[2020-11-05 17:32:56,791] {scheduler_job.py:155} INFO - Started process (PID=12699) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:56,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:32:56,798] {logging_mixin.py:112} INFO - [2020-11-05 17:32:56,798] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:57,033] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:32:57,078] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:32:57,112] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:32:57,118] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.327 seconds
[2020-11-05 17:33:10,112] {scheduler_job.py:155} INFO - Started process (PID=12757) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:10,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:33:10,117] {logging_mixin.py:112} INFO - [2020-11-05 17:33:10,117] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:10,266] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:10,289] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:33:10,308] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:33:10,311] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 17:33:23,401] {scheduler_job.py:155} INFO - Started process (PID=12830) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:23,419] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:33:23,420] {logging_mixin.py:112} INFO - [2020-11-05 17:33:23,420] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:23,606] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:23,637] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:33:23,656] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:33:23,661] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.261 seconds
[2020-11-05 17:33:36,782] {scheduler_job.py:155} INFO - Started process (PID=12895) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:36,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:33:36,788] {logging_mixin.py:112} INFO - [2020-11-05 17:33:36,787] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:36,903] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:36,937] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:33:36,967] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:33:36,973] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.190 seconds
[2020-11-05 17:33:50,040] {scheduler_job.py:155} INFO - Started process (PID=12958) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:50,044] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:33:50,045] {logging_mixin.py:112} INFO - [2020-11-05 17:33:50,044] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:50,157] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:33:50,195] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:33:50,221] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:33:50,231] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 17:34:03,300] {scheduler_job.py:155} INFO - Started process (PID=13022) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:03,305] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:34:03,305] {logging_mixin.py:112} INFO - [2020-11-05 17:34:03,305] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:03,430] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:03,457] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:34:03,473] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:34:03,477] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 17:34:16,594] {scheduler_job.py:155} INFO - Started process (PID=13086) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:16,597] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:34:16,598] {logging_mixin.py:112} INFO - [2020-11-05 17:34:16,598] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:16,725] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:16,751] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:34:16,767] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:34:16,771] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.177 seconds
[2020-11-05 17:34:29,899] {scheduler_job.py:155} INFO - Started process (PID=13150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:29,905] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:34:29,906] {logging_mixin.py:112} INFO - [2020-11-05 17:34:29,906] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:30,028] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:30,050] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:34:30,069] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:34:30,073] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:34:43,178] {scheduler_job.py:155} INFO - Started process (PID=13219) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:43,182] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:34:43,182] {logging_mixin.py:112} INFO - [2020-11-05 17:34:43,182] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:43,308] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:43,332] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:34:43,351] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:34:43,355] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.178 seconds
[2020-11-05 17:34:56,471] {scheduler_job.py:155} INFO - Started process (PID=13304) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:56,475] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:34:56,476] {logging_mixin.py:112} INFO - [2020-11-05 17:34:56,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:56,657] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:34:56,695] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:34:56,718] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:34:56,734] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.274 seconds
[2020-11-05 17:35:09,776] {scheduler_job.py:155} INFO - Started process (PID=13364) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:09,784] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:35:09,787] {logging_mixin.py:112} INFO - [2020-11-05 17:35:09,786] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:10,062] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:10,105] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:35:10,129] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:35:10,135] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.359 seconds
[2020-11-05 17:35:23,144] {scheduler_job.py:155} INFO - Started process (PID=13431) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:23,152] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:35:23,153] {logging_mixin.py:112} INFO - [2020-11-05 17:35:23,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:23,363] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:23,448] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:35:23,505] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:35:23,512] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.368 seconds
[2020-11-05 17:35:36,384] {scheduler_job.py:155} INFO - Started process (PID=13495) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:36,390] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:35:36,391] {logging_mixin.py:112} INFO - [2020-11-05 17:35:36,390] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:36,579] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:36,628] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:35:36,653] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:35:36,659] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.275 seconds
[2020-11-05 17:35:49,692] {scheduler_job.py:155} INFO - Started process (PID=13560) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:49,696] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:35:49,697] {logging_mixin.py:112} INFO - [2020-11-05 17:35:49,696] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:49,851] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:35:49,920] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:35:49,952] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:35:49,967] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.275 seconds
[2020-11-05 17:36:03,026] {scheduler_job.py:155} INFO - Started process (PID=13620) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:03,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:36:03,030] {logging_mixin.py:112} INFO - [2020-11-05 17:36:03,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:03,145] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:03,169] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:36:03,191] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:36:03,196] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 17:36:16,300] {scheduler_job.py:155} INFO - Started process (PID=13681) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:16,306] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:36:16,308] {logging_mixin.py:112} INFO - [2020-11-05 17:36:16,307] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:16,473] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:16,511] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:36:16,546] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:36:16,554] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.254 seconds
[2020-11-05 17:36:29,598] {scheduler_job.py:155} INFO - Started process (PID=13744) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:29,610] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:36:29,613] {logging_mixin.py:112} INFO - [2020-11-05 17:36:29,612] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:29,874] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:29,920] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:36:29,943] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:36:29,950] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.354 seconds
[2020-11-05 17:36:42,880] {scheduler_job.py:155} INFO - Started process (PID=13805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:42,885] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:36:42,886] {logging_mixin.py:112} INFO - [2020-11-05 17:36:42,886] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:43,018] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:43,056] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:36:43,085] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:36:43,093] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 17:36:56,287] {scheduler_job.py:155} INFO - Started process (PID=13882) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:56,303] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:36:56,304] {logging_mixin.py:112} INFO - [2020-11-05 17:36:56,304] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:56,552] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:36:56,597] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:36:56,634] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:36:56,642] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.355 seconds
[2020-11-05 17:37:23,078] {scheduler_job.py:155} INFO - Started process (PID=14014) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:23,117] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:37:23,127] {logging_mixin.py:112} INFO - [2020-11-05 17:37:23,127] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:23,551] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:23,614] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:37:23,674] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:37:23,703] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.625 seconds
[2020-11-05 17:37:36,442] {scheduler_job.py:155} INFO - Started process (PID=14081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:36,478] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:37:36,478] {logging_mixin.py:112} INFO - [2020-11-05 17:37:36,478] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:36,745] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:36,796] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:37:36,836] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:37:36,841] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.398 seconds
[2020-11-05 17:37:49,811] {scheduler_job.py:155} INFO - Started process (PID=14141) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:49,819] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:37:49,820] {logging_mixin.py:112} INFO - [2020-11-05 17:37:49,820] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:49,952] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:37:49,977] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:37:49,993] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:37:49,997] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-05 17:38:03,159] {scheduler_job.py:155} INFO - Started process (PID=14208) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:03,166] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:38:03,166] {logging_mixin.py:112} INFO - [2020-11-05 17:38:03,166] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:03,450] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:03,509] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:38:03,543] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:38:03,549] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.391 seconds
[2020-11-05 17:38:15,729] {scheduler_job.py:155} INFO - Started process (PID=14265) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:15,738] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:38:15,745] {logging_mixin.py:112} INFO - [2020-11-05 17:38:15,745] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:15,947] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:15,975] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:38:16,002] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:38:16,009] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.280 seconds
[2020-11-05 17:38:29,073] {scheduler_job.py:155} INFO - Started process (PID=14324) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:29,084] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:38:29,085] {logging_mixin.py:112} INFO - [2020-11-05 17:38:29,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:29,252] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:29,280] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:38:29,309] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:38:29,313] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.240 seconds
[2020-11-05 17:38:42,405] {scheduler_job.py:155} INFO - Started process (PID=14390) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:42,409] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:38:42,411] {logging_mixin.py:112} INFO - [2020-11-05 17:38:42,411] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:42,513] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:42,556] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:38:42,576] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:38:42,580] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.174 seconds
[2020-11-05 17:38:55,759] {scheduler_job.py:155} INFO - Started process (PID=14454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:55,764] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:38:55,765] {logging_mixin.py:112} INFO - [2020-11-05 17:38:55,764] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:55,882] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:38:55,909] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:38:55,948] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:38:55,952] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.192 seconds
[2020-11-05 17:39:09,109] {scheduler_job.py:155} INFO - Started process (PID=14525) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:09,126] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:39:09,127] {logging_mixin.py:112} INFO - [2020-11-05 17:39:09,126] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:09,486] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:09,528] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:39:09,549] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:39:09,554] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.446 seconds
[2020-11-05 17:39:22,494] {scheduler_job.py:155} INFO - Started process (PID=14584) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:22,501] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:39:22,504] {logging_mixin.py:112} INFO - [2020-11-05 17:39:22,503] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:22,701] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:22,753] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:39:22,792] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:39:22,802] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.307 seconds
[2020-11-05 17:39:34,913] {scheduler_job.py:155} INFO - Started process (PID=14644) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:34,926] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:39:34,929] {logging_mixin.py:112} INFO - [2020-11-05 17:39:34,928] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:35,141] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:35,200] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:39:35,244] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:39:35,251] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.337 seconds
[2020-11-05 17:39:48,374] {scheduler_job.py:155} INFO - Started process (PID=14708) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:48,379] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:39:48,380] {logging_mixin.py:112} INFO - [2020-11-05 17:39:48,379] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:48,568] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:39:48,619] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:39:48,646] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:39:48,654] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.280 seconds
[2020-11-05 17:40:01,706] {scheduler_job.py:155} INFO - Started process (PID=14768) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:01,713] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:40:01,724] {logging_mixin.py:112} INFO - [2020-11-05 17:40:01,723] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:02,050] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:02,100] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:40:02,131] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:40:02,136] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.431 seconds
[2020-11-05 17:40:15,109] {scheduler_job.py:155} INFO - Started process (PID=14831) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:15,114] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:40:15,115] {logging_mixin.py:112} INFO - [2020-11-05 17:40:15,115] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:15,231] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:15,267] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:40:15,286] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:40:15,290] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.181 seconds
[2020-11-05 17:40:28,446] {scheduler_job.py:155} INFO - Started process (PID=14902) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:28,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:40:28,452] {logging_mixin.py:112} INFO - [2020-11-05 17:40:28,452] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:28,612] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:28,671] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:40:28,716] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:40:28,722] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.275 seconds
[2020-11-05 17:40:41,712] {scheduler_job.py:155} INFO - Started process (PID=14974) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:41,718] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:40:41,719] {logging_mixin.py:112} INFO - [2020-11-05 17:40:41,718] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:41,883] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:41,911] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:40:41,955] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:40:41,960] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.248 seconds
[2020-11-05 17:40:55,130] {scheduler_job.py:155} INFO - Started process (PID=15036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:55,136] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:40:55,137] {logging_mixin.py:112} INFO - [2020-11-05 17:40:55,137] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:55,401] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:40:55,457] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:40:55,501] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:40:55,508] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.378 seconds
[2020-11-05 17:41:08,487] {scheduler_job.py:155} INFO - Started process (PID=15094) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:08,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:41:08,492] {logging_mixin.py:112} INFO - [2020-11-05 17:41:08,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:08,761] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:08,808] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:41:08,837] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:41:08,843] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.356 seconds
[2020-11-05 17:41:21,917] {scheduler_job.py:155} INFO - Started process (PID=15150) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:21,923] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:41:21,924] {logging_mixin.py:112} INFO - [2020-11-05 17:41:21,924] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:22,056] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:22,086] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:41:22,116] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:41:22,121] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-05 17:41:35,275] {scheduler_job.py:155} INFO - Started process (PID=15212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:35,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:41:35,280] {logging_mixin.py:112} INFO - [2020-11-05 17:41:35,279] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:35,379] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:35,406] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:41:35,438] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:41:35,445] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.170 seconds
[2020-11-05 17:41:48,590] {scheduler_job.py:155} INFO - Started process (PID=15277) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:48,596] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:41:48,597] {logging_mixin.py:112} INFO - [2020-11-05 17:41:48,596] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:48,767] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:41:48,795] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:41:48,817] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:41:48,826] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.237 seconds
[2020-11-05 17:42:01,990] {scheduler_job.py:155} INFO - Started process (PID=15342) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:01,997] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:42:01,998] {logging_mixin.py:112} INFO - [2020-11-05 17:42:01,998] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:02,118] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:02,146] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:42:02,171] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:42:02,178] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 17:42:15,321] {scheduler_job.py:155} INFO - Started process (PID=15404) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:15,327] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:42:15,328] {logging_mixin.py:112} INFO - [2020-11-05 17:42:15,328] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:15,462] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:15,487] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:42:15,504] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:42:15,509] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.188 seconds
[2020-11-05 17:42:28,654] {scheduler_job.py:155} INFO - Started process (PID=15467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:28,660] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:42:28,661] {logging_mixin.py:112} INFO - [2020-11-05 17:42:28,661] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:28,863] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:28,898] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:42:28,925] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:42:28,931] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.277 seconds
[2020-11-05 17:42:41,984] {scheduler_job.py:155} INFO - Started process (PID=15538) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:41,987] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:42:41,988] {logging_mixin.py:112} INFO - [2020-11-05 17:42:41,988] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:42,195] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:42,220] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:42:42,242] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:42:42,248] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.264 seconds
[2020-11-05 17:42:55,322] {scheduler_job.py:155} INFO - Started process (PID=15601) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:55,326] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:42:55,327] {logging_mixin.py:112} INFO - [2020-11-05 17:42:55,327] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:55,455] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:42:55,500] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:42:55,536] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:42:55,542] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.219 seconds
[2020-11-05 17:43:08,622] {scheduler_job.py:155} INFO - Started process (PID=15666) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:08,627] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:43:08,628] {logging_mixin.py:112} INFO - [2020-11-05 17:43:08,628] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:08,793] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:08,816] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:43:08,835] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:43:08,839] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.216 seconds
[2020-11-05 17:43:21,975] {scheduler_job.py:155} INFO - Started process (PID=15736) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:21,980] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:43:21,980] {logging_mixin.py:112} INFO - [2020-11-05 17:43:21,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:22,083] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:22,119] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:43:22,138] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:43:22,142] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.167 seconds
[2020-11-05 17:43:35,317] {scheduler_job.py:155} INFO - Started process (PID=15805) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:35,325] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:43:35,326] {logging_mixin.py:112} INFO - [2020-11-05 17:43:35,325] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:35,475] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:35,499] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:43:35,519] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:43:35,522] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.205 seconds
[2020-11-05 17:43:48,734] {scheduler_job.py:155} INFO - Started process (PID=15871) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:48,751] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:43:48,751] {logging_mixin.py:112} INFO - [2020-11-05 17:43:48,751] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:48,956] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:43:48,996] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:43:49,028] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:43:49,034] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.300 seconds
[2020-11-05 17:44:02,161] {scheduler_job.py:155} INFO - Started process (PID=15932) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:02,168] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:44:02,170] {logging_mixin.py:112} INFO - [2020-11-05 17:44:02,170] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:02,379] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:02,410] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:44:02,442] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:44:02,452] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.291 seconds
[2020-11-05 17:44:15,515] {scheduler_job.py:155} INFO - Started process (PID=15990) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:15,522] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:44:15,522] {logging_mixin.py:112} INFO - [2020-11-05 17:44:15,522] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:15,792] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:15,824] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:44:15,863] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:44:15,871] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.356 seconds
[2020-11-05 17:44:28,960] {scheduler_job.py:155} INFO - Started process (PID=16051) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:28,975] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:44:28,976] {logging_mixin.py:112} INFO - [2020-11-05 17:44:28,976] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:29,330] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:29,378] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:44:29,409] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:44:29,420] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.460 seconds
[2020-11-05 17:44:42,316] {scheduler_job.py:155} INFO - Started process (PID=16111) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:42,324] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:44:42,328] {logging_mixin.py:112} INFO - [2020-11-05 17:44:42,328] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:42,442] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:42,501] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:44:42,525] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:44:42,535] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.219 seconds
[2020-11-05 17:44:55,709] {scheduler_job.py:155} INFO - Started process (PID=16175) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:55,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:44:55,721] {logging_mixin.py:112} INFO - [2020-11-05 17:44:55,715] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:55,794] {logging_mixin.py:112} INFO - [2020-11-05 17:44:55,792] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 34, in <module>
    with DAG('S3_task', default_args=default_args, schedule_interval=schedule) as dag:
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 280, in __init__
    self.timezone = self.default_args['start_date'].tzinfo
AttributeError: 'datetime.timedelta' object has no attribute 'tzinfo'
[2020-11-05 17:44:55,803] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:44:55,848] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.139 seconds
[2020-11-05 17:45:09,056] {scheduler_job.py:155} INFO - Started process (PID=16236) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:09,060] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:45:09,061] {logging_mixin.py:112} INFO - [2020-11-05 17:45:09,060] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:09,086] {logging_mixin.py:112} INFO - [2020-11-05 17:45:09,084] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 34, in <module>
    with DAG('S3_task', default_args=default_args, schedule_interval=schedule) as dag:
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 280, in __init__
    self.timezone = self.default_args['start_date'].tzinfo
AttributeError: 'datetime.timedelta' object has no attribute 'tzinfo'
[2020-11-05 17:45:09,086] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:09,146] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.090 seconds
[2020-11-05 17:45:22,470] {scheduler_job.py:155} INFO - Started process (PID=16295) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:22,476] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:45:22,480] {logging_mixin.py:112} INFO - [2020-11-05 17:45:22,480] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:22,517] {logging_mixin.py:112} INFO - [2020-11-05 17:45:22,514] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 34, in <module>
    with DAG('S3_task', default_args=default_args, schedule_interval=schedule) as dag:
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 280, in __init__
    self.timezone = self.default_args['start_date'].tzinfo
AttributeError: 'datetime.timedelta' object has no attribute 'tzinfo'
[2020-11-05 17:45:22,517] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:22,559] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.089 seconds
[2020-11-05 17:45:35,787] {scheduler_job.py:155} INFO - Started process (PID=16354) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:35,793] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:45:35,793] {logging_mixin.py:112} INFO - [2020-11-05 17:45:35,793] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:35,819] {logging_mixin.py:112} INFO - [2020-11-05 17:45:35,817] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 34, in <module>
    with DAG('S3_task', default_args=default_args, schedule_interval=schedule) as dag:
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 280, in __init__
    self.timezone = self.default_args['start_date'].tzinfo
AttributeError: 'datetime.timedelta' object has no attribute 'tzinfo'
[2020-11-05 17:45:35,819] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:35,853] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.066 seconds
[2020-11-05 17:45:49,264] {scheduler_job.py:155} INFO - Started process (PID=16414) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:49,279] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:45:49,284] {logging_mixin.py:112} INFO - [2020-11-05 17:45:49,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:49,472] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:45:49,518] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:45:49,576] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:07:00+00:00: scheduled__2020-11-02T10:07:00+00:00, externally triggered: False>
[2020-11-05 17:45:49,583] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:07:00+00:00: scheduled__2020-11-02T10:07:00+00:00, externally triggered: False>
[2020-11-05 17:45:49,604] {logging_mixin.py:112} INFO - [2020-11-05 17:45:49,604] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:07:00+00:00: scheduled__2020-11-02T10:07:00+00:00, externally triggered: False> successful
[2020-11-05 17:45:49,608] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:45:49,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.352 seconds
[2020-11-05 17:46:02,592] {scheduler_job.py:155} INFO - Started process (PID=16477) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:02,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:46:02,596] {logging_mixin.py:112} INFO - [2020-11-05 17:46:02,595] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:02,709] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:02,731] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:46:02,766] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:12:00+00:00: scheduled__2020-11-02T10:12:00+00:00, externally triggered: False>
[2020-11-05 17:46:02,772] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:12:00+00:00: scheduled__2020-11-02T10:12:00+00:00, externally triggered: False>
[2020-11-05 17:46:02,783] {logging_mixin.py:112} INFO - [2020-11-05 17:46:02,783] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:12:00+00:00: scheduled__2020-11-02T10:12:00+00:00, externally triggered: False> successful
[2020-11-05 17:46:02,788] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:46:02,794] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 17:46:15,967] {scheduler_job.py:155} INFO - Started process (PID=16535) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:15,979] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:46:15,980] {logging_mixin.py:112} INFO - [2020-11-05 17:46:15,980] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:16,313] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:16,363] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:46:16,445] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:17:00+00:00: scheduled__2020-11-02T10:17:00+00:00, externally triggered: False>
[2020-11-05 17:46:16,460] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:17:00+00:00: scheduled__2020-11-02T10:17:00+00:00, externally triggered: False>
[2020-11-05 17:46:16,487] {logging_mixin.py:112} INFO - [2020-11-05 17:46:16,487] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:17:00+00:00: scheduled__2020-11-02T10:17:00+00:00, externally triggered: False> successful
[2020-11-05 17:46:16,495] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:46:16,502] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.535 seconds
[2020-11-05 17:46:29,322] {scheduler_job.py:155} INFO - Started process (PID=16594) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:29,329] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:46:29,330] {logging_mixin.py:112} INFO - [2020-11-05 17:46:29,330] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:29,527] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:29,602] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:46:29,670] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:22:00+00:00: scheduled__2020-11-02T10:22:00+00:00, externally triggered: False>
[2020-11-05 17:46:29,678] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:22:00+00:00: scheduled__2020-11-02T10:22:00+00:00, externally triggered: False>
[2020-11-05 17:46:29,700] {logging_mixin.py:112} INFO - [2020-11-05 17:46:29,699] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:22:00+00:00: scheduled__2020-11-02T10:22:00+00:00, externally triggered: False> successful
[2020-11-05 17:46:29,706] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:46:29,723] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.401 seconds
[2020-11-05 17:46:42,695] {scheduler_job.py:155} INFO - Started process (PID=16653) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:42,706] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:46:42,707] {logging_mixin.py:112} INFO - [2020-11-05 17:46:42,707] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:43,026] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:43,085] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:46:43,141] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:27:00+00:00: scheduled__2020-11-02T10:27:00+00:00, externally triggered: False>
[2020-11-05 17:46:43,146] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:27:00+00:00: scheduled__2020-11-02T10:27:00+00:00, externally triggered: False>
[2020-11-05 17:46:43,159] {logging_mixin.py:112} INFO - [2020-11-05 17:46:43,159] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:27:00+00:00: scheduled__2020-11-02T10:27:00+00:00, externally triggered: False> successful
[2020-11-05 17:46:43,164] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:46:43,170] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.475 seconds
[2020-11-05 17:46:56,105] {scheduler_job.py:155} INFO - Started process (PID=16724) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:56,113] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:46:56,114] {logging_mixin.py:112} INFO - [2020-11-05 17:46:56,113] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:56,379] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:46:56,406] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:46:56,452] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:32:00+00:00: scheduled__2020-11-02T10:32:00+00:00, externally triggered: False>
[2020-11-05 17:46:56,456] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:32:00+00:00: scheduled__2020-11-02T10:32:00+00:00, externally triggered: False>
[2020-11-05 17:46:56,471] {logging_mixin.py:112} INFO - [2020-11-05 17:46:56,471] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:32:00+00:00: scheduled__2020-11-02T10:32:00+00:00, externally triggered: False> successful
[2020-11-05 17:46:56,475] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:46:56,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.377 seconds
[2020-11-05 17:47:09,413] {scheduler_job.py:155} INFO - Started process (PID=16783) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:09,415] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:47:09,416] {logging_mixin.py:112} INFO - [2020-11-05 17:47:09,416] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:09,512] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:09,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:47:09,565] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:37:00+00:00: scheduled__2020-11-02T10:37:00+00:00, externally triggered: False>
[2020-11-05 17:47:09,569] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:37:00+00:00: scheduled__2020-11-02T10:37:00+00:00, externally triggered: False>
[2020-11-05 17:47:09,580] {logging_mixin.py:112} INFO - [2020-11-05 17:47:09,580] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:37:00+00:00: scheduled__2020-11-02T10:37:00+00:00, externally triggered: False> successful
[2020-11-05 17:47:09,585] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:47:09,592] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.179 seconds
[2020-11-05 17:47:22,790] {scheduler_job.py:155} INFO - Started process (PID=16842) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:22,794] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:47:22,795] {logging_mixin.py:112} INFO - [2020-11-05 17:47:22,795] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:22,998] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:23,034] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:47:23,083] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:42:00+00:00: scheduled__2020-11-02T10:42:00+00:00, externally triggered: False>
[2020-11-05 17:47:23,089] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:42:00+00:00: scheduled__2020-11-02T10:42:00+00:00, externally triggered: False>
[2020-11-05 17:47:23,105] {logging_mixin.py:112} INFO - [2020-11-05 17:47:23,104] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:42:00+00:00: scheduled__2020-11-02T10:42:00+00:00, externally triggered: False> successful
[2020-11-05 17:47:23,110] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:47:23,117] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.327 seconds
[2020-11-05 17:47:36,176] {scheduler_job.py:155} INFO - Started process (PID=16897) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:36,181] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:47:36,181] {logging_mixin.py:112} INFO - [2020-11-05 17:47:36,181] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:36,467] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:36,577] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:47:36,651] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:47:00+00:00: scheduled__2020-11-02T10:47:00+00:00, externally triggered: False>
[2020-11-05 17:47:36,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:47:00+00:00: scheduled__2020-11-02T10:47:00+00:00, externally triggered: False>
[2020-11-05 17:47:36,675] {logging_mixin.py:112} INFO - [2020-11-05 17:47:36,675] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:47:00+00:00: scheduled__2020-11-02T10:47:00+00:00, externally triggered: False> successful
[2020-11-05 17:47:36,680] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:47:36,685] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.510 seconds
[2020-11-05 17:47:49,542] {scheduler_job.py:155} INFO - Started process (PID=16956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:49,553] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:47:49,554] {logging_mixin.py:112} INFO - [2020-11-05 17:47:49,554] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:49,828] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:47:49,858] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:47:49,915] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:52:00+00:00: scheduled__2020-11-02T10:52:00+00:00, externally triggered: False>
[2020-11-05 17:47:49,919] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:52:00+00:00: scheduled__2020-11-02T10:52:00+00:00, externally triggered: False>
[2020-11-05 17:47:49,930] {logging_mixin.py:112} INFO - [2020-11-05 17:47:49,930] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:52:00+00:00: scheduled__2020-11-02T10:52:00+00:00, externally triggered: False> successful
[2020-11-05 17:47:49,933] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:47:49,937] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.396 seconds
[2020-11-05 17:48:02,976] {scheduler_job.py:155} INFO - Started process (PID=17020) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:02,983] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:48:02,988] {logging_mixin.py:112} INFO - [2020-11-05 17:48:02,988] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:03,278] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:03,311] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:48:03,354] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 10:57:00+00:00: scheduled__2020-11-02T10:57:00+00:00, externally triggered: False>
[2020-11-05 17:48:03,358] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 10:57:00+00:00: scheduled__2020-11-02T10:57:00+00:00, externally triggered: False>
[2020-11-05 17:48:03,374] {logging_mixin.py:112} INFO - [2020-11-05 17:48:03,374] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 10:57:00+00:00: scheduled__2020-11-02T10:57:00+00:00, externally triggered: False> successful
[2020-11-05 17:48:03,379] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:48:03,383] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.407 seconds
[2020-11-05 17:48:16,304] {scheduler_job.py:155} INFO - Started process (PID=17089) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:16,311] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:48:16,312] {logging_mixin.py:112} INFO - [2020-11-05 17:48:16,312] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:16,450] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:16,482] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:48:16,527] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:02:00+00:00: scheduled__2020-11-02T11:02:00+00:00, externally triggered: False>
[2020-11-05 17:48:16,535] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:02:00+00:00: scheduled__2020-11-02T11:02:00+00:00, externally triggered: False>
[2020-11-05 17:48:16,551] {logging_mixin.py:112} INFO - [2020-11-05 17:48:16,550] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:02:00+00:00: scheduled__2020-11-02T11:02:00+00:00, externally triggered: False> successful
[2020-11-05 17:48:16,558] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:48:16,565] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.261 seconds
[2020-11-05 17:48:29,641] {scheduler_job.py:155} INFO - Started process (PID=17148) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:29,646] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:48:29,647] {logging_mixin.py:112} INFO - [2020-11-05 17:48:29,646] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:29,880] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:29,902] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:48:29,928] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:07:00+00:00: scheduled__2020-11-02T11:07:00+00:00, externally triggered: False>
[2020-11-05 17:48:29,932] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:07:00+00:00: scheduled__2020-11-02T11:07:00+00:00, externally triggered: False>
[2020-11-05 17:48:29,948] {logging_mixin.py:112} INFO - [2020-11-05 17:48:29,948] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:07:00+00:00: scheduled__2020-11-02T11:07:00+00:00, externally triggered: False> successful
[2020-11-05 17:48:29,953] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:48:29,962] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.321 seconds
[2020-11-05 17:48:43,045] {scheduler_job.py:155} INFO - Started process (PID=17202) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:43,068] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:48:43,069] {logging_mixin.py:112} INFO - [2020-11-05 17:48:43,068] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:43,320] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:43,356] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:48:43,402] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:12:00+00:00: scheduled__2020-11-02T11:12:00+00:00, externally triggered: False>
[2020-11-05 17:48:43,406] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:12:00+00:00: scheduled__2020-11-02T11:12:00+00:00, externally triggered: False>
[2020-11-05 17:48:43,424] {logging_mixin.py:112} INFO - [2020-11-05 17:48:43,424] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:12:00+00:00: scheduled__2020-11-02T11:12:00+00:00, externally triggered: False> successful
[2020-11-05 17:48:43,432] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:48:43,437] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.391 seconds
[2020-11-05 17:48:56,365] {scheduler_job.py:155} INFO - Started process (PID=17265) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:56,370] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:48:56,372] {logging_mixin.py:112} INFO - [2020-11-05 17:48:56,372] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:56,748] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:48:56,783] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:48:56,815] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:17:00+00:00: scheduled__2020-11-02T11:17:00+00:00, externally triggered: False>
[2020-11-05 17:48:56,820] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:17:00+00:00: scheduled__2020-11-02T11:17:00+00:00, externally triggered: False>
[2020-11-05 17:48:56,841] {logging_mixin.py:112} INFO - [2020-11-05 17:48:56,840] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:17:00+00:00: scheduled__2020-11-02T11:17:00+00:00, externally triggered: False> successful
[2020-11-05 17:48:56,844] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:48:56,850] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.485 seconds
[2020-11-05 17:49:09,745] {scheduler_job.py:155} INFO - Started process (PID=17328) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:09,749] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:49:09,750] {logging_mixin.py:112} INFO - [2020-11-05 17:49:09,750] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:09,883] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:09,918] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:49:09,963] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:22:00+00:00: scheduled__2020-11-02T11:22:00+00:00, externally triggered: False>
[2020-11-05 17:49:09,970] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:22:00+00:00: scheduled__2020-11-02T11:22:00+00:00, externally triggered: False>
[2020-11-05 17:49:09,980] {logging_mixin.py:112} INFO - [2020-11-05 17:49:09,980] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:22:00+00:00: scheduled__2020-11-02T11:22:00+00:00, externally triggered: False> successful
[2020-11-05 17:49:09,984] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:49:09,988] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.244 seconds
[2020-11-05 17:49:23,073] {scheduler_job.py:155} INFO - Started process (PID=17389) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:23,076] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:49:23,077] {logging_mixin.py:112} INFO - [2020-11-05 17:49:23,077] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:23,286] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:23,307] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:49:23,343] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:27:00+00:00: scheduled__2020-11-02T11:27:00+00:00, externally triggered: False>
[2020-11-05 17:49:23,347] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:27:00+00:00: scheduled__2020-11-02T11:27:00+00:00, externally triggered: False>
[2020-11-05 17:49:23,361] {logging_mixin.py:112} INFO - [2020-11-05 17:49:23,361] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:27:00+00:00: scheduled__2020-11-02T11:27:00+00:00, externally triggered: False> successful
[2020-11-05 17:49:23,365] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:49:23,370] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.309 seconds
[2020-11-05 17:49:36,461] {scheduler_job.py:155} INFO - Started process (PID=17450) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:36,465] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:49:36,465] {logging_mixin.py:112} INFO - [2020-11-05 17:49:36,465] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:36,583] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:36,608] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:49:36,652] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:32:00+00:00: scheduled__2020-11-02T11:32:00+00:00, externally triggered: False>
[2020-11-05 17:49:36,657] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:32:00+00:00: scheduled__2020-11-02T11:32:00+00:00, externally triggered: False>
[2020-11-05 17:49:36,670] {logging_mixin.py:112} INFO - [2020-11-05 17:49:36,670] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:32:00+00:00: scheduled__2020-11-02T11:32:00+00:00, externally triggered: False> successful
[2020-11-05 17:49:36,674] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:49:36,680] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.220 seconds
[2020-11-05 17:49:49,872] {scheduler_job.py:155} INFO - Started process (PID=17512) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:49,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:49:49,888] {logging_mixin.py:112} INFO - [2020-11-05 17:49:49,887] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:50,007] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:49:50,031] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:49:50,067] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:37:00+00:00: scheduled__2020-11-02T11:37:00+00:00, externally triggered: False>
[2020-11-05 17:49:50,072] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:37:00+00:00: scheduled__2020-11-02T11:37:00+00:00, externally triggered: False>
[2020-11-05 17:49:50,081] {logging_mixin.py:112} INFO - [2020-11-05 17:49:50,081] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:37:00+00:00: scheduled__2020-11-02T11:37:00+00:00, externally triggered: False> successful
[2020-11-05 17:49:50,085] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:49:50,089] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.216 seconds
[2020-11-05 17:50:03,161] {scheduler_job.py:155} INFO - Started process (PID=17576) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:03,176] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:50:03,179] {logging_mixin.py:112} INFO - [2020-11-05 17:50:03,177] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:03,333] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:03,355] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:50:03,386] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:42:00+00:00: scheduled__2020-11-02T11:42:00+00:00, externally triggered: False>
[2020-11-05 17:50:03,389] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:42:00+00:00: scheduled__2020-11-02T11:42:00+00:00, externally triggered: False>
[2020-11-05 17:50:03,401] {logging_mixin.py:112} INFO - [2020-11-05 17:50:03,400] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:42:00+00:00: scheduled__2020-11-02T11:42:00+00:00, externally triggered: False> successful
[2020-11-05 17:50:03,404] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:50:03,407] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.247 seconds
[2020-11-05 17:50:16,489] {scheduler_job.py:155} INFO - Started process (PID=17636) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:16,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:50:16,501] {logging_mixin.py:112} INFO - [2020-11-05 17:50:16,501] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:16,698] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:16,720] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:50:16,747] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:47:00+00:00: scheduled__2020-11-02T11:47:00+00:00, externally triggered: False>
[2020-11-05 17:50:16,751] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:47:00+00:00: scheduled__2020-11-02T11:47:00+00:00, externally triggered: False>
[2020-11-05 17:50:16,761] {logging_mixin.py:112} INFO - [2020-11-05 17:50:16,761] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:47:00+00:00: scheduled__2020-11-02T11:47:00+00:00, externally triggered: False> successful
[2020-11-05 17:50:16,764] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:50:16,768] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.279 seconds
[2020-11-05 17:50:29,892] {scheduler_job.py:155} INFO - Started process (PID=17697) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:29,896] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:50:29,897] {logging_mixin.py:112} INFO - [2020-11-05 17:50:29,897] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:30,076] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:30,103] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:50:30,148] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:52:00+00:00: scheduled__2020-11-02T11:52:00+00:00, externally triggered: False>
[2020-11-05 17:50:30,153] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:52:00+00:00: scheduled__2020-11-02T11:52:00+00:00, externally triggered: False>
[2020-11-05 17:50:30,175] {logging_mixin.py:112} INFO - [2020-11-05 17:50:30,174] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:52:00+00:00: scheduled__2020-11-02T11:52:00+00:00, externally triggered: False> successful
[2020-11-05 17:50:30,180] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:50:30,187] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.295 seconds
[2020-11-05 17:50:43,311] {scheduler_job.py:155} INFO - Started process (PID=17754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:43,348] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:50:43,349] {logging_mixin.py:112} INFO - [2020-11-05 17:50:43,349] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:43,846] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:43,899] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:50:43,951] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 11:57:00+00:00: scheduled__2020-11-02T11:57:00+00:00, externally triggered: False>
[2020-11-05 17:50:43,967] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 11:57:00+00:00: scheduled__2020-11-02T11:57:00+00:00, externally triggered: False>
[2020-11-05 17:50:43,981] {logging_mixin.py:112} INFO - [2020-11-05 17:50:43,981] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 11:57:00+00:00: scheduled__2020-11-02T11:57:00+00:00, externally triggered: False> successful
[2020-11-05 17:50:43,990] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:50:43,996] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.686 seconds
[2020-11-05 17:50:56,747] {scheduler_job.py:155} INFO - Started process (PID=17818) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:56,756] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:50:56,757] {logging_mixin.py:112} INFO - [2020-11-05 17:50:56,757] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:57,104] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:50:57,145] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:50:57,198] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:02:00+00:00: scheduled__2020-11-02T12:02:00+00:00, externally triggered: False>
[2020-11-05 17:50:57,206] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:02:00+00:00: scheduled__2020-11-02T12:02:00+00:00, externally triggered: False>
[2020-11-05 17:50:57,230] {logging_mixin.py:112} INFO - [2020-11-05 17:50:57,229] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:02:00+00:00: scheduled__2020-11-02T12:02:00+00:00, externally triggered: False> successful
[2020-11-05 17:50:57,234] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:50:57,240] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.493 seconds
[2020-11-05 17:51:10,159] {scheduler_job.py:155} INFO - Started process (PID=17875) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:10,166] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:51:10,167] {logging_mixin.py:112} INFO - [2020-11-05 17:51:10,166] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:10,291] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:10,330] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:51:10,371] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:07:00+00:00: scheduled__2020-11-02T12:07:00+00:00, externally triggered: False>
[2020-11-05 17:51:10,375] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:07:00+00:00: scheduled__2020-11-02T12:07:00+00:00, externally triggered: False>
[2020-11-05 17:51:10,396] {logging_mixin.py:112} INFO - [2020-11-05 17:51:10,396] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:07:00+00:00: scheduled__2020-11-02T12:07:00+00:00, externally triggered: False> successful
[2020-11-05 17:51:10,400] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:51:10,410] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.251 seconds
[2020-11-05 17:51:23,506] {scheduler_job.py:155} INFO - Started process (PID=17928) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:23,510] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:51:23,512] {logging_mixin.py:112} INFO - [2020-11-05 17:51:23,511] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:23,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:23,722] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:51:23,763] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:12:00+00:00: scheduled__2020-11-02T12:12:00+00:00, externally triggered: False>
[2020-11-05 17:51:23,769] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:12:00+00:00: scheduled__2020-11-02T12:12:00+00:00, externally triggered: False>
[2020-11-05 17:51:23,797] {logging_mixin.py:112} INFO - [2020-11-05 17:51:23,797] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:12:00+00:00: scheduled__2020-11-02T12:12:00+00:00, externally triggered: False> successful
[2020-11-05 17:51:23,803] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:51:23,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.304 seconds
[2020-11-05 17:51:36,868] {scheduler_job.py:155} INFO - Started process (PID=17990) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:36,875] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:51:36,876] {logging_mixin.py:112} INFO - [2020-11-05 17:51:36,876] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:37,124] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:37,147] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:51:37,193] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:17:00+00:00: scheduled__2020-11-02T12:17:00+00:00, externally triggered: False>
[2020-11-05 17:51:37,202] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:17:00+00:00: scheduled__2020-11-02T12:17:00+00:00, externally triggered: False>
[2020-11-05 17:51:37,214] {logging_mixin.py:112} INFO - [2020-11-05 17:51:37,214] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:17:00+00:00: scheduled__2020-11-02T12:17:00+00:00, externally triggered: False> successful
[2020-11-05 17:51:37,217] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:51:37,228] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.360 seconds
[2020-11-05 17:51:50,193] {scheduler_job.py:155} INFO - Started process (PID=18051) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:50,199] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:51:50,200] {logging_mixin.py:112} INFO - [2020-11-05 17:51:50,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:50,384] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:51:50,405] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:51:50,445] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:22:00+00:00: scheduled__2020-11-02T12:22:00+00:00, externally triggered: False>
[2020-11-05 17:51:50,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:22:00+00:00: scheduled__2020-11-02T12:22:00+00:00, externally triggered: False>
[2020-11-05 17:51:50,468] {logging_mixin.py:112} INFO - [2020-11-05 17:51:50,467] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:22:00+00:00: scheduled__2020-11-02T12:22:00+00:00, externally triggered: False> successful
[2020-11-05 17:51:50,472] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:51:50,476] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.284 seconds
[2020-11-05 17:52:03,581] {scheduler_job.py:155} INFO - Started process (PID=18112) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:03,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:52:03,598] {logging_mixin.py:112} INFO - [2020-11-05 17:52:03,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:03,751] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:03,798] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:52:03,841] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:27:00+00:00: scheduled__2020-11-02T12:27:00+00:00, externally triggered: False>
[2020-11-05 17:52:03,846] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:27:00+00:00: scheduled__2020-11-02T12:27:00+00:00, externally triggered: False>
[2020-11-05 17:52:03,865] {logging_mixin.py:112} INFO - [2020-11-05 17:52:03,865] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:27:00+00:00: scheduled__2020-11-02T12:27:00+00:00, externally triggered: False> successful
[2020-11-05 17:52:03,870] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:52:03,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.295 seconds
[2020-11-05 17:52:16,991] {scheduler_job.py:155} INFO - Started process (PID=18171) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:16,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:52:16,996] {logging_mixin.py:112} INFO - [2020-11-05 17:52:16,996] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:17,102] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:17,132] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:52:17,160] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:32:00+00:00: scheduled__2020-11-02T12:32:00+00:00, externally triggered: False>
[2020-11-05 17:52:17,168] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:32:00+00:00: scheduled__2020-11-02T12:32:00+00:00, externally triggered: False>
[2020-11-05 17:52:17,182] {logging_mixin.py:112} INFO - [2020-11-05 17:52:17,181] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:32:00+00:00: scheduled__2020-11-02T12:32:00+00:00, externally triggered: False> successful
[2020-11-05 17:52:17,188] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:52:17,193] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 17:52:37,330] {scheduler_job.py:155} INFO - Started process (PID=18265) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:37,334] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:52:37,335] {logging_mixin.py:112} INFO - [2020-11-05 17:52:37,334] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:37,454] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:37,480] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:52:37,516] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:37:00+00:00: scheduled__2020-11-02T12:37:00+00:00, externally triggered: False>
[2020-11-05 17:52:37,522] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:37:00+00:00: scheduled__2020-11-02T12:37:00+00:00, externally triggered: False>
[2020-11-05 17:52:37,532] {logging_mixin.py:112} INFO - [2020-11-05 17:52:37,532] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:37:00+00:00: scheduled__2020-11-02T12:37:00+00:00, externally triggered: False> successful
[2020-11-05 17:52:37,540] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:52:37,544] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.214 seconds
[2020-11-05 17:52:50,677] {scheduler_job.py:155} INFO - Started process (PID=18328) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:50,681] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:52:50,682] {logging_mixin.py:112} INFO - [2020-11-05 17:52:50,681] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:50,827] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:52:50,861] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:52:50,893] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:42:00+00:00: scheduled__2020-11-02T12:42:00+00:00, externally triggered: False>
[2020-11-05 17:52:50,897] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:42:00+00:00: scheduled__2020-11-02T12:42:00+00:00, externally triggered: False>
[2020-11-05 17:52:50,907] {logging_mixin.py:112} INFO - [2020-11-05 17:52:50,907] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:42:00+00:00: scheduled__2020-11-02T12:42:00+00:00, externally triggered: False> successful
[2020-11-05 17:52:50,910] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:52:50,914] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.237 seconds
[2020-11-05 17:53:04,137] {scheduler_job.py:155} INFO - Started process (PID=18390) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:04,140] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:53:04,141] {logging_mixin.py:112} INFO - [2020-11-05 17:53:04,141] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:04,270] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:04,303] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:53:04,337] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:47:00+00:00: scheduled__2020-11-02T12:47:00+00:00, externally triggered: False>
[2020-11-05 17:53:04,343] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:47:00+00:00: scheduled__2020-11-02T12:47:00+00:00, externally triggered: False>
[2020-11-05 17:53:04,358] {logging_mixin.py:112} INFO - [2020-11-05 17:53:04,357] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:47:00+00:00: scheduled__2020-11-02T12:47:00+00:00, externally triggered: False> successful
[2020-11-05 17:53:04,361] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:53:04,366] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-05 17:53:17,450] {scheduler_job.py:155} INFO - Started process (PID=18455) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:17,455] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:53:17,456] {logging_mixin.py:112} INFO - [2020-11-05 17:53:17,455] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:17,681] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:17,720] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:53:17,783] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:52:00+00:00: scheduled__2020-11-02T12:52:00+00:00, externally triggered: False>
[2020-11-05 17:53:17,803] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:52:00+00:00: scheduled__2020-11-02T12:52:00+00:00, externally triggered: False>
[2020-11-05 17:53:17,826] {logging_mixin.py:112} INFO - [2020-11-05 17:53:17,826] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:52:00+00:00: scheduled__2020-11-02T12:52:00+00:00, externally triggered: False> successful
[2020-11-05 17:53:17,833] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:53:17,839] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.389 seconds
[2020-11-05 17:53:30,941] {scheduler_job.py:155} INFO - Started process (PID=18520) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:30,944] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:53:30,944] {logging_mixin.py:112} INFO - [2020-11-05 17:53:30,944] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:31,285] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:31,348] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:53:31,446] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 12:57:00+00:00: scheduled__2020-11-02T12:57:00+00:00, externally triggered: False>
[2020-11-05 17:53:31,455] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 12:57:00+00:00: scheduled__2020-11-02T12:57:00+00:00, externally triggered: False>
[2020-11-05 17:53:31,474] {logging_mixin.py:112} INFO - [2020-11-05 17:53:31,474] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 12:57:00+00:00: scheduled__2020-11-02T12:57:00+00:00, externally triggered: False> successful
[2020-11-05 17:53:31,478] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:53:31,483] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.542 seconds
[2020-11-05 17:53:44,274] {scheduler_job.py:155} INFO - Started process (PID=18577) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:44,287] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:53:44,288] {logging_mixin.py:112} INFO - [2020-11-05 17:53:44,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:44,431] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:44,460] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:53:44,488] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:02:00+00:00: scheduled__2020-11-02T13:02:00+00:00, externally triggered: False>
[2020-11-05 17:53:44,492] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:02:00+00:00: scheduled__2020-11-02T13:02:00+00:00, externally triggered: False>
[2020-11-05 17:53:44,502] {logging_mixin.py:112} INFO - [2020-11-05 17:53:44,502] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:02:00+00:00: scheduled__2020-11-02T13:02:00+00:00, externally triggered: False> successful
[2020-11-05 17:53:44,505] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:53:44,510] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-05 17:53:57,616] {scheduler_job.py:155} INFO - Started process (PID=18645) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:57,621] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:53:57,622] {logging_mixin.py:112} INFO - [2020-11-05 17:53:57,622] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:57,818] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:53:57,877] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:53:57,937] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:07:00+00:00: scheduled__2020-11-02T13:07:00+00:00, externally triggered: False>
[2020-11-05 17:53:57,950] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:07:00+00:00: scheduled__2020-11-02T13:07:00+00:00, externally triggered: False>
[2020-11-05 17:53:57,968] {logging_mixin.py:112} INFO - [2020-11-05 17:53:57,968] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:07:00+00:00: scheduled__2020-11-02T13:07:00+00:00, externally triggered: False> successful
[2020-11-05 17:53:57,976] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:53:57,981] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.365 seconds
[2020-11-05 17:54:11,025] {scheduler_job.py:155} INFO - Started process (PID=18707) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:11,034] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:54:11,035] {logging_mixin.py:112} INFO - [2020-11-05 17:54:11,034] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:11,242] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:11,272] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:54:11,332] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:12:00+00:00: scheduled__2020-11-02T13:12:00+00:00, externally triggered: False>
[2020-11-05 17:54:11,344] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:12:00+00:00: scheduled__2020-11-02T13:12:00+00:00, externally triggered: False>
[2020-11-05 17:54:11,359] {logging_mixin.py:112} INFO - [2020-11-05 17:54:11,359] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:12:00+00:00: scheduled__2020-11-02T13:12:00+00:00, externally triggered: False> successful
[2020-11-05 17:54:11,364] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:54:11,368] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.344 seconds
[2020-11-05 17:54:24,352] {scheduler_job.py:155} INFO - Started process (PID=18761) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:24,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:54:24,366] {logging_mixin.py:112} INFO - [2020-11-05 17:54:24,366] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:24,570] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:24,597] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:54:24,628] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:17:00+00:00: scheduled__2020-11-02T13:17:00+00:00, externally triggered: False>
[2020-11-05 17:54:24,632] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:17:00+00:00: scheduled__2020-11-02T13:17:00+00:00, externally triggered: False>
[2020-11-05 17:54:24,642] {logging_mixin.py:112} INFO - [2020-11-05 17:54:24,642] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:17:00+00:00: scheduled__2020-11-02T13:17:00+00:00, externally triggered: False> successful
[2020-11-05 17:54:24,647] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:54:24,655] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.303 seconds
[2020-11-05 17:54:37,753] {scheduler_job.py:155} INFO - Started process (PID=18833) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:37,759] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:54:37,760] {logging_mixin.py:112} INFO - [2020-11-05 17:54:37,760] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:37,954] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:37,982] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:54:38,027] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:22:00+00:00: scheduled__2020-11-02T13:22:00+00:00, externally triggered: False>
[2020-11-05 17:54:38,033] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:22:00+00:00: scheduled__2020-11-02T13:22:00+00:00, externally triggered: False>
[2020-11-05 17:54:38,046] {logging_mixin.py:112} INFO - [2020-11-05 17:54:38,046] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:22:00+00:00: scheduled__2020-11-02T13:22:00+00:00, externally triggered: False> successful
[2020-11-05 17:54:38,051] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:54:38,059] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.306 seconds
[2020-11-05 17:54:51,231] {scheduler_job.py:155} INFO - Started process (PID=18894) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:51,247] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:54:51,251] {logging_mixin.py:112} INFO - [2020-11-05 17:54:51,250] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:51,384] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:54:51,434] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:54:51,484] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:27:00+00:00: scheduled__2020-11-02T13:27:00+00:00, externally triggered: False>
[2020-11-05 17:54:51,491] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:27:00+00:00: scheduled__2020-11-02T13:27:00+00:00, externally triggered: False>
[2020-11-05 17:54:51,512] {logging_mixin.py:112} INFO - [2020-11-05 17:54:51,511] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:27:00+00:00: scheduled__2020-11-02T13:27:00+00:00, externally triggered: False> successful
[2020-11-05 17:54:51,518] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:54:51,524] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.293 seconds
[2020-11-05 17:55:04,611] {scheduler_job.py:155} INFO - Started process (PID=18962) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:04,614] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:55:04,615] {logging_mixin.py:112} INFO - [2020-11-05 17:55:04,615] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:04,727] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:04,749] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:55:04,780] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:32:00+00:00: scheduled__2020-11-02T13:32:00+00:00, externally triggered: False>
[2020-11-05 17:55:04,785] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:32:00+00:00: scheduled__2020-11-02T13:32:00+00:00, externally triggered: False>
[2020-11-05 17:55:04,794] {logging_mixin.py:112} INFO - [2020-11-05 17:55:04,794] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:32:00+00:00: scheduled__2020-11-02T13:32:00+00:00, externally triggered: False> successful
[2020-11-05 17:55:04,798] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:55:04,802] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 17:55:17,926] {scheduler_job.py:155} INFO - Started process (PID=19023) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:17,931] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:55:17,932] {logging_mixin.py:112} INFO - [2020-11-05 17:55:17,932] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:18,160] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:18,204] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:55:18,245] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:37:00+00:00: scheduled__2020-11-02T13:37:00+00:00, externally triggered: False>
[2020-11-05 17:55:18,252] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:37:00+00:00: scheduled__2020-11-02T13:37:00+00:00, externally triggered: False>
[2020-11-05 17:55:18,266] {logging_mixin.py:112} INFO - [2020-11-05 17:55:18,266] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:37:00+00:00: scheduled__2020-11-02T13:37:00+00:00, externally triggered: False> successful
[2020-11-05 17:55:18,272] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:55:18,276] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.350 seconds
[2020-11-05 17:55:31,274] {scheduler_job.py:155} INFO - Started process (PID=19089) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:31,288] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:55:31,292] {logging_mixin.py:112} INFO - [2020-11-05 17:55:31,292] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:31,470] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:31,497] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:55:31,538] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:42:00+00:00: scheduled__2020-11-02T13:42:00+00:00, externally triggered: False>
[2020-11-05 17:55:31,543] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:42:00+00:00: scheduled__2020-11-02T13:42:00+00:00, externally triggered: False>
[2020-11-05 17:55:31,564] {logging_mixin.py:112} INFO - [2020-11-05 17:55:31,564] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:42:00+00:00: scheduled__2020-11-02T13:42:00+00:00, externally triggered: False> successful
[2020-11-05 17:55:31,572] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:55:31,577] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.303 seconds
[2020-11-05 17:55:44,793] {scheduler_job.py:155} INFO - Started process (PID=19148) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:44,805] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:55:44,806] {logging_mixin.py:112} INFO - [2020-11-05 17:55:44,806] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:45,153] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:45,217] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:55:45,280] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:47:00+00:00: scheduled__2020-11-02T13:47:00+00:00, externally triggered: False>
[2020-11-05 17:55:45,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:47:00+00:00: scheduled__2020-11-02T13:47:00+00:00, externally triggered: False>
[2020-11-05 17:55:45,308] {logging_mixin.py:112} INFO - [2020-11-05 17:55:45,307] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:47:00+00:00: scheduled__2020-11-02T13:47:00+00:00, externally triggered: False> successful
[2020-11-05 17:55:45,313] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:55:45,328] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.535 seconds
[2020-11-05 17:55:58,168] {scheduler_job.py:155} INFO - Started process (PID=19205) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:58,178] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:55:58,184] {logging_mixin.py:112} INFO - [2020-11-05 17:55:58,184] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:58,374] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:55:58,417] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:55:58,471] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:52:00+00:00: scheduled__2020-11-02T13:52:00+00:00, externally triggered: False>
[2020-11-05 17:55:58,475] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:52:00+00:00: scheduled__2020-11-02T13:52:00+00:00, externally triggered: False>
[2020-11-05 17:55:58,499] {logging_mixin.py:112} INFO - [2020-11-05 17:55:58,499] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:52:00+00:00: scheduled__2020-11-02T13:52:00+00:00, externally triggered: False> successful
[2020-11-05 17:55:58,511] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:55:58,517] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.349 seconds
[2020-11-05 17:56:11,558] {scheduler_job.py:155} INFO - Started process (PID=19262) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:11,572] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:56:11,573] {logging_mixin.py:112} INFO - [2020-11-05 17:56:11,573] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:11,780] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:11,816] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:56:11,885] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 13:57:00+00:00: scheduled__2020-11-02T13:57:00+00:00, externally triggered: False>
[2020-11-05 17:56:11,890] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 13:57:00+00:00: scheduled__2020-11-02T13:57:00+00:00, externally triggered: False>
[2020-11-05 17:56:11,909] {logging_mixin.py:112} INFO - [2020-11-05 17:56:11,908] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 13:57:00+00:00: scheduled__2020-11-02T13:57:00+00:00, externally triggered: False> successful
[2020-11-05 17:56:11,915] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:56:11,924] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.366 seconds
[2020-11-05 17:56:24,913] {scheduler_job.py:155} INFO - Started process (PID=19327) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:24,919] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:56:24,920] {logging_mixin.py:112} INFO - [2020-11-05 17:56:24,920] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:25,032] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:25,058] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:56:25,086] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:02:00+00:00: scheduled__2020-11-02T14:02:00+00:00, externally triggered: False>
[2020-11-05 17:56:25,090] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:02:00+00:00: scheduled__2020-11-02T14:02:00+00:00, externally triggered: False>
[2020-11-05 17:56:25,099] {logging_mixin.py:112} INFO - [2020-11-05 17:56:25,099] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:02:00+00:00: scheduled__2020-11-02T14:02:00+00:00, externally triggered: False> successful
[2020-11-05 17:56:25,102] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:56:25,106] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.193 seconds
[2020-11-05 17:56:38,261] {scheduler_job.py:155} INFO - Started process (PID=19395) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:38,264] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:56:38,264] {logging_mixin.py:112} INFO - [2020-11-05 17:56:38,264] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:38,378] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:38,400] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:56:38,444] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:07:00+00:00: scheduled__2020-11-02T14:07:00+00:00, externally triggered: False>
[2020-11-05 17:56:38,449] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:07:00+00:00: scheduled__2020-11-02T14:07:00+00:00, externally triggered: False>
[2020-11-05 17:56:38,461] {logging_mixin.py:112} INFO - [2020-11-05 17:56:38,461] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:07:00+00:00: scheduled__2020-11-02T14:07:00+00:00, externally triggered: False> successful
[2020-11-05 17:56:38,467] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:56:38,473] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.212 seconds
[2020-11-05 17:56:51,564] {scheduler_job.py:155} INFO - Started process (PID=19460) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:51,569] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:56:51,571] {logging_mixin.py:112} INFO - [2020-11-05 17:56:51,570] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:51,744] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:56:51,777] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:56:51,809] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:12:00+00:00: scheduled__2020-11-02T14:12:00+00:00, externally triggered: False>
[2020-11-05 17:56:51,813] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:12:00+00:00: scheduled__2020-11-02T14:12:00+00:00, externally triggered: False>
[2020-11-05 17:56:51,823] {logging_mixin.py:112} INFO - [2020-11-05 17:56:51,823] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:12:00+00:00: scheduled__2020-11-02T14:12:00+00:00, externally triggered: False> successful
[2020-11-05 17:56:51,827] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:56:51,832] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-05 17:57:04,863] {scheduler_job.py:155} INFO - Started process (PID=19528) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:04,869] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:57:04,872] {logging_mixin.py:112} INFO - [2020-11-05 17:57:04,871] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:05,067] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:05,096] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:57:05,128] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:17:00+00:00: scheduled__2020-11-02T14:17:00+00:00, externally triggered: False>
[2020-11-05 17:57:05,133] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:17:00+00:00: scheduled__2020-11-02T14:17:00+00:00, externally triggered: False>
[2020-11-05 17:57:05,147] {logging_mixin.py:112} INFO - [2020-11-05 17:57:05,146] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:17:00+00:00: scheduled__2020-11-02T14:17:00+00:00, externally triggered: False> successful
[2020-11-05 17:57:05,150] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:57:05,155] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.292 seconds
[2020-11-05 17:57:18,133] {scheduler_job.py:155} INFO - Started process (PID=19595) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:18,139] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:57:18,140] {logging_mixin.py:112} INFO - [2020-11-05 17:57:18,140] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:18,240] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:18,275] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:57:18,310] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:22:00+00:00: scheduled__2020-11-02T14:22:00+00:00, externally triggered: False>
[2020-11-05 17:57:18,314] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:22:00+00:00: scheduled__2020-11-02T14:22:00+00:00, externally triggered: False>
[2020-11-05 17:57:18,329] {logging_mixin.py:112} INFO - [2020-11-05 17:57:18,329] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:22:00+00:00: scheduled__2020-11-02T14:22:00+00:00, externally triggered: False> successful
[2020-11-05 17:57:18,332] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:57:18,336] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.203 seconds
[2020-11-05 17:57:31,423] {scheduler_job.py:155} INFO - Started process (PID=19661) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:31,427] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:57:31,428] {logging_mixin.py:112} INFO - [2020-11-05 17:57:31,428] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:31,534] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:31,563] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:57:31,599] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:27:00+00:00: scheduled__2020-11-02T14:27:00+00:00, externally triggered: False>
[2020-11-05 17:57:31,606] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:27:00+00:00: scheduled__2020-11-02T14:27:00+00:00, externally triggered: False>
[2020-11-05 17:57:31,618] {logging_mixin.py:112} INFO - [2020-11-05 17:57:31,618] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:27:00+00:00: scheduled__2020-11-02T14:27:00+00:00, externally triggered: False> successful
[2020-11-05 17:57:31,621] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:57:31,626] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.204 seconds
[2020-11-05 17:57:44,747] {scheduler_job.py:155} INFO - Started process (PID=19727) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:44,752] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:57:44,753] {logging_mixin.py:112} INFO - [2020-11-05 17:57:44,753] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:44,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:44,971] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:57:45,008] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:32:00+00:00: scheduled__2020-11-02T14:32:00+00:00, externally triggered: False>
[2020-11-05 17:57:45,012] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:32:00+00:00: scheduled__2020-11-02T14:32:00+00:00, externally triggered: False>
[2020-11-05 17:57:45,025] {logging_mixin.py:112} INFO - [2020-11-05 17:57:45,024] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:32:00+00:00: scheduled__2020-11-02T14:32:00+00:00, externally triggered: False> successful
[2020-11-05 17:57:45,029] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:57:45,038] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.291 seconds
[2020-11-05 17:57:58,049] {scheduler_job.py:155} INFO - Started process (PID=19793) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:58,056] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:57:58,057] {logging_mixin.py:112} INFO - [2020-11-05 17:57:58,056] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:58,230] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:57:58,264] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:57:58,310] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:37:00+00:00: scheduled__2020-11-02T14:37:00+00:00, externally triggered: False>
[2020-11-05 17:57:58,315] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:37:00+00:00: scheduled__2020-11-02T14:37:00+00:00, externally triggered: False>
[2020-11-05 17:57:58,328] {logging_mixin.py:112} INFO - [2020-11-05 17:57:58,328] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:37:00+00:00: scheduled__2020-11-02T14:37:00+00:00, externally triggered: False> successful
[2020-11-05 17:57:58,331] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:57:58,338] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.289 seconds
[2020-11-05 17:58:11,349] {scheduler_job.py:155} INFO - Started process (PID=19860) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:11,353] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:58:11,354] {logging_mixin.py:112} INFO - [2020-11-05 17:58:11,354] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:11,470] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:11,490] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:58:11,519] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:42:00+00:00: scheduled__2020-11-02T14:42:00+00:00, externally triggered: False>
[2020-11-05 17:58:11,523] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:42:00+00:00: scheduled__2020-11-02T14:42:00+00:00, externally triggered: False>
[2020-11-05 17:58:11,531] {logging_mixin.py:112} INFO - [2020-11-05 17:58:11,531] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:42:00+00:00: scheduled__2020-11-02T14:42:00+00:00, externally triggered: False> successful
[2020-11-05 17:58:11,535] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:58:11,538] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.189 seconds
[2020-11-05 17:58:24,654] {scheduler_job.py:155} INFO - Started process (PID=19925) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:24,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:58:24,659] {logging_mixin.py:112} INFO - [2020-11-05 17:58:24,659] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:24,778] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:24,814] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:58:24,855] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:47:00+00:00: scheduled__2020-11-02T14:47:00+00:00, externally triggered: False>
[2020-11-05 17:58:24,864] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:47:00+00:00: scheduled__2020-11-02T14:47:00+00:00, externally triggered: False>
[2020-11-05 17:58:24,884] {logging_mixin.py:112} INFO - [2020-11-05 17:58:24,884] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:47:00+00:00: scheduled__2020-11-02T14:47:00+00:00, externally triggered: False> successful
[2020-11-05 17:58:24,891] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:58:24,897] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.244 seconds
[2020-11-05 17:58:38,041] {scheduler_job.py:155} INFO - Started process (PID=19992) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:38,051] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:58:38,060] {logging_mixin.py:112} INFO - [2020-11-05 17:58:38,060] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:38,376] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:38,429] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:58:38,494] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:52:00+00:00: scheduled__2020-11-02T14:52:00+00:00, externally triggered: False>
[2020-11-05 17:58:38,503] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:52:00+00:00: scheduled__2020-11-02T14:52:00+00:00, externally triggered: False>
[2020-11-05 17:58:38,537] {logging_mixin.py:112} INFO - [2020-11-05 17:58:38,537] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:52:00+00:00: scheduled__2020-11-02T14:52:00+00:00, externally triggered: False> successful
[2020-11-05 17:58:38,546] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:58:38,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.517 seconds
[2020-11-05 17:58:51,351] {scheduler_job.py:155} INFO - Started process (PID=20054) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:51,357] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:58:51,358] {logging_mixin.py:112} INFO - [2020-11-05 17:58:51,358] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:51,558] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:58:51,586] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:58:51,621] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 14:57:00+00:00: scheduled__2020-11-02T14:57:00+00:00, externally triggered: False>
[2020-11-05 17:58:51,628] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 14:57:00+00:00: scheduled__2020-11-02T14:57:00+00:00, externally triggered: False>
[2020-11-05 17:58:51,643] {logging_mixin.py:112} INFO - [2020-11-05 17:58:51,643] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 14:57:00+00:00: scheduled__2020-11-02T14:57:00+00:00, externally triggered: False> successful
[2020-11-05 17:58:51,648] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:58:51,656] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.305 seconds
[2020-11-05 17:59:04,680] {scheduler_job.py:155} INFO - Started process (PID=20121) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:04,706] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:59:04,707] {logging_mixin.py:112} INFO - [2020-11-05 17:59:04,707] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:04,909] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:04,936] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:59:04,994] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:02:00+00:00: scheduled__2020-11-02T15:02:00+00:00, externally triggered: False>
[2020-11-05 17:59:05,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:02:00+00:00: scheduled__2020-11-02T15:02:00+00:00, externally triggered: False>
[2020-11-05 17:59:05,028] {logging_mixin.py:112} INFO - [2020-11-05 17:59:05,027] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:02:00+00:00: scheduled__2020-11-02T15:02:00+00:00, externally triggered: False> successful
[2020-11-05 17:59:05,043] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:59:05,050] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.370 seconds
[2020-11-05 17:59:18,048] {scheduler_job.py:155} INFO - Started process (PID=20184) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:18,064] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:59:18,065] {logging_mixin.py:112} INFO - [2020-11-05 17:59:18,065] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:18,237] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:18,271] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:59:18,308] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:07:00+00:00: scheduled__2020-11-02T15:07:00+00:00, externally triggered: False>
[2020-11-05 17:59:18,312] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:07:00+00:00: scheduled__2020-11-02T15:07:00+00:00, externally triggered: False>
[2020-11-05 17:59:18,324] {logging_mixin.py:112} INFO - [2020-11-05 17:59:18,324] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:07:00+00:00: scheduled__2020-11-02T15:07:00+00:00, externally triggered: False> successful
[2020-11-05 17:59:18,327] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:59:18,332] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.285 seconds
[2020-11-05 17:59:31,327] {scheduler_job.py:155} INFO - Started process (PID=20252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:31,332] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:59:31,332] {logging_mixin.py:112} INFO - [2020-11-05 17:59:31,332] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:31,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:31,465] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:59:31,504] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:12:00+00:00: scheduled__2020-11-02T15:12:00+00:00, externally triggered: False>
[2020-11-05 17:59:31,510] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:12:00+00:00: scheduled__2020-11-02T15:12:00+00:00, externally triggered: False>
[2020-11-05 17:59:31,520] {logging_mixin.py:112} INFO - [2020-11-05 17:59:31,519] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:12:00+00:00: scheduled__2020-11-02T15:12:00+00:00, externally triggered: False> successful
[2020-11-05 17:59:31,522] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:59:31,527] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.199 seconds
[2020-11-05 17:59:44,741] {scheduler_job.py:155} INFO - Started process (PID=20318) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:44,748] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:59:44,749] {logging_mixin.py:112} INFO - [2020-11-05 17:59:44,748] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:44,864] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:44,886] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:59:44,916] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:17:00+00:00: scheduled__2020-11-02T15:17:00+00:00, externally triggered: False>
[2020-11-05 17:59:44,919] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:17:00+00:00: scheduled__2020-11-02T15:17:00+00:00, externally triggered: False>
[2020-11-05 17:59:44,928] {logging_mixin.py:112} INFO - [2020-11-05 17:59:44,928] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:17:00+00:00: scheduled__2020-11-02T15:17:00+00:00, externally triggered: False> successful
[2020-11-05 17:59:44,932] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:59:44,935] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.194 seconds
[2020-11-05 17:59:58,059] {scheduler_job.py:155} INFO - Started process (PID=20385) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:58,063] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 17:59:58,064] {logging_mixin.py:112} INFO - [2020-11-05 17:59:58,064] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:58,194] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 17:59:58,225] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 17:59:58,253] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:22:00+00:00: scheduled__2020-11-02T15:22:00+00:00, externally triggered: False>
[2020-11-05 17:59:58,258] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:22:00+00:00: scheduled__2020-11-02T15:22:00+00:00, externally triggered: False>
[2020-11-05 17:59:58,270] {logging_mixin.py:112} INFO - [2020-11-05 17:59:58,270] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:22:00+00:00: scheduled__2020-11-02T15:22:00+00:00, externally triggered: False> successful
[2020-11-05 17:59:58,275] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 17:59:58,282] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.223 seconds
[2020-11-05 18:00:11,446] {scheduler_job.py:155} INFO - Started process (PID=20457) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:11,460] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:00:11,461] {logging_mixin.py:112} INFO - [2020-11-05 18:00:11,460] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:11,813] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:11,860] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:00:11,926] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:27:00+00:00: scheduled__2020-11-02T15:27:00+00:00, externally triggered: False>
[2020-11-05 18:00:11,936] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:27:00+00:00: scheduled__2020-11-02T15:27:00+00:00, externally triggered: False>
[2020-11-05 18:00:11,950] {logging_mixin.py:112} INFO - [2020-11-05 18:00:11,950] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:27:00+00:00: scheduled__2020-11-02T15:27:00+00:00, externally triggered: False> successful
[2020-11-05 18:00:11,958] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:00:11,963] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.517 seconds
[2020-11-05 18:00:24,819] {scheduler_job.py:155} INFO - Started process (PID=20517) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:24,849] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:00:24,850] {logging_mixin.py:112} INFO - [2020-11-05 18:00:24,850] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:24,964] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:24,997] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:00:25,066] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:32:00+00:00: scheduled__2020-11-02T15:32:00+00:00, externally triggered: False>
[2020-11-05 18:00:25,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:32:00+00:00: scheduled__2020-11-02T15:32:00+00:00, externally triggered: False>
[2020-11-05 18:00:25,098] {logging_mixin.py:112} INFO - [2020-11-05 18:00:25,098] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:32:00+00:00: scheduled__2020-11-02T15:32:00+00:00, externally triggered: False> successful
[2020-11-05 18:00:25,111] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:00:25,118] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.299 seconds
[2020-11-05 18:00:38,174] {scheduler_job.py:155} INFO - Started process (PID=20584) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:38,182] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:00:38,184] {logging_mixin.py:112} INFO - [2020-11-05 18:00:38,183] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:38,391] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:38,465] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:00:38,547] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:37:00+00:00: scheduled__2020-11-02T15:37:00+00:00, externally triggered: False>
[2020-11-05 18:00:38,555] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:37:00+00:00: scheduled__2020-11-02T15:37:00+00:00, externally triggered: False>
[2020-11-05 18:00:38,575] {logging_mixin.py:112} INFO - [2020-11-05 18:00:38,574] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:37:00+00:00: scheduled__2020-11-02T15:37:00+00:00, externally triggered: False> successful
[2020-11-05 18:00:38,584] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:00:38,591] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.417 seconds
[2020-11-05 18:00:51,572] {scheduler_job.py:155} INFO - Started process (PID=20645) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:51,586] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:00:51,588] {logging_mixin.py:112} INFO - [2020-11-05 18:00:51,588] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:51,835] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:00:51,878] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:00:51,917] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:42:00+00:00: scheduled__2020-11-02T15:42:00+00:00, externally triggered: False>
[2020-11-05 18:00:51,922] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:42:00+00:00: scheduled__2020-11-02T15:42:00+00:00, externally triggered: False>
[2020-11-05 18:00:51,934] {logging_mixin.py:112} INFO - [2020-11-05 18:00:51,934] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:42:00+00:00: scheduled__2020-11-02T15:42:00+00:00, externally triggered: False> successful
[2020-11-05 18:00:51,939] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:00:51,944] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.372 seconds
[2020-11-05 18:01:04,952] {scheduler_job.py:155} INFO - Started process (PID=20702) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:04,956] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:01:04,958] {logging_mixin.py:112} INFO - [2020-11-05 18:01:04,957] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:05,159] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:05,196] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:01:05,229] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:47:00+00:00: scheduled__2020-11-02T15:47:00+00:00, externally triggered: False>
[2020-11-05 18:01:05,233] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:47:00+00:00: scheduled__2020-11-02T15:47:00+00:00, externally triggered: False>
[2020-11-05 18:01:05,246] {logging_mixin.py:112} INFO - [2020-11-05 18:01:05,246] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:47:00+00:00: scheduled__2020-11-02T15:47:00+00:00, externally triggered: False> successful
[2020-11-05 18:01:05,249] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:01:05,253] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.301 seconds
[2020-11-05 18:01:18,362] {scheduler_job.py:155} INFO - Started process (PID=20760) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:18,374] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:01:18,375] {logging_mixin.py:112} INFO - [2020-11-05 18:01:18,374] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:18,473] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:18,494] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:01:18,524] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:52:00+00:00: scheduled__2020-11-02T15:52:00+00:00, externally triggered: False>
[2020-11-05 18:01:18,527] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:52:00+00:00: scheduled__2020-11-02T15:52:00+00:00, externally triggered: False>
[2020-11-05 18:01:18,539] {logging_mixin.py:112} INFO - [2020-11-05 18:01:18,539] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:52:00+00:00: scheduled__2020-11-02T15:52:00+00:00, externally triggered: False> successful
[2020-11-05 18:01:18,544] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:01:18,548] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.186 seconds
[2020-11-05 18:01:31,744] {scheduler_job.py:155} INFO - Started process (PID=20837) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:31,754] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:01:31,757] {logging_mixin.py:112} INFO - [2020-11-05 18:01:31,757] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:31,936] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:31,963] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:01:31,998] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 15:57:00+00:00: scheduled__2020-11-02T15:57:00+00:00, externally triggered: False>
[2020-11-05 18:01:32,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 15:57:00+00:00: scheduled__2020-11-02T15:57:00+00:00, externally triggered: False>
[2020-11-05 18:01:32,014] {logging_mixin.py:112} INFO - [2020-11-05 18:01:32,014] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 15:57:00+00:00: scheduled__2020-11-02T15:57:00+00:00, externally triggered: False> successful
[2020-11-05 18:01:32,018] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:01:32,022] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.278 seconds
[2020-11-05 18:01:44,242] {scheduler_job.py:155} INFO - Started process (PID=20894) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:44,247] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:01:44,249] {logging_mixin.py:112} INFO - [2020-11-05 18:01:44,249] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:44,515] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:44,548] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:01:44,593] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:02:00+00:00: scheduled__2020-11-02T16:02:00+00:00, externally triggered: False>
[2020-11-05 18:01:44,603] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:02:00+00:00: scheduled__2020-11-02T16:02:00+00:00, externally triggered: False>
[2020-11-05 18:01:44,631] {logging_mixin.py:112} INFO - [2020-11-05 18:01:44,630] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:02:00+00:00: scheduled__2020-11-02T16:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:01:44,637] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:01:44,645] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.402 seconds
[2020-11-05 18:01:57,597] {scheduler_job.py:155} INFO - Started process (PID=20952) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:57,603] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:01:57,603] {logging_mixin.py:112} INFO - [2020-11-05 18:01:57,603] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:57,858] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:01:57,933] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:01:57,976] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:07:00+00:00: scheduled__2020-11-02T16:07:00+00:00, externally triggered: False>
[2020-11-05 18:01:57,983] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:07:00+00:00: scheduled__2020-11-02T16:07:00+00:00, externally triggered: False>
[2020-11-05 18:01:58,009] {logging_mixin.py:112} INFO - [2020-11-05 18:01:58,009] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:07:00+00:00: scheduled__2020-11-02T16:07:00+00:00, externally triggered: False> successful
[2020-11-05 18:01:58,024] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:01:58,030] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.434 seconds
[2020-11-05 18:02:10,941] {scheduler_job.py:155} INFO - Started process (PID=21016) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:10,948] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:02:10,948] {logging_mixin.py:112} INFO - [2020-11-05 18:02:10,948] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:11,087] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:11,122] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:02:11,162] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:12:00+00:00: scheduled__2020-11-02T16:12:00+00:00, externally triggered: False>
[2020-11-05 18:02:11,168] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:12:00+00:00: scheduled__2020-11-02T16:12:00+00:00, externally triggered: False>
[2020-11-05 18:02:11,187] {logging_mixin.py:112} INFO - [2020-11-05 18:02:11,187] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:12:00+00:00: scheduled__2020-11-02T16:12:00+00:00, externally triggered: False> successful
[2020-11-05 18:02:11,192] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:02:11,197] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.256 seconds
[2020-11-05 18:02:24,262] {scheduler_job.py:155} INFO - Started process (PID=21072) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:24,268] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:02:24,268] {logging_mixin.py:112} INFO - [2020-11-05 18:02:24,268] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:24,506] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:24,574] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:02:24,642] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:17:00+00:00: scheduled__2020-11-02T16:17:00+00:00, externally triggered: False>
[2020-11-05 18:02:24,653] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:17:00+00:00: scheduled__2020-11-02T16:17:00+00:00, externally triggered: False>
[2020-11-05 18:02:24,679] {logging_mixin.py:112} INFO - [2020-11-05 18:02:24,678] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:17:00+00:00: scheduled__2020-11-02T16:17:00+00:00, externally triggered: False> successful
[2020-11-05 18:02:24,688] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:02:24,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.439 seconds
[2020-11-05 18:02:37,593] {scheduler_job.py:155} INFO - Started process (PID=21127) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:37,602] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:02:37,603] {logging_mixin.py:112} INFO - [2020-11-05 18:02:37,602] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:37,825] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:37,857] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:02:37,922] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:22:00+00:00: scheduled__2020-11-02T16:22:00+00:00, externally triggered: False>
[2020-11-05 18:02:37,940] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:22:00+00:00: scheduled__2020-11-02T16:22:00+00:00, externally triggered: False>
[2020-11-05 18:02:37,963] {logging_mixin.py:112} INFO - [2020-11-05 18:02:37,962] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:22:00+00:00: scheduled__2020-11-02T16:22:00+00:00, externally triggered: False> successful
[2020-11-05 18:02:37,975] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:02:37,981] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.388 seconds
[2020-11-05 18:02:51,058] {scheduler_job.py:155} INFO - Started process (PID=21186) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:51,062] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:02:51,062] {logging_mixin.py:112} INFO - [2020-11-05 18:02:51,062] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:51,224] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:02:51,249] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:02:51,278] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:27:00+00:00: scheduled__2020-11-02T16:27:00+00:00, externally triggered: False>
[2020-11-05 18:02:51,282] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:27:00+00:00: scheduled__2020-11-02T16:27:00+00:00, externally triggered: False>
[2020-11-05 18:02:51,292] {logging_mixin.py:112} INFO - [2020-11-05 18:02:51,292] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:27:00+00:00: scheduled__2020-11-02T16:27:00+00:00, externally triggered: False> successful
[2020-11-05 18:02:51,295] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:02:51,299] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.242 seconds
[2020-11-05 18:03:04,366] {scheduler_job.py:155} INFO - Started process (PID=21248) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:04,371] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:03:04,372] {logging_mixin.py:112} INFO - [2020-11-05 18:03:04,372] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:04,576] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:04,651] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:03:04,745] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:32:00+00:00: scheduled__2020-11-02T16:32:00+00:00, externally triggered: False>
[2020-11-05 18:03:04,754] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:32:00+00:00: scheduled__2020-11-02T16:32:00+00:00, externally triggered: False>
[2020-11-05 18:03:04,775] {logging_mixin.py:112} INFO - [2020-11-05 18:03:04,774] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:32:00+00:00: scheduled__2020-11-02T16:32:00+00:00, externally triggered: False> successful
[2020-11-05 18:03:04,780] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:03:04,792] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.425 seconds
[2020-11-05 18:03:18,371] {scheduler_job.py:155} INFO - Started process (PID=21483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:18,375] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:03:18,375] {logging_mixin.py:112} INFO - [2020-11-05 18:03:18,375] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:18,487] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:18,511] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:03:18,545] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:37:00+00:00: scheduled__2020-11-02T16:37:00+00:00, externally triggered: False>
[2020-11-05 18:03:18,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:37:00+00:00: scheduled__2020-11-02T16:37:00+00:00, externally triggered: False>
[2020-11-05 18:03:18,566] {logging_mixin.py:112} INFO - [2020-11-05 18:03:18,565] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:37:00+00:00: scheduled__2020-11-02T16:37:00+00:00, externally triggered: False> successful
[2020-11-05 18:03:18,569] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:03:18,573] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 18:03:29,525] {scheduler_job.py:155} INFO - Started process (PID=21647) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:29,541] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:03:29,541] {logging_mixin.py:112} INFO - [2020-11-05 18:03:29,541] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:29,692] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:29,714] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:03:29,740] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:42:00+00:00: scheduled__2020-11-02T16:42:00+00:00, externally triggered: False>
[2020-11-05 18:03:29,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:42:00+00:00: scheduled__2020-11-02T16:42:00+00:00, externally triggered: False>
[2020-11-05 18:03:29,754] {logging_mixin.py:112} INFO - [2020-11-05 18:03:29,754] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:42:00+00:00: scheduled__2020-11-02T16:42:00+00:00, externally triggered: False> successful
[2020-11-05 18:03:29,757] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:03:29,761] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-05 18:03:41,845] {scheduler_job.py:155} INFO - Started process (PID=21820) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:41,854] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:03:41,855] {logging_mixin.py:112} INFO - [2020-11-05 18:03:41,855] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:42,183] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:42,228] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:03:42,263] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:47:00+00:00: scheduled__2020-11-02T16:47:00+00:00, externally triggered: False>
[2020-11-05 18:03:42,267] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:47:00+00:00: scheduled__2020-11-02T16:47:00+00:00, externally triggered: False>
[2020-11-05 18:03:42,279] {logging_mixin.py:112} INFO - [2020-11-05 18:03:42,279] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:47:00+00:00: scheduled__2020-11-02T16:47:00+00:00, externally triggered: False> successful
[2020-11-05 18:03:42,288] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:03:42,293] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.448 seconds
[2020-11-05 18:03:55,378] {scheduler_job.py:155} INFO - Started process (PID=21985) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:55,387] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:03:55,393] {logging_mixin.py:112} INFO - [2020-11-05 18:03:55,393] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:55,562] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:03:55,589] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:03:55,618] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:52:00+00:00: scheduled__2020-11-02T16:52:00+00:00, externally triggered: False>
[2020-11-05 18:03:55,622] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:52:00+00:00: scheduled__2020-11-02T16:52:00+00:00, externally triggered: False>
[2020-11-05 18:03:55,633] {logging_mixin.py:112} INFO - [2020-11-05 18:03:55,633] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:52:00+00:00: scheduled__2020-11-02T16:52:00+00:00, externally triggered: False> successful
[2020-11-05 18:03:55,636] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:03:55,641] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.263 seconds
[2020-11-05 18:04:08,731] {scheduler_job.py:155} INFO - Started process (PID=22154) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:08,736] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:04:08,740] {logging_mixin.py:112} INFO - [2020-11-05 18:04:08,740] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:08,923] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:08,948] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:04:08,985] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 16:57:00+00:00: scheduled__2020-11-02T16:57:00+00:00, externally triggered: False>
[2020-11-05 18:04:08,991] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 16:57:00+00:00: scheduled__2020-11-02T16:57:00+00:00, externally triggered: False>
[2020-11-05 18:04:09,005] {logging_mixin.py:112} INFO - [2020-11-05 18:04:09,004] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 16:57:00+00:00: scheduled__2020-11-02T16:57:00+00:00, externally triggered: False> successful
[2020-11-05 18:04:09,010] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:04:09,015] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.284 seconds
[2020-11-05 18:04:22,092] {scheduler_job.py:155} INFO - Started process (PID=22313) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:22,099] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:04:22,099] {logging_mixin.py:112} INFO - [2020-11-05 18:04:22,099] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:22,390] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:22,446] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:04:22,511] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:02:00+00:00: scheduled__2020-11-02T17:02:00+00:00, externally triggered: False>
[2020-11-05 18:04:22,519] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:02:00+00:00: scheduled__2020-11-02T17:02:00+00:00, externally triggered: False>
[2020-11-05 18:04:22,544] {logging_mixin.py:112} INFO - [2020-11-05 18:04:22,544] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:02:00+00:00: scheduled__2020-11-02T17:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:04:22,552] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:04:22,561] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.469 seconds
[2020-11-05 18:04:35,490] {scheduler_job.py:155} INFO - Started process (PID=22483) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:35,494] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:04:35,502] {logging_mixin.py:112} INFO - [2020-11-05 18:04:35,502] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:35,690] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:35,716] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:04:35,759] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:07:00+00:00: scheduled__2020-11-02T17:07:00+00:00, externally triggered: False>
[2020-11-05 18:04:35,766] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:07:00+00:00: scheduled__2020-11-02T17:07:00+00:00, externally triggered: False>
[2020-11-05 18:04:35,777] {logging_mixin.py:112} INFO - [2020-11-05 18:04:35,777] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:07:00+00:00: scheduled__2020-11-02T17:07:00+00:00, externally triggered: False> successful
[2020-11-05 18:04:35,781] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:04:35,788] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.298 seconds
[2020-11-05 18:04:48,887] {scheduler_job.py:155} INFO - Started process (PID=22643) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:48,891] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:04:48,892] {logging_mixin.py:112} INFO - [2020-11-05 18:04:48,892] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:49,088] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:04:49,121] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:04:49,153] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:12:00+00:00: scheduled__2020-11-02T17:12:00+00:00, externally triggered: False>
[2020-11-05 18:04:49,157] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:12:00+00:00: scheduled__2020-11-02T17:12:00+00:00, externally triggered: False>
[2020-11-05 18:04:49,169] {logging_mixin.py:112} INFO - [2020-11-05 18:04:49,169] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:12:00+00:00: scheduled__2020-11-02T17:12:00+00:00, externally triggered: False> successful
[2020-11-05 18:04:49,173] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:04:49,177] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.290 seconds
[2020-11-05 18:05:02,273] {scheduler_job.py:155} INFO - Started process (PID=22817) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:02,278] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:05:02,284] {logging_mixin.py:112} INFO - [2020-11-05 18:05:02,284] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:02,625] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:02,691] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:05:02,757] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:17:00+00:00: scheduled__2020-11-02T17:17:00+00:00, externally triggered: False>
[2020-11-05 18:05:02,770] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:17:00+00:00: scheduled__2020-11-02T17:17:00+00:00, externally triggered: False>
[2020-11-05 18:05:02,797] {logging_mixin.py:112} INFO - [2020-11-05 18:05:02,796] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:17:00+00:00: scheduled__2020-11-02T17:17:00+00:00, externally triggered: False> successful
[2020-11-05 18:05:02,804] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:05:02,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.549 seconds
[2020-11-05 18:05:15,506] {scheduler_job.py:155} INFO - Started process (PID=22980) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:15,509] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:05:15,510] {logging_mixin.py:112} INFO - [2020-11-05 18:05:15,509] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:15,676] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:15,748] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:05:15,782] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:22:00+00:00: scheduled__2020-11-02T17:22:00+00:00, externally triggered: False>
[2020-11-05 18:05:15,787] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:22:00+00:00: scheduled__2020-11-02T17:22:00+00:00, externally triggered: False>
[2020-11-05 18:05:15,799] {logging_mixin.py:112} INFO - [2020-11-05 18:05:15,799] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:22:00+00:00: scheduled__2020-11-02T17:22:00+00:00, externally triggered: False> successful
[2020-11-05 18:05:15,804] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:05:15,809] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.304 seconds
[2020-11-05 18:05:28,822] {scheduler_job.py:155} INFO - Started process (PID=23142) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:28,825] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:05:28,825] {logging_mixin.py:112} INFO - [2020-11-05 18:05:28,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:28,946] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:28,977] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:05:29,016] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:27:00+00:00: scheduled__2020-11-02T17:27:00+00:00, externally triggered: False>
[2020-11-05 18:05:29,022] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:27:00+00:00: scheduled__2020-11-02T17:27:00+00:00, externally triggered: False>
[2020-11-05 18:05:29,036] {logging_mixin.py:112} INFO - [2020-11-05 18:05:29,035] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:27:00+00:00: scheduled__2020-11-02T17:27:00+00:00, externally triggered: False> successful
[2020-11-05 18:05:29,040] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:05:29,044] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.222 seconds
[2020-11-05 18:05:42,197] {scheduler_job.py:155} INFO - Started process (PID=23311) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:42,200] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:05:42,205] {logging_mixin.py:112} INFO - [2020-11-05 18:05:42,205] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:42,459] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:42,496] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:05:42,545] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:32:00+00:00: scheduled__2020-11-02T17:32:00+00:00, externally triggered: False>
[2020-11-05 18:05:42,549] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:32:00+00:00: scheduled__2020-11-02T17:32:00+00:00, externally triggered: False>
[2020-11-05 18:05:42,562] {logging_mixin.py:112} INFO - [2020-11-05 18:05:42,562] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:32:00+00:00: scheduled__2020-11-02T17:32:00+00:00, externally triggered: False> successful
[2020-11-05 18:05:42,569] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:05:42,580] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.383 seconds
[2020-11-05 18:05:55,472] {scheduler_job.py:155} INFO - Started process (PID=23474) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:55,475] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:05:55,476] {logging_mixin.py:112} INFO - [2020-11-05 18:05:55,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:55,600] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:05:55,633] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:05:55,662] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:37:00+00:00: scheduled__2020-11-02T17:37:00+00:00, externally triggered: False>
[2020-11-05 18:05:55,666] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:37:00+00:00: scheduled__2020-11-02T17:37:00+00:00, externally triggered: False>
[2020-11-05 18:05:55,676] {logging_mixin.py:112} INFO - [2020-11-05 18:05:55,676] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:37:00+00:00: scheduled__2020-11-02T17:37:00+00:00, externally triggered: False> successful
[2020-11-05 18:05:55,679] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:05:55,682] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.210 seconds
[2020-11-05 18:06:08,899] {scheduler_job.py:155} INFO - Started process (PID=23637) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:08,904] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:06:08,906] {logging_mixin.py:112} INFO - [2020-11-05 18:06:08,906] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:09,171] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:09,203] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:06:09,243] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:42:00+00:00: scheduled__2020-11-02T17:42:00+00:00, externally triggered: False>
[2020-11-05 18:06:09,249] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:42:00+00:00: scheduled__2020-11-02T17:42:00+00:00, externally triggered: False>
[2020-11-05 18:06:09,262] {logging_mixin.py:112} INFO - [2020-11-05 18:06:09,262] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:42:00+00:00: scheduled__2020-11-02T17:42:00+00:00, externally triggered: False> successful
[2020-11-05 18:06:09,268] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:06:09,290] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.392 seconds
[2020-11-05 18:06:22,283] {scheduler_job.py:155} INFO - Started process (PID=23799) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:22,287] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:06:22,288] {logging_mixin.py:112} INFO - [2020-11-05 18:06:22,288] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:22,433] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:22,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:06:22,481] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:47:00+00:00: scheduled__2020-11-02T17:47:00+00:00, externally triggered: False>
[2020-11-05 18:06:22,485] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:47:00+00:00: scheduled__2020-11-02T17:47:00+00:00, externally triggered: False>
[2020-11-05 18:06:22,494] {logging_mixin.py:112} INFO - [2020-11-05 18:06:22,494] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:47:00+00:00: scheduled__2020-11-02T17:47:00+00:00, externally triggered: False> successful
[2020-11-05 18:06:22,498] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:06:22,502] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.219 seconds
[2020-11-05 18:06:35,591] {scheduler_job.py:155} INFO - Started process (PID=23961) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:35,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:06:35,595] {logging_mixin.py:112} INFO - [2020-11-05 18:06:35,595] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:35,741] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:35,761] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:06:35,788] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:52:00+00:00: scheduled__2020-11-02T17:52:00+00:00, externally triggered: False>
[2020-11-05 18:06:35,792] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:52:00+00:00: scheduled__2020-11-02T17:52:00+00:00, externally triggered: False>
[2020-11-05 18:06:35,808] {logging_mixin.py:112} INFO - [2020-11-05 18:06:35,808] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:52:00+00:00: scheduled__2020-11-02T17:52:00+00:00, externally triggered: False> successful
[2020-11-05 18:06:35,812] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:06:35,821] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-05 18:06:48,851] {scheduler_job.py:155} INFO - Started process (PID=24131) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:48,854] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:06:48,855] {logging_mixin.py:112} INFO - [2020-11-05 18:06:48,855] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:48,990] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:06:49,016] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:06:49,054] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 17:57:00+00:00: scheduled__2020-11-02T17:57:00+00:00, externally triggered: False>
[2020-11-05 18:06:49,059] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 17:57:00+00:00: scheduled__2020-11-02T17:57:00+00:00, externally triggered: False>
[2020-11-05 18:06:49,074] {logging_mixin.py:112} INFO - [2020-11-05 18:06:49,074] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 17:57:00+00:00: scheduled__2020-11-02T17:57:00+00:00, externally triggered: False> successful
[2020-11-05 18:06:49,077] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:06:49,081] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.230 seconds
[2020-11-05 18:07:02,185] {scheduler_job.py:155} INFO - Started process (PID=24293) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:02,189] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:07:02,190] {logging_mixin.py:112} INFO - [2020-11-05 18:07:02,189] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:02,317] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:02,364] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:07:02,402] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:02:00+00:00: scheduled__2020-11-02T18:02:00+00:00, externally triggered: False>
[2020-11-05 18:07:02,409] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:02:00+00:00: scheduled__2020-11-02T18:02:00+00:00, externally triggered: False>
[2020-11-05 18:07:02,430] {logging_mixin.py:112} INFO - [2020-11-05 18:07:02,430] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:02:00+00:00: scheduled__2020-11-02T18:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:07:02,434] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:07:02,441] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.257 seconds
[2020-11-05 18:07:15,547] {scheduler_job.py:155} INFO - Started process (PID=24455) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:15,551] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:07:15,552] {logging_mixin.py:112} INFO - [2020-11-05 18:07:15,552] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:15,766] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:15,795] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:07:15,851] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:07:00+00:00: scheduled__2020-11-02T18:07:00+00:00, externally triggered: False>
[2020-11-05 18:07:15,863] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:07:00+00:00: scheduled__2020-11-02T18:07:00+00:00, externally triggered: False>
[2020-11-05 18:07:15,875] {logging_mixin.py:112} INFO - [2020-11-05 18:07:15,874] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:07:00+00:00: scheduled__2020-11-02T18:07:00+00:00, externally triggered: False> successful
[2020-11-05 18:07:15,884] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:07:15,891] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.344 seconds
[2020-11-05 18:07:28,843] {scheduler_job.py:155} INFO - Started process (PID=24614) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:28,850] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:07:28,851] {logging_mixin.py:112} INFO - [2020-11-05 18:07:28,851] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:28,857] {logging_mixin.py:112} INFO - [2020-11-05 18:07:28,856] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 19
    today = yday = datetime.combine(datetime.today() ,
        ^
SyntaxError: invalid syntax
[2020-11-05 18:07:28,858] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:28,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.052 seconds
[2020-11-05 18:07:42,228] {scheduler_job.py:155} INFO - Started process (PID=24777) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:42,241] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:07:42,242] {logging_mixin.py:112} INFO - [2020-11-05 18:07:42,241] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:42,261] {logging_mixin.py:112} INFO - [2020-11-05 18:07:42,250] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 19
    today = yday = datetime.combine(datetime.today() ,
        ^
SyntaxError: invalid syntax
[2020-11-05 18:07:42,262] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:42,335] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.107 seconds
[2020-11-05 18:07:55,538] {scheduler_job.py:155} INFO - Started process (PID=24941) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:55,544] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:07:55,545] {logging_mixin.py:112} INFO - [2020-11-05 18:07:55,545] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:55,555] {logging_mixin.py:112} INFO - [2020-11-05 18:07:55,552] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 19
    today = yday = datetime.combine(datetime.today() ,
        ^
SyntaxError: invalid syntax
[2020-11-05 18:07:55,556] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:07:55,590] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.052 seconds
[2020-11-05 18:08:08,860] {scheduler_job.py:155} INFO - Started process (PID=25104) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:08,865] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:08:08,865] {logging_mixin.py:112} INFO - [2020-11-05 18:08:08,865] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:08,872] {logging_mixin.py:112} INFO - [2020-11-05 18:08:08,870] {dagbag.py:259} ERROR - Failed to import: /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/dagbag.py", line 256, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/usr/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 781, in get_code
  File "<frozen importlib._bootstrap_external>", line 741, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 19
    today = yday = datetime.combine(datetime.today() ,
        ^
SyntaxError: invalid syntax
[2020-11-05 18:08:08,872] {scheduler_job.py:1603} WARNING - No viable dags retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:08,914] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.054 seconds
[2020-11-05 18:08:22,240] {scheduler_job.py:155} INFO - Started process (PID=25272) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:22,243] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:08:22,243] {logging_mixin.py:112} INFO - [2020-11-05 18:08:22,243] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:22,360] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:22,381] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:08:22,407] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:12:00+00:00: scheduled__2020-11-02T18:12:00+00:00, externally triggered: False>
[2020-11-05 18:08:22,412] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:12:00+00:00: scheduled__2020-11-02T18:12:00+00:00, externally triggered: False>
[2020-11-05 18:08:22,421] {logging_mixin.py:112} INFO - [2020-11-05 18:08:22,421] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:12:00+00:00: scheduled__2020-11-02T18:12:00+00:00, externally triggered: False> successful
[2020-11-05 18:08:22,425] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:08:22,430] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.191 seconds
[2020-11-05 18:08:35,534] {scheduler_job.py:155} INFO - Started process (PID=25434) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:35,537] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:08:35,538] {logging_mixin.py:112} INFO - [2020-11-05 18:08:35,538] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:35,666] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:35,690] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:08:35,717] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:17:00+00:00: scheduled__2020-11-02T18:17:00+00:00, externally triggered: False>
[2020-11-05 18:08:35,721] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:17:00+00:00: scheduled__2020-11-02T18:17:00+00:00, externally triggered: False>
[2020-11-05 18:08:35,730] {logging_mixin.py:112} INFO - [2020-11-05 18:08:35,730] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:17:00+00:00: scheduled__2020-11-02T18:17:00+00:00, externally triggered: False> successful
[2020-11-05 18:08:35,733] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:08:35,736] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.202 seconds
[2020-11-05 18:08:48,811] {scheduler_job.py:155} INFO - Started process (PID=25604) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:48,836] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:08:48,839] {logging_mixin.py:112} INFO - [2020-11-05 18:08:48,839] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:49,025] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:08:49,123] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:08:49,194] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:22:00+00:00: scheduled__2020-11-02T18:22:00+00:00, externally triggered: False>
[2020-11-05 18:08:49,201] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:22:00+00:00: scheduled__2020-11-02T18:22:00+00:00, externally triggered: False>
[2020-11-05 18:08:49,218] {logging_mixin.py:112} INFO - [2020-11-05 18:08:49,218] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:22:00+00:00: scheduled__2020-11-02T18:22:00+00:00, externally triggered: False> successful
[2020-11-05 18:08:49,224] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:08:49,228] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.417 seconds
[2020-11-05 18:09:02,151] {scheduler_job.py:155} INFO - Started process (PID=25767) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:02,154] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:09:02,155] {logging_mixin.py:112} INFO - [2020-11-05 18:09:02,155] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:02,276] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:02,321] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:09:02,361] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:27:00+00:00: scheduled__2020-11-02T18:27:00+00:00, externally triggered: False>
[2020-11-05 18:09:02,366] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:27:00+00:00: scheduled__2020-11-02T18:27:00+00:00, externally triggered: False>
[2020-11-05 18:09:02,378] {logging_mixin.py:112} INFO - [2020-11-05 18:09:02,378] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:27:00+00:00: scheduled__2020-11-02T18:27:00+00:00, externally triggered: False> successful
[2020-11-05 18:09:02,382] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:09:02,386] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.235 seconds
[2020-11-05 18:09:15,429] {scheduler_job.py:155} INFO - Started process (PID=25935) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:15,434] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:09:15,435] {logging_mixin.py:112} INFO - [2020-11-05 18:09:15,434] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:15,607] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:15,641] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:09:15,688] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:32:00+00:00: scheduled__2020-11-02T18:32:00+00:00, externally triggered: False>
[2020-11-05 18:09:15,693] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:32:00+00:00: scheduled__2020-11-02T18:32:00+00:00, externally triggered: False>
[2020-11-05 18:09:15,702] {logging_mixin.py:112} INFO - [2020-11-05 18:09:15,702] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:32:00+00:00: scheduled__2020-11-02T18:32:00+00:00, externally triggered: False> successful
[2020-11-05 18:09:15,705] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:09:15,710] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.281 seconds
[2020-11-05 18:09:28,884] {scheduler_job.py:155} INFO - Started process (PID=26096) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:28,887] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:09:28,888] {logging_mixin.py:112} INFO - [2020-11-05 18:09:28,888] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:29,067] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:29,096] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:09:29,133] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:37:00+00:00: scheduled__2020-11-02T18:37:00+00:00, externally triggered: False>
[2020-11-05 18:09:29,144] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:37:00+00:00: scheduled__2020-11-02T18:37:00+00:00, externally triggered: False>
[2020-11-05 18:09:29,154] {logging_mixin.py:112} INFO - [2020-11-05 18:09:29,154] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:37:00+00:00: scheduled__2020-11-02T18:37:00+00:00, externally triggered: False> successful
[2020-11-05 18:09:29,158] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:09:29,164] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.280 seconds
[2020-11-05 18:09:42,155] {scheduler_job.py:155} INFO - Started process (PID=26256) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:42,158] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:09:42,159] {logging_mixin.py:112} INFO - [2020-11-05 18:09:42,159] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:42,339] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:42,359] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:09:42,394] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:42:00+00:00: scheduled__2020-11-02T18:42:00+00:00, externally triggered: False>
[2020-11-05 18:09:42,398] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:42:00+00:00: scheduled__2020-11-02T18:42:00+00:00, externally triggered: False>
[2020-11-05 18:09:42,407] {logging_mixin.py:112} INFO - [2020-11-05 18:09:42,406] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:42:00+00:00: scheduled__2020-11-02T18:42:00+00:00, externally triggered: False> successful
[2020-11-05 18:09:42,410] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:09:42,415] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-05 18:09:55,495] {scheduler_job.py:155} INFO - Started process (PID=26420) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:55,498] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:09:55,498] {logging_mixin.py:112} INFO - [2020-11-05 18:09:55,498] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:55,624] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:09:55,648] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:09:55,684] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:47:00+00:00: scheduled__2020-11-02T18:47:00+00:00, externally triggered: False>
[2020-11-05 18:09:55,690] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:47:00+00:00: scheduled__2020-11-02T18:47:00+00:00, externally triggered: False>
[2020-11-05 18:09:55,699] {logging_mixin.py:112} INFO - [2020-11-05 18:09:55,699] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:47:00+00:00: scheduled__2020-11-02T18:47:00+00:00, externally triggered: False> successful
[2020-11-05 18:09:55,702] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:09:55,706] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.212 seconds
[2020-11-05 18:10:08,788] {scheduler_job.py:155} INFO - Started process (PID=26588) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:08,795] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:10:08,796] {logging_mixin.py:112} INFO - [2020-11-05 18:10:08,795] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:08,917] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:08,949] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:10:08,989] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:52:00+00:00: scheduled__2020-11-02T18:52:00+00:00, externally triggered: False>
[2020-11-05 18:10:08,993] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:52:00+00:00: scheduled__2020-11-02T18:52:00+00:00, externally triggered: False>
[2020-11-05 18:10:09,006] {logging_mixin.py:112} INFO - [2020-11-05 18:10:09,006] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:52:00+00:00: scheduled__2020-11-02T18:52:00+00:00, externally triggered: False> successful
[2020-11-05 18:10:09,011] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:10:09,017] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.230 seconds
[2020-11-05 18:10:22,198] {scheduler_job.py:155} INFO - Started process (PID=26754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:22,213] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:10:22,215] {logging_mixin.py:112} INFO - [2020-11-05 18:10:22,215] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:22,546] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:22,601] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:10:22,664] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 18:57:00+00:00: scheduled__2020-11-02T18:57:00+00:00, externally triggered: False>
[2020-11-05 18:10:22,676] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 18:57:00+00:00: scheduled__2020-11-02T18:57:00+00:00, externally triggered: False>
[2020-11-05 18:10:22,708] {logging_mixin.py:112} INFO - [2020-11-05 18:10:22,707] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 18:57:00+00:00: scheduled__2020-11-02T18:57:00+00:00, externally triggered: False> successful
[2020-11-05 18:10:22,713] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:10:22,721] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.523 seconds
[2020-11-05 18:10:35,549] {scheduler_job.py:155} INFO - Started process (PID=26915) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:35,553] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:10:35,553] {logging_mixin.py:112} INFO - [2020-11-05 18:10:35,553] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:35,672] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:35,703] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:10:35,745] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:02:00+00:00: scheduled__2020-11-02T19:02:00+00:00, externally triggered: False>
[2020-11-05 18:10:35,749] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:02:00+00:00: scheduled__2020-11-02T19:02:00+00:00, externally triggered: False>
[2020-11-05 18:10:35,764] {logging_mixin.py:112} INFO - [2020-11-05 18:10:35,764] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:02:00+00:00: scheduled__2020-11-02T19:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:10:35,768] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:10:35,774] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.226 seconds
[2020-11-05 18:10:48,968] {scheduler_job.py:155} INFO - Started process (PID=27081) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:48,986] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:10:48,987] {logging_mixin.py:112} INFO - [2020-11-05 18:10:48,987] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:49,275] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:10:49,307] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:10:49,346] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:07:00+00:00: scheduled__2020-11-02T19:07:00+00:00, externally triggered: False>
[2020-11-05 18:10:49,351] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:07:00+00:00: scheduled__2020-11-02T19:07:00+00:00, externally triggered: False>
[2020-11-05 18:10:49,367] {logging_mixin.py:112} INFO - [2020-11-05 18:10:49,367] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:07:00+00:00: scheduled__2020-11-02T19:07:00+00:00, externally triggered: False> successful
[2020-11-05 18:10:49,372] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:10:49,382] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.414 seconds
[2020-11-05 18:11:02,465] {scheduler_job.py:155} INFO - Started process (PID=27242) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:02,472] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:11:02,472] {logging_mixin.py:112} INFO - [2020-11-05 18:11:02,472] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:02,657] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:02,691] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:11:02,742] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:12:00+00:00: scheduled__2020-11-02T19:12:00+00:00, externally triggered: False>
[2020-11-05 18:11:02,750] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:12:00+00:00: scheduled__2020-11-02T19:12:00+00:00, externally triggered: False>
[2020-11-05 18:11:02,769] {logging_mixin.py:112} INFO - [2020-11-05 18:11:02,769] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:12:00+00:00: scheduled__2020-11-02T19:12:00+00:00, externally triggered: False> successful
[2020-11-05 18:11:02,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:11:02,777] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.312 seconds
[2020-11-05 18:11:15,818] {scheduler_job.py:155} INFO - Started process (PID=27404) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:15,822] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:11:15,823] {logging_mixin.py:112} INFO - [2020-11-05 18:11:15,822] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:15,931] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:15,962] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:11:15,996] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:17:00+00:00: scheduled__2020-11-02T19:17:00+00:00, externally triggered: False>
[2020-11-05 18:11:16,001] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:17:00+00:00: scheduled__2020-11-02T19:17:00+00:00, externally triggered: False>
[2020-11-05 18:11:16,015] {logging_mixin.py:112} INFO - [2020-11-05 18:11:16,015] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:17:00+00:00: scheduled__2020-11-02T19:17:00+00:00, externally triggered: False> successful
[2020-11-05 18:11:16,018] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:11:16,027] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.210 seconds
[2020-11-05 18:11:29,152] {scheduler_job.py:155} INFO - Started process (PID=27568) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:29,156] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:11:29,157] {logging_mixin.py:112} INFO - [2020-11-05 18:11:29,157] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:29,291] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:29,314] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:11:29,342] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:22:00+00:00: scheduled__2020-11-02T19:22:00+00:00, externally triggered: False>
[2020-11-05 18:11:29,347] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:22:00+00:00: scheduled__2020-11-02T19:22:00+00:00, externally triggered: False>
[2020-11-05 18:11:29,356] {logging_mixin.py:112} INFO - [2020-11-05 18:11:29,355] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:22:00+00:00: scheduled__2020-11-02T19:22:00+00:00, externally triggered: False> successful
[2020-11-05 18:11:29,359] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:11:29,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.211 seconds
[2020-11-05 18:11:42,441] {scheduler_job.py:155} INFO - Started process (PID=27731) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:42,445] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:11:42,446] {logging_mixin.py:112} INFO - [2020-11-05 18:11:42,446] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:42,606] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:42,648] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:11:42,702] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:27:00+00:00: scheduled__2020-11-02T19:27:00+00:00, externally triggered: False>
[2020-11-05 18:11:42,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:27:00+00:00: scheduled__2020-11-02T19:27:00+00:00, externally triggered: False>
[2020-11-05 18:11:42,741] {logging_mixin.py:112} INFO - [2020-11-05 18:11:42,740] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:27:00+00:00: scheduled__2020-11-02T19:27:00+00:00, externally triggered: False> successful
[2020-11-05 18:11:42,745] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:11:42,752] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.311 seconds
[2020-11-05 18:11:55,755] {scheduler_job.py:155} INFO - Started process (PID=27895) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:55,758] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:11:55,759] {logging_mixin.py:112} INFO - [2020-11-05 18:11:55,759] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:55,966] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:11:56,017] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:11:56,058] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:32:00+00:00: scheduled__2020-11-02T19:32:00+00:00, externally triggered: False>
[2020-11-05 18:11:56,064] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:32:00+00:00: scheduled__2020-11-02T19:32:00+00:00, externally triggered: False>
[2020-11-05 18:11:56,094] {logging_mixin.py:112} INFO - [2020-11-05 18:11:56,094] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:32:00+00:00: scheduled__2020-11-02T19:32:00+00:00, externally triggered: False> successful
[2020-11-05 18:11:56,099] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:11:56,108] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.352 seconds
[2020-11-05 18:12:09,083] {scheduler_job.py:155} INFO - Started process (PID=28064) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:09,086] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:12:09,087] {logging_mixin.py:112} INFO - [2020-11-05 18:12:09,086] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:09,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:09,242] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:12:09,270] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:37:00+00:00: scheduled__2020-11-02T19:37:00+00:00, externally triggered: False>
[2020-11-05 18:12:09,275] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:37:00+00:00: scheduled__2020-11-02T19:37:00+00:00, externally triggered: False>
[2020-11-05 18:12:09,288] {logging_mixin.py:112} INFO - [2020-11-05 18:12:09,288] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:37:00+00:00: scheduled__2020-11-02T19:37:00+00:00, externally triggered: False> successful
[2020-11-05 18:12:09,295] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:12:09,300] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.217 seconds
[2020-11-05 18:12:22,407] {scheduler_job.py:155} INFO - Started process (PID=28229) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:22,414] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:12:22,415] {logging_mixin.py:112} INFO - [2020-11-05 18:12:22,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:22,640] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:22,681] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:12:22,743] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:42:00+00:00: scheduled__2020-11-02T19:42:00+00:00, externally triggered: False>
[2020-11-05 18:12:22,750] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:42:00+00:00: scheduled__2020-11-02T19:42:00+00:00, externally triggered: False>
[2020-11-05 18:12:22,771] {logging_mixin.py:112} INFO - [2020-11-05 18:12:22,771] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:42:00+00:00: scheduled__2020-11-02T19:42:00+00:00, externally triggered: False> successful
[2020-11-05 18:12:22,775] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:12:22,781] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.374 seconds
[2020-11-05 18:12:35,744] {scheduler_job.py:155} INFO - Started process (PID=28401) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:35,749] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:12:35,751] {logging_mixin.py:112} INFO - [2020-11-05 18:12:35,750] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:35,986] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:36,040] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:12:36,103] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:47:00+00:00: scheduled__2020-11-02T19:47:00+00:00, externally triggered: False>
[2020-11-05 18:12:36,110] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:47:00+00:00: scheduled__2020-11-02T19:47:00+00:00, externally triggered: False>
[2020-11-05 18:12:36,131] {logging_mixin.py:112} INFO - [2020-11-05 18:12:36,130] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:47:00+00:00: scheduled__2020-11-02T19:47:00+00:00, externally triggered: False> successful
[2020-11-05 18:12:36,139] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:12:36,144] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.401 seconds
[2020-11-05 18:12:49,040] {scheduler_job.py:155} INFO - Started process (PID=28560) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:49,044] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:12:49,045] {logging_mixin.py:112} INFO - [2020-11-05 18:12:49,044] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:49,167] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:12:49,194] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:12:49,228] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02 19:52:00+00:00: scheduled__2020-11-02T19:52:00+00:00, externally triggered: False>
[2020-11-05 18:12:49,232] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 19:52:00+00:00: scheduled__2020-11-02T19:52:00+00:00, externally triggered: False>
[2020-11-05 18:12:49,242] {logging_mixin.py:112} INFO - [2020-11-05 18:12:49,242] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 19:52:00+00:00: scheduled__2020-11-02T19:52:00+00:00, externally triggered: False> successful
[2020-11-05 18:12:49,247] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:12:49,253] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.213 seconds
[2020-11-05 18:13:02,370] {scheduler_job.py:155} INFO - Started process (PID=28722) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:02,385] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:13:02,387] {logging_mixin.py:112} INFO - [2020-11-05 18:13:02,387] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:02,565] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:02,631] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:13:02,726] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T20:02:00+00:00: scheduled__2020-11-02T20:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:02,735] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 20:02:00+00:00: scheduled__2020-11-02T20:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:02,749] {logging_mixin.py:112} INFO - [2020-11-05 18:13:02,749] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 20:02:00+00:00: scheduled__2020-11-02T20:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:13:02,754] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:13:02,760] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.390 seconds
[2020-11-05 18:13:15,696] {scheduler_job.py:155} INFO - Started process (PID=28882) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:15,713] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:13:15,713] {logging_mixin.py:112} INFO - [2020-11-05 18:13:15,713] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:15,887] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:15,915] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:13:15,965] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T21:02:00+00:00: scheduled__2020-11-02T21:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:15,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 21:02:00+00:00: scheduled__2020-11-02T21:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:15,992] {logging_mixin.py:112} INFO - [2020-11-05 18:13:15,991] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 21:02:00+00:00: scheduled__2020-11-02T21:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:13:15,997] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:13:16,006] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.310 seconds
[2020-11-05 18:13:29,139] {scheduler_job.py:155} INFO - Started process (PID=29055) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:29,148] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:13:29,149] {logging_mixin.py:112} INFO - [2020-11-05 18:13:29,148] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:29,388] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:29,424] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:13:29,512] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T22:02:00+00:00: scheduled__2020-11-02T22:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:29,519] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 22:02:00+00:00: scheduled__2020-11-02T22:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:29,541] {logging_mixin.py:112} INFO - [2020-11-05 18:13:29,541] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 22:02:00+00:00: scheduled__2020-11-02T22:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:13:29,552] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:13:29,566] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.427 seconds
[2020-11-05 18:13:42,593] {scheduler_job.py:155} INFO - Started process (PID=29217) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:42,607] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:13:42,607] {logging_mixin.py:112} INFO - [2020-11-05 18:13:42,607] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:42,965] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:43,022] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:13:43,097] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-02T23:02:00+00:00: scheduled__2020-11-02T23:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:43,104] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-02 23:02:00+00:00: scheduled__2020-11-02T23:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:43,116] {logging_mixin.py:112} INFO - [2020-11-05 18:13:43,115] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-02 23:02:00+00:00: scheduled__2020-11-02T23:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:13:43,125] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:13:43,133] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.541 seconds
[2020-11-05 18:13:55,918] {scheduler_job.py:155} INFO - Started process (PID=29383) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:55,927] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:13:55,928] {logging_mixin.py:112} INFO - [2020-11-05 18:13:55,927] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:56,162] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:13:56,214] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:13:56,280] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T00:02:00+00:00: scheduled__2020-11-03T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:56,285] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 00:02:00+00:00: scheduled__2020-11-03T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:13:56,331] {logging_mixin.py:112} INFO - [2020-11-05 18:13:56,331] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 00:02:00+00:00: scheduled__2020-11-03T00:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:13:56,335] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:13:56,345] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.427 seconds
[2020-11-05 18:14:09,252] {scheduler_job.py:155} INFO - Started process (PID=29550) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:09,256] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:14:09,268] {logging_mixin.py:112} INFO - [2020-11-05 18:14:09,266] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:09,491] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:09,522] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:14:09,564] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T01:02:00+00:00: scheduled__2020-11-03T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:09,572] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 01:02:00+00:00: scheduled__2020-11-03T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:09,584] {logging_mixin.py:112} INFO - [2020-11-05 18:14:09,584] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 01:02:00+00:00: scheduled__2020-11-03T01:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:14:09,588] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:14:09,594] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.342 seconds
[2020-11-05 18:14:22,532] {scheduler_job.py:155} INFO - Started process (PID=29716) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:22,535] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:14:22,535] {logging_mixin.py:112} INFO - [2020-11-05 18:14:22,535] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:22,722] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:22,778] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:14:22,866] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T02:02:00+00:00: scheduled__2020-11-03T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:22,872] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 02:02:00+00:00: scheduled__2020-11-03T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:22,906] {logging_mixin.py:112} INFO - [2020-11-05 18:14:22,906] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 02:02:00+00:00: scheduled__2020-11-03T02:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:14:22,914] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 17:14:15.972321+00:00: manual__2020-11-05T17:14:15.972321+00:00, externally triggered: True>
[2020-11-05 18:14:22,981] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:14:23,000] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 17:14:15.972321+00:00 [success]> in ORM
[2020-11-05 18:14:23,013] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 17:14:15.972321+00:00 [scheduled]> in ORM
[2020-11-05 18:14:23,044] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.512 seconds
[2020-11-05 18:14:36,094] {scheduler_job.py:155} INFO - Started process (PID=29930) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:36,098] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:14:36,099] {logging_mixin.py:112} INFO - [2020-11-05 18:14:36,099] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:36,295] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:36,331] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:14:36,383] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T03:02:00+00:00: scheduled__2020-11-03T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:36,388] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 03:02:00+00:00: scheduled__2020-11-03T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:36,406] {logging_mixin.py:112} INFO - [2020-11-05 18:14:36,406] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 03:02:00+00:00: scheduled__2020-11-03T03:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:14:36,414] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 17:14:15.972321+00:00: manual__2020-11-05T17:14:15.972321+00:00, externally triggered: True>
[2020-11-05 18:14:36,493] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:14:36,501] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.upload_file_to_S3 2020-11-05 17:14:15.972321+00:00 [scheduled]> in ORM
[2020-11-05 18:14:36,513] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.419 seconds
[2020-11-05 18:14:49,465] {scheduler_job.py:155} INFO - Started process (PID=30139) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:49,470] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:14:49,472] {logging_mixin.py:112} INFO - [2020-11-05 18:14:49,471] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:49,624] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:14:49,649] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:14:49,692] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T04:02:00+00:00: scheduled__2020-11-03T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:49,696] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 04:02:00+00:00: scheduled__2020-11-03T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:14:49,707] {logging_mixin.py:112} INFO - [2020-11-05 18:14:49,707] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 04:02:00+00:00: scheduled__2020-11-03T04:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:14:49,711] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 17:14:15.972321+00:00: manual__2020-11-05T17:14:15.972321+00:00, externally triggered: True>
[2020-11-05 18:14:49,727] {logging_mixin.py:112} INFO - [2020-11-05 18:14:49,727] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-05 17:14:15.972321+00:00: manual__2020-11-05T17:14:15.972321+00:00, externally triggered: True> successful
[2020-11-05 18:14:49,733] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:14:49,738] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.274 seconds
[2020-11-05 18:15:02,891] {scheduler_job.py:155} INFO - Started process (PID=30305) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:02,910] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:15:02,911] {logging_mixin.py:112} INFO - [2020-11-05 18:15:02,911] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:03,273] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:03,313] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:15:03,384] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T05:02:00+00:00: scheduled__2020-11-03T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:03,397] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 05:02:00+00:00: scheduled__2020-11-03T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:03,442] {logging_mixin.py:112} INFO - [2020-11-05 18:15:03,441] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 05:02:00+00:00: scheduled__2020-11-03T05:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:15:03,451] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:15:03,465] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.574 seconds
[2020-11-05 18:15:16,185] {scheduler_job.py:155} INFO - Started process (PID=30468) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:16,188] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:15:16,189] {logging_mixin.py:112} INFO - [2020-11-05 18:15:16,189] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:16,354] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:16,385] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:15:16,433] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T06:02:00+00:00: scheduled__2020-11-03T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:16,437] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 06:02:00+00:00: scheduled__2020-11-03T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:16,447] {logging_mixin.py:112} INFO - [2020-11-05 18:15:16,447] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 06:02:00+00:00: scheduled__2020-11-03T06:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:15:16,453] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:15:16,457] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.272 seconds
[2020-11-05 18:15:29,579] {scheduler_job.py:155} INFO - Started process (PID=30631) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:29,590] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:15:29,595] {logging_mixin.py:112} INFO - [2020-11-05 18:15:29,590] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:29,918] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:29,960] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:15:30,055] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T07:02:00+00:00: scheduled__2020-11-03T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:30,068] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 07:02:00+00:00: scheduled__2020-11-03T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:30,101] {logging_mixin.py:112} INFO - [2020-11-05 18:15:30,101] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 07:02:00+00:00: scheduled__2020-11-03T07:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:15:30,110] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:15:30,119] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.540 seconds
[2020-11-05 18:15:42,985] {scheduler_job.py:155} INFO - Started process (PID=30792) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:42,988] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:15:42,989] {logging_mixin.py:112} INFO - [2020-11-05 18:15:42,989] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:43,257] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:43,303] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:15:43,353] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T08:02:00+00:00: scheduled__2020-11-03T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:43,366] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 08:02:00+00:00: scheduled__2020-11-03T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:43,384] {logging_mixin.py:112} INFO - [2020-11-05 18:15:43,384] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 08:02:00+00:00: scheduled__2020-11-03T08:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:15:43,390] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:15:43,397] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.412 seconds
[2020-11-05 18:15:56,276] {scheduler_job.py:155} INFO - Started process (PID=30956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:56,280] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:15:56,281] {logging_mixin.py:112} INFO - [2020-11-05 18:15:56,280] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:56,430] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:15:56,457] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:15:56,512] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T09:02:00+00:00: scheduled__2020-11-03T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:56,519] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 09:02:00+00:00: scheduled__2020-11-03T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:15:56,535] {logging_mixin.py:112} INFO - [2020-11-05 18:15:56,535] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 09:02:00+00:00: scheduled__2020-11-03T09:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:15:56,538] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:15:56,544] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.268 seconds
[2020-11-05 18:16:09,628] {scheduler_job.py:155} INFO - Started process (PID=31119) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:09,632] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:16:09,633] {logging_mixin.py:112} INFO - [2020-11-05 18:16:09,633] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:09,800] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:09,824] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:16:09,857] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T10:02:00+00:00: scheduled__2020-11-03T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:16:09,861] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 10:02:00+00:00: scheduled__2020-11-03T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:16:09,870] {logging_mixin.py:112} INFO - [2020-11-05 18:16:09,870] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 10:02:00+00:00: scheduled__2020-11-03T10:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:16:09,874] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:16:09,877] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.249 seconds
[2020-11-05 18:16:22,960] {scheduler_job.py:155} INFO - Started process (PID=31285) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:22,963] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:16:22,965] {logging_mixin.py:112} INFO - [2020-11-05 18:16:22,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:23,185] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:23,223] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:16:23,283] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T11:02:00+00:00: scheduled__2020-11-03T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:16:23,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 11:02:00+00:00: scheduled__2020-11-03T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:16:23,301] {logging_mixin.py:112} INFO - [2020-11-05 18:16:23,301] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 11:02:00+00:00: scheduled__2020-11-03T11:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:16:23,305] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:16:23,310] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.350 seconds
[2020-11-05 18:16:48,594] {scheduler_job.py:155} INFO - Started process (PID=31676) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:48,599] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:16:48,600] {logging_mixin.py:112} INFO - [2020-11-05 18:16:48,600] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:48,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:16:48,779] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:16:48,816] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T12:02:00+00:00: scheduled__2020-11-03T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:16:48,820] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 12:02:00+00:00: scheduled__2020-11-03T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:16:48,831] {logging_mixin.py:112} INFO - [2020-11-05 18:16:48,831] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 12:02:00+00:00: scheduled__2020-11-03T12:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:16:48,834] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:16:48,838] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.244 seconds
[2020-11-05 18:17:01,868] {scheduler_job.py:155} INFO - Started process (PID=31845) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:01,876] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:17:01,877] {logging_mixin.py:112} INFO - [2020-11-05 18:17:01,876] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:02,123] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:02,145] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:17:02,180] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T13:02:00+00:00: scheduled__2020-11-03T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:02,184] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 13:02:00+00:00: scheduled__2020-11-03T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:02,198] {logging_mixin.py:112} INFO - [2020-11-05 18:17:02,198] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 13:02:00+00:00: scheduled__2020-11-03T13:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:17:02,204] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:17:02,212] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.345 seconds
[2020-11-05 18:17:15,193] {scheduler_job.py:155} INFO - Started process (PID=32018) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:15,196] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:17:15,196] {logging_mixin.py:112} INFO - [2020-11-05 18:17:15,196] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:15,321] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:15,349] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:17:15,386] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T14:02:00+00:00: scheduled__2020-11-03T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:15,393] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 14:02:00+00:00: scheduled__2020-11-03T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:15,408] {logging_mixin.py:112} INFO - [2020-11-05 18:17:15,408] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 14:02:00+00:00: scheduled__2020-11-03T14:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:17:15,412] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:17:15,417] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.224 seconds
[2020-11-05 18:17:28,504] {scheduler_job.py:155} INFO - Started process (PID=32183) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:28,508] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:17:28,509] {logging_mixin.py:112} INFO - [2020-11-05 18:17:28,508] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:28,643] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:28,669] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:17:28,703] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T15:02:00+00:00: scheduled__2020-11-03T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:28,707] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 15:02:00+00:00: scheduled__2020-11-03T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:28,721] {logging_mixin.py:112} INFO - [2020-11-05 18:17:28,720] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 15:02:00+00:00: scheduled__2020-11-03T15:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:17:28,726] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:17:28,732] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.228 seconds
[2020-11-05 18:17:41,838] {scheduler_job.py:155} INFO - Started process (PID=32349) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:41,841] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:17:41,842] {logging_mixin.py:112} INFO - [2020-11-05 18:17:41,842] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:41,983] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:42,005] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:17:42,038] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T16:02:00+00:00: scheduled__2020-11-03T16:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:42,042] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 16:02:00+00:00: scheduled__2020-11-03T16:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:42,052] {logging_mixin.py:112} INFO - [2020-11-05 18:17:42,052] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 16:02:00+00:00: scheduled__2020-11-03T16:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:17:42,055] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:17:42,059] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.221 seconds
[2020-11-05 18:17:55,173] {scheduler_job.py:155} INFO - Started process (PID=32514) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:55,176] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:17:55,177] {logging_mixin.py:112} INFO - [2020-11-05 18:17:55,177] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:55,331] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:17:55,353] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:17:55,395] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T17:02:00+00:00: scheduled__2020-11-03T17:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:55,403] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 17:02:00+00:00: scheduled__2020-11-03T17:02:00+00:00, externally triggered: False>
[2020-11-05 18:17:55,425] {logging_mixin.py:112} INFO - [2020-11-05 18:17:55,425] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 17:02:00+00:00: scheduled__2020-11-03T17:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:17:55,428] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:17:55,433] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.260 seconds
[2020-11-05 18:18:08,471] {scheduler_job.py:155} INFO - Started process (PID=32679) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:08,476] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:18:08,476] {logging_mixin.py:112} INFO - [2020-11-05 18:18:08,476] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:08,613] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:08,635] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:18:08,678] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T18:02:00+00:00: scheduled__2020-11-03T18:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:08,682] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 18:02:00+00:00: scheduled__2020-11-03T18:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:08,692] {logging_mixin.py:112} INFO - [2020-11-05 18:18:08,692] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 18:02:00+00:00: scheduled__2020-11-03T18:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:18:08,696] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:18:08,700] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.229 seconds
[2020-11-05 18:18:21,816] {scheduler_job.py:155} INFO - Started process (PID=392) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:21,826] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:18:21,833] {logging_mixin.py:112} INFO - [2020-11-05 18:18:21,833] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:22,051] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:22,072] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:18:22,109] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T19:02:00+00:00: scheduled__2020-11-03T19:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:22,114] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 19:02:00+00:00: scheduled__2020-11-03T19:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:22,125] {logging_mixin.py:112} INFO - [2020-11-05 18:18:22,125] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 19:02:00+00:00: scheduled__2020-11-03T19:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:18:22,128] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:18:22,133] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.317 seconds
[2020-11-05 18:18:35,165] {scheduler_job.py:155} INFO - Started process (PID=619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:35,169] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:18:35,180] {logging_mixin.py:112} INFO - [2020-11-05 18:18:35,180] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:35,495] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:35,532] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:18:35,608] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T20:02:00+00:00: scheduled__2020-11-03T20:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:35,612] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 20:02:00+00:00: scheduled__2020-11-03T20:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:35,627] {logging_mixin.py:112} INFO - [2020-11-05 18:18:35,627] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 20:02:00+00:00: scheduled__2020-11-03T20:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:18:35,632] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:18:35,640] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.475 seconds
[2020-11-05 18:18:48,615] {scheduler_job.py:155} INFO - Started process (PID=793) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:48,619] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:18:48,619] {logging_mixin.py:112} INFO - [2020-11-05 18:18:48,619] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:48,760] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:18:48,785] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:18:48,826] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T21:02:00+00:00: scheduled__2020-11-03T21:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:48,832] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 21:02:00+00:00: scheduled__2020-11-03T21:02:00+00:00, externally triggered: False>
[2020-11-05 18:18:48,847] {logging_mixin.py:112} INFO - [2020-11-05 18:18:48,846] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 21:02:00+00:00: scheduled__2020-11-03T21:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:18:48,851] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:18:48,855] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.240 seconds
[2020-11-05 18:19:01,990] {scheduler_job.py:155} INFO - Started process (PID=972) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:01,995] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:19:01,996] {logging_mixin.py:112} INFO - [2020-11-05 18:19:01,996] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:02,132] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:02,156] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:19:02,186] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T22:02:00+00:00: scheduled__2020-11-03T22:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:02,190] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 22:02:00+00:00: scheduled__2020-11-03T22:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:02,200] {logging_mixin.py:112} INFO - [2020-11-05 18:19:02,200] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 22:02:00+00:00: scheduled__2020-11-03T22:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:19:02,203] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:19:02,207] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.217 seconds
[2020-11-05 18:19:15,317] {scheduler_job.py:155} INFO - Started process (PID=1138) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:15,321] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:19:15,321] {logging_mixin.py:112} INFO - [2020-11-05 18:19:15,321] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:15,528] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:15,568] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:19:15,620] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-03T23:02:00+00:00: scheduled__2020-11-03T23:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:15,624] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-03 23:02:00+00:00: scheduled__2020-11-03T23:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:15,637] {logging_mixin.py:112} INFO - [2020-11-05 18:19:15,637] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-03 23:02:00+00:00: scheduled__2020-11-03T23:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:19:15,640] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:19:15,644] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.327 seconds
[2020-11-05 18:19:28,599] {scheduler_job.py:155} INFO - Started process (PID=1300) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:28,622] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:19:28,628] {logging_mixin.py:112} INFO - [2020-11-05 18:19:28,628] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:28,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:28,986] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:19:29,041] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T00:02:00+00:00: scheduled__2020-11-04T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:29,047] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 00:02:00+00:00: scheduled__2020-11-04T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:29,058] {logging_mixin.py:112} INFO - [2020-11-05 18:19:29,058] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 00:02:00+00:00: scheduled__2020-11-04T00:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:19:29,062] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:19:29,066] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.467 seconds
[2020-11-05 18:19:41,881] {scheduler_job.py:155} INFO - Started process (PID=1463) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:41,885] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:19:41,886] {logging_mixin.py:112} INFO - [2020-11-05 18:19:41,886] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:42,036] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:42,059] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:19:42,093] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T01:02:00+00:00: scheduled__2020-11-04T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:42,099] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 01:02:00+00:00: scheduled__2020-11-04T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:42,108] {logging_mixin.py:112} INFO - [2020-11-05 18:19:42,107] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 01:02:00+00:00: scheduled__2020-11-04T01:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:19:42,110] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:19:42,115] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.233 seconds
[2020-11-05 18:19:55,193] {scheduler_job.py:155} INFO - Started process (PID=1662) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:55,199] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:19:55,200] {logging_mixin.py:112} INFO - [2020-11-05 18:19:55,200] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:55,393] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:19:55,445] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:19:55,516] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T02:02:00+00:00: scheduled__2020-11-04T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:55,525] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 02:02:00+00:00: scheduled__2020-11-04T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:19:55,558] {logging_mixin.py:112} INFO - [2020-11-05 18:19:55,558] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 02:02:00+00:00: scheduled__2020-11-04T02:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:19:55,566] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:19:55,574] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.381 seconds
[2020-11-05 18:20:08,726] {scheduler_job.py:155} INFO - Started process (PID=2046) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:08,746] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:20:08,746] {logging_mixin.py:112} INFO - [2020-11-05 18:20:08,746] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:09,150] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:09,220] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:20:09,417] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T03:02:00+00:00: scheduled__2020-11-04T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:09,445] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 03:02:00+00:00: scheduled__2020-11-04T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:09,477] {logging_mixin.py:112} INFO - [2020-11-05 18:20:09,477] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 03:02:00+00:00: scheduled__2020-11-04T03:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:20:09,485] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:20:09,495] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.769 seconds
[2020-11-05 18:20:22,040] {scheduler_job.py:155} INFO - Started process (PID=2252) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:22,044] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:20:22,046] {logging_mixin.py:112} INFO - [2020-11-05 18:20:22,045] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:22,218] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:22,242] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:20:22,280] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T04:02:00+00:00: scheduled__2020-11-04T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:22,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 04:02:00+00:00: scheduled__2020-11-04T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:22,298] {logging_mixin.py:112} INFO - [2020-11-05 18:20:22,298] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 04:02:00+00:00: scheduled__2020-11-04T04:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:20:22,302] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:20:22,311] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.271 seconds
[2020-11-05 18:20:35,502] {scheduler_job.py:155} INFO - Started process (PID=2499) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:35,506] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:20:35,507] {logging_mixin.py:112} INFO - [2020-11-05 18:20:35,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:35,845] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:35,888] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:20:35,958] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T05:02:00+00:00: scheduled__2020-11-04T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:35,967] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 05:02:00+00:00: scheduled__2020-11-04T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:35,998] {logging_mixin.py:112} INFO - [2020-11-05 18:20:35,998] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 05:02:00+00:00: scheduled__2020-11-04T05:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:20:36,003] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:20:36,013] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.511 seconds
[2020-11-05 18:20:48,805] {scheduler_job.py:155} INFO - Started process (PID=2691) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:48,819] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:20:48,820] {logging_mixin.py:112} INFO - [2020-11-05 18:20:48,819] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:48,957] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:20:48,979] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:20:49,021] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T06:02:00+00:00: scheduled__2020-11-04T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:49,025] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 06:02:00+00:00: scheduled__2020-11-04T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:20:49,036] {logging_mixin.py:112} INFO - [2020-11-05 18:20:49,035] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 06:02:00+00:00: scheduled__2020-11-04T06:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:20:49,040] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:20:49,047] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.242 seconds
[2020-11-05 18:21:02,149] {scheduler_job.py:155} INFO - Started process (PID=2862) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:02,153] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:21:02,161] {logging_mixin.py:112} INFO - [2020-11-05 18:21:02,153] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:02,404] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:02,437] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:21:02,482] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T07:02:00+00:00: scheduled__2020-11-04T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:02,488] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 07:02:00+00:00: scheduled__2020-11-04T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:02,498] {logging_mixin.py:112} INFO - [2020-11-05 18:21:02,497] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 07:02:00+00:00: scheduled__2020-11-04T07:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:21:02,501] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:21:02,504] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.355 seconds
[2020-11-05 18:21:14,477] {scheduler_job.py:155} INFO - Started process (PID=3015) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:14,484] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:21:14,485] {logging_mixin.py:112} INFO - [2020-11-05 18:21:14,485] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:14,617] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:14,659] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:21:14,725] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T08:02:00+00:00: scheduled__2020-11-04T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:14,731] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 08:02:00+00:00: scheduled__2020-11-04T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:14,748] {logging_mixin.py:112} INFO - [2020-11-05 18:21:14,748] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 08:02:00+00:00: scheduled__2020-11-04T08:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:21:14,752] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:21:14,764] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.287 seconds
[2020-11-05 18:21:27,753] {scheduler_job.py:155} INFO - Started process (PID=3177) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:27,757] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:21:27,758] {logging_mixin.py:112} INFO - [2020-11-05 18:21:27,758] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:27,886] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:27,910] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:21:27,952] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T09:02:00+00:00: scheduled__2020-11-04T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:27,958] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 09:02:00+00:00: scheduled__2020-11-04T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:27,977] {logging_mixin.py:112} INFO - [2020-11-05 18:21:27,977] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 09:02:00+00:00: scheduled__2020-11-04T09:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:21:27,983] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:21:27,989] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.236 seconds
[2020-11-05 18:21:41,042] {scheduler_job.py:155} INFO - Started process (PID=3364) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:41,047] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:21:41,047] {logging_mixin.py:112} INFO - [2020-11-05 18:21:41,047] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:41,187] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:41,229] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:21:41,282] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T10:02:00+00:00: scheduled__2020-11-04T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:41,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 10:02:00+00:00: scheduled__2020-11-04T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:41,297] {logging_mixin.py:112} INFO - [2020-11-05 18:21:41,297] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 10:02:00+00:00: scheduled__2020-11-04T10:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:21:41,307] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:21:41,312] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.269 seconds
[2020-11-05 18:21:54,528] {scheduler_job.py:155} INFO - Started process (PID=3525) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:54,538] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:21:54,539] {logging_mixin.py:112} INFO - [2020-11-05 18:21:54,539] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:54,929] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:21:55,035] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:21:55,102] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T11:02:00+00:00: scheduled__2020-11-04T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:55,108] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 11:02:00+00:00: scheduled__2020-11-04T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:21:55,127] {logging_mixin.py:112} INFO - [2020-11-05 18:21:55,127] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 11:02:00+00:00: scheduled__2020-11-04T11:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:21:55,135] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:21:55,141] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.614 seconds
[2020-11-05 18:22:07,929] {scheduler_job.py:155} INFO - Started process (PID=3692) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:07,934] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:22:07,935] {logging_mixin.py:112} INFO - [2020-11-05 18:22:07,934] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:08,190] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:08,222] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:22:08,309] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T12:02:00+00:00: scheduled__2020-11-04T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:08,321] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 12:02:00+00:00: scheduled__2020-11-04T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:08,350] {logging_mixin.py:112} INFO - [2020-11-05 18:22:08,350] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 12:02:00+00:00: scheduled__2020-11-04T12:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:22:08,358] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:22:08,370] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.441 seconds
[2020-11-05 18:22:21,315] {scheduler_job.py:155} INFO - Started process (PID=3850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:21,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:22:21,320] {logging_mixin.py:112} INFO - [2020-11-05 18:22:21,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:21,469] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:21,497] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:22:21,536] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T13:02:00+00:00: scheduled__2020-11-04T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:21,540] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 13:02:00+00:00: scheduled__2020-11-04T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:21,551] {logging_mixin.py:112} INFO - [2020-11-05 18:22:21,550] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 13:02:00+00:00: scheduled__2020-11-04T13:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:22:21,554] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:22:21,558] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.243 seconds
[2020-11-05 18:22:34,707] {scheduler_job.py:155} INFO - Started process (PID=4013) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:34,718] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:22:34,719] {logging_mixin.py:112} INFO - [2020-11-05 18:22:34,718] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:34,923] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:34,955] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:22:34,994] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T14:02:00+00:00: scheduled__2020-11-04T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:34,999] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 14:02:00+00:00: scheduled__2020-11-04T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:35,016] {logging_mixin.py:112} INFO - [2020-11-05 18:22:35,016] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 14:02:00+00:00: scheduled__2020-11-04T14:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:22:35,019] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:22:35,023] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.317 seconds
[2020-11-05 18:22:47,969] {scheduler_job.py:155} INFO - Started process (PID=4180) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:47,972] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:22:47,976] {logging_mixin.py:112} INFO - [2020-11-05 18:22:47,975] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:48,144] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:22:48,171] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:22:48,214] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T15:02:00+00:00: scheduled__2020-11-04T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:48,220] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 15:02:00+00:00: scheduled__2020-11-04T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:22:48,236] {logging_mixin.py:112} INFO - [2020-11-05 18:22:48,236] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 15:02:00+00:00: scheduled__2020-11-04T15:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:22:48,252] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:22:48,259] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.290 seconds
[2020-11-05 18:23:01,463] {scheduler_job.py:155} INFO - Started process (PID=4341) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:01,466] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:23:01,467] {logging_mixin.py:112} INFO - [2020-11-05 18:23:01,467] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:01,663] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:01,719] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:23:01,762] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T16:02:00+00:00: scheduled__2020-11-04T16:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:01,766] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 16:02:00+00:00: scheduled__2020-11-04T16:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:01,778] {logging_mixin.py:112} INFO - [2020-11-05 18:23:01,778] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 16:02:00+00:00: scheduled__2020-11-04T16:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:23:01,782] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:23:01,786] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.323 seconds
[2020-11-05 18:23:14,785] {scheduler_job.py:155} INFO - Started process (PID=4505) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:14,787] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:23:14,788] {logging_mixin.py:112} INFO - [2020-11-05 18:23:14,788] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:14,998] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:15,033] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:23:15,096] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T17:02:00+00:00: scheduled__2020-11-04T17:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:15,105] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 17:02:00+00:00: scheduled__2020-11-04T17:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:15,133] {logging_mixin.py:112} INFO - [2020-11-05 18:23:15,132] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 17:02:00+00:00: scheduled__2020-11-04T17:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:23:15,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:23:15,145] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.360 seconds
[2020-11-05 18:23:28,066] {scheduler_job.py:155} INFO - Started process (PID=4669) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:28,070] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:23:28,071] {logging_mixin.py:112} INFO - [2020-11-05 18:23:28,070] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:28,226] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:28,255] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:23:28,326] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T18:02:00+00:00: scheduled__2020-11-04T18:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:28,330] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 18:02:00+00:00: scheduled__2020-11-04T18:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:28,340] {logging_mixin.py:112} INFO - [2020-11-05 18:23:28,340] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 18:02:00+00:00: scheduled__2020-11-04T18:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:23:28,344] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:23:28,348] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.282 seconds
[2020-11-05 18:23:41,459] {scheduler_job.py:155} INFO - Started process (PID=4831) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:41,463] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:23:41,463] {logging_mixin.py:112} INFO - [2020-11-05 18:23:41,463] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:41,671] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:41,699] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:23:41,763] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T19:02:00+00:00: scheduled__2020-11-04T19:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:41,773] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 19:02:00+00:00: scheduled__2020-11-04T19:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:41,785] {logging_mixin.py:112} INFO - [2020-11-05 18:23:41,785] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 19:02:00+00:00: scheduled__2020-11-04T19:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:23:41,789] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:23:41,794] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.335 seconds
[2020-11-05 18:23:54,771] {scheduler_job.py:155} INFO - Started process (PID=5003) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:54,773] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:23:54,774] {logging_mixin.py:112} INFO - [2020-11-05 18:23:54,774] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:54,913] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:23:54,938] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:23:54,973] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T20:02:00+00:00: scheduled__2020-11-04T20:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:54,982] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 20:02:00+00:00: scheduled__2020-11-04T20:02:00+00:00, externally triggered: False>
[2020-11-05 18:23:54,992] {logging_mixin.py:112} INFO - [2020-11-05 18:23:54,992] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 20:02:00+00:00: scheduled__2020-11-04T20:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:23:54,997] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:23:55,005] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.234 seconds
[2020-11-05 18:24:08,287] {scheduler_job.py:155} INFO - Started process (PID=5174) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:08,319] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:24:08,320] {logging_mixin.py:112} INFO - [2020-11-05 18:24:08,320] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:08,968] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:09,008] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:24:09,122] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T21:02:00+00:00: scheduled__2020-11-04T21:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:09,137] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 21:02:00+00:00: scheduled__2020-11-04T21:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:09,179] {logging_mixin.py:112} INFO - [2020-11-05 18:24:09,178] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 21:02:00+00:00: scheduled__2020-11-04T21:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:24:09,184] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:24:09,195] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.908 seconds
[2020-11-05 18:24:21,628] {scheduler_job.py:155} INFO - Started process (PID=5340) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:21,632] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:24:21,633] {logging_mixin.py:112} INFO - [2020-11-05 18:24:21,633] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:21,833] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:21,886] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:24:21,951] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T22:02:00+00:00: scheduled__2020-11-04T22:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:21,958] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 22:02:00+00:00: scheduled__2020-11-04T22:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:21,978] {logging_mixin.py:112} INFO - [2020-11-05 18:24:21,978] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 22:02:00+00:00: scheduled__2020-11-04T22:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:24:21,987] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:24:21,993] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.365 seconds
[2020-11-05 18:24:34,944] {scheduler_job.py:155} INFO - Started process (PID=5502) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:34,950] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:24:34,952] {logging_mixin.py:112} INFO - [2020-11-05 18:24:34,951] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:35,293] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:35,329] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:24:35,409] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-04T23:02:00+00:00: scheduled__2020-11-04T23:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:35,415] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-04 23:02:00+00:00: scheduled__2020-11-04T23:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:35,456] {logging_mixin.py:112} INFO - [2020-11-05 18:24:35,455] {dagrun.py:320} INFO - Marking run <DagRun S3_task @ 2020-11-04 23:02:00+00:00: scheduled__2020-11-04T23:02:00+00:00, externally triggered: False> successful
[2020-11-05 18:24:35,472] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:24:35,482] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.538 seconds
[2020-11-05 18:24:48,305] {scheduler_job.py:155} INFO - Started process (PID=5664) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:48,308] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:24:48,309] {logging_mixin.py:112} INFO - [2020-11-05 18:24:48,308] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:48,573] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:24:48,600] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:24:48,648] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:48,652] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:24:48,695] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:24:48,703] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 00:02:00+00:00 [success]> in ORM
[2020-11-05 18:24:48,710] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 00:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:24:48,720] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.415 seconds
[2020-11-05 18:25:01,694] {scheduler_job.py:155} INFO - Started process (PID=5873) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:01,698] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:25:01,698] {logging_mixin.py:112} INFO - [2020-11-05 18:25:01,698] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:01,869] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:01,906] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:25:01,953] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:01,957] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:01,986] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:02,050] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:25:02,058] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 01:02:00+00:00 [success]> in ORM
[2020-11-05 18:25:02,069] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 01:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:25:02,080] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.386 seconds
[2020-11-05 18:25:15,135] {scheduler_job.py:155} INFO - Started process (PID=6080) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:15,139] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:25:15,140] {logging_mixin.py:112} INFO - [2020-11-05 18:25:15,139] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:15,273] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:15,298] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:25:15,346] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:15,353] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:15,389] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:15,446] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:15,522] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:25:15,528] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 02:02:00+00:00 [success]> in ORM
[2020-11-05 18:25:15,534] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 02:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:25:15,547] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.412 seconds
[2020-11-05 18:25:28,448] {scheduler_job.py:155} INFO - Started process (PID=6299) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:28,451] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:25:28,451] {logging_mixin.py:112} INFO - [2020-11-05 18:25:28,451] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:28,609] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:28,640] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:25:28,694] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:28,698] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:28,730] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:28,758] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:28,786] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:28,876] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:25:28,887] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 03:02:00+00:00 [success]> in ORM
[2020-11-05 18:25:28,895] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 03:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:25:28,913] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.465 seconds
[2020-11-05 18:25:41,906] {scheduler_job.py:155} INFO - Started process (PID=6524) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:41,918] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:25:41,935] {logging_mixin.py:112} INFO - [2020-11-05 18:25:41,935] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:42,381] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:42,409] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:25:42,460] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:42,465] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:42,496] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:42,525] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:42,554] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:42,589] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:42,825] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:25:42,833] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 04:02:00+00:00 [success]> in ORM
[2020-11-05 18:25:42,847] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 04:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:25:42,880] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.974 seconds
[2020-11-05 18:25:56,211] {scheduler_job.py:155} INFO - Started process (PID=6751) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:56,216] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:25:56,216] {logging_mixin.py:112} INFO - [2020-11-05 18:25:56,216] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:56,432] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:25:56,465] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:25:56,520] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,525] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,554] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,581] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,619] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,662] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,700] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:25:56,838] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:25:56,844] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 05:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:25:56,851] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 05:02:00+00:00 [success]> in ORM
[2020-11-05 18:25:56,860] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.649 seconds
[2020-11-05 18:26:08,524] {scheduler_job.py:155} INFO - Started process (PID=6971) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:08,529] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:26:08,530] {logging_mixin.py:112} INFO - [2020-11-05 18:26:08,530] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:08,839] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:08,870] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:26:08,985] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,006] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,110] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,195] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,306] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,441] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,477] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:09,567] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:10,196] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:26:10,203] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 06:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:26:10,216] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 06:02:00+00:00 [success]> in ORM
[2020-11-05 18:26:10,232] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.707 seconds
[2020-11-05 18:26:23,011] {scheduler_job.py:155} INFO - Started process (PID=7212) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:23,020] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:26:23,025] {logging_mixin.py:112} INFO - [2020-11-05 18:26:23,021] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:23,175] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:23,200] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:26:23,264] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,268] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,298] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,378] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,409] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,442] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,465] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:23,614] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:26:23,621] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 07:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:26:23,627] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 07:02:00+00:00 [success]> in ORM
[2020-11-05 18:26:23,637] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.626 seconds
[2020-11-05 18:26:35,358] {scheduler_job.py:155} INFO - Started process (PID=7443) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:35,365] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:26:35,366] {logging_mixin.py:112} INFO - [2020-11-05 18:26:35,365] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:35,520] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:35,539] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:26:35,577] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,582] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,663] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,697] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,725] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,741] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,756] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,774] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,791] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:35,949] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:26:35,957] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 08:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:26:35,967] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 08:02:00+00:00 [success]> in ORM
[2020-11-05 18:26:35,979] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.621 seconds
[2020-11-05 18:26:48,789] {scheduler_job.py:155} INFO - Started process (PID=7699) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:48,792] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:26:48,793] {logging_mixin.py:112} INFO - [2020-11-05 18:26:48,793] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:48,948] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:26:48,972] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:26:49,024] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,028] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,065] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,095] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,127] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,160] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,184] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,200] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,215] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,230] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:26:49,479] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:26:49,485] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 09:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:26:49,492] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 09:02:00+00:00 [success]> in ORM
[2020-11-05 18:26:49,503] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.714 seconds
[2020-11-05 18:27:02,074] {scheduler_job.py:155} INFO - Started process (PID=7957) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:02,088] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:27:02,089] {logging_mixin.py:112} INFO - [2020-11-05 18:27:02,089] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:02,252] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:02,275] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:27:02,319] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,385] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,418] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,451] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,489] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,503] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,529] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,552] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,569] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,586] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,601] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:02,836] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:27:02,843] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 10:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:27:02,850] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 10:02:00+00:00 [success]> in ORM
[2020-11-05 18:27:02,859] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.785 seconds
[2020-11-05 18:27:15,452] {scheduler_job.py:155} INFO - Started process (PID=8247) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:15,463] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:27:15,464] {logging_mixin.py:112} INFO - [2020-11-05 18:27:15,464] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:15,689] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:15,749] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:27:15,832] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:15,846] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:15,894] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:15,941] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:15,990] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,030] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,047] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,077] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,100] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,117] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,136] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,152] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,170] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:16,456] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:27:16,467] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 11:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:27:16,480] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 11:02:00+00:00 [success]> in ORM
[2020-11-05 18:27:16,499] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.047 seconds
[2020-11-05 18:27:30,075] {scheduler_job.py:155} INFO - Started process (PID=8540) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:30,079] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:27:30,080] {logging_mixin.py:112} INFO - [2020-11-05 18:27:30,079] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:30,222] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:30,249] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:27:30,300] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,306] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,364] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,498] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,600] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,645] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,671] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,711] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,734] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,758] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,777] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,807] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,827] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:30,843] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:31,221] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:27:31,229] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 12:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:27:31,238] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 12:02:00+00:00 [success]> in ORM
[2020-11-05 18:27:31,250] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.175 seconds
[2020-11-05 18:27:43,796] {scheduler_job.py:155} INFO - Started process (PID=8836) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:43,818] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:27:43,819] {logging_mixin.py:112} INFO - [2020-11-05 18:27:43,818] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:44,077] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:44,124] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:27:44,202] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,209] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,276] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,306] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,340] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,369] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,395] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,418] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,438] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,466] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,487] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,507] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,523] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,542] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:44,973] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:27:44,985] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 13:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:27:44,999] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 13:02:00+00:00 [success]> in ORM
[2020-11-05 18:27:45,015] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.219 seconds
[2020-11-05 18:27:57,188] {scheduler_job.py:155} INFO - Started process (PID=9115) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:57,198] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:27:57,199] {logging_mixin.py:112} INFO - [2020-11-05 18:27:57,198] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:57,408] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:27:57,438] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:27:57,523] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,534] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,588] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,655] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,800] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,874] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,916] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,943] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:57,982] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,010] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,062] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,090] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,132] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,160] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,194] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,276] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:27:58,928] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:27:58,940] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 14:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:27:58,972] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 14:02:00+00:00 [success]> in ORM
[2020-11-05 18:27:59,512] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.323 seconds
[2020-11-05 18:28:12,398] {scheduler_job.py:155} INFO - Started process (PID=9395) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:12,454] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:28:12,455] {logging_mixin.py:112} INFO - [2020-11-05 18:28:12,455] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:14,354] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:14,655] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:28:14,977] {scheduler_job.py:1315} INFO - Created <DagRun S3_task @ 2020-11-05T15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,015] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,105] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,355] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,455] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,495] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,519] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,544] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,584] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,668] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,787] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,836] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,930] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:15,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:16,010] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:16,038] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:16,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:17,349] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:28:17,375] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.get_new_json 2020-11-05 15:02:00+00:00 [scheduled]> in ORM
[2020-11-05 18:28:17,386] {scheduler_job.py:1668} INFO - Creating / updating <TaskInstance: S3_task.dummy_start 2020-11-05 15:02:00+00:00 [success]> in ORM
[2020-11-05 18:28:17,865] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 5.467 seconds
[2020-11-05 18:28:31,504] {scheduler_job.py:155} INFO - Started process (PID=9743) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:31,512] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:28:31,530] {logging_mixin.py:112} INFO - [2020-11-05 18:28:31,529] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:32,310] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:32,422] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:28:32,482] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:32,860] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,064] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,269] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,358] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,484] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,594] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,806] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,875] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,920] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:33,997] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:34,060] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:34,166] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:34,237] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:34,282] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:34,771] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:28:34,777] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.273 seconds
[2020-11-05 18:28:45,887] {scheduler_job.py:155} INFO - Started process (PID=10068) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:45,890] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:28:45,891] {logging_mixin.py:112} INFO - [2020-11-05 18:28:45,891] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:46,209] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:46,240] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:28:46,258] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,303] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,336] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,369] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,405] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,457] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,515] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,555] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,592] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,622] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,654] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,698] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,736] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,768] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,808] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:46,888] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:47,365] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:28:47,369] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.482 seconds
[2020-11-05 18:28:59,203] {scheduler_job.py:155} INFO - Started process (PID=10357) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:59,214] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:28:59,214] {logging_mixin.py:112} INFO - [2020-11-05 18:28:59,214] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:59,373] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:28:59,395] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:28:59,425] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,470] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,505] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,535] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,573] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,614] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,643] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,671] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,719] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,752] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,786] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,827] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,864] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,900] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,929] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:28:59,962] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:00,441] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:29:00,448] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.245 seconds
[2020-11-05 18:29:12,543] {scheduler_job.py:155} INFO - Started process (PID=10643) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:12,548] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:29:12,549] {logging_mixin.py:112} INFO - [2020-11-05 18:29:12,549] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:12,868] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:12,923] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:29:12,953] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,099] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,157] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,236] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,301] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,416] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,593] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,791] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,859] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:13,934] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,005] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,050] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,088] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,126] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,182] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,228] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:14,591] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:29:14,595] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.052 seconds
[2020-11-05 18:29:26,026] {scheduler_job.py:155} INFO - Started process (PID=10933) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:26,029] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:29:26,030] {logging_mixin.py:112} INFO - [2020-11-05 18:29:26,030] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:26,177] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:26,200] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:29:26,214] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,269] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,311] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,342] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,377] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,416] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,445] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,483] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,511] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,545] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,580] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,615] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,689] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,730] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:26,759] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:27,087] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:29:27,091] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.065 seconds
[2020-11-05 18:29:39,284] {scheduler_job.py:155} INFO - Started process (PID=11223) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:39,292] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:29:39,296] {logging_mixin.py:112} INFO - [2020-11-05 18:29:39,295] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:39,524] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:39,553] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:29:39,572] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,633] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,672] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,722] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,770] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,815] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,874] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:39,987] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,061] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,108] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,166] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,213] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,264] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,337] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,387] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:40,730] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:29:40,734] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.450 seconds
[2020-11-05 18:29:52,573] {scheduler_job.py:155} INFO - Started process (PID=11507) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:52,578] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:29:52,579] {logging_mixin.py:112} INFO - [2020-11-05 18:29:52,579] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:52,794] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:29:52,822] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:29:52,838] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:52,873] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:52,899] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:52,925] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:52,959] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:52,993] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,030] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,072] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,104] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,137] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,171] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,199] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,230] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,261] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,294] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,323] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:29:53,683] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:29:53,690] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.117 seconds
[2020-11-05 18:30:05,904] {scheduler_job.py:155} INFO - Started process (PID=11791) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:05,907] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:30:05,908] {logging_mixin.py:112} INFO - [2020-11-05 18:30:05,908] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:06,103] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:06,135] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:30:06,158] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,202] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,230] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,255] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,289] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,329] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,361] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,396] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,440] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,475] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,508] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,541] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,571] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,606] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,634] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:06,666] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:07,157] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:30:07,181] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.276 seconds
[2020-11-05 18:30:19,190] {scheduler_job.py:155} INFO - Started process (PID=12087) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:19,193] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:30:19,194] {logging_mixin.py:112} INFO - [2020-11-05 18:30:19,194] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:19,306] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:19,326] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:30:19,339] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,380] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,410] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,491] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,531] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,573] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,599] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,626] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,665] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,714] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,762] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,802] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,847] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,890] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:19,921] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:20,407] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:30:20,417] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.227 seconds
[2020-11-05 18:30:32,481] {scheduler_job.py:155} INFO - Started process (PID=12377) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:32,484] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:30:32,485] {logging_mixin.py:112} INFO - [2020-11-05 18:30:32,484] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:32,619] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:32,649] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:30:32,666] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,703] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,739] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,767] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,804] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,830] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,856] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,884] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,919] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,954] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:32,987] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:33,020] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:33,063] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:33,101] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:33,147] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:33,192] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:33,554] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:30:33,560] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.079 seconds
[2020-11-05 18:30:45,811] {scheduler_job.py:155} INFO - Started process (PID=12664) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:45,816] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:30:45,825] {logging_mixin.py:112} INFO - [2020-11-05 18:30:45,825] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:46,063] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:46,173] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:30:46,203] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,253] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,322] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,387] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,424] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,584] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,614] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,657] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,699] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,731] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,773] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,830] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,910] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:46,976] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:47,455] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:30:47,460] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.649 seconds
[2020-11-05 18:30:59,100] {scheduler_job.py:155} INFO - Started process (PID=12956) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:59,104] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:30:59,105] {logging_mixin.py:112} INFO - [2020-11-05 18:30:59,105] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:59,279] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:30:59,318] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:30:59,337] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,373] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,434] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,469] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,503] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,539] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,613] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,641] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,682] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,730] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,765] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,824] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,863] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:30:59,907] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:00,293] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:31:00,296] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.196 seconds
[2020-11-05 18:31:12,359] {scheduler_job.py:155} INFO - Started process (PID=13237) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:12,363] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:31:12,363] {logging_mixin.py:112} INFO - [2020-11-05 18:31:12,363] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:12,519] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:12,548] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:31:12,565] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,600] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,628] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,696] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,722] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,749] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,775] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,806] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,840] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,876] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,905] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:12,948] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:13,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:13,027] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:13,060] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:13,351] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:31:13,355] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.996 seconds
[2020-11-05 18:31:24,653] {scheduler_job.py:155} INFO - Started process (PID=13533) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:24,658] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:31:24,658] {logging_mixin.py:112} INFO - [2020-11-05 18:31:24,658] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:24,853] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:24,879] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:31:24,896] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:24,934] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:24,965] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:24,995] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,026] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,062] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,089] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,135] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,178] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,243] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,285] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,344] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,374] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,411] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,501] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:25,547] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:26,002] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:31:26,009] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.355 seconds
[2020-11-05 18:31:39,038] {scheduler_job.py:155} INFO - Started process (PID=13843) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:39,042] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:31:39,043] {logging_mixin.py:112} INFO - [2020-11-05 18:31:39,043] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:39,240] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:39,273] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:31:39,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,326] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,376] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,455] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,501] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,538] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,578] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,607] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,636] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,671] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,700] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,738] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,782] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,822] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:39,883] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:40,175] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:31:40,182] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.144 seconds
[2020-11-05 18:31:52,335] {scheduler_job.py:155} INFO - Started process (PID=14127) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:52,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:31:52,343] {logging_mixin.py:112} INFO - [2020-11-05 18:31:52,343] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:52,499] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:31:52,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:31:52,571] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,633] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,672] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,758] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,802] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,846] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,880] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:52,943] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,037] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,192] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,239] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,281] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,313] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,344] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,382] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:31:53,677] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:31:53,682] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.347 seconds
[2020-11-05 18:32:05,648] {scheduler_job.py:155} INFO - Started process (PID=14414) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:05,651] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:32:05,652] {logging_mixin.py:112} INFO - [2020-11-05 18:32:05,652] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:05,771] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:05,797] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:32:05,821] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:05,859] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:05,890] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:05,928] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:05,959] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:05,983] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,009] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,037] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,090] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,126] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,161] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,197] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,226] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,262] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,292] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,321] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:06,605] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:32:06,609] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.962 seconds
[2020-11-05 18:32:17,940] {scheduler_job.py:155} INFO - Started process (PID=14684) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:17,945] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:32:17,946] {logging_mixin.py:112} INFO - [2020-11-05 18:32:17,946] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:18,098] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:18,124] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:32:18,153] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,188] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,268] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,328] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,367] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,395] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,433] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,462] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,489] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,519] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,560] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,697] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,735] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:18,774] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:19,174] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:32:19,199] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.259 seconds
[2020-11-05 18:32:32,244] {scheduler_job.py:155} INFO - Started process (PID=15001) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:32,249] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:32:32,250] {logging_mixin.py:112} INFO - [2020-11-05 18:32:32,250] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:32,404] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:32,425] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:32:32,437] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,466] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,492] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,526] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,612] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,640] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,687] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,723] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,749] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,773] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,811] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,843] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,878] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,913] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:32,944] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:33,327] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:32:33,343] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.100 seconds
[2020-11-05 18:32:45,499] {scheduler_job.py:155} INFO - Started process (PID=15284) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:45,507] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:32:45,507] {logging_mixin.py:112} INFO - [2020-11-05 18:32:45,507] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:45,652] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:45,679] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:32:45,700] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,785] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,819] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,848] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,882] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,913] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,948] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:45,984] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,017] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,049] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,083] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,138] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,176] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,215] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,271] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:46,603] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:32:46,607] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.108 seconds
[2020-11-05 18:32:58,943] {scheduler_job.py:155} INFO - Started process (PID=15575) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:58,947] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:32:58,947] {logging_mixin.py:112} INFO - [2020-11-05 18:32:58,947] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:59,131] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:32:59,153] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:32:59,166] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,209] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,238] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,267] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,329] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,380] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,421] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,478] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,544] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,585] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,632] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,720] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,796] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,853] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:32:59,915] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:00,477] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:33:00,483] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.541 seconds
[2020-11-05 18:33:12,263] {scheduler_job.py:155} INFO - Started process (PID=15862) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:12,269] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:33:12,269] {logging_mixin.py:112} INFO - [2020-11-05 18:33:12,269] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:12,456] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:12,479] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:33:12,494] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,546] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,580] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,653] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,681] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,708] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,741] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,775] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,814] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,862] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,902] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,948] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:12,994] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:13,038] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:13,099] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:13,436] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:33:13,442] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.178 seconds
[2020-11-05 18:33:25,584] {scheduler_job.py:155} INFO - Started process (PID=16149) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:25,589] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:33:25,590] {logging_mixin.py:112} INFO - [2020-11-05 18:33:25,589] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:25,724] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:25,750] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:33:25,764] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:25,801] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:25,836] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:25,869] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:25,905] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:25,946] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:25,973] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,085] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,147] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,179] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,218] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,247] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,274] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,300] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,326] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:26,673] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:33:26,678] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.094 seconds
[2020-11-05 18:33:38,866] {scheduler_job.py:155} INFO - Started process (PID=16438) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:38,871] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:33:38,872] {logging_mixin.py:112} INFO - [2020-11-05 18:33:38,872] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:39,022] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:39,046] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:33:39,059] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,116] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,145] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,178] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,210] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,245] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,282] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,310] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,338] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,367] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,393] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,437] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,466] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,510] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,544] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:39,861] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:33:39,870] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.004 seconds
[2020-11-05 18:33:52,152] {scheduler_job.py:155} INFO - Started process (PID=16731) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:52,155] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:33:52,156] {logging_mixin.py:112} INFO - [2020-11-05 18:33:52,156] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:52,427] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:33:52,449] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:33:52,473] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,538] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,625] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,689] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,721] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,750] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,786] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,828] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,857] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,898] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,934] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:52,981] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:53,018] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:33:53,721] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:33:53,727] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.575 seconds
[2020-11-05 18:34:05,549] {scheduler_job.py:155} INFO - Started process (PID=17016) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:05,554] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:34:05,555] {logging_mixin.py:112} INFO - [2020-11-05 18:34:05,554] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:05,744] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:05,783] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:34:05,806] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:05,859] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:05,913] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:05,959] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:05,992] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,028] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,063] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,101] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,143] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,192] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,296] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,336] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,371] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,416] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,464] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,504] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:06,890] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:34:06,895] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.346 seconds
[2020-11-05 18:34:18,869] {scheduler_job.py:155} INFO - Started process (PID=17310) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:18,874] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:34:18,875] {logging_mixin.py:112} INFO - [2020-11-05 18:34:18,874] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:19,032] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:19,059] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:34:19,072] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,129] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,163] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,219] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,258] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,333] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,367] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,395] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,427] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,456] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,484] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,516] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,552] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,586] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:19,893] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:34:19,899] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.030 seconds
[2020-11-05 18:34:32,180] {scheduler_job.py:155} INFO - Started process (PID=17604) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:32,185] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:34:32,186] {logging_mixin.py:112} INFO - [2020-11-05 18:34:32,186] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:32,623] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:32,681] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:34:32,706] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:32,778] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:32,839] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:32,895] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:32,951] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:32,993] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,031] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,070] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,107] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,145] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,242] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,280] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,316] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,342] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,371] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,423] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:33,778] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:34:33,786] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.606 seconds
[2020-11-05 18:34:45,530] {scheduler_job.py:155} INFO - Started process (PID=17943) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:45,534] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:34:45,535] {logging_mixin.py:112} INFO - [2020-11-05 18:34:45,535] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:45,687] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:45,709] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:34:45,724] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,764] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,792] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,819] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,849] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,891] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,920] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,957] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:45,993] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,021] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,056] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,085] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,112] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,139] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,166] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,202] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:46,520] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:34:46,526] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.996 seconds
[2020-11-05 18:34:57,826] {scheduler_job.py:155} INFO - Started process (PID=18218) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:57,830] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:34:57,830] {logging_mixin.py:112} INFO - [2020-11-05 18:34:57,830] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:57,943] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:34:57,967] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:34:57,990] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,030] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,059] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,113] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,141] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,172] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,203] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,237] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,279] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,332] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,360] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,391] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,423] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,453] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,479] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:34:58,782] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:34:58,786] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.960 seconds
[2020-11-05 18:35:11,119] {scheduler_job.py:155} INFO - Started process (PID=18510) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:11,125] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:35:11,126] {logging_mixin.py:112} INFO - [2020-11-05 18:35:11,125] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:11,305] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:11,334] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:35:11,352] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,396] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,422] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,453] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,506] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,553] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,592] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,635] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,677] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,706] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,740] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,773] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,811] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,839] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,870] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:11,925] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:12,355] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:35:12,363] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.244 seconds
[2020-11-05 18:35:25,484] {scheduler_job.py:155} INFO - Started process (PID=18823) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:25,491] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:35:25,492] {logging_mixin.py:112} INFO - [2020-11-05 18:35:25,492] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:25,770] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:25,863] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:35:25,885] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:25,939] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:25,987] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,028] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,082] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,126] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,167] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,218] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,250] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,293] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,341] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,472] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,527] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,709] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:26,855] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:27,392] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:35:27,398] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.914 seconds
[2020-11-05 18:35:38,920] {scheduler_job.py:155} INFO - Started process (PID=19107) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:38,929] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:35:38,929] {logging_mixin.py:112} INFO - [2020-11-05 18:35:38,929] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:39,094] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:39,118] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:35:39,149] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,191] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,230] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,276] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,390] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,433] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,477] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,517] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,555] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,592] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,618] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,650] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,680] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,715] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:39,751] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:40,134] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:35:40,141] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.221 seconds
[2020-11-05 18:35:52,179] {scheduler_job.py:155} INFO - Started process (PID=19397) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:52,182] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:35:52,183] {logging_mixin.py:112} INFO - [2020-11-05 18:35:52,183] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:52,348] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:35:52,369] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:35:52,383] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,424] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,456] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,489] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,539] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,569] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,604] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,630] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,656] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,684] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,717] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,747] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,781] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,832] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,871] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:52,914] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:35:53,321] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:35:53,326] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.148 seconds
[2020-11-05 18:36:05,592] {scheduler_job.py:155} INFO - Started process (PID=19689) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:05,596] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:36:05,598] {logging_mixin.py:112} INFO - [2020-11-05 18:36:05,597] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:05,824] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:05,867] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:36:05,887] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:05,926] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:05,966] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,024] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,076] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,130] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,184] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,235] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,289] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,346] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,405] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,462] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,516] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,572] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,640] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:06,754] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:07,199] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:36:07,205] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.613 seconds
[2020-11-05 18:36:18,937] {scheduler_job.py:155} INFO - Started process (PID=19970) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:18,941] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:36:18,942] {logging_mixin.py:112} INFO - [2020-11-05 18:36:18,942] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:19,131] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:19,152] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:36:19,170] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,222] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,265] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,319] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,378] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,416] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,452] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,489] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,530] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,583] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,627] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,665] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,692] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,719] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,746] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:19,802] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:20,107] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:36:20,113] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.176 seconds
[2020-11-05 18:36:32,250] {scheduler_job.py:155} INFO - Started process (PID=20260) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:32,260] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:36:32,264] {logging_mixin.py:112} INFO - [2020-11-05 18:36:32,264] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:32,429] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:32,448] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:36:32,461] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,490] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,517] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,553] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,586] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,615] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,680] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,707] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,737] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,766] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,797] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,827] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,860] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,894] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:32,924] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:33,237] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:36:33,241] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.991 seconds
[2020-11-05 18:36:44,527] {scheduler_job.py:155} INFO - Started process (PID=20529) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:44,532] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:36:44,533] {logging_mixin.py:112} INFO - [2020-11-05 18:36:44,532] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:44,802] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:44,843] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:36:44,863] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:44,932] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:44,985] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,031] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,065] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,135] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,170] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,208] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,246] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,283] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,311] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,337] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,366] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,429] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:45,745] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:36:45,749] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.221 seconds
[2020-11-05 18:36:59,029] {scheduler_job.py:155} INFO - Started process (PID=20850) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:59,047] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:36:59,048] {logging_mixin.py:112} INFO - [2020-11-05 18:36:59,048] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:59,387] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:36:59,430] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:36:59,448] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:59,492] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:59,651] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:59,720] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:36:59,920] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,059] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,186] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,409] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,694] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,798] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,857] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:00,923] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:01,021] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:01,115] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:01,174] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:01,453] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:02,520] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:37:02,529] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.500 seconds
[2020-11-05 18:37:13,495] {scheduler_job.py:155} INFO - Started process (PID=21157) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:13,498] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:37:13,499] {logging_mixin.py:112} INFO - [2020-11-05 18:37:13,499] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:13,653] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:13,679] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:37:13,693] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,732] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,759] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,792] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,827] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,861] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,894] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,923] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,948] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:13,999] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:14,030] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:14,065] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:14,094] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:14,134] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:14,166] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:14,444] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:37:14,448] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 0.953 seconds
[2020-11-05 18:37:25,908] {scheduler_job.py:155} INFO - Started process (PID=21428) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:25,934] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:37:25,949] {logging_mixin.py:112} INFO - [2020-11-05 18:37:25,945] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:27,454] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:27,578] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:37:27,687] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:27,813] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:27,872] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:27,944] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,045] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,216] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,443] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,498] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,554] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,609] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,655] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,700] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,737] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,780] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,818] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:28,851] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:29,480] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:37:29,489] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 3.580 seconds
[2020-11-05 18:37:41,281] {scheduler_job.py:155} INFO - Started process (PID=21741) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:41,285] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:37:41,285] {logging_mixin.py:112} INFO - [2020-11-05 18:37:41,285] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:41,579] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:41,630] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:37:41,668] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:41,724] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:41,779] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:41,833] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:41,883] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,027] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,090] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,133] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,163] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,242] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,449] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,534] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,601] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,660] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:42,715] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:43,251] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:37:43,257] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.976 seconds
[2020-11-05 18:37:54,536] {scheduler_job.py:155} INFO - Started process (PID=22018) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:54,541] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:37:54,542] {logging_mixin.py:112} INFO - [2020-11-05 18:37:54,542] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:54,675] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:37:54,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:37:54,723] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:54,788] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:54,852] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:54,888] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:54,926] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:54,977] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,024] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,067] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,095] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,127] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,175] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,218] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,278] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,316] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,376] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:37:55,699] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:37:55,702] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.167 seconds
[2020-11-05 18:38:07,801] {scheduler_job.py:155} INFO - Started process (PID=22307) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:07,805] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:38:07,806] {logging_mixin.py:112} INFO - [2020-11-05 18:38:07,805] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:07,958] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:07,988] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:38:08,002] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,042] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,070] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,100] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,130] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,158] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,185] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,215] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,252] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,307] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,355] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,400] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,491] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,536] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,568] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,604] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:08,924] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:38:08,929] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.128 seconds
[2020-11-05 18:38:21,049] {scheduler_job.py:155} INFO - Started process (PID=22610) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:21,053] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:38:21,054] {logging_mixin.py:112} INFO - [2020-11-05 18:38:21,054] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:21,183] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:21,215] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:38:21,235] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,273] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,309] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,344] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,381] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,412] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,448] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,487] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,523] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,552] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,588] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,632] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,689] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,730] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:21,765] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:22,051] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:38:22,055] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.006 seconds
[2020-11-05 18:38:34,335] {scheduler_job.py:155} INFO - Started process (PID=22894) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:34,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:38:34,343] {logging_mixin.py:112} INFO - [2020-11-05 18:38:34,343] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:34,584] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:34,611] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:38:34,627] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,674] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,718] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,766] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,799] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,835] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,884] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,940] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:34,978] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,026] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,054] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,092] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,130] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,161] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,205] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,242] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:35,611] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:38:35,615] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.281 seconds
[2020-11-05 18:38:47,564] {scheduler_job.py:155} INFO - Started process (PID=23178) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:47,569] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:38:47,569] {logging_mixin.py:112} INFO - [2020-11-05 18:38:47,569] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:47,866] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:38:47,895] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:38:47,917] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:47,998] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,034] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,061] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,087] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,116] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,144] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,190] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,232] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,269] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,311] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,340] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,368] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,397] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,425] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,451] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:38:48,870] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:38:48,876] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.313 seconds
[2020-11-05 18:39:00,871] {scheduler_job.py:155} INFO - Started process (PID=23467) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:00,877] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:39:00,879] {logging_mixin.py:112} INFO - [2020-11-05 18:39:00,878] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:01,145] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:01,196] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:39:01,211] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,248] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,284] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,317] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,356] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,386] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,434] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,477] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,515] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,544] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,603] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,636] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,667] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,710] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:01,750] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:02,142] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:39:02,154] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.283 seconds
[2020-11-05 18:39:14,081] {scheduler_job.py:155} INFO - Started process (PID=23758) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:14,093] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:39:14,094] {logging_mixin.py:112} INFO - [2020-11-05 18:39:14,094] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:14,289] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:14,323] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:39:14,338] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,391] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,425] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,456] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,483] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,513] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,541] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,574] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,603] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,631] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,659] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,691] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,717] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,771] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:14,843] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:15,348] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:39:15,353] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.272 seconds
[2020-11-05 18:39:27,439] {scheduler_job.py:155} INFO - Started process (PID=24036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:27,443] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:39:27,444] {logging_mixin.py:112} INFO - [2020-11-05 18:39:27,443] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:27,595] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:27,622] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:39:27,639] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,682] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,744] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,781] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,807] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,835] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,867] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,919] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:27,977] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,018] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,050] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,076] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,109] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,151] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,183] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,217] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:28,578] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:39:28,583] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.144 seconds
[2020-11-05 18:39:40,671] {scheduler_job.py:155} INFO - Started process (PID=24333) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:40,675] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:39:40,675] {logging_mixin.py:112} INFO - [2020-11-05 18:39:40,675] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:40,810] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:40,839] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:39:40,860] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:40,902] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:40,941] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,050] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,119] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,254] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,290] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,341] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,402] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,465] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,531] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,564] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,595] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,667] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,718] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:41,802] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:42,482] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:39:42,491] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.820 seconds
[2020-11-05 18:39:54,023] {scheduler_job.py:155} INFO - Started process (PID=24619) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:54,026] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:39:54,027] {logging_mixin.py:112} INFO - [2020-11-05 18:39:54,027] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:54,195] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:39:54,217] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:39:54,231] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,321] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,354] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,389] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,425] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,458] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,486] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,518] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,547] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,613] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,646] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,680] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,723] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:54,759] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:39:55,140] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:39:55,144] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.121 seconds
[2020-11-05 18:40:07,320] {scheduler_job.py:155} INFO - Started process (PID=24900) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:07,343] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:40:07,345] {logging_mixin.py:112} INFO - [2020-11-05 18:40:07,345] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:07,505] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:07,534] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:40:07,548] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,582] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,619] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,654] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,689] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,721] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,748] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,780] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,829] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,867] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,914] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:07,950] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:08,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:08,049] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:08,088] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:08,126] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:08,552] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:40:08,556] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.236 seconds
[2020-11-05 18:40:20,579] {scheduler_job.py:155} INFO - Started process (PID=25182) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:20,584] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:40:20,585] {logging_mixin.py:112} INFO - [2020-11-05 18:40:20,585] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:20,771] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:20,858] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:40:20,879] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:20,916] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:20,941] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:20,968] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:20,998] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,041] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,079] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,115] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,161] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,191] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,223] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,254] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,319] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,372] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:21,732] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:40:21,736] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.157 seconds
[2020-11-05 18:40:33,842] {scheduler_job.py:155} INFO - Started process (PID=25469) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:33,845] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:40:33,845] {logging_mixin.py:112} INFO - [2020-11-05 18:40:33,845] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:33,992] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:34,064] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:40:34,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,111] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,141] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,168] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,196] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,241] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,304] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,335] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,369] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,403] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,431] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,460] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,488] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,525] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,566] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,602] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:34,937] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:40:34,941] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.099 seconds
[2020-11-05 18:40:47,084] {scheduler_job.py:155} INFO - Started process (PID=25754) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:47,090] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:40:47,091] {logging_mixin.py:112} INFO - [2020-11-05 18:40:47,091] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:47,225] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:40:47,259] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:40:47,280] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,347] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,399] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,478] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,530] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,589] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,707] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,743] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,769] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,794] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,825] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,857] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,889] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,926] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:47,961] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:40:48,313] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:40:48,318] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.233 seconds
[2020-11-05 18:41:00,338] {scheduler_job.py:155} INFO - Started process (PID=26033) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:00,345] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:41:00,346] {logging_mixin.py:112} INFO - [2020-11-05 18:41:00,345] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:00,533] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:00,566] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:41:00,626] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:00,681] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:00,726] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:00,780] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:00,832] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:00,891] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:00,963] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,026] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,072] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,109] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,154] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,194] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,253] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,313] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,446] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,531] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:01,827] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:41:01,833] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.494 seconds
[2020-11-05 18:41:13,564] {scheduler_job.py:155} INFO - Started process (PID=26314) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:13,572] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:41:13,574] {logging_mixin.py:112} INFO - [2020-11-05 18:41:13,573] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:13,753] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:13,790] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:41:13,812] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:13,852] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:13,902] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:13,939] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:13,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,030] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,057] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,084] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,118] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,154] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,216] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,330] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,414] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,468] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,524] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:14,936] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:41:14,940] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.376 seconds
[2020-11-05 18:41:26,845] {scheduler_job.py:155} INFO - Started process (PID=26610) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:26,855] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:41:26,856] {logging_mixin.py:112} INFO - [2020-11-05 18:41:26,856] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:27,217] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:27,258] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:41:27,281] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,344] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,401] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,446] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,479] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,520] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,560] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,597] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,642] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,705] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,745] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,790] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,821] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,855] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,898] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:27,959] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:28,277] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:41:28,282] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.437 seconds
[2020-11-05 18:41:40,135] {scheduler_job.py:155} INFO - Started process (PID=26888) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:40,146] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:41:40,147] {logging_mixin.py:112} INFO - [2020-11-05 18:41:40,146] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:40,317] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:40,337] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:41:40,350] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,388] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,425] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,457] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,498] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,541] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,568] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,597] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,623] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,651] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,722] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,754] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,805] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,834] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,865] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:40,893] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:41,216] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:41:41,223] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.088 seconds
[2020-11-05 18:41:53,395] {scheduler_job.py:155} INFO - Started process (PID=27171) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:53,404] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:41:53,405] {logging_mixin.py:112} INFO - [2020-11-05 18:41:53,405] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:53,568] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:41:53,591] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:41:53,605] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,636] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,664] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,697] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,749] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,784] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,825] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,855] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,881] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,908] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,937] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:53,978] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:54,006] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:54,040] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:54,076] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:54,102] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:41:54,445] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:41:54,462] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.066 seconds
[2020-11-05 18:42:06,596] {scheduler_job.py:155} INFO - Started process (PID=27454) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:06,601] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:42:06,602] {logging_mixin.py:112} INFO - [2020-11-05 18:42:06,602] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:06,815] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:06,836] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:42:06,850] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:06,886] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:06,938] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:06,973] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,010] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,041] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,078] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,116] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,149] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,178] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,213] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,245] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,280] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,308] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,339] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,372] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:07,646] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:42:07,650] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.054 seconds
[2020-11-05 18:42:19,868] {scheduler_job.py:155} INFO - Started process (PID=27738) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:19,884] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:42:19,886] {logging_mixin.py:112} INFO - [2020-11-05 18:42:19,885] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:20,083] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:20,122] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:42:20,161] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,228] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,263] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,291] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,319] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,360] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,401] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,465] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,503] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,536] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,586] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,611] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,637] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,670] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,703] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:20,742] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:21,236] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:42:21,249] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.382 seconds
[2020-11-05 18:42:33,080] {scheduler_job.py:155} INFO - Started process (PID=28021) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:33,085] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:42:33,086] {logging_mixin.py:112} INFO - [2020-11-05 18:42:33,085] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:33,242] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:33,271] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:42:33,287] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,325] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,354] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,386] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,531] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,570] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,614] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,649] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,686] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,729] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,759] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,797] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,836] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,863] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:33,899] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:34,214] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:42:34,219] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.139 seconds
[2020-11-05 18:42:46,324] {scheduler_job.py:155} INFO - Started process (PID=28306) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:46,328] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:42:46,329] {logging_mixin.py:112} INFO - [2020-11-05 18:42:46,328] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:46,494] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:46,537] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:42:46,558] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,605] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,666] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,724] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,788] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,848] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,896] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:46,957] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,037] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,085] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,145] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,187] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,218] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,258] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,303] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:42:47,726] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:42:47,732] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.408 seconds
[2020-11-05 18:42:59,587] {scheduler_job.py:155} INFO - Started process (PID=28588) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:59,595] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:42:59,596] {logging_mixin.py:112} INFO - [2020-11-05 18:42:59,596] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:59,940] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:42:59,990] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:43:00,022] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,087] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,136] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,227] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,294] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,357] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,389] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,437] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,498] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,592] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,633] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,682] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,721] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,763] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,806] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:00,854] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:01,333] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:43:01,338] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.751 seconds
[2020-11-05 18:43:12,968] {scheduler_job.py:155} INFO - Started process (PID=28869) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:12,976] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:43:12,977] {logging_mixin.py:112} INFO - [2020-11-05 18:43:12,977] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:13,212] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:13,241] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:43:13,273] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,327] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,403] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,474] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,511] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,576] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,673] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,776] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,816] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,867] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,925] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:13,999] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:14,107] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:14,196] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:14,254] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:14,293] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:14,626] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:43:14,634] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.666 seconds
[2020-11-05 18:43:26,284] {scheduler_job.py:155} INFO - Started process (PID=29161) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:26,289] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:43:26,290] {logging_mixin.py:112} INFO - [2020-11-05 18:43:26,289] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:26,423] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:26,450] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:43:26,464] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,511] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,552] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,602] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,662] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,712] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,760] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,792] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,827] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,863] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,900] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,953] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:26,997] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:27,035] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:27,074] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:27,105] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:27,485] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:43:27,492] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.207 seconds
[2020-11-05 18:43:39,569] {scheduler_job.py:155} INFO - Started process (PID=29440) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:39,575] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:43:39,577] {logging_mixin.py:112} INFO - [2020-11-05 18:43:39,576] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:39,738] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:39,765] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:43:39,787] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:39,865] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:39,985] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,046] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,111] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,182] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,247] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,295] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,333] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,392] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,490] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,595] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,714] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,860] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:40,941] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:41,000] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:41,480] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:43:41,494] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.924 seconds
[2020-11-05 18:43:52,957] {scheduler_job.py:155} INFO - Started process (PID=29731) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:52,963] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:43:52,964] {logging_mixin.py:112} INFO - [2020-11-05 18:43:52,964] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:53,141] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:43:53,165] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:43:53,192] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,238] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,321] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,398] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,450] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,524] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,597] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,655] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,700] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,851] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:53,981] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:54,101] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:54,177] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:54,220] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:54,256] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:54,325] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:43:54,704] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:43:54,709] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.753 seconds
[2020-11-05 18:44:06,409] {scheduler_job.py:155} INFO - Started process (PID=30036) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:06,412] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:44:06,415] {logging_mixin.py:112} INFO - [2020-11-05 18:44:06,414] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:06,656] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:06,708] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:44:06,738] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:06,805] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:06,889] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:06,935] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:06,990] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,054] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,103] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,220] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,295] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,341] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,377] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,484] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,579] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,668] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,734] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:07,788] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:08,260] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:44:08,280] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.871 seconds
[2020-11-05 18:44:19,820] {scheduler_job.py:155} INFO - Started process (PID=30316) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:19,833] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:44:19,844] {logging_mixin.py:112} INFO - [2020-11-05 18:44:19,844] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:20,213] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:20,260] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:44:20,288] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,345] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,404] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,457] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,497] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,546] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,595] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,711] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,776] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,847] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:20,906] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:21,012] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:21,058] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:21,100] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:21,152] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:21,828] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:44:21,844] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 2.024 seconds
[2020-11-05 18:44:33,163] {scheduler_job.py:155} INFO - Started process (PID=30601) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:33,173] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:44:33,173] {logging_mixin.py:112} INFO - [2020-11-05 18:44:33,173] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:33,325] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:33,347] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:44:33,360] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,400] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,435] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,476] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,513] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,556] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,592] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,629] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,667] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,724] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,773] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,818] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,877] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,930] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:33,986] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:34,122] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:34,795] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:44:34,810] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.647 seconds
[2020-11-05 18:44:46,377] {scheduler_job.py:155} INFO - Started process (PID=30880) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:46,382] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:44:46,383] {logging_mixin.py:112} INFO - [2020-11-05 18:44:46,382] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:46,644] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:46,707] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:44:46,727] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:46,787] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:46,866] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:46,894] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:46,926] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:46,967] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:46,996] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,029] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,065] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,119] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,164] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,208] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,243] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,297] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,344] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,407] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:44:47,790] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:44:47,794] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.418 seconds
[2020-11-05 18:44:59,706] {scheduler_job.py:155} INFO - Started process (PID=31167) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:59,714] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:44:59,724] {logging_mixin.py:112} INFO - [2020-11-05 18:44:59,724] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:59,928] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:44:59,956] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:44:59,975] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,034] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,087] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,121] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,156] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,198] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,233] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,274] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,317] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,352] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,393] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,444] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,487] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,520] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,562] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,592] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:00,916] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:45:00,925] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.220 seconds
[2020-11-05 18:45:12,933] {scheduler_job.py:155} INFO - Started process (PID=31494) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:12,939] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:45:12,940] {logging_mixin.py:112} INFO - [2020-11-05 18:45:12,939] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:13,244] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:13,279] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:45:13,299] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,347] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,377] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,424] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,454] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,495] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,534] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,570] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,603] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,634] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,667] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,704] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,747] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,790] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,830] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:13,856] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:14,168] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:45:14,172] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.238 seconds
[2020-11-05 18:45:26,209] {scheduler_job.py:155} INFO - Started process (PID=31804) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:26,214] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:45:26,215] {logging_mixin.py:112} INFO - [2020-11-05 18:45:26,215] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:26,434] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:26,456] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:45:26,468] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,511] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,543] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,598] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,648] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,698] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,745] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,771] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,796] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,823] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,848] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,882] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,912] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:26,965] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:27,003] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:27,031] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:27,335] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:45:27,339] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.130 seconds
[2020-11-05 18:45:39,469] {scheduler_job.py:155} INFO - Started process (PID=32087) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:39,473] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:45:39,474] {logging_mixin.py:112} INFO - [2020-11-05 18:45:39,473] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:39,615] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:39,644] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:45:39,657] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,688] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,716] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,749] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,787] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,835] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,865] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,910] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:39,956] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,015] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,073] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,134] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,180] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,221] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,263] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,315] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:40,784] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:45:40,789] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.320 seconds
[2020-11-05 18:45:52,738] {scheduler_job.py:155} INFO - Started process (PID=32388) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:52,742] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:45:52,743] {logging_mixin.py:112} INFO - [2020-11-05 18:45:52,743] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:52,882] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:45:52,908] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:45:52,921] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:52,978] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,013] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,045] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,077] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,107] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,135] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,161] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,192] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,229] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,264] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,309] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,337] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,378] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,416] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,452] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:45:53,774] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:45:53,783] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.046 seconds
[2020-11-05 18:46:05,004] {scheduler_job.py:155} INFO - Started process (PID=32653) to work on /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:46:05,011] {scheduler_job.py:1589} INFO - Processing file /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py for tasks to queue
[2020-11-05 18:46:05,013] {logging_mixin.py:112} INFO - [2020-11-05 18:46:05,012] {dagbag.py:417} INFO - Filling up the DagBag from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:46:05,177] {scheduler_job.py:1601} INFO - DAG(s) dict_keys(['S3_task']) retrieved from /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py
[2020-11-05 18:46:05,204] {scheduler_job.py:1305} INFO - Processing S3_task
[2020-11-05 18:46:05,222] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 00:02:00+00:00: scheduled__2020-11-05T00:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,254] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 01:02:00+00:00: scheduled__2020-11-05T01:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,294] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 02:02:00+00:00: scheduled__2020-11-05T02:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,324] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 03:02:00+00:00: scheduled__2020-11-05T03:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,353] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 04:02:00+00:00: scheduled__2020-11-05T04:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,390] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 05:02:00+00:00: scheduled__2020-11-05T05:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,430] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 06:02:00+00:00: scheduled__2020-11-05T06:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,464] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 07:02:00+00:00: scheduled__2020-11-05T07:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,512] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 08:02:00+00:00: scheduled__2020-11-05T08:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,554] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 09:02:00+00:00: scheduled__2020-11-05T09:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,596] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 10:02:00+00:00: scheduled__2020-11-05T10:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,629] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 11:02:00+00:00: scheduled__2020-11-05T11:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,675] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 12:02:00+00:00: scheduled__2020-11-05T12:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,713] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 13:02:00+00:00: scheduled__2020-11-05T13:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,743] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 14:02:00+00:00: scheduled__2020-11-05T14:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:05,776] {scheduler_job.py:771} INFO - Examining DAG run <DagRun S3_task @ 2020-11-05 15:02:00+00:00: scheduled__2020-11-05T15:02:00+00:00, externally triggered: False>
[2020-11-05 18:46:06,093] {scheduler_job.py:459} INFO - Skipping SLA check for <DAG: S3_task> because no tasks in DAG have SLAs
[2020-11-05 18:46:06,101] {scheduler_job.py:163} INFO - Processing /home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py took 1.097 seconds

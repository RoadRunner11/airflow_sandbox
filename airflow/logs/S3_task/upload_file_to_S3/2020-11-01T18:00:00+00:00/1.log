[2020-11-01 20:00:45,896] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: S3_task.upload_file_to_S3 2020-11-01T18:00:00+00:00 [queued]>
[2020-11-01 20:00:45,951] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: S3_task.upload_file_to_S3 2020-11-01T18:00:00+00:00 [queued]>
[2020-11-01 20:00:45,951] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2020-11-01 20:00:45,951] {taskinstance.py:881} INFO - Starting attempt 1 of 4
[2020-11-01 20:00:45,951] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2020-11-01 20:00:45,962] {taskinstance.py:901} INFO - Executing <Task(PythonOperator): upload_file_to_S3> on 2020-11-01T18:00:00+00:00
[2020-11-01 20:00:45,983] {standard_task_runner.py:54} INFO - Started process 14579 to run task
[2020-11-01 20:00:46,034] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'S3_task', 'upload_file_to_S3', '2020-11-01T18:00:00+00:00', '--job_id', '91', '--pool', 'default_pool', '--raw', '-sd', '/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py', '--cfg_path', '/tmp/tmp35o9jbfr']
[2020-11-01 20:00:46,037] {standard_task_runner.py:78} INFO - Job 91: Subtask upload_file_to_S3
[2020-11-01 20:00:46,129] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: S3_task.upload_file_to_S3 2020-11-01T18:00:00+00:00 [running]> akorede-VirtualBox
[2020-11-01 20:00:47,698] {taskinstance.py:1150} ERROR - The key mycsv.csv already exists.
Traceback (most recent call last):
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/akorede/Documents/airflow_sandbox/airflow/dags/s3_dag.py", line 50, in upload_file_to_S3_with_hook
    hook.load_file(filename, key, bucket_name)
  File "/home/akorede/.local/lib/python3.6/site-packages/airflow/hooks/S3_hook.py", line 358, in load_file
    raise ValueError("The key {key} already exists.".format(key=key))
ValueError: The key mycsv.csv already exists.
[2020-11-01 20:00:47,701] {taskinstance.py:1194} INFO - Marking task as UP_FOR_RETRY. dag_id=S3_task, task_id=upload_file_to_S3, execution_date=20201101T180000, start_date=20201101T190045, end_date=20201101T190047
[2020-11-01 20:00:50,870] {local_task_job.py:102} INFO - Task exited with return code 1
